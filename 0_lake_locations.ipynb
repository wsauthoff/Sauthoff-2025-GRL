{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37f3e2df-7e19-4277-8089-3cdabfa97d64",
   "metadata": {
    "tags": []
   },
   "source": [
    "Notebook collates Antarctic active subglacial lakes from past inventories that included polygons (Smith and others, 2009; Siegfried & Fricker, 2018), latest inventory that includes point lake locations (Livingstone and others, 2022), and individual studies that are not included past inventories to generate the most recent active subglacial lake inventory.\n",
    "\n",
    "Written 2023-01-17 by W. Sauthoff (sauthoff@mines.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db871748-1df8-4f86-bc58-b30467eac86d",
   "metadata": {},
   "source": [
    "# Setup computing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7fe055-4f23-4648-a1d8-cd1138d5deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies not pre-installed\n",
    "%pip install openpyxl --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e84277d-9816-47d3-9f78-b763b43ca327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import h5py\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon\n",
    "from shapely.ops import unary_union\n",
    "import pyproj\n",
    "from pyproj import CRS, Geod, Transformer\n",
    "\n",
    "# Magic functions\n",
    "%matplotlib widget\n",
    "\n",
    "# Define utility functions\n",
    "def ll2ps(lon, lat):\n",
    "    \"\"\"\n",
    "    Transform coordinates from geodetic coordinates (lon, lat)\n",
    "    to Antarctic Polar Stereograph coordinates (x, y)\n",
    "    x, y = ll2ps(lon, lat)\n",
    "    \"\"\"\n",
    "    crs_ll = CRS(\"EPSG:4326\")\n",
    "    crs_xy = CRS(\"EPSG:3031\")\n",
    "    ll_to_xy = Transformer.from_crs(crs_ll, crs_xy, always_xy = True)\n",
    "    x, y = ll_to_xy.transform(lon, lat)\n",
    "    return x, y\n",
    "\n",
    "def ps2ll(x, y):\n",
    "    \"\"\"\n",
    "    Transform coordinates from Antarctic Polar Stereograph\n",
    "    to geodetic (lon, lat) coordinates\n",
    "    \n",
    "    lon, lat = ps2ll(x, y)\n",
    "    \"\"\"\n",
    "    crs_ll = CRS(\"EPSG:4326\")\n",
    "    crs_xy = CRS(\"EPSG:3031\")\n",
    "    xy_to_ll = Transformer.from_crs(crs_xy, crs_ll, always_xy = True)\n",
    "    lon, lat = xy_to_ll.transform(x, y)\n",
    "    return lon, lat\n",
    "\n",
    "def find_intersections(gdf1, gdf2):\n",
    "    # Create an empty list to store the results\n",
    "    intersections = []\n",
    "\n",
    "    # Iterate over each geometry in gdf1\n",
    "    for index1, geom1 in gdf1.geometry.items():\n",
    "        # Compare with each geometry in gdf2\n",
    "        for index2, geom2 in gdf2.geometry.items():\n",
    "            if geom1.intersects(geom2):\n",
    "                # If they intersect, add the indices to the list\n",
    "                intersections.append((index1, index2))\n",
    "    return intersections\n",
    "\n",
    "# Create a Geod object for calculating area on the WGS84 ellipsoid\n",
    "geod = Geod(ellps=\"WGS84\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923fbc06-326d-4249-a510-a5c4f762b740",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace1ecf-d763-450d-8954-5161d878bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_area(geometry, crs=None):\n",
    "    '''\n",
    "    Calculate geodesic area of polygon or multipolygon\n",
    "    Can accept either a geometry with associated CRS or a geometry and separate CRS\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    geometry : shapely.geometry.Polygon/MultiPolygon or GeoDataFrame/GeoSeries\n",
    "        Input geometry\n",
    "    crs : pyproj.CRS or str, optional\n",
    "        Coordinate reference system. Required if geometry doesn't have CRS\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float or None\n",
    "        Area in square meters, or None if geometry is invalid\n",
    "    '''\n",
    "    # Handle GeoDataFrame/GeoSeries input\n",
    "    if hasattr(geometry, 'crs'):\n",
    "        crs = geometry.crs\n",
    "        if isinstance(geometry, (gpd.GeoDataFrame, gpd.GeoSeries)):\n",
    "            geometry = geometry.geometry.iloc[0]\n",
    "    \n",
    "    # Validate CRS\n",
    "    if crs is None:\n",
    "        raise ValueError(\"No CRS provided. Input must either have CRS or CRS must be provided separately\")\n",
    "        \n",
    "    # Convert CRS to string for comparison if it isn't already\n",
    "    if not isinstance(crs, str):\n",
    "        crs = str(crs)\n",
    "    \n",
    "    # Check if geometry is in EPSG:4326\n",
    "    if 'EPSG:4326' not in crs.upper():\n",
    "        raise ValueError(f\"CRS must be EPSG:4326, got {crs}\")\n",
    "    \n",
    "    # Check if geometry is None or invalid\n",
    "    if geometry is None or not geometry.is_valid:\n",
    "        return None\n",
    "    \n",
    "    # Calculate area based on geometry type\n",
    "    if isinstance(geometry, Polygon):\n",
    "        return abs(geod.polygon_area_perimeter(geometry.exterior.coords.xy[0], geometry.exterior.coords.xy[1])[0])\n",
    "    elif isinstance(geometry, MultiPolygon):\n",
    "        total_area = 0\n",
    "        for part in geometry.geoms:\n",
    "            total_area += abs(geod.polygon_area_perimeter(part.exterior.coords.xy[0], part.exterior.coords.xy[1])[0])\n",
    "        return total_area\n",
    "    else:\n",
    "        raise ValueError(\"Input must be either Polygon or MultiPolygon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4779ab-bb27-45da-a523-48c83c395771",
   "metadata": {},
   "source": [
    "# Import past inventories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160c636b-d154-4e1c-9e6a-4a6c0a4297a1",
   "metadata": {},
   "source": [
    "## Smith and others, 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d12e4-f3d5-451e-abae-e6de71a35b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import active subglacial lake outlines from Smith and others (2009) (S09)\n",
    "# As released in Smith and others, 2012 dataset (https://doi.org/10.15784/601439)\n",
    "fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "S09_outlines_lonlat = gpd.read_file('input/lake_outlines/Smith2009_lakes/Antarctic_lakes.kml', driver='KML')\n",
    "\n",
    "# Ensure GeoDataFrame is in EPSG:4326 for geodesic area calculation\n",
    "if S09_outlines_lonlat.crs != 'EPSG:4326':\n",
    "    S09_outlines_lonlat = S09_outlines_lonlat.to_crs('EPSG:4326')\n",
    "\n",
    "# Calculate the geodesic area for each polygon\n",
    "S09_outlines_lonlat['area (m^2)'] = S09_outlines_lonlat['geometry'].apply(    \n",
    "    lambda poly: abs(geod.polygon_area_perimeter(\n",
    "    poly.exterior.coords.xy[0], poly.exterior.coords.xy[1])[0]) if poly is not None and poly.is_valid else None)\n",
    "\n",
    "# Convert to CRS EPSG:3031\n",
    "S09_outlines = S09_outlines_lonlat.to_crs(3031)\n",
    "\n",
    "# Delete original geodataframe\n",
    "del S09_outlines_lonlat\n",
    "\n",
    "# Strip the \\n characters from the name column\n",
    "S09_outlines['Name'] = S09_outlines['Name'].str.strip()\n",
    "\n",
    "# Select only the 'Name', 'geometry', and 'area (m^2)' columns\n",
    "S09_outlines = S09_outlines[['Name', 'geometry', 'area (m^2)']]\n",
    "\n",
    "# Display the modified geodataframe\n",
    "S09_outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fb70e7-80bc-496a-94fd-df06632aa3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Smith and others, 2009 outlines to GeoJSON\n",
    "S09_outlines.to_file('output/lake_outlines/stationary_outlines/Smith2009_outlines.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e3c261-7d88-4011-8d26-b47e879a0242",
   "metadata": {},
   "source": [
    "## Siegfried and Fricker, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b142d-667e-45bb-90ca-31d1a2c2bf6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import active subglacial lake outlines from Siegfried & Fricker (2018) (SF18)\n",
    "# Original pub: https://doi.org/10.1017/aog.2017.36 \n",
    "# Code for loading lake outlines available in code bank associated with Siegfried & Fricker (2021), https://doi.org/10.1029/2020GL091089: \n",
    "# https://github.com/mrsiegfried/Siegfried2021-GRL/blob/main/data/outlines/load_lakes.ipynb\n",
    "\n",
    "# import subglacial lake outlines (Siegfried & Fricker, 2018)\n",
    "h5f = h5py.File('input/lake_outlines/SiegfriedFricker2018_lakes/SiegfriedFricker2018-outlines.h5', 'r')\n",
    "outline_geometries = [] # store polygons\n",
    "citations = [] # store citation information\n",
    "\n",
    "# we're going to calculate geodesic lake area because that is often screwed up\n",
    "# and occasionally incorrect in the literature\n",
    "areas = []\n",
    "\n",
    "# we're going to need to do some coordinate transforms for the geodesic area\n",
    "# define CRS for Antarcica and make a converter from xy to ll\n",
    "CRS_LL = \"EPSG:4326\" # wgs84 in lon,lat\n",
    "CRS_XY = h5f.attrs.get('proj_crs') # get projection from hdf5 file\n",
    "XY_TO_LL = Transformer.from_crs(CRS_XY, CRS_LL, always_xy = True) # make coord transformer\n",
    "geod = CRS(CRS_LL).get_geod() # geod object for calculating geodesic area on defined ellipsoid\n",
    "\n",
    "# look through each lake and load all of it's info\n",
    "for lake in h5f.keys():\n",
    "    outline_x = h5f[lake]['x'][:]\n",
    "    outline_y = h5f[lake]['y'][:]\n",
    "    outlines_xy = np.stack((outline_x, outline_y),axis=2).reshape(outline_x.shape[1], 2)\n",
    "\n",
    "    # A single lake with multiple polygons is NaN broken---need to identify and\n",
    "    # load as a MultiPolygon. Otherwise it's easy (just load as polygon)\n",
    "    if np.isnan(outlines_xy)[:,0].sum() == 0:\n",
    "        geometry = Polygon(outlines_xy)\n",
    "        lon, lat = XY_TO_LL.transform(outlines_xy[:,0], outlines_xy[:,1])\n",
    "        this_area = abs(geod.polygon_area_perimeter(lon,lat)[0])\n",
    "    else:\n",
    "        this_area = 0\n",
    "        # break at NaN values and load each as separate polygons\n",
    "        idx = np.where(np.isnan(outlines_xy[:,0]))[0]\n",
    "\n",
    "        # grab outline of first lake before getting into the loop\n",
    "        this_outline = outlines_xy[0:idx[0],:]\n",
    "        pgons = [Polygon(this_outline)] # put the first polygon in a list\n",
    "        lon,lat = XY_TO_LL.transform(this_outline[:,0], this_outline[:,1])\n",
    "        this_area += abs(geod.polygon_area_perimeter(lon,lat)[0])/1e6 # add its area\n",
    "        for i in np.arange(0,len(idx)):\n",
    "            if i == len(idx)-1:\n",
    "                this_outline = outlines_xy[idx[i]+1:,:]\n",
    "            else:\n",
    "                this_outline = outlines_xy[idx[i]+1:idx[i+1]]\n",
    "\n",
    "            pgons.append(Polygon(this_outline))\n",
    "            lon,lat = XY_TO_LL.transform(this_outline[:,0], this_outline[:,1])\n",
    "            this_area += abs(geod.polygon_area_perimeter(lon,lat)[0])/1e6\n",
    "        geometry = MultiPolygon(pgons)\n",
    "\n",
    "    # append all the results in the right place\n",
    "    outline_geometries.append(geometry)\n",
    "    citations.append(h5f[lake].attrs.get('citation')[0].decode('UTF-8'))\n",
    "    areas.append(this_area)\n",
    "\n",
    "# make a pandas dataframe with all the necessary info\n",
    "df = pd.DataFrame(zip(h5f.keys(), outline_geometries, areas, citations),\n",
    "                  columns=['name', 'geometry', 'area (m^2)', 'cite'])\n",
    "# convert to geopands geodataframe\n",
    "SF18_outlines = gpd.GeoDataFrame(df, crs=CRS_XY, geometry=outline_geometries)\n",
    "# close HDF5 file\n",
    "h5f.close()\n",
    "\n",
    "# Display geodataframe\n",
    "SF18_outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384fae5b-2e64-4bf1-9ac4-315321e748a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Siegfried & Fricker, 2018 outlines to GeoJSON\n",
    "SF18_outlines.to_file('output/lake_outlines/stationary_outlines/SiegfriedFricker2018_outlines.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d90f8e-0f8a-4431-b706-951a74a2c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy SF18_outlines to isolate outlines new in SF18 to view added lakes\n",
    "SF18_outlines_SF18only = SF18_outlines.copy(deep=True)\n",
    "\n",
    "# Drop rows where 'cite' column equals the specified string\n",
    "SF18_outlines_SF18only = SF18_outlines_SF18only[~SF18_outlines_SF18only['cite'].eq('Smith and others, 2009, J. Glac., doi:10.3189/002214309789470879')]\n",
    "\n",
    "# View dataframe\n",
    "SF18_outlines_SF18only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa3d7b6-a16e-44d0-928f-6bc617b7a3c6",
   "metadata": {},
   "source": [
    "## Livingstone and others, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3999bed9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read subglacial lake point data from Livingstone and others (2022) (L22), https://doi.org/10.1038/s43017-021-00246-9\n",
    "url = 'https://static-content.springer.com/esm/art%3A10.1038%2Fs43017-021-00246-9/MediaObjects/43017_2021_246_MOESM1_ESM.xlsx'\n",
    "use_cols = ['Name', 'Lat.  oN', 'Lon. oE', 'Lake Type', 'References', 'Prior Inventory']\n",
    "import_rows = np.arange(0,676)\n",
    "L22_points = pd.read_excel(url, sheet_name='Antarctica', usecols=use_cols, skiprows = lambda x: x not in import_rows)\n",
    "\n",
    "# View just the active lakes of the pandas dataset\n",
    "L22_activelake_points = L22_points[L22_points['Lake Type'].str.strip().isin(['Active'])]\n",
    "\n",
    "# Reset the index, dropping the old one\n",
    "L22_activelake_points = L22_activelake_points.reset_index(drop=True)\n",
    "\n",
    "# Display geodataframe\n",
    "L22_activelake_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb743ef-9aa3-42cd-9883-ff0f2bd552e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate lakes not previously included in Smith and others (2009; 2017) or Siegfried and Fricker (2018)\n",
    "L22_activelake_points_new = L22_activelake_points.copy(deep=True)\n",
    "\n",
    "# Drop rows containing either 'Smith et al. (2009)' or 'Siegfried & Fricker (2018)' with escaped parentheses\n",
    "L22_activelake_points_new.drop(\n",
    "    L22_activelake_points_new.loc[L22_activelake_points['References'].str.contains(\n",
    "        'Smith et al\\\\. \\\\(2009\\\\)|Siegfried & Fricker \\\\(2018\\\\)', regex=True)].index, inplace=True)\n",
    "\n",
    "# Reset the index after removing rows\n",
    "L22_activelake_points_new = L22_activelake_points_new.reset_index(drop=True)\n",
    "\n",
    "# Display the updated dataframe\n",
    "L22_activelake_points_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c4892b-28d2-480c-b183-b12892caaa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Byrd1 (DL) Byrd1 (DL) were included in Smith and others, 2009 and thus Siegfried & Fricker, 2018 inventories\n",
    "# Let's confirm this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cfe275-eba1-42f4-90c6-ceabd3cb5440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for rows where the S09 'Name' column contains Byrd lakes\n",
    "S09_outlines[S09_outlines['Name'].str.contains('Byrd_1|Byrd_2', case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f050d2-1df8-4847-a0ff-1d341060b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for rows where the SF18 'name' column contains Byrd_1 or Byrd_2 and Thwaites lakes\n",
    "SF18_outlines[SF18_outlines['name'].str.contains('Byrd_1|Byrd_2|Thw_70|Thw_124|Thw_142|Thw_170', case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48325b3-00ba-4950-ad4c-ebdaf1a441c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'Name' is one of the specified values\n",
    "names_to_remove = ['Byrd1 (DL)', 'Byrd2 (UL)', 'THW70', 'THW170', 'THW142', 'THW124']\n",
    "\n",
    "L22_activelake_points_new = L22_activelake_points_new[~L22_activelake_points_new['Name'].isin(names_to_remove)]\n",
    "\n",
    "# Reset the index after removing the rows\n",
    "L22_activelake_points_new = L22_activelake_points_new.reset_index(drop=True)\n",
    "\n",
    "# Display the updated dataframe\n",
    "L22_activelake_points_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f81380e-b1ce-452b-91bf-b59a71445faf",
   "metadata": {},
   "source": [
    "# Generate Sauthoff and others (2025) inventory of previously identified lakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f2af51-7147-4bb4-ae89-97893e3d4f68",
   "metadata": {},
   "source": [
    "## Add lakes from past inventories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53836768-0bd8-43a0-a970-ee18b3639105",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copy SF18 outline inventory to add entries from more recent publications\n",
    "# Starting with the most recent active subglacial lake inventory that includes stationary lake outlines\n",
    "stationary_outlines_gdf = SF18_outlines.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f55f548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Examine citations column\n",
    "stationary_outlines_gdf['cite'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb43ff84-c43b-4896-b381-128016c5e5a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change journal abbreviations to ISO4 standard\n",
    "# Replace TC with Cryosphere\n",
    "stationary_outlines_gdf = stationary_outlines_gdf.replace('Kim and others, 2016, TC, doi:10.5194/tc-10-2971-2016', 'Kim and others, 2016, Cryosphere, doi:10.5194/tc-10-2971-2016')\n",
    "stationary_outlines_gdf = stationary_outlines_gdf.replace('Smith and others, 2017, TC, doi:10.5194/tc-11-451-2017', 'Smith and others, 2017, Cryosphere, doi:10.5194/tc-11-451-2017')\n",
    "\n",
    "# Replace GRL with Geophys. Res. Lett.\n",
    "stationary_outlines_gdf = stationary_outlines_gdf.replace('McMillan and others, 2013, GRL, doi:10.1002/grl.50689', 'McMillan and others, 2013, Geophys. Res. Lett., doi:10.1002/grl.50689')\n",
    "\n",
    "# Ensure replacements worked as expected\n",
    "stationary_outlines_gdf['cite'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d86489-c991-4669-af96-a8df2cbb8c50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find area of previously identified lakes inventoried in SF18 to use as guesstimate \n",
    "# for lakes without this information or figures allowing for closer guesstimate\n",
    "print('mean lake area: ', np.round(np.mean(SF18_outlines['area (m^2)'])/1e6, 1), ' km^2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27fad4e-b874-4422-898d-d10077ee6ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add entries for previously identified lakes not included in SF18 inventory \n",
    "# Used approximated centroid point when lake outline was unavailable\n",
    "\n",
    "# Smith 2009 Recovery_8 was argued to not be an active subglacial lake in Fricker and others (2010),\n",
    "# so not included Siegfried & Fricker (2018) outlines; including here to determine if there's been activity since\n",
    "lake_gdf = S09_outlines[S09_outlines['Name'] == 'Recovery_8']\n",
    "name = lake_gdf['Name'].values[0]\n",
    "\n",
    "# S09 outline inventory uses 3D polygons with z dimension vs. 2D polygons in SF18 inventory\n",
    "# Extract the point values that define the perimeter of the polygon to make polygon without third z dimension\n",
    "# Extract 2D coordinates (X, Y) from the 3D polygon (X, Y, Z)\n",
    "xy_coords = [(x, y) for x, y, z in lake_gdf.geometry.values[0].exterior.coords]\n",
    "\n",
    "# Create a new 2D polygon from these coordinates\n",
    "lake_poly_2d = Polygon(xy_coords)\n",
    "geometry = lake_poly_2d\n",
    "\n",
    "# Store area that was previously calculated to be geodesic area\n",
    "area = lake_gdf['area (m^2)'].values[0]\n",
    "\n",
    "# Store citation info from another lake from the same S09 study in the SF18 citation format\n",
    "cite = SF18_outlines[SF18_outlines['name'] == 'Bindschadler_1'].cite.values[0]\n",
    "\n",
    "# Make entry into pandas dataframe to concatenate to inventory\n",
    "gdf = gpd.GeoDataFrame([[name, geometry, area, cite]], columns=stationary_outlines_gdf.columns)\n",
    "\n",
    "# Set CRS\n",
    "gdf.crs = stationary_outlines_gdf.crs\n",
    "\n",
    "# Ensure that new entry isn't already in inventory before adding\n",
    "gdf_diff = gdf[~gdf['name'].isin(stationary_outlines_gdf['name'])]\n",
    "\n",
    "# Add entry to inventory\n",
    "stationary_outlines_gdf = pd.concat([stationary_outlines_gdf, gdf_diff], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb3b61a-f102-47ac-802e-dad3106a5973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Livingstone and others, 2022 active lakes points (no outlines available):\n",
    "\n",
    "# Wingham and others, 2006 (within Livingstone and others, 2022 inventory)\n",
    "# Lakes L1, U1, U2, U3\n",
    "lake_names = ['L1', 'U1', 'U2', 'U3']\n",
    "areas = [600e6, 200e6, 200e6, 400e6]  # Lake L1 area estimated in paper, U1-3 guestimated from their Fig. 1\n",
    "\n",
    "for i, lake_name in enumerate(lake_names):\n",
    "    # Access the row where the 'Name' matches the current lake name\n",
    "    lake_row = L22_activelake_points_new.loc[L22_activelake_points_new['Name'] == lake_name]\n",
    "    \n",
    "    if not lake_row.empty:  # Proceed only if the lake is found in the dataframe\n",
    "        name = lake_row.iloc[0]['Name']\n",
    "        lon = lake_row.iloc[0]['Lon. oE']\n",
    "        lat = lake_row.iloc[0]['Lat.  oN']\n",
    "        geometry = Point(ll2ps(lon, lat)).buffer(math.sqrt(areas[i] / math.pi))\n",
    "        area = areas[i]\n",
    "        cite = 'Wingham and others, 2006, Nature, doi:10.1038/nature04660'\n",
    "        \n",
    "        # Make entry into GeoDataFrame to concatenate to inventory\n",
    "        gdf = gpd.GeoDataFrame([[name, geometry, area, cite]], columns=stationary_outlines_gdf.columns, geometry=[geometry], crs=stationary_outlines_gdf.crs)\n",
    "        \n",
    "        # Ensure that new entry isn't already in inventory before adding to avoid duplicate entry\n",
    "        gdf_diff = gdf[~gdf['name'].isin(stationary_outlines_gdf['name'])]\n",
    "        \n",
    "        # Add entry to inventory\n",
    "        stationary_outlines_gdf = gpd.GeoDataFrame(pd.concat([stationary_outlines_gdf, gdf_diff], ignore_index=True), crs=stationary_outlines_gdf.crs)\n",
    "\n",
    "# N. Young personal comm. (within Wright & Siegert, 2012 and Livingstone and others, 2022 inventories)\n",
    "# Lakes Site A, Site B, Site C\n",
    "lake_names = ['“Site A”', '“Site B”', '“Site C”']\n",
    "area = 200e6  # Estimate based on average of previously identified lakes\n",
    "\n",
    "for lake_name in lake_names:\n",
    "    # Access the row where the 'Name' matches the current lake name\n",
    "    lake_row = L22_activelake_points_new.loc[L22_activelake_points_new['Name'] == lake_name]\n",
    "\n",
    "    if not lake_row.empty:  # Proceed only if the lake is found in the dataframe\n",
    "        # Strip curly double quotation marks from names and replace spaces with underscores\n",
    "        name = lake_row.iloc[0]['Name'].strip('“”').replace(\" \", \"_\")\n",
    "        lon = lake_row.iloc[0]['Lon. oE']\n",
    "        lat = lake_row.iloc[0]['Lat.  oN']\n",
    "        geometry = Point(ll2ps(lon, lat)).buffer(math.sqrt(area / math.pi))\n",
    "        cite = 'Wright & Siegert, 2012, Antarct. Sci., doi:10.1017/S095410201200048X'\n",
    "        \n",
    "        # Create a GeoDataFrame for the new lake\n",
    "        gdf = gpd.GeoDataFrame([[name, geometry, area, cite]], columns=stationary_outlines_gdf.columns)\n",
    "        gdf.crs = stationary_outlines_gdf.crs\n",
    "        \n",
    "        # Ensure that new entry isn't already in the inventory before adding\n",
    "        gdf_diff = gdf[~gdf['name'].isin(stationary_outlines_gdf['name'])]\n",
    "        \n",
    "        # Add entry to inventory\n",
    "        stationary_outlines_gdf = gpd.GeoDataFrame(pd.concat([stationary_outlines_gdf, gdf_diff], ignore_index=True), crs=stationary_outlines_gdf.crs)\n",
    "\n",
    "# Lacked sufficient data density in altimetry datasets data_counts to include:\n",
    "# Scambos and others, 2011, Ann. Glac.\n",
    "# https://doi.org/10.3189/172756411799096204 \n",
    "# (within Livingstone and others, 2022 inventory)\n",
    "\n",
    "# Crane Glacier lake\n",
    "\n",
    "area = 4.5e6  # Area reported in original publication\n",
    "\n",
    "for i in range(7, 8):\n",
    "    # name = L22_activelake_points_new.iloc[i]['Name'].split()[0]  # Only use first word to avoid spaces\n",
    "    name = '_'.join(L22_activelake_points_new.iloc[i]['Name'].split())  # Use all words connected with underscores\n",
    "    lon = L22_activelake_points_new.iloc[i]['Lon. oE']\n",
    "    lat = L22_activelake_points_new.iloc[i]['Lat.  oN']\n",
    "    geometry = Point(ll2ps(lon, lat)).buffer(math.sqrt(area / math.pi))\n",
    "    cite = 'Scambos and others, 2011, Ann. Glaciol., doi:10.3189/172756411799096204'\n",
    "    gdf = gpd.GeoDataFrame([[name, geometry, area, cite]], columns=stationary_outlines_gdf.columns)\n",
    "    gdf.crs = stationary_outlines_gdf.crs\n",
    "    gdf_diff = gdf[~gdf['name'].isin(stationary_outlines_gdf['name'])]\n",
    "    stationary_outlines_gdf = pd.concat([stationary_outlines_gdf, gdf_diff], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55126547-2d79-4b23-9806-179f880823cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display newly add lakes \n",
    "stationary_outlines_gdf.iloc[len(SF18_outlines):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095904b2-4d5e-47f3-98f4-acc662c61d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot newly added lake outlines\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Filter rows on the 'name' column\n",
    "filtered_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'].isin(['Recovery_8', 'U1', 'U2', 'U3', 'Site_A', 'Site_B', 'Site_C', 'Crane_Glacier'])]\n",
    "\n",
    "filtered_gdf.boundary.plot(ax=ax, color='blue')\n",
    "# Iterate through the GeoDataFrame to annotate each polygon.\n",
    "for idx, row in filtered_gdf.iterrows():\n",
    "    # Use the centroid of each polygon for the annotation location.\n",
    "    centroid = row['geometry'].centroid\n",
    "    ax.annotate(text=row['name'], xy=(centroid.x, centroid.y), xytext=(3, 3), textcoords=\"offset points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa2136e-f29d-47b9-976d-9af196726cc3",
   "metadata": {},
   "source": [
    "### Hoffman and others, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0f44a-7254-4d23-a8eb-d3e21d0742af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hoffman and others, 2020, Cryosphere\n",
    "# https://doi.org/10.5194/tc-14-4603-2020\n",
    "\n",
    "# Redelineations of two Thwaites lakes\n",
    "\n",
    "# Obtained lake outlines via email from corresponding author, Hoffman\n",
    "\n",
    "# Read lake outline geojsons into geodataframes\n",
    "file_path = 'input/lake_outlines/Hoffman2020_subglacial_lakes/Thw124.geojson'\n",
    "Hoffman2020_Thw124 = gpd.read_file(file_path)\n",
    "\n",
    "file_path = 'input/lake_outlines/Hoffman2020_subglacial_lakes/Thw142.geojson'\n",
    "Hoffman2020_Thw142 = gpd.read_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7341618c-bf72-4099-8afc-422bda45e55b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "stationary_outlines_gdf[stationary_outlines_gdf['name'].isin(['Thw_70', 'Thw_124', 'Thw_142', 'Thw_170'])].boundary.plot(ax=ax)\n",
    "Hoffman2020_Thw124.boundary.plot(ax=ax, color='red')\n",
    "Hoffman2020_Thw142.boundary.plot(ax=ax, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6865845-4247-486b-a544-6ded1d62fdd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Hoffman and others redelineated Thw124 and Thw142; however outlines are largely similar to Smith and others, 2017 delineations and outlines were not publicly available at time of investigation, so we use the Smith others, 2017 delineations contained in SF18 inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbc0ba1-3bb0-451b-9432-20d0a602ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hoffman and others, 2020 (within Livingstone and others, 2022 inventory as lake points)\n",
    "# but multiple lakes aggregated into single Haynes Glacier (HG) point\n",
    "\n",
    "# Takahe lakes (TL; named after nearby Mt. Takahe) on Haynes Glacier (HG) & Western Thwaites (WT) lake\n",
    "# detailed in Hoffman and others, 2020 supplement\n",
    "# https://tc.copernicus.org/articles/14/4603/2020/tc-14-4603-2020-supplement.pdf\n",
    "\n",
    "# Make list of lake names\n",
    "lake_names = ['TL96', 'TL108', 'TL115', 'TL122', 'WT']\n",
    "\n",
    "for lake_name in lake_names:\n",
    "    # Load geojson into geodataframe\n",
    "    lake_geojson_path = f'input/lake_outlines/Hoffman2020_subglacial_lakes/{lake_name}.geojson'\n",
    "    lake_gdf = gpd.read_file(lake_geojson_path)\n",
    "\n",
    "    # Extract name and geometry\n",
    "    name = lake_gdf['id'].values[0]\n",
    "    geometry = lake_gdf['geometry'].iloc[0]\n",
    "    \n",
    "    # Convert CRS to 4326 for geodesic area calculations\n",
    "    lake_gdf = lake_gdf.to_crs('EPSG:4326')\n",
    "    area = lake_gdf['geometry'].apply(lambda poly: abs(geod.polygon_area_perimeter(poly.exterior.coords.xy[0], poly.exterior.coords.xy[1])[0]) if poly is not None and poly.is_valid else None)[0]\n",
    "    \n",
    "    # Convert CRS back to 3031 for plotting\n",
    "    lake_gdf = lake_gdf.to_crs('EPSG:3031')\n",
    "    cite = 'Hoffman and others, 2020, Cryosphere, doi:10.5194/tc-14-4603-2020'\n",
    "    \n",
    "    # Create a GeoDataFrame for the current lake and append it if it's not already present\n",
    "    gdf = gpd.GeoDataFrame([[name, geometry, area, cite]], columns=stationary_outlines_gdf.columns)\n",
    "    gdf.crs = stationary_outlines_gdf.crs\n",
    "    gdf_diff = gdf[~gdf['name'].isin(stationary_outlines_gdf['name'])]\n",
    "    stationary_outlines_gdf = pd.concat([stationary_outlines_gdf, gdf_diff], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95175bf5-5f07-4ef2-9c3d-9ebb24650ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Filter rows on the 'name' column\n",
    "filtered_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'].isin(lake_names)]\n",
    "\n",
    "filtered_gdf.boundary.plot(ax=ax, color='blue')\n",
    "# Iterate through the GeoDataFrame to annotate each polygon.\n",
    "for idx, row in filtered_gdf.iterrows():\n",
    "    # Use the centroid of each polygon for the annotation location.\n",
    "    centroid = row['geometry'].centroid\n",
    "    ax.annotate(text=row['name'], xy=(centroid.x, centroid.y), xytext=(3, 3), textcoords=\"offset points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904440ed-ae2b-4226-b086-5fa64dd838aa",
   "metadata": {},
   "source": [
    "### Neckel and others, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821cdaf2-4328-4318-a412-d6da8f5daba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neckel and others, 2021, Geophys. Res. Lett.\n",
    "# (publication not included in Livingstone and others, 2022 inventory)\n",
    "# https://doi.org/10.1029/2021GL094472\n",
    "\n",
    "# New active lakes on Jutulstraumen Glacier (Antarctica)\n",
    "\n",
    "# Data set\n",
    "# https://doi.org/10.1594/PANGAEA.927120\n",
    "\n",
    "# Read shape file into geodataframe\n",
    "file_path = 'input/lake_outlines/Neckel2021_Jutulstraumen_lakes/JG_interferometry_lake_outlines.shp'\n",
    "Neckel2021_outlines = gpd.read_file(file_path)\n",
    "\n",
    "# Display geodataframe\n",
    "Neckel2021_outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b718e9-4717-43ae-bbf2-263e649cfe6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop columns unnecessary to our analysis\n",
    "Neckel2021_outlines.drop(columns=['Date', 'Year', 'Month', 'Movement', 'Lon', 'Lat'], inplace=True)\n",
    "\n",
    "# Rename 'Feature' col to 'Name' to match stationary_outlines_gdf\n",
    "Neckel2021_outlines = Neckel2021_outlines.rename(columns={'Feature': 'name'})\n",
    "\n",
    "# Display geodataframe\n",
    "Neckel2021_outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c969804-7f2f-4ff4-a116-73406543b0c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "Neckel2021_outlines.boundary.plot(ax=ax)\n",
    "\n",
    "# Adding labels to the center of each polygon\n",
    "for idx, row in Neckel2021_outlines.iterrows():\n",
    "    centroid = row.geometry.centroid\n",
    "    ax.text(centroid.x, centroid.y, row['name'], fontsize=12, ha='center', va='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce9db3b-0abe-47cf-b338-c31f497fd627",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the only set of previously identified lakes that has spatiotemporally evolving outlines available as a data set\n",
    "# For simplicity, we collapse this spatiotemporal variability into a stationary outline that is \n",
    "# the unary union of all outlines of each lake\n",
    "\n",
    "# Define the two sets of criteria\n",
    "criterias = [['D2_b', 'E1'], ['E2', 'F2']]\n",
    "\n",
    "for criteria in criterias:\n",
    "    # Filter rows where 'Feature' matches the current set of criteria\n",
    "    rows_to_combine = Neckel2021_outlines[Neckel2021_outlines['name'].isin(criteria)]\n",
    "\n",
    "    # Check if we have at least two polygons to combine for the current criteria\n",
    "    if len(rows_to_combine) >= 2:\n",
    "        # Perform the union of the polygons\n",
    "        combined_polygon = rows_to_combine['geometry'].union_all()\n",
    "\n",
    "        # Create a new row with the combined geometry and any other necessary attributes\n",
    "        new_row_data = {\n",
    "            'geometry': combined_polygon,\n",
    "            'name': 'Combined_' + '_'.join(criteria),  # Example of setting the Feature column\n",
    "            # Set other attributes as needed\n",
    "        }\n",
    "        new_row = gpd.GeoDataFrame([new_row_data], crs=gdf.crs)\n",
    "\n",
    "        # Use pandas.concat instead of append\n",
    "        Neckel2021_outlines = pd.concat([Neckel2021_outlines, new_row], ignore_index=True)\n",
    "\n",
    "    # Ensure you reference the correct GeoDataFrame when removing rows\n",
    "    Neckel2021_outlines = Neckel2021_outlines[~Neckel2021_outlines['name'].isin(criteria)]\n",
    "\n",
    "# Add citation column\n",
    "Neckel2021_outlines['cite'] = 'Neckel and others, 2021, Geophys. Res. Lett., doi:10.1029/2021GL094472'\n",
    "\n",
    "# Reset the index of the GeoDataFrame after all operations\n",
    "Neckel2021_outlines = Neckel2021_outlines.reset_index(drop=True)\n",
    "\n",
    "# Ensure GeoDataFrame is in EPSG:4326 for geodesic area calculation\n",
    "if Neckel2021_outlines.crs != 'EPSG:4326':\n",
    "    Neckel2021_outlines = Neckel2021_outlines.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "# Calculate the geodesic area for each polygon\n",
    "Neckel2021_outlines['area (m^2)'] = Neckel2021_outlines.geometry.apply(\n",
    "    lambda poly: abs(geod.polygon_area_perimeter(\n",
    "        poly.exterior.coords.xy[0], poly.exterior.coords.xy[1])[0]) if poly is not None and poly.is_valid else None)\n",
    "\n",
    "# Convert to crs to epsg:3031\n",
    "Neckel2021_outlines = Neckel2021_outlines.to_crs(3031)\n",
    "\n",
    "# Add a prefix to all the names to indicate their locations on Jutulstraumen Glacier\n",
    "Neckel2021_outlines['name'] = 'JG_' + Neckel2021_outlines['name']\n",
    "\n",
    "# Add to stationary_outlines_gdf\n",
    "gdf_diff = Neckel2021_outlines[~Neckel2021_outlines['name'].isin(stationary_outlines_gdf['name'])]\n",
    "stationary_outlines_gdf = pd.concat([stationary_outlines_gdf, gdf_diff], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9236a8-5d47-4667-bad9-8411d9ac1c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "Neckel2021_outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4308b621-20e4-41e5-aaff-b0f2df26f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Filter rows on the 'name' column\n",
    "filtered_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'].isin(Neckel2021_outlines['name'].tolist())]\n",
    "\n",
    "filtered_gdf.boundary.plot(ax=ax, color='blue')\n",
    "# Iterate through the GeoDataFrame to annotate each polygon.\n",
    "for idx, row in filtered_gdf.iterrows():\n",
    "    # Use the centroid of each polygon for the annotation location.\n",
    "    centroid = row['geometry'].centroid\n",
    "    ax.annotate(text=row['name'], xy=(centroid.x, centroid.y), xytext=(3, 3), textcoords=\"offset points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef240d0-4e0c-45b2-8529-444984514d13",
   "metadata": {},
   "source": [
    "### Siegfried and Fricker, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be5e348-c450-4c08-ba25-7adf1fca37d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Siegfried and Fricker, 2021, Geophys. Res. Lett.\n",
    "# (publication not included in Livingstone and others, 2022 inventory)\n",
    "# https://doi.org/10.1029/2020GL091089\n",
    "\n",
    "# New active lakes near previously identified lakes on Mercer and Whillans Ice Streams\n",
    "\n",
    "# These are discussed but no figures contain outlines; Lead author, Siegfried, confirmed no outlines were generated\n",
    "# Guestimated centroid point and approximate area\n",
    "\n",
    "# Lower Conway Subglacial Lake\n",
    "name = 'LowerConwaySubglacialLake'\n",
    "area = 10e6\n",
    "geometry = Point(-308000, -509000).buffer(math.sqrt(area / math.pi))\n",
    "cite = 'Siegfried and Fricker, 2021, Geophys. Res. Lett., doi:10.1029/2020GL091089'\n",
    "gdf = gpd.GeoDataFrame([[name, geometry, area, cite]], columns=stationary_outlines_gdf.columns, geometry=[geometry])\n",
    "gdf.crs = stationary_outlines_gdf.crs\n",
    "gdf_diff = gdf[~gdf['name'].isin(stationary_outlines_gdf['name'])]\n",
    "stationary_outlines_gdf = gpd.GeoDataFrame(pd.concat([stationary_outlines_gdf, gdf_diff], ignore_index=True), crs=stationary_outlines_gdf.crs)\n",
    "\n",
    "# Lower Subglacial Lake Mercer\n",
    "name = 'LowerMercerSubglacialLake'\n",
    "area = 10e6\n",
    "geometry = Point(-282000, -502000).buffer(math.sqrt(area / math.pi))\n",
    "cite = 'Siegfried and Fricker, 2021, Geophys. Res. Lett., doi:10.1029/2020GL091089'\n",
    "gdf = gpd.GeoDataFrame([[name, geometry, area, cite]], columns=stationary_outlines_gdf.columns, geometry=[geometry])\n",
    "gdf.crs = stationary_outlines_gdf.crs\n",
    "gdf_diff = gdf[~gdf['name'].isin(stationary_outlines_gdf['name'])]\n",
    "stationary_outlines_gdf = gpd.GeoDataFrame(pd.concat([stationary_outlines_gdf, gdf_diff], ignore_index=True), crs=stationary_outlines_gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fbbbd6-91b0-4169-9407-0847de4618f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Filter rows on the 'name' column\n",
    "filtered_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'].isin(['MercerSubglacialLake', 'LowerMercerSubglacialLake', 'ConwaySubglacialLake', 'LowerConwaySubglacialLake'])]\n",
    "\n",
    "filtered_gdf.boundary.plot(ax=ax, color='blue')\n",
    "# Iterate through the GeoDataFrame to annotate each polygon.\n",
    "for idx, row in filtered_gdf.iterrows():\n",
    "    # Use the centroid of each polygon for the annotation location.\n",
    "    centroid = row['geometry'].centroid\n",
    "    ax.annotate(text=row['name'], xy=(centroid.x, centroid.y), xytext=(3, 3), textcoords=\"offset points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4000dd-3bc3-4f14-8d86-e18c82870940",
   "metadata": {},
   "source": [
    "### Freer and others, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9699d8cf-66a4-44cf-97bb-6665bc50daf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freer and others, 2024, J. Geophys. Res. Earth Surf.\n",
    "# https://doi.org/10.1029/2024JF007724\n",
    "# (study published after Livingstone and others, 2022 inventory)\n",
    "\n",
    "# New active lakes near previously identified lakes on Whillans ice plain\n",
    "\n",
    "# Lake outline files obtained via email from corresponding author, Freer\n",
    "\n",
    "# Read shape files into separate geodataframes\n",
    "sle_outline = gpd.read_file('input/lake_outlines/Freer2024_lakes/SLE-outline-BF.shp')\n",
    "feeder_outline = gpd.read_file('input/lake_outlines/Freer2024_lakes/feeder-lake-outline-BF.shp')\n",
    "\n",
    "# Concatenate the two geodataframes\n",
    "Freer2024_outlines = pd.concat([sle_outline, feeder_outline], ignore_index=True)\n",
    "\n",
    "# Convert CRS to 3031\n",
    "Freer2024_outlines = Freer2024_outlines.to_crs('EPSG:3031')\n",
    "\n",
    "# Display geodataframe\n",
    "Freer2024_outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b50759-36d2-48e7-9c2c-23c81a0737fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "Freer2024_outlines.boundary.plot(ax=ax, color='blue')\n",
    "SF18_outlines[SF18_outlines['name'] == 'EngelhardtSubglacialLake'].boundary.plot(ax=ax, color='k')\n",
    "\n",
    "# Adding labels to the center of each polygon\n",
    "for idx, row in Freer2024_outlines.iterrows():\n",
    "    centroid = row.geometry.centroid\n",
    "    ax.text(centroid.x, centroid.y, row['names'], fontsize=12, ha='center', va='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0043e2d1-8a77-44af-8a0b-271fe865189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View current SLE lake\n",
    "stationary_outlines_gdf[stationary_outlines_gdf['name'] == 'EngelhardtSubglacialLake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aff6cc-f94e-4e19-81bb-e7ad7ac94688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store SLE original name and citation\n",
    "SLE_original_name = stationary_outlines_gdf[stationary_outlines_gdf['name'] == 'EngelhardtSubglacialLake']['name'].iloc[0]\n",
    "SLE_original_cite = stationary_outlines_gdf[stationary_outlines_gdf['name'] == 'EngelhardtSubglacialLake']['cite'].iloc[0]\n",
    "\n",
    "# Remove older SLE delineation from stationary_outlines_gdf\n",
    "print(len(stationary_outlines_gdf[stationary_outlines_gdf['name'] == 'EngelhardtSubglacialLake']))\n",
    "stationary_outlines_gdf = stationary_outlines_gdf.drop(stationary_outlines_gdf[stationary_outlines_gdf['name'] == 'EngelhardtSubglacialLake'].index)\n",
    "print(len(stationary_outlines_gdf[stationary_outlines_gdf['name'] == 'EngelhardtSubglacialLake']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec6dbf-db1b-43fc-b54e-e5f2ffe24037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each row of the Freer2024_outlines GeoDataFrame\n",
    "for i, row in Freer2024_outlines.iterrows():\n",
    "    # Extract name and geometry for each row\n",
    "    if i == 0:\n",
    "        name = SLE_original_name\n",
    "    else:\n",
    "        name = 'UpperEngelhardtSubglacialLake'\n",
    "    geometry = row['geometry']\n",
    "    \n",
    "    # Create a temporary GeoDataFrame for the current lake geometry\n",
    "    lake_gdf = gpd.GeoDataFrame([[geometry]], columns=['geometry'], geometry='geometry', crs=Freer2024_outlines.crs)\n",
    "\n",
    "    # Convert CRS to 4326 for geodesic area calculations\n",
    "    lake_gdf = lake_gdf.to_crs('EPSG:4326')\n",
    "    \n",
    "    # Calculate the area using geodesic area calculation\n",
    "    area = lake_gdf['geometry'].apply(\n",
    "        lambda poly: abs(geod.polygon_area_perimeter(\n",
    "            poly.exterior.coords.xy[0], poly.exterior.coords.xy[1])[0]) if poly is not None and poly.is_valid else None)[0]\n",
    "    \n",
    "    # Convert CRS to 3031 for plotting\n",
    "    lake_gdf = lake_gdf.to_crs('EPSG:3031')\n",
    "    \n",
    "    # Set citation information\n",
    "    if i == 0:\n",
    "        cite = SLE_original_cite + '; Freer and others, 2024, J. Geophys. Res. Earth Surf., doi:10.1029/2024JF007724'\n",
    "    else:\n",
    "        cite = 'Freer and others, 2024, J. Geophys. Res. Earth Surf., doi:10.1029/2024JF007724'\n",
    "    \n",
    "    # Create a GeoDataFrame for the current lake and append it to the static lakes inventory\n",
    "    gdf = gpd.GeoDataFrame([[name, geometry, area, cite]], columns=stationary_outlines_gdf.columns, geometry=[geometry], crs=lake_gdf.crs)\n",
    "    \n",
    "    # Ensure the new lake entry isn't already in the stationary_outlines_gdf (to avoid duplicates)\n",
    "    gdf_diff = gdf[~gdf['name'].isin(stationary_outlines_gdf['name'])]\n",
    "    \n",
    "    # Concatenate the new entry to the stationary_outlines_gdf\n",
    "    stationary_outlines_gdf = gpd.GeoDataFrame(pd.concat([stationary_outlines_gdf, gdf_diff], ignore_index=True), crs=stationary_outlines_gdf.crs)\n",
    "\n",
    "# Display the updated stationary_outlines_gdf\n",
    "stationary_outlines_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbb14b-46db-4f32-b2ba-6d2813b5881a",
   "metadata": {},
   "source": [
    "### Arthur and others, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b6df2-d85f-4c45-b686-c05f53c8f76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arthur and others, 2025, Active subglacial lakes along Dronning Maud Land\n",
    "# (study published after Livingstone and others, 2022 inventory)\n",
    "# https://doi.org/10.5194/tc-19-375-2025\n",
    "# Lake outline files:\n",
    "# Arthur and others, 2024, Active subglacial lakes along Dronning Maud Land derived from ICESat-2 and ICESat \n",
    "# https://doi.org/10.21334/npolar.2024.ab777130\n",
    "\n",
    "# Read shape file into geodataframe\n",
    "file_path = 'input/lake_outlines/Arthur2025_lakes/DML_Lakes_Combined.shp'\n",
    "Arthur2025_outlines = gpd.read_file(file_path)\n",
    "\n",
    "# Display geodataframe\n",
    "Arthur2025_outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad0273-39bf-4221-9160-4196b113a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the 'name' and 'geometry' columns\n",
    "Arthur2025_outlines = Arthur2025_outlines[['Name', 'geometry']]\n",
    "\n",
    "# Rename the column 'OBJECTID' to 'name'\n",
    "Arthur2025_outlines = Arthur2025_outlines.rename(columns={'Name': 'name'})\n",
    "\n",
    "# Display the updated GeoDataFrame\n",
    "Arthur2025_outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b316e3d-e5d8-4fdd-b73d-4fa8f1c4db4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "Arthur2025_outlines.boundary.plot(ax=ax)\n",
    "\n",
    "# Adding labels to the center of each polygon\n",
    "for idx, row in Arthur2025_outlines.iterrows():\n",
    "    centroid = row.geometry.centroid\n",
    "    ax.text(centroid.x, centroid.y, row['name'], fontsize=12, ha='center', va='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b0f94-3086-4ed7-8d6d-19c5f72dd310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each row of the Arthur2025_outlines GeoDataFrame\n",
    "for index, row in Arthur2025_outlines.iterrows():\n",
    "    # Extract name and geometry for each row\n",
    "    name = row['name']\n",
    "    geometry = row['geometry']\n",
    "    \n",
    "    # Create a temporary GeoDataFrame for the current lake geometry\n",
    "    lake_gdf = gpd.GeoDataFrame([[geometry]], columns=['geometry'], geometry='geometry', crs=Arthur2025_outlines.crs)\n",
    "    \n",
    "    # Convert CRS to 4326 for geodesic area calculations\n",
    "    lake_gdf = lake_gdf.to_crs('EPSG:4326')\n",
    "    \n",
    "    # Calculate the area using geodesic area calculation\n",
    "    area = lake_gdf['geometry'].apply(\n",
    "        lambda poly: abs(geod.polygon_area_perimeter(\n",
    "            poly.exterior.coords.xy[0], poly.exterior.coords.xy[1])[0]) if poly is not None and poly.is_valid else None\n",
    "    )[0]\n",
    "    \n",
    "    # Convert CRS back to 3031 for plotting\n",
    "    lake_gdf = lake_gdf.to_crs('EPSG:3031')\n",
    "    \n",
    "    # Set citation information\n",
    "    cite = 'Arthur and others, 2025, Cryosphere, doi:10.5194/tc-19-375-2025'\n",
    "    \n",
    "    # Create a GeoDataFrame for the current lake and append it to the static lakes inventory\n",
    "    gdf = gpd.GeoDataFrame([[name, geometry, area, cite]], columns=stationary_outlines_gdf.columns, geometry=[geometry], crs=lake_gdf.crs)\n",
    "    \n",
    "    # Ensure the new lake entry isn't already in the stationary_outlines_gdf (to avoid duplicates)\n",
    "    gdf_diff = gdf[~gdf['name'].isin(stationary_outlines_gdf['name'])]\n",
    "    \n",
    "    # Concatenate the new entry to the stationary_outlines_gdf\n",
    "    stationary_outlines_gdf = gpd.GeoDataFrame(pd.concat([stationary_outlines_gdf, gdf_diff], ignore_index=True), crs=stationary_outlines_gdf.crs)\n",
    "\n",
    "# Display the updated stationary_outlines_gdf\n",
    "stationary_outlines_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4049d8e-789a-4188-9ce9-44de943d6780",
   "metadata": {},
   "source": [
    "# Finalize Sauthoff and others (2025) inventory of re-examined lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b61b334-a910-4bdb-abe6-835362218071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find invalid geometries\n",
    "invalid_rows = stationary_outlines_gdf[~stationary_outlines_gdf.is_valid]\n",
    "\n",
    "# Display the invalid rows\n",
    "print(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfac7b95-9c08-429a-8f2a-52357930ff8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Repair invalid geometries\n",
    "stationary_outlines_gdf.loc[~stationary_outlines_gdf.is_valid, 'geometry'] = stationary_outlines_gdf[~stationary_outlines_gdf.is_valid].geometry.buffer(0)\n",
    "\n",
    "# Check if there are still any invalid geometries\n",
    "print(stationary_outlines_gdf[~stationary_outlines_gdf.is_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ebb241-0736-4084-8e23-9fe37e18b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detemine if there are any duplicates\n",
    "def find_overlapping_geometries(gdf):\n",
    "    \"\"\"\n",
    "    Find overlapping geometries in a GeoDataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf : geopandas.GeoDataFrame\n",
    "        Input GeoDataFrame with a 'geometry' column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of tuples\n",
    "        Each tuple is (index1, index2) where geometries overlap.\n",
    "    \"\"\"\n",
    "    overlaps = []\n",
    "    sindex = gdf.sindex  # spatial index for speed\n",
    "\n",
    "    for idx, geom in gdf.geometry.items():  # <-- changed here\n",
    "        if geom is None:\n",
    "            continue\n",
    "\n",
    "        # Candidate matches by bounding box\n",
    "        possible_matches_index = list(sindex.intersection(geom.bounds))\n",
    "        possible_matches = gdf.iloc[possible_matches_index]\n",
    "\n",
    "        for jdx, other_geom in possible_matches.geometry.items():  # <-- changed here\n",
    "            if idx >= jdx:  # avoid self & duplicate pairs\n",
    "                continue\n",
    "            if geom.intersects(other_geom):\n",
    "                overlaps.append((idx, jdx))\n",
    "\n",
    "    return overlaps\n",
    "\n",
    "overlaps = find_overlapping_geometries(stationary_outlines_gdf)\n",
    "\n",
    "if overlaps:\n",
    "    print(\"Overlapping geometry pairs found:\")\n",
    "    for i, j in overlaps:\n",
    "        print(f\"Row {i} overlaps with Row {j}\")\n",
    "else:\n",
    "    print(\"No overlaps found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c2fc77-4e89-4b2f-9a7d-ddcfa896bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View lakes determined to have overlap\n",
    "print(stationary_outlines_gdf.iloc[0]['name'], 'overlaps with', stationary_outlines_gdf.iloc[90]['name'])\n",
    "print(stationary_outlines_gdf.iloc[1]['name'], 'overlaps with', stationary_outlines_gdf.iloc[91]['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f6b5d-82f3-4e14-aeaa-75ecebcbdea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print citation information for lakes determined to have overlap\n",
    "print(stationary_outlines_gdf.iloc[0]['cite'])\n",
    "print(stationary_outlines_gdf.iloc[90]['cite'])\n",
    "print(stationary_outlines_gdf.iloc[1]['cite'])\n",
    "print(stationary_outlines_gdf.iloc[91]['cite'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a6c62-4919-4625-9292-93bd1e39ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "S09_outlines[S09_outlines['Name'].str.contains('Mac', case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3431a63-090a-4ba7-8e9b-3d402ed6e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the four rows\n",
    "rows_to_plot = stationary_outlines_gdf.iloc[[0, 90, 1, 91]]\n",
    "\n",
    "bindschadlers = S09_outlines[S09_outlines['Name'].isin(['Bindschadler_1', 'Bindschadler_2', 'Bindschadler_3', \n",
    "                                                        'Bindschadler_4', 'Bindschadler_5', 'Bindschadler_6'])]\n",
    "\n",
    "# Select the specific geometry from S09_outlines\n",
    "bindschadler1 = S09_outlines[S09_outlines['Name'] == 'Bindschadler_1']\n",
    "bindschadler2 = S09_outlines[S09_outlines['Name'] == 'Bindschadler_2']\n",
    "bindschadler5 = S09_outlines[S09_outlines['Name'] == 'Bindschadler_5']\n",
    "\n",
    "# Select the specific geometry from SF18_outlines\n",
    "Bindschadler_1 = SF18_outlines[SF18_outlines['name'] == 'Bindschadler_1']\n",
    "Bindschadler_2 = SF18_outlines[SF18_outlines['name'] == 'Bindschadler_2']\n",
    "Mac7 = SF18_outlines[SF18_outlines['name'] == 'Mac7']\n",
    "Mac8 = SF18_outlines[SF18_outlines['name'] == 'Mac8']\n",
    "\n",
    "# Create a figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "bindschadlers.plot(ax=ax, facecolor='blue', edgecolor='none', linewidth=2)\n",
    "\n",
    "# Plot S09 geometries in a distinct color\n",
    "bindschadler1.plot(ax=ax, facecolor='none', edgecolor='red', lw=2)\n",
    "bindschadler2.plot(ax=ax, facecolor='none', edgecolor='green', lw=2)\n",
    "bindschadler5.plot(ax=ax, facecolor='none', edgecolor='purple', lw=2)\n",
    "\n",
    "# Plot SF18 geometries in a distinct color\n",
    "Bindschadler_1.plot(ax=ax, facecolor='red', edgecolor='none', lw=2, ls='--')\n",
    "Bindschadler_2.plot(ax=ax, facecolor='green', edgecolor='none', lw=2, ls='--')\n",
    "Mac7.plot(ax=ax, facecolor='none', edgecolor='blue', lw=2, ls='--')\n",
    "Mac8.plot(ax=ax, facecolor='none', edgecolor='blue', lw=2, ls='--')\n",
    "\n",
    "ax.set_title('Duplicated Stationary Outlines')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb65a8bb-71fd-4e7c-a61d-c82102296921",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Two lakes (Bindschadler_1 and Bindschadler_2) were renamed in [Carter and others (2011)](https://doi.org/10.3189/002214311798843421) to Mac7 and Mac8 likely under the assumption that these two lakes are within the subglacial watershed of MacAyeal Ice Stream  instead of the Bindschadler Ice Stream; however, the renaming was not documented in that paper and so these lake outlines were duplicative to Bindschadler_1 and _2 in the SF18 inventory. \n",
    "\n",
    "Watershed delineation indicates that one of these lakes is likely in the MacAyeal subglacial watershed; however, we feel this renaming is unnecessary and confusing, so we opt to use the original names for these two lakes (Bindschadler_1 and _2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d0e18-1f00-4a66-a994-643602564417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Mac7 and Mac8 outlines from stationary_outlines_gdf that were renamed Bindschadler_1 and _2, \n",
    "# which were not removed from the SF18 stationary outlines inventory and thus duplicated in that inventory\n",
    "print(len(stationary_outlines_gdf))\n",
    "stationary_outlines_gdf = stationary_outlines_gdf[~stationary_outlines_gdf['name'].isin(['Mac7', 'Mac8'])]\n",
    "print(len(stationary_outlines_gdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000acbd5-39d6-4f24-ad12-4ad2caa89292",
   "metadata": {},
   "source": [
    "[Carter and others (2013)](https://doi.org/10.3189/2013JoG13J085) defined Lake7 and Lake8 (originally from [Fricker and Scambos (2009)](https://doi.org10.3189/002214309788608813)) as a lake system, Lake78."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d5891-10e3-4f1a-8139-4ebc41f75c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the reference for this lake reflects that\n",
    "SF18_outlines[SF18_outlines['cite'].str.contains('Carter and others, 2013', case=False, na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d84a67a-5542-494d-abe3-a236b2be9792",
   "metadata": {},
   "source": [
    "## Double check area calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d901543-9e55-46aa-9843-64739b1cc43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrepancies = []\n",
    "# Convert GeoDataFrame to EPSG:4326 for geodesic area calculation\n",
    "stationary_outlines_gdf = stationary_outlines_gdf.to_crs('4326')\n",
    "# Define tolerance\n",
    "tolerance = 1e-5\n",
    "\n",
    "for idx, row in stationary_outlines_gdf.iterrows():\n",
    "    calculated_area = calculate_area(row.geometry, stationary_outlines_gdf.crs)\n",
    "    stored_area = row['area (m^2)']\n",
    "    \n",
    "    # Skip if either value is None\n",
    "    if calculated_area is None or stored_area is None:\n",
    "        continue\n",
    "        \n",
    "    # Compare areas with relative tolerance\n",
    "    relative_diff = abs(calculated_area - stored_area) / stored_area\n",
    "    if relative_diff > tolerance:\n",
    "        discrepancies.append({\n",
    "            'name': row['name'],\n",
    "            'stored_area_m2': stored_area,\n",
    "            'calculated_area_m2': calculated_area,\n",
    "            'relative_difference': relative_diff,\n",
    "            'difference_percentage': relative_diff * 100\n",
    "        })\n",
    "\n",
    "# Create DataFrame from discrepancies\n",
    "discrepancies_df = pd.DataFrame(discrepancies)\n",
    "\n",
    "# Display the DataFrame with better formatting\n",
    "if not discrepancies_df.empty:\n",
    "    # Round numerical columns for better readability\n",
    "    discrepancies_df['stored_area_m2'] = discrepancies_df['stored_area_m2'].round(2)\n",
    "    discrepancies_df['calculated_area_m2'] = discrepancies_df['calculated_area_m2'].round(2)\n",
    "    discrepancies_df['relative_difference'] = discrepancies_df['relative_difference'].round(2)\n",
    "    discrepancies_df['difference_percentage'] = discrepancies_df['difference_percentage'].round(2)\n",
    "    \n",
    "    # Set pandas display options to prevent scientific notation\n",
    "    pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "    \n",
    "    print(f\"Found {len(discrepancies_df)} discrepancies:\")\n",
    "    print(discrepancies_df.to_string(index=False))\n",
    "    \n",
    "    # Reset display options to default\n",
    "    pd.reset_option('display.float_format')\n",
    "else:\n",
    "    print(\"No discrepancies found above the tolerance threshold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d458b-b747-4d3d-aced-f56f08193610",
   "metadata": {},
   "source": [
    "Discrepency at Lake78 appears to be a conversion from m^2 to km^2 because the relative difference is nearly 1e6. We will replace with a new calculation of geodesic area.\n",
    "\n",
    "Discrepencies at Mac3-4 and Rec4 are small (around 1% or less), but still large enough to replace with a new calculation of geodesic area. \n",
    "\n",
    "Discrepencies at L1 and other lakes are due to the fact that we used a planar area to estimate a perfect circle outline around the reported point location of the lake to a reported or estimated area to have a stationary outline to use as a target for the evolving outlines algorithm. These outlines do not need to be altered as they were mere estimates and only used in the interim as a target for finding evolving outlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97317677-101c-457a-99ed-3c9d75e2a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of lakes to update\n",
    "lakes_to_update = ['Lake78', 'Mac3', 'Mac4', 'Rec4']\n",
    "\n",
    "# Ensure GDF is in EPSG:4326\n",
    "if stationary_outlines_gdf.crs.to_string().upper() != 'EPSG:4326':\n",
    "    stationary_outlines_gdf = stationary_outlines_gdf.to_crs('4326')\n",
    "\n",
    "# Update areas for specified lakes\n",
    "for idx, row in stationary_outlines_gdf.iterrows():\n",
    "    if row['name'] in lakes_to_update:\n",
    "        calculated_area = calculate_area(row.geometry, stationary_outlines_gdf.crs)\n",
    "        stationary_outlines_gdf.at[idx, 'area (m^2)'] = calculated_area\n",
    "\n",
    "# Reproject to Antarctic polar stereographic projection\n",
    "stationary_outlines_gdf = stationary_outlines_gdf.to_crs('3031')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3010440-2be4-481e-bf44-7ec36629f226",
   "metadata": {},
   "source": [
    "## Determine CryoSat-2 SARIn coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4daa8e7-2f73-4675-82d6-608c7e163f97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import CryoSat-2 SARIn mode mask\n",
    "# See 0_preprocess_data.ipynb for data source and pre-processing steps\n",
    "gdf_SARIn_3_1 = gpd.read_file('output/CryoSat2_SARIn_mode_masks/gdf_SARIn_3_1.geojson')\n",
    "gdf_SARIn_3_1_3_6_diff= gpd.read_file('output/CryoSat2_SARIn_mode_masks/gdf_SARIn_3_1_3_6_diff.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd767104-15b6-428f-8299-c5f2a8f10966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot stationary_outlines_gdf with CS2 SARIn mode mask\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "gdf_SARIn_3_1.plot(ax=ax, edgecolor='red', facecolor='red', label='SARIn mode mask 3.1', alpha=0.5)\n",
    "gdf_SARIn_3_1_3_6_diff.plot(ax=ax, edgecolor='red', facecolor='red', label='SARIn mode mask 3.6', alpha=0.5)\n",
    "stationary_outlines_gdf.plot(ax=ax, edgecolor='cyan', facecolor='cyan')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575f52ae-9d35-495f-9b81-02c0fbff1dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a column to indicate the time period each lake has CS2 SARIn coverage if any\n",
    "\n",
    "# Combining the SARIn mode masks the occur during the CS2-IS2 analysis time period (2010-2023)\n",
    "combined_geom = unary_union([gdf_SARIn_3_1, gdf_SARIn_3_1_3_6_diff])\n",
    "\n",
    "# Creating a new GeoDataFrame with the combined geometry\n",
    "gdf_SARIn_3_1_3_6 = gpd.GeoDataFrame(geometry=[combined_geom])\n",
    "gdf_SARIn_3_1_3_6.crs = 'epsg:3031'\n",
    "\n",
    "# Use the 'within' method to find lakes that have CS2 SARIn coverage during the different SARIn mode mask versions\n",
    "# See 0_preprocess_data.ipynb for more info\n",
    "CS2_SARIn_3_1_col = stationary_outlines_gdf.geometry.within(gdf_SARIn_3_1.geometry.iloc[0])\n",
    "CS2_SARIn_3_1_3_6_col = stationary_outlines_gdf.geometry.within(gdf_SARIn_3_1_3_6.geometry.iloc[0])\n",
    "\n",
    "# Conditions\n",
    "condition1 = (CS2_SARIn_3_1_col == True) & (CS2_SARIn_3_1_3_6_col == True)\n",
    "condition2 = (CS2_SARIn_3_1_col == False) & (CS2_SARIn_3_1_3_6_col == True)\n",
    "\n",
    "# Choices based on the conditions\n",
    "choices = ['2010.5', '2013.75']\n",
    "\n",
    "# Use np.select to apply conditions and choices\n",
    "stationary_outlines_gdf['CS2_SARIn_start'] = np.select([condition1, condition2], choices, default=pd.NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e6c12e-549b-442a-9cef-45e9f082bc02",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d946c-c1a0-45b7-84bb-b5fa35dd658e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View lakes added to Siegfried & Fricker, 2018 inventory to create revised inventory\n",
    "\n",
    "# Drop outlines from stationary_outlines_gdf that are part of SF18_outlines (indexes 0:130)\n",
    "# and Recovery_8 (idx 131) originally from S09_outlines\n",
    "stationary_outlines_notinSF18_gdf = stationary_outlines_gdf.copy(deep=True)\n",
    "stationary_outlines_notinSF18_gdf.drop(stationary_outlines_notinSF18_gdf.iloc[0:132].index, inplace=True)\n",
    "\n",
    "# View dataframe\n",
    "stationary_outlines_notinSF18_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea27f97e-e081-4f01-a06c-f49cc8cc8857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort the GeoDataFrame by the 'name' column\n",
    "stationary_outlines_gdf = stationary_outlines_gdf.sort_values(by='name')\n",
    "\n",
    "# Resetting the index of the sorted GeoDataFrame\n",
    "stationary_outlines_gdf = stationary_outlines_gdf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb8e2f8-20ca-4316-931f-a5bc1dc1cb77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display geodataframe\n",
    "stationary_outlines_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1be2c9-41cf-4f59-a144-94ede6077c83",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ab1265-25ae-47b4-96df-ff3266940773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "stationary_outlines_gdf.to_file('output/lake_outlines/stationary_outlines/stationary_outlines_gdf.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d07161d-c748-496b-98d6-74cf5b07d87a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
