{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91744e7d-64d6-4cc7-a908-e0f607fa8fb3",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* rename notebook to reflect new SI fig numbers\n",
    "* update SLM fill/drain plot to use one in dV section\n",
    "* change stationary_outlines_gdf.geojson to prior_stationary_outlines.geojson throughout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f816f210-8934-4276-a4bb-775da6475a4c",
   "metadata": {},
   "source": [
    "Notebook generates statistics used in article text, data in Table S2, and Figs. 2, 3, S2-5.\n",
    "\n",
    "Written 2023-11-11 by W. Sauthoff (wsauthoff.github.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63862714-5c5b-41ad-9f67-8e8fe91b4c52",
   "metadata": {},
   "source": [
    "# Set up computing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0490b8-0bb2-4bf8-8ea1-d93c28a1610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import datetime\n",
    "import fiona\n",
    "import functools\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "import hvplot.pandas\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "from math import radians\n",
    "import matplotlib\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.legend_handler import HandlerPatch, HandlerTuple\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pyproj import CRS, Geod, Transformer\n",
    "import rioxarray\n",
    "from scipy import odr\n",
    "from scipy.stats import percentileofscore\n",
    "from shapely.geometry import MultiPolygon, Point, Polygon\n",
    "from shapely.ops import transform, unary_union\n",
    "import xarray as xr\n",
    "\n",
    "# Magic functions\n",
    "%matplotlib widget\n",
    "\n",
    "# Define data directories dependent on home environment\n",
    "# Replace with your directory file paths\n",
    "if os.getenv('HOME') == '/home/jovyan':\n",
    "    DATA_DIR = '/home/jovyan/data'\n",
    "    OUTPUT_DIR = '/home/jovyan/1_evolving_lakes/output/Figs23_S23_lake_reexamination_results'\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Define constants and coordinate transforms for the geodesic area calculation\n",
    "CRS_LL = \"EPSG:4326\" # wgs84 in lon,lat\n",
    "GEOD = CRS(CRS_LL).get_geod() # geod object for calculating geodesic area on defined ellipsoid\n",
    "CRS_XY = \"EPSG:3031\" # Antarctic Polar Stereographic in x, y\n",
    "XY_TO_LL = Transformer.from_crs(CRS_XY, CRS_LL, always_xy = True) # make coord transformer\n",
    "geod = Geod(ellps=\"WGS84\") # Create a Geod object for calculating area on the WGS84 ellipsoid\n",
    "\n",
    "# Define utility functions\n",
    "def ll2ps(lon, lat):\n",
    "    \"\"\"\n",
    "    Transform coordinates from geodetic coordinates (lon, lat)\n",
    "    to Antarctic Polar Stereograph coordinates (x, y)\n",
    "    x, y = ll2ps(lon, lat)\n",
    "    \"\"\"\n",
    "    crs_ll = CRS(\"EPSG:4326\")\n",
    "    crs_xy = CRS(\"EPSG:3031\")\n",
    "    ll_to_xy = Transformer.from_crs(crs_ll, crs_xy, always_xy = True)\n",
    "    x, y = ll_to_xy.transform(lon, lat)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6a7d91-6d40-45b7-8ff8-0e29f2e55c5f",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a605abb-9415-47a6-926a-e55cb4807b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantify_lake_extensions(lakes_gdf, evolving_outlines_union_gdf, area_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Quantify lakes with extensions beyond their original stationary outlines using geodesic area calculations.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lakes_gdf : GeoDataFrame\n",
    "        GeoDataFrame containing stationary lake outlines with 'name' column\n",
    "    evolving_outlines_union_gdf : GeoDataFrame\n",
    "        GeoDataFrame containing evolving union outlines with 'name' column\n",
    "    area_threshold : float, default=0.05\n",
    "        Minimum fraction of area increase to consider as a reportable extension\n",
    "        (e.g., 0.05 means 5% area increase)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (lakes_with_extensions, extension_results)\n",
    "        - lakes_with_extensions: count of lakes with reportable extensions\n",
    "        - extension_results: DataFrame with detailed extension metrics for each lake\n",
    "    \"\"\"\n",
    "    # Define the geodesic object for Earth calculations\n",
    "    geod = Geod(ellps=\"WGS84\")\n",
    "    \n",
    "    # Create a transformer to convert from EPSG:3031 to EPSG:4326\n",
    "    project = Transformer.from_crs(\"EPSG:3031\", \"EPSG:4326\", always_xy=True).transform\n",
    "\n",
    "    def transform_to_4326(geometry):\n",
    "        '''\n",
    "        Transform geometry from EPSG:3031 to EPSG:4326\n",
    "        '''\n",
    "        if geometry is None or not geometry.is_valid:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Use functools.partial to create a function that can be used with shapely's transform\n",
    "            project_func = functools.partial(project)\n",
    "            transformed_geom = transform(project_func, geometry)\n",
    "            return transformed_geom\n",
    "        except Exception as e:\n",
    "            print(f\"Error transforming geometry: {e}\")\n",
    "            return None\n",
    "\n",
    "    def calculate_geodesic_area_and_perimeter(geometry):\n",
    "        '''\n",
    "        Calculate geodesic area and perimeter of a polygon or multipolygon.\n",
    "        First transforms geometry from EPSG:3031 to EPSG:4326, then performs calculations.\n",
    "        '''\n",
    "        \n",
    "        # Ensure geometry exists and is valid\n",
    "        if geometry is None or not geometry.is_valid:\n",
    "            return None, None\n",
    "        \n",
    "        # Transform geometry to EPSG:4326\n",
    "        geom_4326 = transform_to_4326(geometry)\n",
    "        if geom_4326 is None:\n",
    "            return None, None\n",
    "            \n",
    "        if isinstance(geom_4326, Polygon):\n",
    "            # Calculate area and perimeter for a single polygon\n",
    "            area, perimeter = geod.polygon_area_perimeter(geom_4326.exterior.coords.xy[0], \n",
    "                                                          geom_4326.exterior.coords.xy[1])\n",
    "            # Subtract areas of holes if any exist\n",
    "            for interior in geom_4326.interiors:\n",
    "                hole_area, _ = geod.polygon_area_perimeter(interior.coords.xy[0], \n",
    "                                                           interior.coords.xy[1])\n",
    "                area -= hole_area\n",
    "                \n",
    "            return abs(area), abs(perimeter)\n",
    "            \n",
    "        elif isinstance(geom_4326, MultiPolygon):\n",
    "            # Calculate combined area and perimeter for multipolygons\n",
    "            total_area = 0\n",
    "            total_perimeter = 0\n",
    "            for part in geom_4326.geoms:\n",
    "                # Add the part's area\n",
    "                part_area, part_perimeter = geod.polygon_area_perimeter(part.exterior.coords.xy[0], \n",
    "                                                                        part.exterior.coords.xy[1])\n",
    "                total_area += abs(part_area)\n",
    "                total_perimeter += abs(part_perimeter)\n",
    "                \n",
    "                # Subtract areas of holes if any exist\n",
    "                for interior in part.interiors:\n",
    "                    hole_area, _ = geod.polygon_area_perimeter(interior.coords.xy[0], \n",
    "                                                              interior.coords.xy[1])\n",
    "                    total_area -= abs(hole_area)\n",
    "            \n",
    "            return total_area, total_perimeter\n",
    "        else:\n",
    "            return None, None\n",
    "            \n",
    "    results = []\n",
    "    \n",
    "    # Process each lake\n",
    "    for lake_name in evolving_outlines_union_gdf['name'].unique():\n",
    "        try:\n",
    "            # Get stationary outline\n",
    "            stationary = lakes_gdf[lakes_gdf['name'] == lake_name]['geometry'].iloc[0]\n",
    "            \n",
    "            # Get evolving union outline\n",
    "            evolving = evolving_outlines_union_gdf[\n",
    "                evolving_outlines_union_gdf['name'] == lake_name]['geometry'].iloc[0]\n",
    "            \n",
    "            # Skip if either geometry is missing\n",
    "            if stationary is None or evolving is None:\n",
    "                continue\n",
    "                \n",
    "            # Calculate geodesic areas\n",
    "            stationary_area, _ = calculate_geodesic_area_and_perimeter(stationary)\n",
    "            evolving_area, _ = calculate_geodesic_area_and_perimeter(evolving)\n",
    "            \n",
    "            # Calculate extension area (area in evolving that's not in stationary)\n",
    "            if evolving.contains(stationary):\n",
    "                # Simple case: evolving completely contains stationary\n",
    "                extension = evolving.difference(stationary)\n",
    "            else:\n",
    "                # More complex case: find areas in evolving that aren't in stationary\n",
    "                extension = evolving.difference(stationary)\n",
    "            \n",
    "            # Get geodesic area of the extension\n",
    "            extension_area, _ = calculate_geodesic_area_and_perimeter(extension)\n",
    "            \n",
    "            # If any area calculation failed, skip this lake\n",
    "            if stationary_area is None or evolving_area is None or extension_area is None:\n",
    "                print(f\"Warning: Could not calculate geodesic area for {lake_name}, skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate relative extension (as percentage of original area)\n",
    "            relative_extension = extension_area / stationary_area if stationary_area > 0 else 0\n",
    "            \n",
    "            # Calculate metrics to determine if this is a reportable extension\n",
    "            has_reportable_extension = relative_extension >= area_threshold\n",
    "            \n",
    "            results.append({\n",
    "                'lake_name': lake_name,\n",
    "                'stationary_area': stationary_area,\n",
    "                'evolving_area': evolving_area,\n",
    "                'extension_area': extension_area,\n",
    "                'relative_extension': relative_extension,\n",
    "                'has_reportable_extension': has_reportable_extension\n",
    "            })\n",
    "\n",
    "            # Clear output\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {lake_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create DataFrame with results\n",
    "    extension_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Count lakes with extensions\n",
    "    lakes_with_extensions = extension_df['has_reportable_extension'].sum()\n",
    "    \n",
    "    return lakes_with_extensions, extension_df\n",
    "\n",
    "def generate_extension_summary(extension_df, km2=True):\n",
    "    \"\"\"\n",
    "    Generate a summary report about lake extensions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    extension_df : DataFrame\n",
    "        DataFrame with lake extension results from quantify_lake_extensions\n",
    "    km2 : bool, default=True\n",
    "        If True, displays area in square kilometers, otherwise in square meters\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Summary text with key findings\n",
    "    \"\"\"\n",
    "    total_lakes = len(extension_df)\n",
    "    lakes_with_extensions = extension_df['has_reportable_extension'].sum()\n",
    "    \n",
    "    # Calculate average extension for lakes that have extensions\n",
    "    extended_lakes = extension_df[extension_df['has_reportable_extension']]\n",
    "    avg_extension = extended_lakes['relative_extension'].mean() if len(extended_lakes) > 0 else 0\n",
    "    \n",
    "    # Calculate total area of extensions\n",
    "    total_extension_area = extended_lakes['extension_area'].sum() if len(extended_lakes) > 0 else 0\n",
    "    area_unit = \"km²\" if km2 else \"m²\"\n",
    "    area_divisor = 1_000_000 if km2 else 1  # Convert to km² if requested\n",
    "    \n",
    "    # Find lake with maximum extension\n",
    "    if len(extended_lakes) > 0:\n",
    "        max_extension_lake = extended_lakes.loc[extended_lakes['relative_extension'].idxmax()]\n",
    "        max_extension_pct = max_extension_lake['relative_extension'] * 100\n",
    "        max_lake_name = max_extension_lake['lake_name']\n",
    "        \n",
    "        # Find lake with largest absolute extension area\n",
    "        max_area_lake = extended_lakes.loc[extended_lakes['extension_area'].idxmax()]\n",
    "        max_area_value = max_area_lake['extension_area'] / area_divisor\n",
    "        max_area_lake_name = max_area_lake['lake_name']\n",
    "    else:\n",
    "        max_extension_pct = 0\n",
    "        max_lake_name = \"None\"\n",
    "        max_area_value = 0\n",
    "        max_area_lake_name = \"None\"\n",
    "    \n",
    "    # Generate summary\n",
    "    summary = f\"\"\"Lake Extension Analysis Summary:\n",
    "    ---------------------------------\n",
    "    We found {lakes_with_extensions} lakes ({lakes_with_extensions/total_lakes*100:.1f}% of {total_lakes} analyzed) \n",
    "    with previously unidentified lake extensions beyond their original stationary outlines.\n",
    "    \n",
    "    For lakes with extensions:\n",
    "    - Total extension area: {total_extension_area/area_divisor:.2f} {area_unit}\n",
    "    - Average extension: {avg_extension*100-100:.1f}% beyond original outline\n",
    "    - Largest relative extension: {max_extension_pct:.1f}% beyond original outline (Lake {max_lake_name})\n",
    "    - Largest absolute extension: {max_area_value:.2f} {area_unit} (Lake {max_area_lake_name})\n",
    "    \"\"\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example usage:\n",
    "# lakes_with_extensions, extension_results = quantify_lake_extensions(\n",
    "#     lakes_gdf, \n",
    "#     evolving_outlines_union_gdf, \n",
    "# )\n",
    "# summary = generate_extension_summary(extension_results, km2=True)\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd84727-a5b2-415a-98bb-f9702819979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def muliple_area_buffer(polygon, area_multiple, precision=100):\n",
    "    \"\"\"\n",
    "    This function takes a polygon and returns a polygon with a buffer such that the area of the buffered polygon\n",
    "    is approximately the specified multiple the area of the original polygon.\n",
    "\n",
    "    :param polygon: Shapely Polygon object\n",
    "    :param area_multiple: the multiple of the original polygon area you wish the buffered polygon to be\n",
    "    :param precision: Precision for the iterative process to find the buffer distance\n",
    "    :return: Buffered Polygon\n",
    "\n",
    "    # Example usage\n",
    "    # Define a simple square polygon\n",
    "    square = Polygon([(0, 0), (1, 0), (1, 1), (0, 1)])\n",
    "    # Apply the function to find the buffered polygon area and bounds\n",
    "    buffered_poly = muliple_area_buffer(square, 2)\n",
    "    \"\"\"\n",
    "    original_area = polygon.area\n",
    "    target_area = area_multiple * original_area\n",
    "    buffer_distance = 0\n",
    "    buffered_polygon = polygon\n",
    "\n",
    "    while True:\n",
    "        buffered_polygon = polygon.buffer(buffer_distance)\n",
    "        if buffered_polygon.area >= target_area:\n",
    "            break\n",
    "        buffer_distance += precision\n",
    "    \n",
    "    # Convert to geodataframe\n",
    "    buffered_polygon_gdf = gpd.GeoDataFrame({'geometry': [buffered_polygon]})\n",
    "\n",
    "    return buffered_polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a766aa3d-cc07-4af3-a19d-e61e7750b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR + '/lake_group_dV_plots', exist_ok=True)\n",
    "\n",
    "def plot_lake_groups_dV(lake_groups):\n",
    "    \"\"\"\n",
    "    Create multi-panel plots for groups of lakes showing spatial overview and volume changes.\n",
    "    Lakes are arranged in rows of three plots of equal size, with valid data checking.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lake_groups : list of tuples\n",
    "        Each tuple contains (group_name, lake_list) where:\n",
    "        - group_name: str, name of the lake group for file naming and identification\n",
    "        - lake_list: list of str, names of lakes to be analyzed together\n",
    "    \"\"\"\n",
    "    \n",
    "    for group_idx, (group_name, lake_list) in enumerate(lake_groups):\n",
    "        print(f\"\\nProcessing lake group: {group_name}\")\n",
    "        \n",
    "        # Lists to store valid lake data\n",
    "        valid_lakes = []\n",
    "        evolving_outlines_gdfs = []\n",
    "        evolving_geom_calcs_dfs = []\n",
    "        stationary_geom_calcs_dfs = []\n",
    "        evolving_union_geom_calcs_dfs = []\n",
    "        lake_gdfs = []\n",
    "        \n",
    "        # First pass: collect all valid lake data\n",
    "        for lake_name in lake_list:\n",
    "            print(f\"Checking data for {lake_name}...\")\n",
    "            \n",
    "            # Get lake data from stationary outlines\n",
    "            lake_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == lake_name]\n",
    "            if lake_gdf.empty:\n",
    "                print(f\"Skipping {lake_name}: not found in stationary outlines\")\n",
    "                continue\n",
    "                \n",
    "            # Try loading evolving outlines gdf\n",
    "            try:\n",
    "                evolving_outlines_gdf = gpd.read_file(os.path.join(\n",
    "                    'output/lake_outlines/evolving_outlines',\n",
    "                    f'{lake_name}.geojson'))\n",
    "            except Exception as e:\n",
    "                print(f\"  Skipping {lake_name}: no evolving outlines file\")\n",
    "                continue\n",
    "\n",
    "            # Attempt to open the geometric calculations CSV files\n",
    "            try:\n",
    "                evolving_geom_calcs_df = pd.read_csv('output/geometric_calcs/evolving_outlines_geom_calc/forward_fill/{}.csv'.format(lake_name))\n",
    "                evolving_union_geom_calcs_df = pd.read_csv('output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/{}.csv'.format(lake_name))\n",
    "                stationary_geom_calcs_df = pd.read_csv('output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes/{}.csv'.format(lake_name))\n",
    "            except FileNotFoundError:\n",
    "                print(f\"At least one of the geometric calculations CSV files for {lake_name} not found. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Convert strings to datetime\n",
    "            evolving_geom_calcs_df['mid_pt_datetime'] = pd.to_datetime(evolving_geom_calcs_df['mid_pt_datetime'])\n",
    "            evolving_union_geom_calcs_df['mid_pt_datetime'] = pd.to_datetime(evolving_union_geom_calcs_df['mid_pt_datetime'])\n",
    "            stationary_geom_calcs_df['mid_pt_datetime'] = pd.to_datetime(stationary_geom_calcs_df['mid_pt_datetime'])\n",
    "\n",
    "            # If we got here, all data is valid\n",
    "            print(f\"Valid data found for {lake_name}\")\n",
    "            valid_lakes.append(lake_name)\n",
    "            lake_gdfs.append(lake_gdf)\n",
    "            evolving_outlines_gdfs.append(evolving_outlines_gdf)\n",
    "            evolving_geom_calcs_dfs.append(evolving_geom_calcs_df)\n",
    "            stationary_geom_calcs_dfs.append(stationary_geom_calcs_df)\n",
    "            evolving_union_geom_calcs_dfs.append(evolving_union_geom_calcs_df)\n",
    "        \n",
    "        # Skip this group if no valid lakes found\n",
    "        if not valid_lakes:\n",
    "            print(f\"Skipping group {group_name}: no valid lakes found\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nCreating plots for valid lakes in group {group_name}: {valid_lakes}\")\n",
    "\n",
    "        # Calculate plot layout (including space for combined plot)\n",
    "        n_lakes = len(valid_lakes)\n",
    "        n_plots = n_lakes + 1  # Add 1 for the combined plot\n",
    "        n_rows = (n_plots + 2) // 3  # Integer division rounded up\n",
    "        \n",
    "        # Create figure\n",
    "        fig = plt.figure(figsize=(15, 5*n_rows + 3))\n",
    "        gs = fig.add_gridspec(n_rows + 1, 3, height_ratios=[1] + [1]*n_rows)\n",
    "        \n",
    "        # Main spatial overview panel\n",
    "        ax_main = fig.add_subplot(gs[0, :])\n",
    "        \n",
    "        # Get combined extent for all valid lakes\n",
    "        x_mins, x_maxs, y_mins, y_maxs = [], [], [], []\n",
    "        \n",
    "        for lake_gdf, evolving_outlines_gdf in zip(lake_gdfs, evolving_outlines_gdfs):\n",
    "            # Find evolving and stationary outlines union for plotting extent\n",
    "            lake_name = lake_gdf['name'].iloc[0]\n",
    "            evolving_stationary_union_gdf = gpd.GeoDataFrame(\n",
    "                geometry=[lake_gdf.geometry.iloc[0].union(evolving_outlines_gdf.geometry.union_all())],\n",
    "                crs=lake_gdf.crs)\n",
    "\n",
    "            # Get extent\n",
    "            x_min, y_min, x_max, y_max = evolving_stationary_union_gdf['geometry'].bounds.iloc[0]\n",
    "            buffer_dist = max(x_max - x_min, y_max - y_min) * 0.05\n",
    "            x_mins.append(x_min - buffer_dist)\n",
    "            x_maxs.append(x_max + buffer_dist)\n",
    "            y_mins.append(y_min - buffer_dist)\n",
    "            y_maxs.append(y_max + buffer_dist)\n",
    "        \n",
    "        # Set plot extent\n",
    "        x_min, x_max = min(x_mins), max(x_maxs)\n",
    "        y_min, y_max = min(y_mins), max(y_maxs)\n",
    "        \n",
    "        # Plot MOA background\n",
    "        mask_x = (moa_highres_da.x >= x_min) & (moa_highres_da.x <= x_max)\n",
    "        mask_y = (moa_highres_da.y >= y_min) & (moa_highres_da.y <= y_max)\n",
    "        moa_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "        ax_main.imshow(moa_subset[0,:,:], cmap='gray', clim=[14000, 17000],\n",
    "                      extent=[x_min, x_max, y_min, y_max])\n",
    "        \n",
    "        # Plot stationary outlines\n",
    "        stationary_color = 'darkturquoise'\n",
    "        for lake_gdf in lake_gdfs:\n",
    "            lake_gdf.boundary.plot(ax=ax_main, color=stationary_color, linewidth=2)\n",
    "\n",
    "        # Plot evolving outlines union\n",
    "        for lake_gdf in lake_gdfs:\n",
    "            lake_name = lake_gdf['name'].iloc[0]\n",
    "            evolving_union_gdf = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]\n",
    "            evolving_union_gdf.boundary.plot(ax=ax_main, color='k', linestyle='dotted', linewidth=2)\n",
    "        \n",
    "        # Plot evolving outlines with time-based coloring\n",
    "        cmap = plt.get_cmap('plasma')\n",
    "        norm = plt.Normalize(mdates.date2num(cyc_start_datetimes[1]), \n",
    "                           mdates.date2num(cyc_start_datetimes[-1]))\n",
    "        \n",
    "        for evolving_outlines_gdf in evolving_outlines_gdfs:\n",
    "            for idx, row in evolving_outlines_gdf.iterrows():\n",
    "                color = cmap(norm(mdates.date2num(pd.to_datetime(row['mid_pt_datetime']))))\n",
    "                gpd.GeoSeries(row['geometry']).boundary.plot(\n",
    "                    ax=ax_main, color=color, linewidth=1)\n",
    "\n",
    "        # Format overview axes\n",
    "        km_scale = 1e3\n",
    "        ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "        ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "        ax_main.xaxis.set_major_formatter(ticks_x)\n",
    "        ax_main.yaxis.set_major_formatter(ticks_y)\n",
    "        ax_main.set_xlabel('x [km]')\n",
    "        ax_main.set_ylabel('y [km]')\n",
    "\n",
    "        # Set up colormap\n",
    "        min_date = pd.to_datetime(cyc_start_datetimes[1])\n",
    "        max_date = pd.to_datetime(cyc_start_datetimes[-1])\n",
    "        date_range = pd.date_range(min_date, max_date, periods=len(cyc_start_datetimes[1:]))\n",
    "        years = date_range.year.unique()\n",
    "        years = pd.to_datetime(years, format='%Y')\n",
    "        n_dates = len(cyc_start_datetimes[1:])\n",
    "        cmap = plt.get_cmap('plasma', n_dates)\n",
    "        norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "        m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        m.set_array(np.linspace(mdates.date2num(min_date), mdates.date2num(max_date), n_dates))\n",
    "        \n",
    "        # Add colorbar\n",
    "        divider = make_axes_locatable(ax_main)\n",
    "        cax = divider.append_axes('bottom', size='2.5%', pad=0.5)\n",
    "        cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "\n",
    "        # Set colorbar ticks\n",
    "        cbar.ax.xaxis.set_major_formatter(year_interval_formatter(interval=4))\n",
    "        cbar.ax.xaxis.set_major_locator(mdates.YearLocator())  # Every year\n",
    "        cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator([1, 4, 7, 10]))  # Quarter year ticks\n",
    "        cbar.set_label('Year')\n",
    "\n",
    "        # # Get y axis limits for volume plots\n",
    "        y_min, y_max = get_overall_y_limits(evolving_geom_calcs_dfs, \n",
    "                                          stationary_geom_calcs_dfs,\n",
    "                                          evolving_union_geom_calcs_dfs)        \n",
    "        # Calculate limits with buffer\n",
    "        y_range = y_max - y_min\n",
    "        buffer = y_range * 0.05\n",
    "        y_limits = (y_min - buffer, y_max + buffer)\n",
    "        \n",
    "        # Create axes for all plots\n",
    "        axes = []\n",
    "        for idx in range(n_plots):\n",
    "            row = (idx // 3) + 1\n",
    "            col = idx % 3\n",
    "            ax = fig.add_subplot(gs[row, col])\n",
    "            axes.append(ax)\n",
    "        \n",
    "        # Plot individual lakes\n",
    "        for idx, (lake_name, evolving_df, stationary_df, union_df) in enumerate(zip(\n",
    "                valid_lakes, evolving_geom_calcs_dfs, stationary_geom_calcs_dfs, evolving_union_geom_calcs_dfs)):\n",
    "            ax = axes[idx]\n",
    "            ax.axhline(0, color='k', linestyle='--')\n",
    "            \n",
    "            dates = mdates.date2num(evolving_df['mid_pt_datetime'])\n",
    "            \n",
    "            # Plot stationary outline\n",
    "            stationary_cumsum = np.cumsum(np.divide(stationary_df['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "            ax.plot(dates, stationary_cumsum, color=stationary_color, label='Stationary', linewidth=2)\n",
    "            ax.scatter(dates, stationary_cumsum, color=stationary_color, s=5)\n",
    "\n",
    "            # Plot evolving outlines union\n",
    "            union_cumsum = np.cumsum(np.divide(union_df['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "            ax.plot(dates, union_cumsum, color='k', linestyle='dotted', label='Union', linewidth=2)\n",
    "            ax.scatter(dates, union_cumsum, color='k', s=5)\n",
    "\n",
    "            # Store line segments for multi-colored line in legend\n",
    "            lines = []\n",
    "            for i, dt in enumerate(dates):\n",
    "                line = ax.plot(1, 1, color=cmap(norm(mdates.date2num(cyc_start_datetimes[i]))), linewidth=2)[0]\n",
    "                lines.append(line)\n",
    "                line.remove()  # Remove the dummy lines after creating them\n",
    "\n",
    "            # Store line segments for multi-colored line in legend\n",
    "            onlake_lines = []\n",
    "            for i, dt in enumerate(dates):\n",
    "                x, y = 1, 1\n",
    "                onlake_line, = ax.plot(x, y, color=cmap(norm(mdates.date2num(cyc_start_datetimes[i]))), linewidth=2)\n",
    "                onlake_lines.append(onlake_line)\n",
    "\n",
    "            # Plot evolving outlines (multi-colored line)\n",
    "            x = dates\n",
    "            y = np.cumsum(np.divide(evolving_df['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "            points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "            segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "            lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "            lc.set_array(x)\n",
    "            lc.set_linewidth(2)\n",
    "            ax.add_collection(lc)\n",
    "            ax.scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "            # Plot bias\n",
    "            bias = np.cumsum(np.divide(evolving_df['evolving_outlines_dV_corr (m^3)'] - \n",
    "                                     stationary_df['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "            ax.plot(dates, bias, color='r', label='Bias', linewidth=2)\n",
    "            ax.scatter(dates, bias, color='r', linewidth=2, s=5)\n",
    "\n",
    "            # Add legend only to the first plot\n",
    "            if idx == 0:\n",
    "                stationary_line = plt.Line2D([], [], color=stationary_color, linestyle='solid', linewidth=2)\n",
    "                evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=2)\n",
    "                bias_line = plt.Line2D([], [], color='red', linestyle='solid', linewidth=2)\n",
    "                legend = ax.legend(\n",
    "                    [stationary_line,\n",
    "                     tuple(lines), \n",
    "                     evolving_union_line,\n",
    "                     bias_line],\n",
    "                    ['stationary outline',\n",
    "                     'evolving outlines',\n",
    "                     'updated stationary outline',\n",
    "                     'bias (evolving − stationary)'],\n",
    "                    handlelength=3,\n",
    "                    handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "                    fontsize=12,\n",
    "                    loc='upper center')\n",
    "\n",
    "            # Format axes\n",
    "            ax.xaxis.set_major_formatter(year_interval_formatter())\n",
    "            ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "            ax.xaxis.set_minor_locator(mdates.MonthLocator([1, 4, 7, 10]))\n",
    "            ax.set_xlabel('Year')\n",
    "            ax.set_title(lake_name)\n",
    "\n",
    "            # Set x and y axes limit\n",
    "            ax.set_xlim(cyc_start_datetimes[0], cyc_end_datetimes[-1])\n",
    "            ax.set_ylim(y_min, y_max)    \n",
    "\n",
    "            # Handle y-axis labels and ticks\n",
    "            if idx % 3 == 0:  # Leftmost column\n",
    "                ax.set_ylabel('cumulative dV [km$^3$]')\n",
    "            else:  # Middle and right columns\n",
    "                ax.set_yticklabels([])\n",
    "            \n",
    "            # Handle x-axis labels\n",
    "            # Calculate if this is the last plot in its column\n",
    "            current_row = (idx // 3) + 1\n",
    "            current_col = idx % 3\n",
    "            is_last_in_column = True\n",
    "            for next_idx in range(idx + 1, n_plots):\n",
    "                if next_idx % 3 == current_col:  # Same column\n",
    "                    is_last_in_column = False\n",
    "                    break\n",
    "            \n",
    "            if not is_last_in_column:\n",
    "                ax.set_xticklabels([])\n",
    "                ax.set_xlabel('')\n",
    "            else:\n",
    "                ax.set_xlabel('Year')\n",
    "            \n",
    "            ax.set_title(lake_name)\n",
    "        \n",
    "        # Format last plot (combined data)\n",
    "        last_ax = axes[-1]\n",
    "        last_col = (n_plots - 1) % 3\n",
    "\n",
    "        # Plot combined data\n",
    "        # Combine all dataframes by summing values for each timestamp\n",
    "        combined_evolving = pd.concat(evolving_geom_calcs_dfs).groupby('mid_pt_datetime').sum().reset_index()\n",
    "        combined_stationary = pd.concat(stationary_geom_calcs_dfs).groupby('mid_pt_datetime').sum().reset_index()\n",
    "        combined_union = pd.concat(evolving_union_geom_calcs_dfs).groupby('mid_pt_datetime').sum().reset_index()\n",
    "        \n",
    "        dates = mdates.date2num(combined_evolving['mid_pt_datetime'])\n",
    "        \n",
    "        # Plot stationary outline\n",
    "        stationary_cumsum = np.cumsum(np.divide(combined_stationary['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "        last_ax.plot(dates, stationary_cumsum, color=stationary_color, label='Stationary', linewidth=2)\n",
    "        last_ax.scatter(dates, stationary_cumsum, color=stationary_color, s=5)\n",
    "\n",
    "        # Plot evolving outlines union\n",
    "        union_cumsum = np.cumsum(np.divide(combined_union['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "        last_ax.plot(dates, union_cumsum, color='k', linestyle='dotted', label='Union', linewidth=2)\n",
    "        last_ax.scatter(dates, union_cumsum, color='k', s=5)\n",
    "\n",
    "        # Plot evolving outlines (multi-colored line)\n",
    "        evolving_cumsum = np.cumsum(np.divide(combined_evolving['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "        points = np.array([dates, evolving_cumsum]).T.reshape(-1, 1, 2)\n",
    "        segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "        lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "        lc.set_array(dates)\n",
    "        lc.set_linewidth(2)\n",
    "        last_ax.add_collection(lc)\n",
    "        last_ax.scatter(dates, evolving_cumsum, c=dates, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "        # Plot bias\n",
    "        bias_cumsum = np.cumsum(np.divide(\n",
    "            combined_evolving['evolving_outlines_dV_corr (m^3)'] - \n",
    "            combined_stationary['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "        last_ax.plot(dates, bias_cumsum, color='r', label='Bias', linewidth=2)\n",
    "        last_ax.scatter(dates, bias_cumsum, color='r', s=5)\n",
    "\n",
    "        # Set axes limits for combined plot\n",
    "        last_ax.set_xlim(cyc_start_datetimes[0], cyc_end_datetimes[-1])\n",
    "        last_ax.set_ylim(y_min, y_max)\n",
    "        last_ax.axhline(0, color='k', linestyle='--')\n",
    "        \n",
    "        # Set y-axis formatting for combined plot\n",
    "        if last_col == 0:  # Leftmost column\n",
    "            last_ax.set_ylabel('Cumulative dV [km$^3$]')\n",
    "        else:\n",
    "            last_ax.set_yticklabels([])\n",
    "        \n",
    "        # Always show x-axis labels for the combined plot as it's the last one\n",
    "        last_ax.set_xlabel('Year')\n",
    "        last_ax.xaxis.set_major_formatter(year_interval_formatter())\n",
    "        last_ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "        last_ax.xaxis.set_minor_locator(mdates.MonthLocator([1, 4, 7, 10]))\n",
    "        last_ax.set_title('Integrated')\n",
    "\n",
    "        # Save the figure using the group name\n",
    "        sanitized_group_name = group_name.replace(' ', '_').replace('/', '_')\n",
    "        plt.savefig(f'{OUTPUT_DIR}/lake_group_dV_plots/{sanitized_group_name}_dV_time_series.jpg', \n",
    "                   dpi=500, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Clear output\n",
    "        clear_output(wait=True)\n",
    "\n",
    "def get_overall_y_limits(evolving_geom_calcs_dfs, stationary_geom_calcs_dfs, evolving_union_geom_calcs_dfs):\n",
    "    \"\"\"\n",
    "    Calculate overall y-axis limits for all lake volume plots based on three types of geometric calculations.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    evolving_geom_calcs_dfs : list of pandas.DataFrame\n",
    "        List of dataframes containing evolving outline calculations\n",
    "    stationary_geom_calcs_dfs : list of pandas.DataFrame\n",
    "        List of dataframes containing stationary outline calculations\n",
    "    evolving_union_geom_calcs_dfs : list of pandas.DataFrame\n",
    "        List of dataframes containing evolving union calculations\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (y_min, y_max)\n",
    "        The minimum and maximum y-axis values with a 5% buffer\n",
    "    \"\"\"\n",
    "    all_y_values = []\n",
    "    \n",
    "    # Process each lake's data\n",
    "    for evolving_df, stationary_df, union_df in zip(evolving_geom_calcs_dfs, \n",
    "                                                   stationary_geom_calcs_dfs,\n",
    "                                                   evolving_union_geom_calcs_dfs):\n",
    "        # Calculate cumulative values for all time series\n",
    "        stationary_cumsum = np.cumsum(np.divide(stationary_df['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "        evolving_cumsum = np.cumsum(np.divide(evolving_df['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "        union_cumsum = np.cumsum(np.divide(union_df['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "        bias_cumsum = np.cumsum(np.divide(evolving_df['evolving_outlines_dV_corr (m^3)'] - \n",
    "                                        stationary_df['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "        \n",
    "        # Extend list with all values\n",
    "        all_y_values.extend(stationary_cumsum)\n",
    "        all_y_values.extend(evolving_cumsum)\n",
    "        all_y_values.extend(union_cumsum)\n",
    "        all_y_values.extend(bias_cumsum)\n",
    "    \n",
    "    # Also include the combined plot values if there are any lakes\n",
    "    if evolving_geom_calcs_dfs:\n",
    "        # Combine all dataframes by summing values for each timestamp\n",
    "        combined_evolving = pd.concat(evolving_geom_calcs_dfs).groupby('mid_pt_datetime').sum().reset_index()\n",
    "        combined_stationary = pd.concat(stationary_geom_calcs_dfs).groupby('mid_pt_datetime').sum().reset_index()\n",
    "        combined_union = pd.concat(evolving_union_geom_calcs_dfs).groupby('mid_pt_datetime').sum().reset_index()\n",
    "        \n",
    "        # Calculate cumulative sums for combined data\n",
    "        stationary_cumsum = np.cumsum(np.divide(combined_stationary['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "        evolving_cumsum = np.cumsum(np.divide(combined_evolving['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "        union_cumsum = np.cumsum(np.divide(combined_union['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "        bias_cumsum = np.cumsum(np.divide(combined_evolving['evolving_outlines_dV_corr (m^3)'] - \n",
    "                                        combined_stationary['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "        \n",
    "        all_y_values.extend(stationary_cumsum)\n",
    "        all_y_values.extend(evolving_cumsum)\n",
    "        all_y_values.extend(union_cumsum)\n",
    "        all_y_values.extend(bias_cumsum)\n",
    "    \n",
    "    # Calculate limits with a small buffer (5% of range)\n",
    "    y_min = min(all_y_values)\n",
    "    y_max = max(all_y_values)\n",
    "    y_range = y_max - y_min\n",
    "    buffer = y_range * 0.05\n",
    "    \n",
    "    return y_min - buffer, y_max + buffer\n",
    "\n",
    "def year_interval_formatter(interval=2, start_year=2012):\n",
    "    '''\n",
    "    Create custom formatter that labels years at specified intervals\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    interval : int, default=2\n",
    "        Interval between labeled years (e.g., 2 for every 2 years, 4 for every 4 years)\n",
    "    start_year : int, optional\n",
    "        Starting year for the interval. If None, uses modulo arithmetic.\n",
    "        If provided, labels years that are start_year + n*interval\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    function : formatter function for matplotlib\n",
    "    \n",
    "    Examples:\n",
    "    ---------\n",
    "    # Every 2 years (even years): 2012, 2014, 2016, 2018, 2020, 2022, 2024\n",
    "    formatter = year_interval_formatter(interval=2)\n",
    "    \n",
    "    # Every 4 years starting from 2012: 2012, 2016, 2020, 2024\n",
    "    formatter = year_interval_formatter(interval=4, start_year=2012)\n",
    "    \n",
    "    # Every 4 years using modulo (years divisible by 4): 2012, 2016, 2020, 2024\n",
    "    formatter = year_interval_formatter(interval=4)\n",
    "\n",
    "    \n",
    "    # Usage examples:\n",
    "    # For specific case (2012, 2016, 2020, 2024):\n",
    "    formatter = year_interval_formatter(interval=4, start_year=2012)\n",
    "    \n",
    "    # For every 4 years using modulo:\n",
    "    formatter = year_interval_formatter(interval=4)\n",
    "    \n",
    "    # For every 2 years using modulo:\n",
    "    formatter = year_interval_formatter(interval=2)\n",
    "    '''\n",
    "    def formatter_func(x, pos):\n",
    "        date = mdates.num2date(x)\n",
    "        year = date.year\n",
    "        \n",
    "        if start_year is not None:\n",
    "            # Use specific starting year and interval\n",
    "            if (year - start_year) % interval == 0 and year >= start_year:\n",
    "                return date.strftime('%Y')\n",
    "        else:\n",
    "            # Use modulo arithmetic\n",
    "            if year % interval == 0:\n",
    "                return date.strftime('%Y')\n",
    "        \n",
    "        return ''\n",
    "    \n",
    "    return formatter_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d46c5-610a-4a83-8316-bc5e755bd714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_gdf_by_folder_contents(gdf, folder_path, exclude=True, prefix=None, suffix=None, suffix_pattern=None, file_extension=None):\n",
    "    '''\n",
    "    Filter the GeoDataFrame based on processed lake names from the folder contents.\n",
    "    \n",
    "    Args:\n",
    "    gdf: GeoDataFrame to be filtered.\n",
    "    folder_path: Path to the directory containing files and/or subdirectories.\n",
    "    exclude: If True, excludes gdf rows where the 'name' is in the folder_path directories or files.\n",
    "             If False, includes only gdf rows where the 'name' is in the folder_path directories or files.\n",
    "    prefix: Optional string to remove from the beginning of filenames.\n",
    "    suffix: Optional string to remove from the end of filenames.\n",
    "    suffix_pattern: Optional regex pattern to remove from the end of filenames.\n",
    "    file_extension: Optional string specifying the file extension to filter (e.g., 'png', 'txt').\n",
    "    \n",
    "    Returns:\n",
    "    GeoDataFrame filtered based on the presence of 'name' in folder_path.\n",
    "\n",
    "    # Example usage:\n",
    "    remaining_lakes = filter_gdf_by_folder_contents(\n",
    "        stationary_outlines_gdf, \n",
    "        folder_path,\n",
    "        # prefix='plot_evolving_outlines_time_series_', \n",
    "        suffix_pattern=r'\\d+\\.\\d+m-level_\\d+x-with',\n",
    "        file_extension='txt'\n",
    "    )\n",
    "    '''\n",
    "    # Return empty GeoDataFrame if input is empty\n",
    "    if gdf is None or gdf.empty:\n",
    "        return gdf\n",
    "\n",
    "    def process_name(name):\n",
    "        '''Helper function to remove prefix and suffix from a name'''\n",
    "        processed_name = name\n",
    "        \n",
    "        # First strip the file extension if it exists\n",
    "        processed_name = os.path.splitext(processed_name)[0]\n",
    "        \n",
    "        if prefix and processed_name.startswith(prefix):\n",
    "            processed_name = processed_name[len(prefix):]\n",
    "            \n",
    "        if suffix_pattern:\n",
    "            processed_name = re.sub(suffix_pattern + '$', '', processed_name)\n",
    "        elif suffix and processed_name.endswith(suffix):\n",
    "            processed_name = processed_name[:-len(suffix)]\n",
    "            \n",
    "        return processed_name.lower().strip()\n",
    "    \n",
    "    # Get all files and filter by extension if specified\n",
    "    all_files = os.listdir(folder_path)\n",
    "    if file_extension:\n",
    "        clean_extension = file_extension.lstrip('.')\n",
    "        all_files = [f for f in all_files if f.lower().endswith(f'.{clean_extension.lower()}')]\n",
    "    \n",
    "    # Process filenames to get lake names\n",
    "    names_in_folder = {\n",
    "        process_name(name)\n",
    "        for name in all_files\n",
    "    }\n",
    "    \n",
    "    # Filter without adding and then dropping a new column\n",
    "    gdf_filtered = gdf[gdf['name'].str.lower().str.strip().apply(\n",
    "        lambda x: (x not in names_in_folder) if exclude else (x in names_in_folder)\n",
    "    )]\n",
    "    \n",
    "    return gdf_filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca12e5-6f58-491a-9f2d-36adf3662939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files_by_suffix(directory_path, suffix=\".csv\"):\n",
    "    \"\"\"\n",
    "    Counts the number of files with a given suffix in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): The path to the directory to search.\n",
    "        suffix (str): The file suffix to match (e.g., \".csv\", \".txt\"). Defaults to \".csv\".\n",
    "\n",
    "    Returns:\n",
    "        int: The number of matching files found in the directory.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"Error: Directory '{directory_path}' does not exist.\")\n",
    "        return 0\n",
    "\n",
    "    return sum(1 for filename in os.listdir(directory_path) if filename.endswith(suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1a529e-b58d-425d-833f-6b0202a9c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_csvs_with_values_in_range(directory, col=\"evolving_outlines_area (m^2)\",\n",
    "                                   date_col=\"mid_pt_datetime\",\n",
    "                                   start=\"2019-01-01 00:00:00\",\n",
    "                                   end=\"2021-07-02 09:00:00\"):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame of CSV files in a directory that contain at least one row\n",
    "    where `col` is not null and `date_col` is within the given date range.\n",
    "    The DataFrame has one column: 'name' (the filename without extension).\n",
    "    \"\"\"\n",
    "    start = pd.to_datetime(start)\n",
    "    end = pd.to_datetime(end)\n",
    "\n",
    "    matching_files = []\n",
    "\n",
    "    for fname in os.listdir(directory):\n",
    "        if fname.endswith(\".csv\"):\n",
    "            fpath = os.path.join(directory, fname)\n",
    "            try:\n",
    "                df = pd.read_csv(fpath, parse_dates=[date_col])\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {fname}, error: {e}\")\n",
    "                continue\n",
    "\n",
    "            if col not in df.columns or date_col not in df.columns:\n",
    "                continue\n",
    "\n",
    "            mask = (df[date_col].between(start, end)) & (df[col].notna())\n",
    "            if mask.any():\n",
    "                matching_files.append(os.path.splitext(fname)[0])  # remove .csv\n",
    "\n",
    "    return pd.DataFrame(matching_files, columns=[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d1083-ac7e-4bc0-be61-e069e7848354",
   "metadata": {},
   "source": [
    "# Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae841541-da2b-4fbf-b92c-6ec5c7e9221b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import subglacial lake outlines \n",
    "stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/stationary_outlines_gdf.geojson')\n",
    "reexamined_stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/reexamined_stationary_outlines_gdf.geojson')\n",
    "evolving_outlines_union_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/evolving_outlines_union_gdf.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a901efcd-04b5-468b-ac6e-f820147c67b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CryoSat-2 SARIn mode mask\n",
    "# See 0_preprocess_data.ipynb for data source and pre-processing steps\n",
    "gdf_SARIn_3_1 = gpd.read_file('output/CryoSat2_SARIn_mode_masks/gdf_SARIn_3_1.geojson')\n",
    "gdf_SARIn_3_1_3_6_diff= gpd.read_file('output/CryoSat2_SARIn_mode_masks/gdf_SARIn_3_1_3_6_diff.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cf07dd-b864-4517-bbc9-60048ea9878a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import cyc_datetimes\n",
    "cyc_datetimes = pd.read_csv('output/cycle_dates.csv', parse_dates=['cyc_start_datetimes', 'cyc_end_datetimes'])\n",
    "\n",
    "# Store the cyc_datetimes columns as a np array with datetime64[ns] data type\n",
    "cyc_start_datetimes = [np.datetime64(ts) for ts in cyc_datetimes['cyc_start_datetimes']]\n",
    "cyc_end_datetimes = [np.datetime64(ts) for ts in cyc_datetimes['cyc_end_datetimes']]\n",
    "# Do not use last row of mid_pt_datetimes since NaT\n",
    "cyc_mid_pt_datetimes = [np.datetime64(ts) for ts in cyc_datetimes['mid_pt_datetimes'][0:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe18173c-0ef2-4897-b7a7-11852526e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MODIS Mosaic of Antarctica (MOA) surface imagery\n",
    "# https://nsidc.org/data/nsidc-0730/versions/1\n",
    "moa_highres = DATA_DIR + '/moa125_2014_hp1_v01.tif' \n",
    "moa_highres_da = rioxarray.open_rasterio(moa_highres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9477ce8b-819f-4bc9-b1aa-94d444759f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIS MOA 2014 coastline and grounding line\n",
    "# https://nsidc.org/data/nsidc-0730/versions/1\n",
    "shp = DATA_DIR + '/moa2014_coastline_v01.shp' \n",
    "moa_2014_coastline = gpd.read_file(shp)\n",
    "shp = DATA_DIR + '/moa2014_grounding_line_v01.shp' \n",
    "moa_2014_groundingline = gpd.read_file(shp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68880d8-2656-4f3f-8d3d-0d80242d9bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scripps Grounding Line (Depoorter and others, 2013)\n",
    "# https://doi.pangaea.de/10.1594/PANGAEA.819147\n",
    "Scripps_gl = gpd.read_file(DATA_DIR + '/boundaries/Depoorter2013/Antarctica_masks/scripps_antarctica_polygons_v1.shp')\n",
    "\n",
    "# Isolate land ice and ice shelf\n",
    "Scripps_landice = Scripps_gl[Scripps_gl['Id_text'] == 'Grounded ice or land']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff157c2-2af5-45eb-b735-215d19ad1074",
   "metadata": {},
   "source": [
    "# Results and discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8498282-b806-459a-a654-1d21ee8340e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many previously identified lakes were analyzed?\n",
    "print(len(stationary_outlines_gdf), 'lakes reanalyzed')\n",
    "print(len(stationary_outlines_gdf[stationary_outlines_gdf['name'] != 'Crane_Glacier']), 'actually lakes reanalyzed due to lake of data at Crane Glacier')\n",
    "print(len(reexamined_stationary_outlines_gdf), \n",
    "    'lakes analyzed in revised inventory due to Site_B and Site_C being combined into Site_BC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6dc095-814b-4b70-987e-7d886b44ca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many lakes are missing CryoSat-2 SARIn coverage?\n",
    "print(stationary_outlines_gdf[stationary_outlines_gdf['CS2_SARIn_start'] == '<NA>'].shape[0])\n",
    "print(reexamined_stationary_outlines_gdf[reexamined_stationary_outlines_gdf['CS2_SARIn_start'] == '<NA>'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f5b237-d443-4dc4-ba68-2c855352751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many lakes have CryoSat-2 SARIn coverage?\n",
    "print(stationary_outlines_gdf[stationary_outlines_gdf['CS2_SARIn_start'] != '<NA>'].shape[0])\n",
    "print(reexamined_stationary_outlines_gdf[reexamined_stationary_outlines_gdf['CS2_SARIn_start'] != '<NA>'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe66e07-5d47-409a-bc73-6d4c280caa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many lakes have CryoSat-2 SARIn coverage from the start of the mission?\n",
    "print(stationary_outlines_gdf[stationary_outlines_gdf['CS2_SARIn_start'] == '2010.5'].shape[0])\n",
    "print(reexamined_stationary_outlines_gdf[reexamined_stationary_outlines_gdf['CS2_SARIn_start'] == '2010.5'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21245f4a-daf0-405f-b9fd-2b1d40fb2682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many lakes have CryoSat-2 SARIn coverage starting when LRM/SARIn boundary moved inland?\n",
    "print(stationary_outlines_gdf[stationary_outlines_gdf['CS2_SARIn_start'] == '2013.75'].shape[0])\n",
    "print(reexamined_stationary_outlines_gdf[reexamined_stationary_outlines_gdf['CS2_SARIn_start'] == '2013.75'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d0795a-f3f7-4627-a64b-869a37a8401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many lakes exhibit evolving outlines?\n",
    "print(len([f for f in os.listdir('output/lake_outlines/evolving_outlines') if f.endswith('.geojson')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819e83df-ed05-400c-a548-b063ef76b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many lakes exhibit evolving outlines?\n",
    "print(len([f for f in os.listdir('output/lake_outlines/evolving_outlines/forward_fill') if f.endswith('.geojson')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fea5e8-ab2c-4132-8c00-cea590bc7519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many lakes exhibit no evolving outlines?\n",
    "print(len([f for f in os.listdir('output/lake_outlines/evolving_outlines') if f.endswith('.txt')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd651217-9c52-4cfe-b1fa-643052315cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm count of non-evolving lakes\n",
    "print(len(stationary_outlines_gdf) - len([f for f in os.listdir('output/lake_outlines/evolving_outlines') if f.endswith('.geojson')]))\n",
    "print(len(reexamined_stationary_outlines_gdf) - len([f for f in os.listdir('output/lake_outlines/evolving_outlines') if f.endswith('.geojson')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0acd1b5-a1d0-4a05-9502-381108cb8daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate discrepancy in numbers\n",
    "\n",
    "# Gather file names (strip extensions so they match \"name\" column)\n",
    "non_evolving_lakes = [os.path.splitext(f)[0] for f in os.listdir(\"output/lake_outlines/evolving_outlines\") if f.endswith(\".txt\")]\n",
    "evolving_lakes = [os.path.splitext(f)[0] for f in os.listdir(\"output/lake_outlines/evolving_outlines\") if f.endswith(\".geojson\")]\n",
    "\n",
    "# Get names from the GeoDataFrames\n",
    "evolving_lakes_names = set(evolving_lakes)\n",
    "non_evolving_lakes_names = set(non_evolving_lakes)\n",
    "prior_stationary_names = set(stationary_outlines_gdf[\"name\"])\n",
    "reexamined_names = set(reexamined_stationary_outlines_gdf[\"name\"])\n",
    "updated_stationary_names = set(evolving_outlines_union_gdf[\"name\"])\n",
    "\n",
    "prior_stationary_evolving_lakes = prior_stationary_names - non_evolving_lakes_names\n",
    "reexamined_evolving_lakes = reexamined_names - non_evolving_lakes_names\n",
    "updated_stationary_evolving_lakes = updated_stationary_names - non_evolving_lakes_names\n",
    "\n",
    "prior_stationary_non_evolving_lakes = prior_stationary_names - evolving_lakes_names\n",
    "reexamined_non_evolving_lakes = reexamined_names - evolving_lakes_names\n",
    "updated_stationary_non_evolving_lakes = updated_stationary_names - evolving_lakes_names\n",
    "\n",
    "# Compare\n",
    "print(\"Evolving lakes\")\n",
    "print(\"In prior stationary gdf but not in reexamined gdf:\", prior_stationary_evolving_lakes - reexamined_evolving_lakes)\n",
    "print(\"In reexamined gdf but not in prior stationary gdf:\", reexamined_evolving_lakes - prior_stationary_evolving_lakes)\n",
    "print(\"In prior stationary gdf but not in updated stationary gdf:\", prior_stationary_evolving_lakes - updated_stationary_evolving_lakes)\n",
    "print(\"In updated stationary gdf but not in prior stationary gdf:\", updated_stationary_evolving_lakes - prior_stationary_evolving_lakes)\n",
    "print(\"In reexamined gdf but not in updated stationary gdf:\", reexamined_evolving_lakes - updated_stationary_evolving_lakes)\n",
    "print(\"In updated stationary gdf but not in reexamined gdf:\", updated_stationary_evolving_lakes - reexamined_evolving_lakes)\n",
    "\n",
    "print(\"\\nNon-evolving lakes\")\n",
    "print(\"In prior stationary gdf but not in reexamined gdf:\", prior_stationary_non_evolving_lakes - reexamined_non_evolving_lakes)\n",
    "print(\"In reexamined gdf but not in prior stationary gdf:\", reexamined_non_evolving_lakes - prior_stationary_non_evolving_lakes)\n",
    "print(\"In prior stationary gdf but not in updated stationary gdf:\", prior_stationary_non_evolving_lakes - updated_stationary_non_evolving_lakes)\n",
    "print(\"In updated stationary gdf but not in prior stationary gdf:\", updated_stationary_non_evolving_lakes - prior_stationary_non_evolving_lakes)\n",
    "print(\"In reexamined gdf but not in updated stationary gdf:\", reexamined_non_evolving_lakes - updated_stationary_non_evolving_lakes)\n",
    "print(\"In updated stationary gdf but not in reexamined gdf:\", updated_stationary_non_evolving_lakes - reexamined_non_evolving_lakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c8289d-68e0-4c7b-81f5-22b5a19adbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lakes with evolving outlines found\n",
    "\n",
    "# Gather file names (strip extensions)\n",
    "txt_names = {os.path.splitext(f)[0] for f in os.listdir(\"output/lake_outlines/evolving_outlines\") if f.endswith(\".txt\")}\n",
    "evolving_outlines_names = {os.path.splitext(f)[0] for f in os.listdir(\"output/lake_outlines/evolving_outlines\") if f.endswith(\".geojson\")}\n",
    "stationary_names = set(stationary_outlines_gdf[\"name\"])\n",
    "reexamined_names = set(reexamined_stationary_outlines_gdf[\"name\"])\n",
    "evolving_outlines_union_names = set(evolving_outlines_union_gdf[\"name\"])\n",
    "\n",
    "# Collect all sets in a dict for easier handling\n",
    "sources = {\n",
    "    \"stationary_at_evolving_subset\": stationary_names - txt_names,\n",
    "    \"reexamined_at_evolving_subset\": reexamined_names - txt_names,\n",
    "    \"evolving_union\": evolving_outlines_union_names,\n",
    "    \"evolving_outlines\": evolving_outlines_names,\n",
    "}\n",
    "\n",
    "# Find common names across all sets\n",
    "common_names = set.intersection(*sources.values())\n",
    "print(f\"Common to all sets ({len(common_names)}):\", sorted(common_names))\n",
    "\n",
    "# Report unique names per set\n",
    "for label, names in sources.items():\n",
    "    unique_names = names - common_names\n",
    "    print(f\"\\nUnique to {label} ({len(unique_names)}):\", sorted(unique_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b800ea-20bc-42ff-a122-19085f3ed414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lakes without evolving outlines found\n",
    "\n",
    "# Collect all sets in a dict for easier handling\n",
    "sources = {\n",
    "    \"stationary_at_evolving_subset\": stationary_names - evolving_outlines_names,\n",
    "    \"reexamined_at_evolving_subset\": reexamined_names - evolving_outlines_names,\n",
    "    \"txt\": txt_names,\n",
    "}\n",
    "\n",
    "# Find common names across all sets\n",
    "common_names = set.intersection(*sources.values())\n",
    "print(f\"Common to all sets ({len(common_names)}):\", sorted(common_names))\n",
    "\n",
    "# Report unique names per set\n",
    "for label, names in sources.items():\n",
    "    unique_names = names - common_names\n",
    "    print(f\"\\nUnique to {label} ({len(unique_names)}):\", sorted(unique_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99f729f-b2a4-42ce-9e11-412c016c60f9",
   "metadata": {},
   "source": [
    "## Evolving outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b029a-f029-4ff6-bed5-83f0b60cd521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze lake extensions\n",
    "lakes_with_extensions, extension_results = quantify_lake_extensions(\n",
    "    reexamined_stationary_outlines_gdf, \n",
    "    evolving_outlines_union_gdf, \n",
    "    area_threshold=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f22eed-a541-4d44-aa8a-a154f93b4ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many lakes have no extension beyond stationary outline?\n",
    "len(extension_results[extension_results['relative_extension'] != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a531e7-f682-4169-9422-fc68d80d24c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = generate_extension_summary(extension_results, km2=True)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10727648-a236-407c-a393-686ab3cf6707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View lakes that exceed the area threshold\n",
    "extension_results[extension_results['relative_extension'] > 0.25].sort_values('relative_extension', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc12128-2aab-44d3-a071-2460f9873c0c",
   "metadata": {},
   "source": [
    "## Active area and carbon export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af05cae5-1947-49f3-81a1-d74b8babb278",
   "metadata": {},
   "source": [
    "### Active area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9584261-bb9b-490b-a03c-2b0483b0c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in continental summation geometric calculation csv files - stationary outlines (all analyzed lakes)\n",
    "base_path = 'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes'\n",
    "filenames = {'superset_IS2_lakes_sum': 'superset_IS2_lakes_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPreExpansion_sum': 'subset_CS2_IS2_lakes_SARInPreExpansion_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPostExpansion_sum': 'subset_CS2_IS2_lakes_SARInPostExpansion_sum.csv'}\n",
    "\n",
    "dfs = {name: pd.read_csv(f'{base_path}/{filename}') \n",
    "       for name, filename in filenames.items()}\n",
    "\n",
    "superset_IS2_stationary_sum_df = dfs['superset_IS2_lakes_sum']\n",
    "subset_CS2_IS2_SARInPreExpansion_stationary_sum_df = dfs['subset_CS2_IS2_lakes_SARInPreExpansion_sum']\n",
    "subset_CS2_IS2_SARInPostExpansion_stationary_sum_df = dfs['subset_CS2_IS2_lakes_SARInPostExpansion_sum']\n",
    "\n",
    "\n",
    "# Read in continental summation geometric calculation csv files - stationary outlines (evolving lakes subset)\n",
    "base_path = 'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes'\n",
    "filenames = {\n",
    "    'superset_IS2_lakes_sum': 'superset_IS2_lakes_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPreExpansion_sum': 'subset_CS2_IS2_lakes_SARInPreExpansion_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPostExpansion_sum': 'subset_CS2_IS2_lakes_SARInPostExpansion_sum.csv',\n",
    "}\n",
    "\n",
    "dfs = {name: pd.read_csv(f'{base_path}/{filename}')\n",
    "       for name, filename in filenames.items()}\n",
    "\n",
    "superset_IS2_stationary_subset_sum_df = dfs['superset_IS2_lakes_sum']\n",
    "subset_CS2_IS2_SARInPreExpansion_stationary_subset_sum_df = dfs['subset_CS2_IS2_lakes_SARInPreExpansion_sum']\n",
    "subset_CS2_IS2_SARInPostExpansion_stationary_subset_sum_df = dfs['subset_CS2_IS2_lakes_SARInPostExpansion_sum']\n",
    "\n",
    "\n",
    "# Read in continental summation geometric calculation csv files - evolving outlines (only lakes with evolving outlines)\n",
    "base_path = 'output/geometric_calcs/evolving_outlines_geom_calc/forward_fill'\n",
    "filenames = {\n",
    "    'superset_IS2_lakes_sum': 'superset_IS2_lakes_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPreExpansion_sum': 'subset_CS2_IS2_lakes_SARInPreExpansion_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPostExpansion_sum': 'subset_CS2_IS2_lakes_SARInPostExpansion_sum.csv',\n",
    "}\n",
    "\n",
    "dfs = {name: pd.read_csv(f'{base_path}/{filename}') \n",
    "       for name, filename in filenames.items()}\n",
    "\n",
    "# Store dataframes from dfs list for code readability\n",
    "superset_IS2_evolving_sum_df = dfs['superset_IS2_lakes_sum']\n",
    "subset_CS2_IS2_SARInPreExpansion_evolving_sum_df = dfs['subset_CS2_IS2_lakes_SARInPreExpansion_sum']\n",
    "subset_CS2_IS2_SARInPostExpansion_evolving_sum_df = dfs['subset_CS2_IS2_lakes_SARInPostExpansion_sum']\n",
    "\n",
    "\n",
    "# Read in continental summation geometric calculation csv files - evolving union (only lakes with evolving outlines)\n",
    "base_path = 'output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes'\n",
    "filenames = {'superset_IS2_lakes_sum': 'superset_IS2_lakes_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPreExpansion_sum': 'subset_CS2_IS2_lakes_SARInPreExpansion_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPostExpansion_sum': 'subset_CS2_IS2_lakes_SARInPostExpansion_sum.csv'}\n",
    "\n",
    "dfs = {name: pd.read_csv(f'{base_path}/{filename}')\n",
    "       for name, filename in filenames.items()}\n",
    "\n",
    "superset_IS2_evolving_union_sum_df = dfs['superset_IS2_lakes_sum']\n",
    "subset_CS2_IS2_SARInPreExpansion_evolving_union_sum_df = dfs['subset_CS2_IS2_lakes_SARInPreExpansion_sum']\n",
    "subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df = dfs['subset_CS2_IS2_lakes_SARInPostExpansion_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8819966-8726-4774-bee2-10bb91623893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolving outlines vs. prior stationary outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9282373b-603d-457c-bedf-ff3a765394f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find minimum/maximum lake bed active area derived from evolving outlines during ICESat-2 observation period\n",
    "print(np.round(subset_CS2_IS2_SARInPreExpansion_evolving_sum_df['evolving_outlines_area (m^2)'].min() / 1e6, 0))\n",
    "print(np.round(subset_CS2_IS2_SARInPreExpansion_evolving_sum_df['evolving_outlines_area (m^2)'].max() / 1e6, 0))\n",
    "\n",
    "# Find lake bed active area derived from prior stationary outlines during ICESat-2 observation period\n",
    "print(np.round(subset_CS2_IS2_SARInPreExpansion_stationary_subset_sum_df['stationary_outline_area (m^2)'].iloc[0] / 1e6, 0))\n",
    "\n",
    "# Find minimum/maximum discrepancy (as a percentage) between lake bed active area derived from evolving outlines \n",
    "# compared to prior stationary outlines during ICESat-2 observation period\n",
    "print(np.round(subset_CS2_IS2_SARInPreExpansion_evolving_sum_df['evolving_outlines_area (m^2)'].min() / \n",
    "               subset_CS2_IS2_SARInPreExpansion_stationary_subset_sum_df['stationary_outline_area (m^2)'].iloc[0] * 100, 0), '%')\n",
    "print(np.round(subset_CS2_IS2_SARInPreExpansion_evolving_sum_df['evolving_outlines_area (m^2)'].max() / \n",
    "               subset_CS2_IS2_SARInPreExpansion_stationary_subset_sum_df['stationary_outline_area (m^2)'].iloc[0] * 100, 0), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ebb75-bf91-4f7e-a37b-3c20611e3353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find minimum/maximum lake bed active area derived from evolving outlines during ICESat-2 observation period\n",
    "print(np.round(subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['evolving_outlines_area (m^2)'].min() / 1e6, 0))\n",
    "print(np.round(subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['evolving_outlines_area (m^2)'].max() / 1e6, 0))\n",
    "\n",
    "# Find lake bed active area derived from prior stationary outlines during ICESat-2 observation period\n",
    "print(np.round(subset_CS2_IS2_SARInPostExpansion_stationary_subset_sum_df['stationary_outline_area (m^2)'].iloc[-1] / 1e6, 0))\n",
    "\n",
    "# Find minimum discrepancy (as a percentage) between lake bed active area derived from evolving outlines \n",
    "# compared to prior stationary outlines during ICESat-2 observation period\n",
    "print(np.round(subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['evolving_outlines_area (m^2)'].min() / \n",
    "               subset_CS2_IS2_SARInPostExpansion_stationary_subset_sum_df['stationary_outline_area (m^2)'].iloc[-1] * 100, 0), '%')\n",
    "print(np.round(subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['evolving_outlines_area (m^2)'].max() / \n",
    "               subset_CS2_IS2_SARInPostExpansion_stationary_subset_sum_df['stationary_outline_area (m^2)'].iloc[-1] * 100, 0), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f3800-4e63-4ce5-848f-0d001e210364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find minimum/maximum lake bed active area derived from evolving outlines during ICESat-2 observation period\n",
    "print(np.round(superset_IS2_evolving_sum_df['evolving_outlines_area (m^2)'].min() / 1e6, 0))\n",
    "print(np.round(superset_IS2_evolving_sum_df['evolving_outlines_area (m^2)'].max() / 1e6, 0))\n",
    "\n",
    "# Find lake bed active area derived from prior stationary outlines during ICESat-2 observation period\n",
    "print(np.round(superset_IS2_stationary_subset_sum_df['stationary_outline_area (m^2)'].iloc[0] / 1e6, 0))\n",
    "\n",
    "# Find minimum/maximum discrepancy (as a percentage) between lake bed active area derived from evolving outlines \n",
    "# compared to prior stationary outlines during ICESat-2 observation period\n",
    "print(np.round(superset_IS2_evolving_sum_df['evolving_outlines_area (m^2)'].min() / \n",
    "               superset_IS2_stationary_subset_sum_df['stationary_outline_area (m^2)'].iloc[0] * 100, 0), '%')\n",
    "print(np.round(superset_IS2_evolving_sum_df['evolving_outlines_area (m^2)'].max() / \n",
    "               superset_IS2_stationary_subset_sum_df['stationary_outline_area (m^2)'].iloc[0] * 100, 0), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c273a0b2-0bca-41af-8dda-02f6f774b675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated stationary outlines vs. prior stationary outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282bad88-28f8-4f78-976e-1a96ab0b3583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find lake bed active area derived from updated stationary outlines during ICESat-2 observation period\n",
    "print(np.round(subset_CS2_IS2_SARInPreExpansion_evolving_union_sum_df['stationary_outline_area (m^2)'].iloc[0] / 1e6, 0))\n",
    "\n",
    "# Find lake bed active area derived from prior stationary outlines during ICESat-2 observation period\n",
    "print(np.round(subset_CS2_IS2_SARInPreExpansion_stationary_subset_sum_df['stationary_outline_area (m^2)'].iloc[0] / 1e6, 0))\n",
    "\n",
    "# Find discrepancy (as a percentage) between lake bed active area derived from updated stationary outlines \n",
    "# compared to prior stationary outlines during ICESat-2 observation period\n",
    "print(np.round(subset_CS2_IS2_SARInPreExpansion_evolving_union_sum_df['stationary_outline_area (m^2)'].iloc[0] / \n",
    "               subset_CS2_IS2_SARInPreExpansion_stationary_subset_sum_df['stationary_outline_area (m^2)'].iloc[0] * 100, 0), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b75fc3-0872-44be-bebd-3538c796723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find lake bed active area derived from updated stationary outlines during ICESat-2 observation period\n",
    "print(np.round(subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['stationary_outline_area (m^2)'].iloc[0] / 1e6, 0))\n",
    "\n",
    "# Find lake bed active area derived from prior stationary outlines during ICESat-2 observation period\n",
    "print(np.round(subset_CS2_IS2_SARInPostExpansion_stationary_subset_sum_df['stationary_outline_area (m^2)'].iloc[-1] / 1e6, 0))\n",
    "\n",
    "# Find discrepancy (as a percentage) between lake bed active area derived from updated stationary outlines\n",
    "# compared to prior stationary outlines during ICESat-2 observation period\n",
    "print(np.round(subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['stationary_outline_area (m^2)'].iloc[0] / \n",
    "               subset_CS2_IS2_SARInPostExpansion_stationary_subset_sum_df['stationary_outline_area (m^2)'].iloc[-1] * 100, 0), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17df816-4538-4ab7-bede-970800957b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find lake bed active area derived from prior stationary outlines during ICESat-2 observation period\n",
    "print(np.round(superset_IS2_evolving_union_sum_df['stationary_outline_area (m^2)'].iloc[0] / 1e6, 0))\n",
    "\n",
    "# Find lake bed active area derived from prior stationary outlines during ICESat-2 observation period\n",
    "print(np.round(superset_IS2_stationary_subset_sum_df['stationary_outline_area (m^2)'].iloc[0] / 1e6, 0))\n",
    "\n",
    "# Find discrepancy (as a percentage) between lake bed active area derived from evolving outlines \n",
    "# compared to prior stationary outlines during ICESat-2 observation period\n",
    "print(np.round(superset_IS2_evolving_union_sum_df['stationary_outline_area (m^2)'].iloc[0] / \n",
    "               superset_IS2_stationary_subset_sum_df['stationary_outline_area (m^2)'].iloc[0] * 100, 0), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202d821b-5f5b-468c-bf5e-60919788eefc",
   "metadata": {},
   "source": [
    "### Dissolved inorganic carbon (DIC) export estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d24baf6-6b93-4aeb-aa80-3f2db19b02ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the microbial respiration rate  of 1.4 x 10^4 g C d-1 measured directly in Mercer Subglacial Lake (SLM; Venturelli and others, 2023) \n",
    "# as an estimate of dissolved inorganic carbon (DIC) flux from saturated sediment to the subglacial water column via respiration\n",
    "# doi.org/10.1029/2022AV000846\n",
    "resp_rate = 1.4 * 10**4  # g C d-1 in SLM\n",
    "print('respiration rate:', resp_rate, 'g C d-1')\n",
    "\n",
    "# Estimate DIC export from SLM per CryoSat-2/ICESat-2 satellite repeat cycle (91 days) time step\n",
    "SLM_DIC_export_per_step = resp_rate * 91\n",
    "print('DIC export per time step at SLM:', SLM_DIC_export_per_step, 'g C / 91 d')  # g C / 91 d in SLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05392b4e-a184-4af4-ace8-3439b5d9afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the stationary areas\n",
    "SLM_stationary_area = reexamined_stationary_outlines_gdf[reexamined_stationary_outlines_gdf['name'] == 'MercerSubglacialLake']['area (m^2)'].values[0]\n",
    "SLM_evolving_union_area = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == 'MercerSubglacialLake']['area (m^2)'].values[0]\n",
    "print('SLM stationary outline area:', np.round(SLM_stationary_area/1e6,1), 'km^2')\n",
    "print('SLM updated stationary area:', np.round(SLM_evolving_union_area/1e6,1), 'km^2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3faee2-fd1c-402c-8c98-3e4c6a11198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the per area DIC export values using stationary outline\n",
    "SLM_stationary_per_area_DIC_export = resp_rate / SLM_stationary_area\n",
    "SLM_stationary_per_area_per_step_DIC_export = resp_rate / SLM_stationary_area * 91\n",
    "\n",
    "print('SLM stationary outline per area DIC export:', np.round(SLM_stationary_per_area_DIC_export,6), 'g C d-1 m-2')\n",
    "print('SLM stationary outline per area per time step DIC export:', np.round(SLM_stationary_per_area_per_step_DIC_export,6), 'g C step-1 m-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1408e64-3648-4829-8ced-b173dfb2a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the per area per time step DIC export values using evolving union outline\n",
    "SLM_evolving_union_per_area_DIC_export = resp_rate / SLM_evolving_union_area\n",
    "SLM_evolving_union_per_area_per_step_DIC_export = resp_rate / SLM_evolving_union_area * 91\n",
    "\n",
    "print('SLM updated stationary outline per area DIC export:', np.round(SLM_evolving_union_per_area_DIC_export,6), 'g C d-1 m-2')\n",
    "print('SLM updated stationary outline per area per time step DIC export:', np.round(SLM_evolving_union_per_area_per_step_DIC_export,6), 'g C step-1 m-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f244d-ccaf-4eb3-af65-ca48f03b02bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we estimate the per area respiration using evolving outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64781e19-e9b5-423a-aced-103fd2690729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dV time series to view filling/draining history\n",
    "\n",
    "# Load SLM geometries dataframe\n",
    "SLM_evolving_outlines_geom = pd.read_csv('/home/jovyan/1_evolving_lakes/Sauthoff-2025-GRL/output/geometric_calcs/evolving_outlines_geom_calc/forward_fill/MercerSubglacialLake.csv')\n",
    "SLM_stationary_outline_geom = pd.read_csv('/home/jovyan/1_evolving_lakes/Sauthoff-2025-GRL/output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes/MercerSubglacialLake.csv')\n",
    "SLM_updated_stationary_outline_geom = pd.read_csv('/home/jovyan/1_evolving_lakes/Sauthoff-2025-GRL/output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/MercerSubglacialLake.csv')\n",
    "\n",
    "# Convert to datetimes\n",
    "SLM_evolving_outlines_geom['mid_pt_datetime'] = pd.to_datetime(SLM_evolving_outlines_geom['mid_pt_datetime'])\n",
    "\n",
    "# Define x and y for plotting\n",
    "x = SLM_evolving_outlines_geom['mid_pt_datetime']\n",
    "y = (SLM_evolving_outlines_geom['evolving_outlines_dV_corr (m^3)'] / 1e9)  # km^3\n",
    "y2 = np.cumsum(SLM_evolving_outlines_geom['evolving_outlines_dV_corr (m^3)'] / 1e9)  # km^3\n",
    "y3 = (SLM_stationary_outline_geom['stationary_outline_dV_corr (m^3)'] / 1e9)  # km^3\n",
    "y4 = np.cumsum(SLM_stationary_outline_geom['stationary_outline_dV_corr (m^3)'] / 1e9)  # km^3\n",
    "y5 = (SLM_updated_stationary_outline_geom['stationary_outline_dV_corr (m^3)'] / 1e9)  # km^3\n",
    "y6 = np.cumsum(SLM_updated_stationary_outline_geom['stationary_outline_dV_corr (m^3)'] / 1e9)  # km^3\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(15, 6))  # Set figure size\n",
    "\n",
    "# Get current axis\n",
    "ax = plt.gca()\n",
    "\n",
    "# Define masks for shading\n",
    "positive_mask = y > 0\n",
    "negative_mask = y < 0\n",
    "\n",
    "# +1 = positive, -1 = negative, 0 = zero\n",
    "sign_series = positive_mask.astype(int) - negative_mask.astype(int)\n",
    "\n",
    "# Find change points\n",
    "changes = np.diff(sign_series, prepend=sign_series.iloc[0])\n",
    "change_points = np.where(changes != 0)[0]\n",
    "change_points = np.concatenate(([0], change_points, [len(x)]))\n",
    "\n",
    "# Compute half time step for proper discrete alignment\n",
    "if len(x) > 1:\n",
    "    dt = (x.iloc[1] - x.iloc[0]) / 2\n",
    "else:\n",
    "    dt = pd.Timedelta(days=0)  # fallback\n",
    "\n",
    "# Shaded regions\n",
    "for i in range(len(change_points) - 1):\n",
    "    start_idx = change_points[i]\n",
    "    end_idx = change_points[i + 1]\n",
    "\n",
    "    # Determine color based on start of interval\n",
    "    val = sign_series.iloc[start_idx]\n",
    "    if val > 0:\n",
    "        color = \"blue\"\n",
    "    elif val < 0:\n",
    "        color = \"red\"\n",
    "    else:\n",
    "        color = \"gray\"\n",
    "\n",
    "    # Use start_idx-1 to fully cover previous regime for perfect alignment\n",
    "    plt.axvspan(x.iloc[start_idx-1], x.iloc[end_idx-1],\n",
    "        alpha=0.2, color=color, zorder=0)\n",
    "    \n",
    "# Plot both lines (with fixed colors)\n",
    "plt.plot(x, y, 'k-', linewidth=2, label='evolving outlines $dV$ ($km^3$)')\n",
    "plt.plot(x, y2, 'k--', linewidth=2, label='evolving outlines cumulative $dV$ ($km^3$)')\n",
    "plt.plot(x, y3, 'b-', linewidth=2, label='stationary outline $dV$ ($km^3$)')\n",
    "plt.plot(x, y4, 'b--', linewidth=2, label='stationary outline cumulative $dV$ ($km^3$)')\n",
    "plt.plot(x, y5, 'g-', linewidth=2, label='updated stationary outline $dV$ ($km^3$)')\n",
    "plt.plot(x, y6, 'g--', linewidth=2, label='updated stationary outline cumulative $dV$ ($km^3$)')\n",
    "\n",
    "# Format the x-axis to show years as major ticks and quarters as minor ticks\n",
    "# Set major ticks to years\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Format as year only\n",
    "\n",
    "# Set minor ticks to quarters\n",
    "ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1, 4, 7, 10]))  # Jan, Apr, Jul, Oct\n",
    "\n",
    "# Add gridlines for better readability\n",
    "ax.grid(True, which='major', linestyle='-', alpha=0.7)\n",
    "ax.grid(True, which='minor', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Add reference line at y=0\n",
    "plt.axhline(y=0, color='k', linestyle='-', lw=0.5)\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Year', fontsize=12)\n",
    "plt.ylabel('$dV$ ($km^3$)', fontsize=12)\n",
    "ax.set_title('Mercer Subglacial Lake $dV$ time series')\n",
    "\n",
    "ax.set_xlim(x.iloc[0], x.iloc[-1])\n",
    "\n",
    "# Add proxy patches for shaded regions\n",
    "pos_patch = mpatches.Patch(color='blue', alpha=0.2, label='filling')\n",
    "neg_patch = mpatches.Patch(color='red', alpha=0.2, label='draining')\n",
    "zero_patch = mpatches.Patch(color='gray', alpha=0.2, label='zero cumulative $dV$')\n",
    "\n",
    "# Add legend with both line plots and shaded patches\n",
    "plt.legend(handles=[pos_patch, neg_patch, zero_patch] + ax.get_legend_handles_labels()[0],\n",
    "           loc='best', bbox_to_anchor=(1, 0.5, 0.25, 0.25))  # (x, y, width, height)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a22009-3a7c-4229-9205-d1ddce767029",
   "metadata": {},
   "source": [
    "We observed a 3.75 year filling period prior to the 2018-2019 subglacial lake access campaign (2014.5 - 2018.25) cf. 4.5 years reported by Venturelli and others (2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8eebc4-411e-4ff9-a236-6fcafde23e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for date range 2014.75 to 2018.25\n",
    "# Filter for SLM fill period prior to Venturelli et al., 2023 sampling (2014.5 to 2018.25)\n",
    "start_date = pd.to_datetime('2014-07-01')  # ~2014.5\n",
    "end_date = pd.to_datetime('2018-04-01')    # ~2018.25\n",
    "SLM_evolving_outlines_geom_filling_period = SLM_evolving_outlines_geom[(SLM_evolving_outlines_geom['mid_pt_datetime'] >= start_date) & (SLM_evolving_outlines_geom['mid_pt_datetime'] <= end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1224a38-d07b-4897-a4a0-a785031941a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Conservation-based integration\n",
    "\n",
    "# Respiration rate [g C d-1]\n",
    "resp_rate = resp_rate  \n",
    "\n",
    "# Duration of each step [days/time step]\n",
    "time_step_days = 91  \n",
    "\n",
    "# Total filling period length [days = (years in filling period) * (days/year)]\n",
    "T_days = 3.75 * 365.25  \n",
    "\n",
    "# Total carbon respired over 3.75-year filling period [g C]\n",
    "C_total = resp_rate * T_days \n",
    "\n",
    "# Area at each time step [m^2]\n",
    "areas = SLM_evolving_outlines_geom_filling_period['evolving_outlines_area (m^2)'].values\n",
    "\n",
    "# Total area–time exposure [m^2 d]\n",
    "A_time_total = np.sum(areas * time_step_days)\n",
    "\n",
    "# Conservation-based per-area per-day rate [g C d-1 m-2]\n",
    "rate_method1 = C_total / A_time_total\n",
    "\n",
    "print('Method 1:', np.round(rate_method1, 6), 'g C d-1 m-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c4991b-c254-4f4e-a188-99b6d697e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2 - Mean of per-step rates\n",
    "rate_method2 = np.average(resp_rate / SLM_evolving_outlines_geom_filling_period['evolving_outlines_area (m^2)'])\n",
    "print('Method 2:', np.round(rate_method2,6), 'g C d-1 m-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd0fb8e-f993-4174-952e-ea52477d9c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3 - Mean area approach\n",
    "rate_method3 = resp_rate / np.average(SLM_evolving_outlines_geom_filling_period['evolving_outlines_area (m^2)'])\n",
    "print('Method 3:', np.round(rate_method3,6), 'g C d-1 m-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f2ab5f-77d8-494a-92ce-e73dc5bbfede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert per-area respiration rate to per-area, per-time-step respiration rate to be compatible \n",
    "# [g C m-2 time step-1 = (g C d-1 m-2) * (days/time step)]\n",
    "SLM_evolving_per_area_per_step_DIC_export_filling_period = rate_method1 * time_step_days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d56ec-96cc-4136-aa0f-f09bcd3003bf",
   "metadata": {},
   "source": [
    "## Volume change (dV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5335d7-ca33-4c11-8cf1-a58a54a7d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "base_dir = \"/home/jovyan/1_evolving_lakes/Sauthoff-2025-GRL/output/geometric_calcs\"\n",
    "evolving_dir = f\"{base_dir}/evolving_outlines_geom_calc/forward_fill\"\n",
    "stationary_dir = f\"{base_dir}/stationary_outline_geom_calc/stationary_outlines_at_all_lakes\"\n",
    "updated_dir = f\"{base_dir}/stationary_outline_geom_calc/evolving_union_at_evolving_lakes\"\n",
    "\n",
    "# Get list of all available lakes\n",
    "evolving_outlines_names = sorted({\n",
    "    os.path.splitext(f)[0]\n",
    "    for f in os.listdir(\"output/lake_outlines/evolving_outlines\")\n",
    "    if f.endswith(\".geojson\")\n",
    "})\n",
    "\n",
    "print(f\"Found {len(evolving_outlines_names)} lakes to plot.\")\n",
    "\n",
    "for lake_name in evolving_outlines_names:\n",
    "    try:\n",
    "        # Build file paths\n",
    "        evolving_path = os.path.join(evolving_dir, f\"{lake_name}.csv\")\n",
    "        stationary_path = os.path.join(stationary_dir, f\"{lake_name}.csv\")\n",
    "        updated_path = os.path.join(updated_dir, f\"{lake_name}.csv\")\n",
    "\n",
    "        # Skip if any file is missing\n",
    "        if not all(os.path.exists(p) for p in [evolving_path, stationary_path, updated_path]):\n",
    "            print(f\"Skipping {lake_name}: one or more CSVs missing.\")\n",
    "            continue\n",
    "\n",
    "        # -----------------------------\n",
    "        # Load data\n",
    "        # -----------------------------\n",
    "        evolving_outlines_geom = pd.read_csv('/home/jovyan/1_evolving_lakes/Sauthoff-2025-GRL/output/geometric_calcs/evolving_outlines_geom_calc/forward_fill/MercerSubglacialLake.csv')\n",
    "        stationary_outline_geom = pd.read_csv('/home/jovyan/1_evolving_lakes/Sauthoff-2025-GRL/output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes/MercerSubglacialLake.csv')\n",
    "        updated_stationary_outline_geom = pd.read_csv('/home/jovyan/1_evolving_lakes/Sauthoff-2025-GRL/output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/MercerSubglacialLake.csv')\n",
    "        \n",
    "        # Convert datetime\n",
    "        evolving_outlines_geom['mid_pt_datetime'] = pd.to_datetime(evolving_outlines_geom['mid_pt_datetime'])\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Define plotting series\n",
    "        # -----------------------------\n",
    "        x = evolving_outlines_geom['mid_pt_datetime']\n",
    "        y = evolving_outlines_geom['evolving_outlines_dV_corr (m^3)'] / 1e9\n",
    "        y2 = np.cumsum(y)\n",
    "        y3 = stationary_outline_geom['stationary_outline_dV_corr (m^3)'] / 1e9\n",
    "        y4 = np.cumsum(y3)\n",
    "        y5 = updated_stationary_outline_geom['stationary_outline_dV_corr (m^3)'] / 1e9\n",
    "        y6 = np.cumsum(y5)\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Define masks for shading\n",
    "        # -----------------------------\n",
    "        positive_mask = y > 0\n",
    "        negative_mask = y < 0\n",
    "        zero_mask = y == 0\n",
    "        \n",
    "        sign_series = positive_mask.astype(int) - negative_mask.astype(int)  # +1, -1, 0\n",
    "        \n",
    "        # Half-step for quarter-year intervals = 0.125 years\n",
    "        half_step_days = 0.125 * 365.25  # days\n",
    "        half_step = pd.Timedelta(days=half_step_days)\n",
    "        \n",
    "        # Define colors and linestyles that will be reused and create lines for legend\n",
    "        stationary_outline_color = 'turquoise'\n",
    "        evolving_outlines_color = 'blue' #TODO change to multi-colored line\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Plot figure\n",
    "        # -----------------------------\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        ax = plt.gca()\n",
    "        \n",
    "        # # Plot horizontal zero line for reference\n",
    "        # ax.axhline(0, color='k', linewidth=0.5)\n",
    "        \n",
    "        # Shade each time step centered on its timestamp\n",
    "        for i in range(len(x)):\n",
    "            val = sign_series.iloc[i]\n",
    "            if val > 0:\n",
    "                color = \"blue\"\n",
    "            elif val < 0:\n",
    "                color = \"red\"\n",
    "            else:\n",
    "                color = \"gray\"\n",
    "        \n",
    "            # Centered rectangle ± half time step\n",
    "            plt.axvspan(x.iloc[i] - half_step, x.iloc[i] + half_step,\n",
    "                        alpha=0.2, facecolor=color, edgecolor=None, zorder=0)\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Plot lines\n",
    "        # -----------------------------\n",
    "        plt.plot(x, y, color=evolving_outlines_color, linewidth=2, label='evolving outlines $dV$ (km$^3$)')\n",
    "        plt.plot(x, y2, color=evolving_outlines_color, linestyle='--', linewidth=2, label='evolving outlines cumulative $dV$ (km$^3$)')\n",
    "        plt.plot(x, y3, color=stationary_outline_color, linewidth=2, label='prior stationary outline $dV$ (km$^3$)')\n",
    "        plt.plot(x, y4, color=stationary_outline_color, linestyle='--', linewidth=2, label='prior stationary outline cumulative $dV$ (km$^3$)')\n",
    "        plt.plot(x, y5, 'k-', linewidth=2, label='updated stationary outline $dV$ (km$^3$)')\n",
    "        plt.plot(x, y6, 'k--', linewidth=2, label='updated stationary outline cumulative $dV$ (km$^3$)')\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Legend for shaded regions\n",
    "        # -----------------------------\n",
    "        pos_patch = mpatches.Patch(facecolor='blue', edgecolor=None, alpha=0.2, label='positive $dV$')\n",
    "        neg_patch = mpatches.Patch(facecolor='red', edgecolor=None, alpha=0.2, label='negative $dV$')\n",
    "        zero_patch = mpatches.Patch(facecolor='gray', edgecolor=None, alpha=0.1, label='zero $dV$')\n",
    "        \n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        plt.legend(handles=[pos_patch, neg_patch, zero_patch] + ax.get_legend_handles_labels()[0],\n",
    "                   loc='best', bbox_to_anchor=(1, 0.5, 0.25, 0.25))\n",
    "        \n",
    "        ax.set(xlim=(cyc_datetimes['cyc_start_datetimes'].iloc[0], cyc_datetimes['cyc_end_datetimes'].iloc[-1]))\n",
    "            # Set righthand x-axis limit slightly earlier to prevent tick mark displaying when there is no data point\n",
    "            # (cyc_datetimes['cyc_end_datetimes'].iloc[-1] - datetime.timedelta(days=2))))\n",
    "\n",
    "        # Axis formatting\n",
    "        ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "        ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1, 4, 7, 10]))\n",
    "        plt.axhline(y=0, color='k', linestyle='-', lw=0.5)\n",
    "        \n",
    "        # Labels and title\n",
    "        plt.xlabel('year')\n",
    "        plt.ylabel('$dV$ (km$^3$)')\n",
    "        plt.title(f'{lake_name} $dV$ time series')\n",
    "\n",
    "        # Save each figure\n",
    "        output_plot_dir = OUTPUT_DIR + \"/lake_individual_dV_plots\"\n",
    "        os.makedirs(output_plot_dir, exist_ok=True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_plot_dir, f\"{lake_name}_dV_time_series.png\"), dpi=200)\n",
    "        plt.close()\n",
    "        print(f\"Saved {lake_name}_dV_time_series.png\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {lake_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469cdb3b-0b96-4848-af0b-6477d6c42706",
   "metadata": {},
   "source": [
    "### Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bc1fce-cf56-455c-914c-8bb2bb1f1edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CS2_last_cyc_date = str(cyc_datetimes[cyc_datetimes['dataset'] == 'CryoSat2_SARIn']['cyc_end_datetimes'].iloc[-1])\n",
    "\n",
    "# Evolving outlines\n",
    "cum_CS2_evolving = np.cumsum(\n",
    "    subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['evolving_outlines_dV_corr (m^3)'].values\n",
    ") / 1e9\n",
    "\n",
    "cum_sum_last_CS2_evolving = cum_CS2_evolving[\n",
    "    subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date\n",
    "][-1]\n",
    "\n",
    "cum_IS2_evolving = np.cumsum(\n",
    "    superset_IS2_evolving_sum_df['evolving_outlines_dV_corr (m^3)'].values\n",
    ") / 1e9 + cum_sum_last_CS2_evolving\n",
    "\n",
    "cum_total_evolving = np.concatenate([cum_CS2_evolving, cum_IS2_evolving])\n",
    "\n",
    "max_abs_cum_dV_evolving = np.max(np.abs(cum_total_evolving))\n",
    "\n",
    "# Stationary outline at all lakes\n",
    "cum_CS2_stationary = np.cumsum(\n",
    "    subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['stationary_outline_dV_corr (m^3)'].values\n",
    ") / 1e9\n",
    "\n",
    "cum_sum_last_CS2_stationary = cum_CS2_stationary[\n",
    "    subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date\n",
    "][-1]\n",
    "\n",
    "cum_IS2_stationary = np.cumsum(\n",
    "    superset_IS2_stationary_sum_df['stationary_outline_dV_corr (m^3)'].values\n",
    ") / 1e9 + cum_sum_last_CS2_stationary\n",
    "\n",
    "cum_total_stationary = np.concatenate([cum_CS2_stationary, cum_IS2_stationary])\n",
    "\n",
    "max_abs_cum_dV_prior_stationary = np.max(np.abs(cum_total_stationary))\n",
    "\n",
    "\n",
    "# Stationary outline at evolving lakes subset\n",
    "cum_CS2_stationary_subset = np.cumsum(\n",
    "    subset_CS2_IS2_SARInPostExpansion_stationary_subset_sum_df['stationary_outline_dV_corr (m^3)'].values\n",
    ") / 1e9\n",
    "\n",
    "cum_sum_last_CS2_stationary_subset = cum_CS2_stationary[\n",
    "    subset_CS2_IS2_SARInPostExpansion_stationary_subset_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date\n",
    "][-1]\n",
    "\n",
    "cum_IS2_stationary_subset = np.cumsum(\n",
    "    superset_IS2_stationary_subset_sum_df['stationary_outline_dV_corr (m^3)'].values\n",
    ") / 1e9 + cum_sum_last_CS2_stationary\n",
    "\n",
    "cum_total_stationary_subset = np.concatenate([cum_CS2_stationary_subset, cum_IS2_stationary_subset])\n",
    "\n",
    "max_abs_cum_dV_prior_stationary_evolving_subset = np.max(np.abs(cum_total_stationary_subset))\n",
    "\n",
    "\n",
    "# Updated stationary (union of evolving outlines)\n",
    "cum_CS2_updated_stationary = np.cumsum(\n",
    "    subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['stationary_outline_dV_corr (m^3)'].values\n",
    ") / 1e9\n",
    "\n",
    "cum_sum_last_CS2_updated_stationary = cum_CS2_updated_stationary[\n",
    "    subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date\n",
    "][-1]\n",
    "\n",
    "cum_IS2_updated_stationary = np.cumsum(\n",
    "    superset_IS2_evolving_union_sum_df['stationary_outline_dV_corr (m^3)'].values\n",
    ") / 1e9 + cum_sum_last_CS2_updated_stationary\n",
    "\n",
    "cum_total_updated_stationary = np.concatenate([cum_CS2_updated_stationary, cum_IS2_updated_stationary])\n",
    "\n",
    "max_abs_cum_dV_updated_stationary = np.max(np.abs(cum_total_updated_stationary))\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"Max absolute cumulative dV (evolving outlines): {max_abs_cum_dV_evolving:.3f} km³\")\n",
    "print(f\"Max absolute cumulative dV (stationary outline at all analyzed lakes): {max_abs_cum_dV_prior_stationary:.3f} km³\")\n",
    "print(f\"Max absolute cumulative dV (stationary outline at evolving lakes subset): {max_abs_cum_dV_prior_stationary_evolving_subset:.3f} km³\")\n",
    "print(f\"Max absolute cumulative dV (updated stationary outline): {max_abs_cum_dV_updated_stationary:.3f} km³\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6c4eb0-39fe-488f-ad86-82a4425a94b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias (evolving - prior stationary at all analyzed lakes)\n",
    "bias_prior_CS2 = np.cumsum(\n",
    "    subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['evolving_outlines_dV_corr (m^3)'].values -\n",
    "    subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['stationary_outline_dV_corr (m^3)'].values\n",
    ") / 1e9\n",
    "\n",
    "cum_sum_last_CS2_midcyc_date_prior = np.cumsum((\n",
    "    subset_CS2_IS2_SARInPostExpansion_evolving_sum_df[\n",
    "        subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date\n",
    "    ]['evolving_outlines_dV_corr (m^3)'] -\n",
    "    subset_CS2_IS2_SARInPostExpansion_stationary_sum_df[\n",
    "        subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date\n",
    "    ]['stationary_outline_dV_corr (m^3)']\n",
    ") / 1e9).iloc[-1]\n",
    "\n",
    "bias_prior_IS2 = np.cumsum((\n",
    "    superset_IS2_evolving_sum_df['evolving_outlines_dV_corr (m^3)'].values -\n",
    "    superset_IS2_stationary_sum_df['stationary_outline_dV_corr (m^3)'].values\n",
    ") / 1e9) + cum_sum_last_CS2_midcyc_date_prior\n",
    "\n",
    "# Combine CS2 + IS2\n",
    "bias_prior_total = np.concatenate([bias_prior_CS2, bias_prior_IS2])\n",
    "\n",
    "# Max absolute bias (prior stationary)\n",
    "max_abs_cum_dV_bias_prior_stationary = np.max(np.abs(bias_prior_total))\n",
    "\n",
    "\n",
    "# Bias (evolving - prior stationary at evolving lakes subset)\n",
    "bias_prior_CS2 = np.cumsum(\n",
    "    subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['evolving_outlines_dV_corr (m^3)'].values -\n",
    "    subset_CS2_IS2_SARInPostExpansion_stationary_subset_sum_df['stationary_outline_dV_corr (m^3)'].values\n",
    ") / 1e9\n",
    "\n",
    "cum_sum_last_CS2_midcyc_date_prior = np.cumsum((\n",
    "    subset_CS2_IS2_SARInPostExpansion_evolving_sum_df[\n",
    "        subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date\n",
    "    ]['evolving_outlines_dV_corr (m^3)'] -\n",
    "    subset_CS2_IS2_SARInPostExpansion_stationary_subset_sum_df[\n",
    "        subset_CS2_IS2_SARInPostExpansion_stationary_subset_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date\n",
    "    ]['stationary_outline_dV_corr (m^3)']\n",
    ") / 1e9).iloc[-1]\n",
    "\n",
    "bias_prior_IS2 = np.cumsum((\n",
    "    superset_IS2_evolving_sum_df['evolving_outlines_dV_corr (m^3)'].values -\n",
    "    superset_IS2_stationary_subset_sum_df['stationary_outline_dV_corr (m^3)'].values\n",
    ") / 1e9) + cum_sum_last_CS2_midcyc_date_prior\n",
    "\n",
    "# Combine CS2 + IS2\n",
    "bias_prior_total = np.concatenate([bias_prior_CS2, bias_prior_IS2])\n",
    "\n",
    "# Max absolute bias (prior stationary)\n",
    "max_abs_cum_dV_bias_prior_stationary_at_evolving_lakes = np.max(np.abs(bias_prior_total))\n",
    "\n",
    "\n",
    "# Bias (evolving - updated stationary)\n",
    "bias_updated_CS2 = np.cumsum(\n",
    "    subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['evolving_outlines_dV_corr (m^3)'].values -\n",
    "    subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['stationary_outline_dV_corr (m^3)'].values\n",
    ") / 1e9\n",
    "\n",
    "cum_sum_last_CS2_midcyc_date_updated = np.cumsum((\n",
    "    subset_CS2_IS2_SARInPostExpansion_evolving_sum_df[\n",
    "        subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date\n",
    "    ]['evolving_outlines_dV_corr (m^3)'] -\n",
    "    subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df[\n",
    "        subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date\n",
    "    ]['stationary_outline_dV_corr (m^3)']\n",
    ") / 1e9).iloc[-1]\n",
    "\n",
    "bias_updated_IS2 = np.cumsum((\n",
    "    superset_IS2_evolving_sum_df['evolving_outlines_dV_corr (m^3)'].values -\n",
    "    superset_IS2_evolving_union_sum_df['stationary_outline_dV_corr (m^3)'].values\n",
    ") / 1e9) + cum_sum_last_CS2_midcyc_date_updated\n",
    "\n",
    "# Combine CS2 + IS2\n",
    "bias_updated_total = np.concatenate([bias_updated_CS2, bias_updated_IS2])\n",
    "\n",
    "# Max absolute bias (updated stationary)\n",
    "max_abs_cum_dV_bias_updated_stationary = np.max(np.abs(bias_updated_total))\n",
    "\n",
    "\n",
    "# --- Print results ---\n",
    "print(f\"Maximum absolute bias (evolving - prior stationary at all analyzed lakes): {max_abs_cum_dV_bias_prior_stationary:.3f} km³\")\n",
    "print(f\"Maximum absolute bias (evolving - prior stationary at evolving lakes subset): {max_abs_cum_dV_bias_prior_stationary_at_evolving_lakes:.3f} km³\")\n",
    "print(f\"Maximum absolute bias (evolving - updated stationary): {max_abs_cum_dV_bias_updated_stationary:.3f} km³\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09b5336-3dac-4a75-a64c-4664f1432873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent underestimation\n",
    "percent_prior = 100 * max_abs_cum_dV_bias_prior_stationary / max_abs_cum_dV_evolving\n",
    "percent_prior_evolving_subset = 100 * max_abs_cum_dV_bias_prior_stationary_at_evolving_lakes / max_abs_cum_dV_evolving\n",
    "percent_updated = 100 * max_abs_cum_dV_bias_updated_stationary / max_abs_cum_dV_evolving\n",
    "\n",
    "print(f\"Maximum cumulative bias (prior stationary at all analyzed lakes) is {percent_prior:.1f}% of the cumulative evolving flux.\")\n",
    "print(f\"Maximum cumulative bias (prior stationary at evolving lakes subset) is {percent_prior_evolving_subset:.1f}% of the cumulative evolving flux.\")\n",
    "print(f\"Maximum cumulative bias (updated stationary) is {percent_updated:.1f}% of the cumulative evolving flux.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e43113-1904-4b87-af1f-1164c0ea7351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent underestimation\n",
    "max_bias = max(max_abs_cum_dV_bias_prior_stationary, max_abs_cum_dV_bias_prior_stationary_at_evolving_lakes, max_abs_cum_dV_bias_updated_stationary)\n",
    "max_bias_percent = 100 *  max_bias / max_abs_cum_dV_evolving\n",
    "print(f'maximum cumulative volume flux bias: {max_bias:.1f} km$^3$')\n",
    "\n",
    "min_bias = min(max_abs_cum_dV_bias_prior_stationary, max_abs_cum_dV_bias_prior_stationary_at_evolving_lakes, max_abs_cum_dV_bias_updated_stationary)\n",
    "min_bias_percent = 100 *  min_bias / max_abs_cum_dV_evolving\n",
    "print(f'minimum cumulative volume flux bias: {min_bias:.1f} km$^3$')\n",
    "\n",
    "# Key point # 3\n",
    "# Find maximum percent of cumulative volume flux underestimated using stationary outlines\n",
    "print(f'Using stationary outlines results in a cumulative underestimation of Antarctic subglacial water volume of $\\sim$ {min_bias_percent:.0f}-{max_bias_percent:.0f}$\\%$ ($\\sim$ {min_bias:.1f}-{max_bias:.1f} $km^3$) compared to evolving outlines.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3799a4e-5417-4d71-9149-0ca4633af348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory paths for the new folder structure\n",
    "evolving_folder = 'output/geometric_calcs/evolving_outlines_geom_calc/forward_fill/'\n",
    "stationary_folder = 'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes/'\n",
    "\n",
    "# Files to ignore in our analysis\n",
    "files_to_ignore = [\n",
    "    'superset_IS2_lakes_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPostExpansion_sum.csv', \n",
    "    'subset_CS2_IS2_lakes_SARInPreExpansion_sum.csv',\n",
    "    'subset_noCS2_IS2_lakes_sum.csv'\n",
    "]\n",
    "\n",
    "# Get lists of CSV files in each directory, excluding the files to ignore\n",
    "evolving_csv_files = [f for f in os.listdir(evolving_folder) \n",
    "                     if f.endswith('.csv') and f not in files_to_ignore]\n",
    "stationary_csv_files = [f for f in os.listdir(stationary_folder) \n",
    "                       if f.endswith('.csv') and f not in files_to_ignore]\n",
    "\n",
    "# Find common lake names (files that exist in both folders)\n",
    "evolving_lake_names = {os.path.splitext(f)[0] for f in evolving_csv_files}\n",
    "stationary_lake_names = {os.path.splitext(f)[0] for f in stationary_csv_files}\n",
    "common_lake_names = evolving_lake_names.intersection(stationary_lake_names)\n",
    "\n",
    "# Initialize DataFrame to store lake-level results\n",
    "lake_results_df = pd.DataFrame({\n",
    "    'lake_name': list(common_lake_names),\n",
    "    'greater_than_125_percent': False,\n",
    "    'less_than_75_percent': False,\n",
    "    'both_conditions': False,\n",
    "    'either_condition': False,\n",
    "    'total_time_steps': 0,\n",
    "    'valid_data_found': False\n",
    "})\n",
    "\n",
    "# Create a list to store the combined data for all lakes (for time step analysis)\n",
    "all_combined_data = []\n",
    "\n",
    "# Loop through each lake and process data\n",
    "for idx, lake_name in enumerate(lake_results_df['lake_name']):\n",
    "    try:\n",
    "        # Read the evolving lake data\n",
    "        evolving_file_path = os.path.join(evolving_folder, f\"{lake_name}.csv\")\n",
    "        evolving_df = pd.read_csv(evolving_file_path)\n",
    "        \n",
    "        # Read the stationary lake data\n",
    "        stationary_file_path = os.path.join(stationary_folder, f\"{lake_name}.csv\")\n",
    "        stationary_df = pd.read_csv(stationary_file_path)\n",
    "        \n",
    "        # Identify the date column and volume column in each dataframe\n",
    "        evolving_date_col = None\n",
    "        evolving_vol_col = None\n",
    "        \n",
    "        for col in evolving_df.columns:\n",
    "            if any(date_indicator in col.lower() for date_indicator in ['date', 'time', 'day']):\n",
    "                evolving_date_col = col\n",
    "            if 'dv_corr' in col.lower():\n",
    "                evolving_vol_col = col\n",
    "        \n",
    "        # For stationary dataframe\n",
    "        stationary_date_col = None\n",
    "        stationary_vol_col = None\n",
    "        \n",
    "        for col in stationary_df.columns:\n",
    "            if any(date_indicator in col.lower() for date_indicator in ['date', 'time', 'day']):\n",
    "                stationary_date_col = col\n",
    "            if 'dv_corr' in col.lower():\n",
    "                stationary_vol_col = col\n",
    "        \n",
    "        # Skip if we couldn't identify the necessary columns\n",
    "        if not all([evolving_date_col, evolving_vol_col, stationary_date_col, stationary_vol_col]):\n",
    "            print(f\"Skipping {lake_name} - could not identify all required columns\")\n",
    "            continue\n",
    "        \n",
    "        # Create standardized dataframes for merging\n",
    "        evolving_std_df = pd.DataFrame({\n",
    "            'lake_name': lake_name,\n",
    "            'date': evolving_df[evolving_date_col],\n",
    "            'evolving_outlines_dV_corr (m^3)': evolving_df[evolving_vol_col]\n",
    "        })\n",
    "        \n",
    "        stationary_std_df = pd.DataFrame({\n",
    "            'lake_name': lake_name,\n",
    "            'date': stationary_df[stationary_date_col],\n",
    "            'stationary_outline_dV_corr (m^3)': stationary_df[stationary_vol_col]\n",
    "        })\n",
    "        \n",
    "        # Merge the dataframes on lake_name and date\n",
    "        merged_df = pd.merge(evolving_std_df, stationary_std_df, on=['lake_name', 'date'])\n",
    "        \n",
    "        # Filter out rows where evolving volume is 0\n",
    "        filtered_df = merged_df[merged_df['evolving_outlines_dV_corr (m^3)'] != 0]\n",
    "        \n",
    "        # Skip lakes with no valid data\n",
    "        if filtered_df.empty:\n",
    "            print(f\"Skipping {lake_name} - no valid data after filtering\")\n",
    "            continue\n",
    "        \n",
    "        # Update lake-level results\n",
    "        lake_results_df.loc[idx, 'valid_data_found'] = True\n",
    "        lake_results_df.loc[idx, 'total_time_steps'] = len(filtered_df)\n",
    "        \n",
    "        # Check conditions for lake-level analysis\n",
    "        condition_greater_than = (filtered_df['evolving_outlines_dV_corr (m^3)'] > \n",
    "                                1.25 * filtered_df['stationary_outline_dV_corr (m^3)'])\n",
    "        condition_less_than = (filtered_df['evolving_outlines_dV_corr (m^3)'] < \n",
    "                             0.75 * filtered_df['stationary_outline_dV_corr (m^3)'])\n",
    "        \n",
    "        # Update DataFrame with lake-level results\n",
    "        lake_results_df.loc[idx, 'greater_than_125_percent'] = condition_greater_than.any()\n",
    "        lake_results_df.loc[idx, 'less_than_75_percent'] = condition_less_than.any()\n",
    "        lake_results_df.loc[idx, 'both_conditions'] = condition_greater_than.any() and condition_less_than.any()\n",
    "        \n",
    "        # Calculate either condition\n",
    "        condition_either = condition_greater_than | condition_less_than\n",
    "        lake_results_df.loc[idx, 'either_condition'] = condition_either.any()\n",
    "        \n",
    "        # Add to combined data for time step analysis\n",
    "        all_combined_data.append(filtered_df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {lake_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# ==================== LAKE-LEVEL ANALYSIS ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LAKE-LEVEL ANALYSIS RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Filter out lakes with no valid data for final analysis\n",
    "valid_lake_results = lake_results_df[lake_results_df['valid_data_found']]\n",
    "\n",
    "# Calculate proportions using vectorized operations\n",
    "total_valid_lakes = len(valid_lake_results)\n",
    "print(f\"Valid lakes analyzed: {total_valid_lakes}\")\n",
    "\n",
    "if total_valid_lakes > 0:\n",
    "    lake_proportions = {\n",
    "        'greater_than_125_percent': valid_lake_results['greater_than_125_percent'].sum() / total_valid_lakes,\n",
    "        'less_than_75_percent': valid_lake_results['less_than_75_percent'].sum() / total_valid_lakes,\n",
    "        'both_conditions': valid_lake_results['both_conditions'].sum() / total_valid_lakes,\n",
    "        'either_condition': valid_lake_results['either_condition'].sum() / total_valid_lakes\n",
    "    }\n",
    "    \n",
    "    # Print lake-level results\n",
    "    print(f\"Lakes meeting condition 1 (greater than 125%): {valid_lake_results['greater_than_125_percent'].sum()} ({lake_proportions['greater_than_125_percent']:.2f})\")\n",
    "    print(f\"Lakes meeting condition 2 (less than 75%): {valid_lake_results['less_than_75_percent'].sum()} ({lake_proportions['less_than_75_percent']:.2f})\")\n",
    "    print(f\"Lakes meeting either condition: {valid_lake_results['either_condition'].sum()} ({lake_proportions['either_condition']:.2f})\")\n",
    "    print(f\"Lakes meeting both conditions: {valid_lake_results['both_conditions'].sum()} ({lake_proportions['both_conditions']:.2f})\")\n",
    "    \n",
    "else:\n",
    "    print(\"No valid lakes found for analysis\")\n",
    "\n",
    "# ==================== TIME STEP ANALYSIS ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TIME STEP ANALYSIS RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Concatenate all the combined data into a single dataframe\n",
    "if all_combined_data:\n",
    "    time_step_df = pd.concat(all_combined_data, ignore_index=True)\n",
    "    \n",
    "    # Define the separate conditions, using absolute values\n",
    "    condition1 = (time_step_df['evolving_outlines_dV_corr (m^3)'].abs() > \n",
    "                  1.25 * time_step_df['stationary_outline_dV_corr (m^3)'].abs())\n",
    "    condition2 = (time_step_df['evolving_outlines_dV_corr (m^3)'].abs() < \n",
    "                  0.75 * time_step_df['stationary_outline_dV_corr (m^3)'].abs())\n",
    "    \n",
    "    # Condition where either condition is met\n",
    "    either_condition = (condition1 | condition2)\n",
    "    \n",
    "    # Condition where neither condition is met\n",
    "    neither_condition = ~(condition1 | condition2)\n",
    "    \n",
    "    # Condition where both conditions are met\n",
    "    both_conditions = condition1 & condition2\n",
    "    \n",
    "    # Count the number of rows meeting each condition\n",
    "    num_rows_condition1 = condition1.sum()\n",
    "    num_rows_condition2 = condition2.sum()\n",
    "    num_rows_either_condition = either_condition.sum()\n",
    "    num_rows_neither_condition = neither_condition.sum()\n",
    "    num_rows_both_conditions = both_conditions.sum()\n",
    "    \n",
    "    # Calculate proportions based on the length of time_step_df\n",
    "    total_time_steps = len(time_step_df)\n",
    "    proportion_condition1 = num_rows_condition1 / total_time_steps if total_time_steps > 0 else 0\n",
    "    proportion_condition2 = num_rows_condition2 / total_time_steps if total_time_steps > 0 else 0\n",
    "    proportion_either_condition = num_rows_either_condition / total_time_steps if total_time_steps > 0 else 0\n",
    "    proportion_neither_condition = num_rows_neither_condition / total_time_steps if total_time_steps > 0 else 0\n",
    "    proportion_both_conditions = num_rows_both_conditions / total_time_steps if total_time_steps > 0 else 0\n",
    "    \n",
    "    # Calculate sum of proportions as a sanity check\n",
    "    sum_of_proportions = np.sum([proportion_condition1, proportion_condition2, proportion_both_conditions, proportion_neither_condition])\n",
    "    \n",
    "    # Print out the time step results\n",
    "    print(f\"Valid time steps analyzed: {total_time_steps}\")\n",
    "    print(f\"Proportion of time steps meeting condition 1 (greater than 125%): {np.round(proportion_condition1, 2)}\")\n",
    "    print(f\"Proportion of time steps meeting condition 2 (less than 75%): {np.round(proportion_condition2, 2)}\")\n",
    "    print(f\"Proportion of time steps meeting either conditions: {np.round(proportion_either_condition, 2)}\")\n",
    "    print(f\"Proportion of time steps meeting neither condition: {np.round(proportion_neither_condition, 2)}\")\n",
    "    print(f\"Proportion of time steps meeting both conditions: {np.round(proportion_both_conditions, 2)}\")\n",
    "    print(f\"Sum of proportions: {np.round(sum_of_proportions, 2)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Failed to create any valid combined data for time step analysis.\")\n",
    "\n",
    "# ==================== SUMMARY ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if total_valid_lakes > 0 and all_combined_data:\n",
    "    print(f\"Analysis completed successfully:\")\n",
    "    print(f\"  • {total_valid_lakes} lakes analyzed\")\n",
    "    print(f\"  • {len(time_step_df)} total time steps analyzed\")\n",
    "    print(f\"\\nKey findings:\")\n",
    "    print(f\"  • {lake_proportions['either_condition']:.1%} of lakes have at least one time step with dV differences ±25%\")\n",
    "    print(f\"  • {proportion_either_condition:.1%} of time steps show dV differences ±25%\")\n",
    "else:\n",
    "    print(\"Analysis could not be completed due to data issues.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a19e91-0833-40ed-b248-a3aebdcaeb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory paths for the new folder structure\n",
    "evolving_folder = 'output/geometric_calcs/evolving_outlines_geom_calc/forward_fill/'\n",
    "stationary_folder = 'output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/'\n",
    "\n",
    "# Files to ignore in our analysis\n",
    "files_to_ignore = [\n",
    "    'superset_IS2_lakes_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPostExpansion_sum.csv', \n",
    "    'subset_CS2_IS2_lakes_SARInPreExpansion_sum.csv',\n",
    "    'subset_noCS2_IS2_lakes_sum.csv'\n",
    "]\n",
    "\n",
    "# Get lists of CSV files in each directory, excluding the files to ignore\n",
    "evolving_csv_files = [f for f in os.listdir(evolving_folder) \n",
    "                     if f.endswith('.csv') and f not in files_to_ignore]\n",
    "stationary_csv_files = [f for f in os.listdir(stationary_folder) \n",
    "                       if f.endswith('.csv') and f not in files_to_ignore]\n",
    "\n",
    "# Find common lake names (files that exist in both folders)\n",
    "evolving_lake_names = {os.path.splitext(f)[0] for f in evolving_csv_files}\n",
    "stationary_lake_names = {os.path.splitext(f)[0] for f in stationary_csv_files}\n",
    "common_lake_names = evolving_lake_names.intersection(stationary_lake_names)\n",
    "\n",
    "# Initialize DataFrame to store lake-level results\n",
    "lake_results_df = pd.DataFrame({\n",
    "    'lake_name': list(common_lake_names),\n",
    "    'greater_than_125_percent': False,\n",
    "    'less_than_75_percent': False,\n",
    "    'both_conditions': False,\n",
    "    'either_condition': False,\n",
    "    'total_time_steps': 0,\n",
    "    'valid_data_found': False\n",
    "})\n",
    "\n",
    "# Create a list to store the combined data for all lakes (for time step analysis)\n",
    "all_combined_data = []\n",
    "\n",
    "# Loop through each lake and process data\n",
    "for idx, lake_name in enumerate(lake_results_df['lake_name']):\n",
    "    try:\n",
    "        # Read the evolving lake data\n",
    "        evolving_file_path = os.path.join(evolving_folder, f\"{lake_name}.csv\")\n",
    "        evolving_df = pd.read_csv(evolving_file_path)\n",
    "        \n",
    "        # Read the stationary lake data\n",
    "        stationary_file_path = os.path.join(stationary_folder, f\"{lake_name}.csv\")\n",
    "        stationary_df = pd.read_csv(stationary_file_path)\n",
    "        \n",
    "        # Identify the date column and volume column in each dataframe\n",
    "        evolving_date_col = None\n",
    "        evolving_vol_col = None\n",
    "        \n",
    "        for col in evolving_df.columns:\n",
    "            if any(date_indicator in col.lower() for date_indicator in ['date', 'time', 'day']):\n",
    "                evolving_date_col = col\n",
    "            if 'dv_corr' in col.lower():\n",
    "                evolving_vol_col = col\n",
    "        \n",
    "        # For stationary dataframe\n",
    "        stationary_date_col = None\n",
    "        stationary_vol_col = None\n",
    "        \n",
    "        for col in stationary_df.columns:\n",
    "            if any(date_indicator in col.lower() for date_indicator in ['date', 'time', 'day']):\n",
    "                stationary_date_col = col\n",
    "            if 'dv_corr' in col.lower():\n",
    "                stationary_vol_col = col\n",
    "        \n",
    "        # Skip if we couldn't identify the necessary columns\n",
    "        if not all([evolving_date_col, evolving_vol_col, stationary_date_col, stationary_vol_col]):\n",
    "            print(f\"Skipping {lake_name} - could not identify all required columns\")\n",
    "            continue\n",
    "        \n",
    "        # Create standardized dataframes for merging\n",
    "        evolving_std_df = pd.DataFrame({\n",
    "            'lake_name': lake_name,\n",
    "            'date': evolving_df[evolving_date_col],\n",
    "            'evolving_outlines_dV_corr (m^3)': evolving_df[evolving_vol_col]\n",
    "        })\n",
    "        \n",
    "        stationary_std_df = pd.DataFrame({\n",
    "            'lake_name': lake_name,\n",
    "            'date': stationary_df[stationary_date_col],\n",
    "            'stationary_outline_dV_corr (m^3)': stationary_df[stationary_vol_col]\n",
    "        })\n",
    "        \n",
    "        # Merge the dataframes on lake_name and date\n",
    "        merged_df = pd.merge(evolving_std_df, stationary_std_df, on=['lake_name', 'date'])\n",
    "        \n",
    "        # Filter out rows where evolving volume is 0\n",
    "        filtered_df = merged_df[merged_df['evolving_outlines_dV_corr (m^3)'] != 0]\n",
    "        \n",
    "        # Skip lakes with no valid data\n",
    "        if filtered_df.empty:\n",
    "            print(f\"Skipping {lake_name} - no valid data after filtering\")\n",
    "            continue\n",
    "        \n",
    "        # Update lake-level results\n",
    "        lake_results_df.loc[idx, 'valid_data_found'] = True\n",
    "        lake_results_df.loc[idx, 'total_time_steps'] = len(filtered_df)\n",
    "        \n",
    "        # Check conditions for lake-level analysis\n",
    "        condition_greater_than = (filtered_df['evolving_outlines_dV_corr (m^3)'] > \n",
    "                                1.25 * filtered_df['stationary_outline_dV_corr (m^3)'])\n",
    "        condition_less_than = (filtered_df['evolving_outlines_dV_corr (m^3)'] < \n",
    "                             0.75 * filtered_df['stationary_outline_dV_corr (m^3)'])\n",
    "        \n",
    "        # Update DataFrame with lake-level results\n",
    "        lake_results_df.loc[idx, 'greater_than_125_percent'] = condition_greater_than.any()\n",
    "        lake_results_df.loc[idx, 'less_than_75_percent'] = condition_less_than.any()\n",
    "        lake_results_df.loc[idx, 'both_conditions'] = condition_greater_than.any() and condition_less_than.any()\n",
    "        \n",
    "        # Calculate either condition\n",
    "        condition_either = condition_greater_than | condition_less_than\n",
    "        lake_results_df.loc[idx, 'either_condition'] = condition_either.any()\n",
    "        \n",
    "        # Add to combined data for time step analysis\n",
    "        all_combined_data.append(filtered_df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {lake_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# ==================== LAKE-LEVEL ANALYSIS ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LAKE-LEVEL ANALYSIS RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Filter out lakes with no valid data for final analysis\n",
    "valid_lake_results = lake_results_df[lake_results_df['valid_data_found']]\n",
    "\n",
    "# Calculate proportions using vectorized operations\n",
    "total_valid_lakes = len(valid_lake_results)\n",
    "print(f\"Valid lakes analyzed: {total_valid_lakes}\")\n",
    "\n",
    "if total_valid_lakes > 0:\n",
    "    lake_proportions = {\n",
    "        'greater_than_125_percent': valid_lake_results['greater_than_125_percent'].sum() / total_valid_lakes,\n",
    "        'less_than_75_percent': valid_lake_results['less_than_75_percent'].sum() / total_valid_lakes,\n",
    "        'both_conditions': valid_lake_results['both_conditions'].sum() / total_valid_lakes,\n",
    "        'either_condition': valid_lake_results['either_condition'].sum() / total_valid_lakes\n",
    "    }\n",
    "    \n",
    "    # Print lake-level results\n",
    "    print(f\"Lakes meeting condition 1 (greater than 125%): {valid_lake_results['greater_than_125_percent'].sum()} ({lake_proportions['greater_than_125_percent']:.2f})\")\n",
    "    print(f\"Lakes meeting condition 2 (less than 75%): {valid_lake_results['less_than_75_percent'].sum()} ({lake_proportions['less_than_75_percent']:.2f})\")\n",
    "    print(f\"Lakes meeting either condition: {valid_lake_results['either_condition'].sum()} ({lake_proportions['either_condition']:.2f})\")\n",
    "    print(f\"Lakes meeting both conditions: {valid_lake_results['both_conditions'].sum()} ({lake_proportions['both_conditions']:.2f})\")\n",
    "    \n",
    "else:\n",
    "    print(\"No valid lakes found for analysis\")\n",
    "\n",
    "# ==================== TIME STEP ANALYSIS ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TIME STEP ANALYSIS RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Concatenate all the combined data into a single dataframe\n",
    "if all_combined_data:\n",
    "    time_step_df = pd.concat(all_combined_data, ignore_index=True)\n",
    "    \n",
    "    # Define the separate conditions, using absolute values\n",
    "    condition1 = (time_step_df['evolving_outlines_dV_corr (m^3)'].abs() > \n",
    "                  1.25 * time_step_df['stationary_outline_dV_corr (m^3)'].abs())\n",
    "    condition2 = (time_step_df['evolving_outlines_dV_corr (m^3)'].abs() < \n",
    "                  0.75 * time_step_df['stationary_outline_dV_corr (m^3)'].abs())\n",
    "    \n",
    "    # Condition where either condition is met\n",
    "    either_condition = (condition1 | condition2)\n",
    "    \n",
    "    # Condition where neither condition is met\n",
    "    neither_condition = ~(condition1 | condition2)\n",
    "    \n",
    "    # Condition where both conditions are met\n",
    "    both_conditions = condition1 & condition2\n",
    "    \n",
    "    # Count the number of rows meeting each condition\n",
    "    num_rows_condition1 = condition1.sum()\n",
    "    num_rows_condition2 = condition2.sum()\n",
    "    num_rows_either_condition = either_condition.sum()\n",
    "    num_rows_neither_condition = neither_condition.sum()\n",
    "    num_rows_both_conditions = both_conditions.sum()\n",
    "    \n",
    "    # Calculate proportions based on the length of time_step_df\n",
    "    total_time_steps = len(time_step_df)\n",
    "    proportion_condition1 = num_rows_condition1 / total_time_steps if total_time_steps > 0 else 0\n",
    "    proportion_condition2 = num_rows_condition2 / total_time_steps if total_time_steps > 0 else 0\n",
    "    proportion_either_condition = num_rows_either_condition / total_time_steps if total_time_steps > 0 else 0\n",
    "    proportion_neither_condition = num_rows_neither_condition / total_time_steps if total_time_steps > 0 else 0\n",
    "    proportion_both_conditions = num_rows_both_conditions / total_time_steps if total_time_steps > 0 else 0\n",
    "    \n",
    "    # Calculate sum of proportions as a sanity check\n",
    "    sum_of_proportions = np.sum([proportion_condition1, proportion_condition2, proportion_both_conditions, proportion_neither_condition])\n",
    "    \n",
    "    # Print out the time step results\n",
    "    print(f\"Valid time steps analyzed: {total_time_steps}\")\n",
    "    print(f\"Proportion of time steps meeting condition 1 (greater than 125%): {np.round(proportion_condition1, 2)}\")\n",
    "    print(f\"Proportion of time steps meeting condition 2 (less than 75%): {np.round(proportion_condition2, 2)}\")\n",
    "    print(f\"Proportion of time steps meeting either conditions: {np.round(proportion_either_condition, 2)}\")\n",
    "    print(f\"Proportion of time steps meeting neither condition: {np.round(proportion_neither_condition, 2)}\")\n",
    "    print(f\"Proportion of time steps meeting both conditions: {np.round(proportion_both_conditions, 2)}\")\n",
    "    print(f\"Sum of proportions: {np.round(sum_of_proportions, 2)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Failed to create any valid combined data for time step analysis.\")\n",
    "\n",
    "# ==================== SUMMARY ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if total_valid_lakes > 0 and all_combined_data:\n",
    "    print(f\"Analysis completed successfully:\")\n",
    "    print(f\"  • {total_valid_lakes} lakes analyzed\")\n",
    "    print(f\"  • {len(time_step_df)} total time steps analyzed\")\n",
    "    print(f\"\\nKey findings:\")\n",
    "    print(f\"  • {lake_proportions['either_condition']:.1%} of lakes have at least one time step with dV differences ±25%\")\n",
    "    print(f\"  • {proportion_either_condition:.1%} of time steps show dV differences ±25%\")\n",
    "else:\n",
    "    print(\"Analysis could not be completed due to data issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35266a7-4cf8-48a7-acf9-de6ffb1e8586",
   "metadata": {},
   "source": [
    "### Explaining continental sum trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8225abee-a0cc-44dd-9750-f6f62583d650",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'output/geometric_calcs/evolving_outlines_geom_calc/forward_fill'\n",
    "\n",
    "def create_interactive_plot(directory, stationary_outlines_gdf):\n",
    "    # Load and process all lake data\n",
    "    dfs = []\n",
    "    lake_names = []\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        # Exclude continental summation files\n",
    "        if \"subset\" in file or \"superset\" in file:\n",
    "            continue\n",
    "\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            lake_name = os.path.splitext(file)[0]\n",
    "            \n",
    "            # Process each lake's data\n",
    "            df['lake_name'] = lake_name\n",
    "            df['datetime'] = pd.to_datetime(df['mid_pt_datetime'])\n",
    "            df['cumsum_vol'] = np.cumsum(df['evolving_outlines_dV_corr (m^3)']/1e9)\n",
    "            dfs.append(df)\n",
    "            lake_names.append(lake_name)\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Create plot using hvplot\n",
    "    plot = combined_df.hvplot.line(\n",
    "        x='datetime',\n",
    "        y='cumsum_vol',\n",
    "        by='lake_name',\n",
    "        width=800,\n",
    "        height=400,\n",
    "        title='Lake Volume Changes Over Time',\n",
    "        xlabel='Date',\n",
    "        ylabel='Cumulative Volume Change (km³)',\n",
    "        grid=True,\n",
    "        legend=False\n",
    "    )\n",
    "    \n",
    "    return plot\n",
    "\n",
    "# Usage:\n",
    "plot = create_interactive_plot(directory, stationary_outlines_gdf)\n",
    "plot  # Display in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345d7bfc-019d-4570-919e-175a1dde2e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'output/geometric_calcs/evolving_outlines_geom_calc/forward_fill'\n",
    "\n",
    "def create_interactive_plot(directory, stationary_outlines_gdf):\n",
    "    # Load and process all lake data\n",
    "    dfs_subset_CS2_IS2_lakes = []\n",
    "    lake_names = []\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            lake_name = os.path.splitext(file)[0]\n",
    "            \n",
    "            # Process each lake's data\n",
    "            df['lake_name'] = lake_name\n",
    "            df['datetime'] = pd.to_datetime(df['mid_pt_datetime'])\n",
    "            df['cumsum_vol'] = np.cumsum(np.divide(df['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "            dfs_subset_CS2_IS2_lakes.append(df)\n",
    "            lake_names.append(lake_name)\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(dfs_subset_CS2_IS2_lakes, ignore_index=True)\n",
    "    \n",
    "    # Create the plot using Dataset and Curve\n",
    "    dataset = hv.Dataset(combined_df)\n",
    "    curves = dataset.to(hv.Curve, \n",
    "                       kdims=['datetime'], \n",
    "                       vdims=['cumsum_vol', 'lake_name'],\n",
    "                       groupby='lake_name')\n",
    "    \n",
    "    # Apply options to the plot\n",
    "    plot = curves.opts(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        tools=['hover'],\n",
    "        title='Lake volume changes over time',\n",
    "        xlabel='Date',\n",
    "        ylabel='Cumulative volume change (km³)',\n",
    "        show_grid=True,\n",
    "        toolbar='above'\n",
    "    )\n",
    "    \n",
    "    return plot\n",
    "\n",
    "# Usage:\n",
    "plot = create_interactive_plot(directory, stationary_outlines_gdf)\n",
    "plot  # Display in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f265cfb6-ce0b-4107-bfb0-e772dcf1e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the lakes driving the deviation of evolving outlines and prior stationary outlines\n",
    "\n",
    "evolving_directory = 'output/geometric_calcs/evolving_outlines_geom_calc/forward_fill'\n",
    "stationary_directory = 'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes'\n",
    "\n",
    "def process_lake_data(directory, is_evolving=True):\n",
    "    dfs = {}\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        # Exclude continental summation files\n",
    "        if \"subset\" in file or \"superset\" in file:\n",
    "            continue\n",
    "\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            lake_name = os.path.splitext(file)[0]\n",
    "            \n",
    "            # Process each lake's data\n",
    "            df['lake_name'] = lake_name\n",
    "            df['datetime'] = pd.to_datetime(df['mid_pt_datetime'])\n",
    "            \n",
    "            # Calculate cumulative volume based on directory type\n",
    "            if is_evolving:\n",
    "                df['cumsum_vol'] = np.cumsum(df['evolving_outlines_dV_corr (m^3)'])\n",
    "            else:\n",
    "                df['cumsum_vol'] = np.cumsum(df['stationary_outline_dV_corr (m^3)'])\n",
    "            \n",
    "            dfs[lake_name] = df\n",
    "    \n",
    "    return dfs\n",
    "\n",
    "def create_interactive_plot(evolving_directory, stationary_directory):\n",
    "    # Load data from both directories\n",
    "    evolving_dfs = process_lake_data(evolving_directory, is_evolving=True)\n",
    "    stationary_dfs = process_lake_data(stationary_directory, is_evolving=False)\n",
    "    \n",
    "    # Initialize lists to store processed dataframes\n",
    "    plot_dfs = []\n",
    "    \n",
    "    # Process common lakes\n",
    "    common_lakes = set(evolving_dfs.keys()) & set(stationary_dfs.keys())\n",
    "    for lake_name in common_lakes:\n",
    "        evolving_df = evolving_dfs[lake_name].copy()\n",
    "        stationary_df = stationary_dfs[lake_name].copy()\n",
    "        \n",
    "        # Calculate difference (evolving - stationary)\n",
    "        merged_df = pd.merge(\n",
    "            evolving_df[['datetime', 'cumsum_vol']], \n",
    "            stationary_df[['datetime', 'cumsum_vol']], \n",
    "            on='datetime', \n",
    "            suffixes=('_evolving', '_stationary')\n",
    "        )\n",
    "        merged_df['cumsum_vol'] = merged_df['cumsum_vol_evolving'] - merged_df['cumsum_vol_stationary']\n",
    "        merged_df['lake_name'] = lake_name + '_difference'\n",
    "        plot_dfs.append(merged_df[['datetime', 'cumsum_vol', 'lake_name']])\n",
    "    \n",
    "    # Process lakes only in stationary directory\n",
    "    stationary_only = set(stationary_dfs.keys()) - set(evolving_dfs.keys())\n",
    "    for lake_name in stationary_only:\n",
    "        df = stationary_dfs[lake_name].copy()\n",
    "        df['lake_name'] = lake_name + '_stationary'\n",
    "        plot_dfs.append(df[['datetime', 'cumsum_vol', 'lake_name']])\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(plot_dfs, ignore_index=True)\n",
    "    \n",
    "    # Create plot using hvplot\n",
    "    plot = combined_df.hvplot.line(\n",
    "        x='datetime',\n",
    "        y='cumsum_vol',\n",
    "        by='lake_name',\n",
    "        width=800,\n",
    "        height=400,\n",
    "        title='Lake volume changes over time',\n",
    "        xlabel='Date',\n",
    "        ylabel='Cumulative volume change (km³)',\n",
    "        grid=True,\n",
    "        legend=False\n",
    "    )\n",
    "    \n",
    "    return plot\n",
    "\n",
    "# Usage:\n",
    "plot = create_interactive_plot(evolving_directory, stationary_directory)\n",
    "plot  # Display in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f72dc8-bf10-418f-b2ee-b6f7e112a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the lakes driving the deviation of evolving outlines and updated stationary outlines\n",
    "\n",
    "evolving_directory = 'output/geometric_calcs/evolving_outlines_geom_calc/forward_fill'\n",
    "stationary_directory = 'output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes'\n",
    "\n",
    "def process_lake_data(directory, is_evolving=True):\n",
    "    dfs = {}\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        # Exclude continental summation files\n",
    "        if \"subset\" in file or \"superset\" in file:\n",
    "            continue\n",
    "\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            lake_name = os.path.splitext(file)[0]\n",
    "            \n",
    "            # Process each lake's data\n",
    "            df['lake_name'] = lake_name\n",
    "            df['datetime'] = pd.to_datetime(df['mid_pt_datetime'])\n",
    "            \n",
    "            # Calculate cumulative volume based on directory type\n",
    "            if is_evolving:\n",
    "                df['cumsum_vol'] = np.cumsum(df['evolving_outlines_dV_corr (m^3)'])\n",
    "            else:\n",
    "                df['cumsum_vol'] = np.cumsum(df['stationary_outline_dV_corr (m^3)'])\n",
    "            \n",
    "            dfs[lake_name] = df\n",
    "    \n",
    "    return dfs\n",
    "\n",
    "def create_interactive_plot(evolving_directory, stationary_directory):\n",
    "    # Load data from both directories\n",
    "    evolving_dfs = process_lake_data(evolving_directory, is_evolving=True)\n",
    "    stationary_dfs = process_lake_data(stationary_directory, is_evolving=False)\n",
    "    \n",
    "    # Initialize lists to store processed dataframes\n",
    "    plot_dfs = []\n",
    "    \n",
    "    # Process common lakes\n",
    "    common_lakes = set(evolving_dfs.keys()) & set(stationary_dfs.keys())\n",
    "    for lake_name in common_lakes:\n",
    "        evolving_df = evolving_dfs[lake_name].copy()\n",
    "        stationary_df = stationary_dfs[lake_name].copy()\n",
    "        \n",
    "        # Calculate difference (evolving - stationary)\n",
    "        merged_df = pd.merge(\n",
    "            evolving_df[['datetime', 'cumsum_vol']], \n",
    "            stationary_df[['datetime', 'cumsum_vol']], \n",
    "            on='datetime', \n",
    "            suffixes=('_evolving', '_stationary')\n",
    "        )\n",
    "        merged_df['cumsum_vol'] = merged_df['cumsum_vol_evolving'] - merged_df['cumsum_vol_stationary']\n",
    "        merged_df['lake_name'] = lake_name + '_difference'\n",
    "        plot_dfs.append(merged_df[['datetime', 'cumsum_vol', 'lake_name']])\n",
    "    \n",
    "    # Process lakes only in stationary directory\n",
    "    stationary_only = set(stationary_dfs.keys()) - set(evolving_dfs.keys())\n",
    "    for lake_name in stationary_only:\n",
    "        df = stationary_dfs[lake_name].copy()\n",
    "        df['lake_name'] = lake_name + '_stationary'\n",
    "        plot_dfs.append(df[['datetime', 'cumsum_vol', 'lake_name']])\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(plot_dfs, ignore_index=True)\n",
    "    \n",
    "    # Create plot using hvplot\n",
    "    plot = combined_df.hvplot.line(\n",
    "        x='datetime',\n",
    "        y='cumsum_vol',\n",
    "        by='lake_name',\n",
    "        width=800,\n",
    "        height=400,\n",
    "        title='Lake volume changes over time',\n",
    "        xlabel='Date',\n",
    "        ylabel='Cumulative volume change (km³)',\n",
    "        grid=True,\n",
    "        legend=False\n",
    "    )\n",
    "    \n",
    "    return plot\n",
    "\n",
    "# Usage:\n",
    "plot = create_interactive_plot(evolving_directory, stationary_directory)\n",
    "plot  # Display in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5780e9-bca9-4c14-b53a-faeef1d88a62",
   "metadata": {},
   "source": [
    "### Comparison to SMB trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5930d092-1c09-41f7-85f0-2b43ae80c4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ef1f7ba-529b-44f2-a6ea-db473f951313",
   "metadata": {},
   "source": [
    "# Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e6519b-1691-49eb-8147-4a9c1c8fb171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stationary subglacial lake outlines\n",
    "stationary_lakes_gdf = gpd.read_file(os.path.join(os.getcwd(), 'output/lake_outlines/stationary_outlines/stationary_outlines_gdf.geojson'))\n",
    "\n",
    "# Create filtered geodataframes of lakes based on whether they have evolving outlines\n",
    "folder_path = os.path.join ('output/lake_outlines/evolving_outlines')\n",
    "\n",
    "# Lakes with evolving outlines (.geojson)\n",
    "evolving_outlines_lakes = filter_gdf_by_folder_contents(stationary_lakes_gdf, folder_path, file_extension='geojson', exclude=False)\n",
    "\n",
    "# For the evolving_outlines_lakes, we must add the special case of Site_B_Site_C that are now a combined lake\n",
    "include_list = ['Site_B', 'Site_C']\n",
    "included_rows = stationary_lakes_gdf[stationary_lakes_gdf['name'].isin(include_list)]\n",
    "evolving_outlines_lakes = pd.concat([evolving_outlines_lakes, included_rows]).drop_duplicates()\n",
    "print('lakes with evolving outlines:',len(evolving_outlines_lakes))\n",
    "\n",
    "# Lakes with non-dynamic outlines (.txt)\n",
    "no_evolving_outlines_lakes = filter_gdf_by_folder_contents(stationary_lakes_gdf, folder_path, file_extension='txt', exclude=False)\n",
    "print('lakes without evolving outlines:',len(no_evolving_outlines_lakes))\n",
    "\n",
    "# All analyzed lakes within SARIn mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18111948-8866-4cdc-97a2-57072245c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_evolving_outlines_lakes.name.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e4e7e-6175-4eda-b85b-944f4d970fde",
   "metadata": {},
   "source": [
    "## Fig. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d07985-5934-4208-a4df-1d4bc1c73b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.family': 'DejaVu Sans',\n",
    "    'font.size': 9,\n",
    "    'axes.labelsize': 22,\n",
    "    'axes.titlesize': 9,\n",
    "    'xtick.labelsize': 14,\n",
    "    'ytick.labelsize': 14,\n",
    "    'legend.fontsize': 12,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d325c8-bf5f-4d59-8d01-3dbb9099f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolen arg on whether to use the forward filled version of the evolving outlines\n",
    "forward_fill = True\n",
    "# forward_fill = False\n",
    "\n",
    "# Select lakes to be included in plot\n",
    "selected_lakes = stationary_outlines_gdf[stationary_outlines_gdf['name'].isin(['ConwaySubglacialLake', 'David_s1', 'Slessor_23'])]\n",
    "desired_order = ['ConwaySubglacialLake', 'David_s1', 'Slessor_23']\n",
    "stationary_outlines_gdf_filtered = gpd.GeoDataFrame(pd.concat([selected_lakes[selected_lakes['name'] == name] for name in desired_order]))\n",
    "\n",
    "# Number of rows and columns\n",
    "nrows, ncols = 4, 3\n",
    "\n",
    "# Create a 4x3 grid of plots (4 metrics, 3 lakes per metric)\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 20), constrained_layout=True)\n",
    "\n",
    "# Define the display names for lakes\n",
    "lake_names = ['Conway Subglacial Lake', r'David$_{\\text{s1}}$', 'Slessor$_{23}$']\n",
    "\n",
    "# Add titles to the top row of subplots\n",
    "for col, title in enumerate(lake_names):\n",
    "    axs[0, col].set_title(title, fontsize=18, pad=12)\n",
    "\n",
    "# Define color that will be reused\n",
    "stationary_outline_color  = 'darkturquoise'\n",
    "\n",
    "for row in range(1, nrows):\n",
    "    # Share y-axis within each row but not between rows\n",
    "    for col in range(ncols):\n",
    "        axs[row, col].sharey(axs[row, 0])\n",
    "\n",
    "# Pick colormap and normalize to cyc_start_datetimes\n",
    "cmap = plt.get_cmap('plasma', len(cyc_start_datetimes))\n",
    "norm = plt.Normalize(mdates.date2num(cyc_start_datetimes[1]), \n",
    "                    mdates.date2num(cyc_end_datetimes[-1]))\n",
    "\n",
    "for idx, (lake_idx, lake) in enumerate(stationary_outlines_gdf_filtered.iterrows()):\n",
    "    # Select the row by index and convert it to a GeoDataFrame\n",
    "    lake_gdf = stationary_outlines_gdf_filtered.loc[[lake_idx]]\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    stationary_outline = lake_gdf['geometry']\n",
    "    print(f\"\\nProcessing lake: {lake_name}\")\n",
    "    \n",
    "    # Attempt to open the evolving outlines GeoJSON file\n",
    "    try:\n",
    "        evolving_outlines_orig_gdf = gpd.read_file('output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name))\n",
    "    except fiona.errors.DriverError:\n",
    "        print(f\"File for {lake_name} not found. Skipping...\")\n",
    "        continue  # Skip the rest of the function if the file doesn't exist\n",
    "    \n",
    "    # Attempt to open the geometric calculations CSV files\n",
    "    try:\n",
    "        if forward_fill==True: \n",
    "            evolving_geom_calcs_orig_df = pd.read_csv('output/geometric_calcs/evolving_outlines_geom_calc/{}.csv'.format(lake_name))\n",
    "            evolving_geom_calcs_df = pd.read_csv('output/geometric_calcs/evolving_outlines_geom_calc/forward_fill/{}.csv'.format(lake_name))\n",
    "        elif forward_fill==False:\n",
    "            print('using not forward fill')\n",
    "            evolving_geom_calcs_df = pd.read_csv('output/geometric_calcs/evolving_outlines_geom_calc/{}.csv'.format(lake_name))\n",
    "        evolving_union_geom_calcs_df = pd.read_csv('output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/{}.csv'.format(lake_name))\n",
    "        stationary_geom_calcs_df = pd.read_csv('output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes/{}.csv'.format(lake_name))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"At least of of the geometric calculations CSV files for {lake_name} not found. Skipping...\")\n",
    "        continue  # Skip the rest of the function if the file doesn't exist\n",
    "    \n",
    "    # Convert of strings to datetime\n",
    "    evolving_geom_calcs_df['mid_pt_datetime'] = pd.to_datetime(evolving_geom_calcs_df['mid_pt_datetime'])\n",
    "\n",
    "    \n",
    "    # Panel - evolving outlines ------------------------------------------------------\n",
    "    \n",
    "    # Find evolving and stationary outlines union for plotting extent\n",
    "    evolving_stationary_union_gdf = gpd.GeoDataFrame(\n",
    "        geometry=[lake_gdf.geometry.iloc[0].union(evolving_outlines_orig_gdf.geometry.union_all())],\n",
    "        crs=lake_gdf.crs)\n",
    "    x_min, y_min, x_max, y_max = evolving_stationary_union_gdf.bounds.iloc[0]\n",
    "\n",
    "    # Make plots a uniform size\n",
    "    # Make x_min, y_min, x_max, and y_max define a square area centered at the original midpoints\n",
    "    # Calculate the midpoints of the current bounds\n",
    "    x_mid = (x_min + x_max) / 2\n",
    "    y_mid = (y_min + y_max) / 2\n",
    "    \n",
    "    # Calculate the current spans of the x and y dimensions\n",
    "    x_span = x_max - x_min\n",
    "    y_span = y_max - y_min\n",
    "    \n",
    "    # Determine the maximum span to ensure square dimensions\n",
    "    max_span = max(x_span, y_span)\n",
    "    \n",
    "    # Update the min and max values to match the new span, keeping the midpoint the same\n",
    "    x_min = x_mid - max_span / 2\n",
    "    x_max = x_mid + max_span / 2\n",
    "    y_min = y_mid - max_span / 2\n",
    "    y_max = y_mid + max_span / 2\n",
    "    \n",
    "    buffer_frac = 0.35\n",
    "    x_buffer = abs(x_max-x_min)*buffer_frac\n",
    "    y_buffer = abs(y_max-y_min)*buffer_frac\n",
    "\n",
    "    # Plot MOA surface imagery\n",
    "    mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "    mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "    moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "    axs[0,idx].imshow(moa_highres_da_subset[0,:,:], cmap='gray', clim=[14000, 17000], extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "    \n",
    "    # Use for loop to store each time slice as line segment to use in legend\n",
    "    # And plot each evolving outline in the geodataframe color by date\n",
    "    lines = []  # list of lines to be used for the legend\n",
    "    for dt_idx, dt in enumerate(cyc_mid_pt_datetimes[0:-1]):\n",
    "        x = 1; y = 1\n",
    "        line, = axs[0,idx].plot(x, y, color=cmap(norm(mdates.date2num(cyc_mid_pt_datetimes[dt_idx]))))\n",
    "        lines.append(line)\n",
    "        \n",
    "        # Filter rows that match the current time step\n",
    "        evolving_outlines_gdf_dt_sub = evolving_outlines_orig_gdf[evolving_outlines_orig_gdf['mid_pt_datetime'] == dt]\n",
    "    \n",
    "        # Plotting the subset if not empty\n",
    "        if not evolving_outlines_gdf_dt_sub.empty:\n",
    "            evolving_outlines_gdf_dt_sub.boundary.plot(ax=axs[0,idx], \n",
    "                color=cmap(norm(mdates.date2num(cyc_mid_pt_datetimes[dt_idx]))), linewidth=1)\n",
    "    \n",
    "    # Plot stationary outline\n",
    "    stationary_outlines_gdf['geometry'].boundary.plot(ax=axs[0,idx], color=stationary_outline_color, linewidth=2)\n",
    "\n",
    "    # Import evolving_outlines_union_gdf and plot\n",
    "    evolving_union_gdf = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]\n",
    "    evolving_union_gdf.boundary.plot(ax=axs[0,idx], color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "    # Plot inset map\n",
    "    axIns = axs[0,idx].inset_axes([0.01, -0.01, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "    axIns.set_aspect('equal')\n",
    "    moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    axIns.axis('off')\n",
    "    # Plot star to indicate location\n",
    "    axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "        linewidth=2, color='k', s=30, zorder=3)\n",
    "\n",
    "    # Change polar stereographic m to km\n",
    "    km_scale = 1e3\n",
    "    ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    axs[0,idx].xaxis.set_major_formatter(ticks_x)\n",
    "    ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    axs[0,idx].yaxis.set_major_formatter(ticks_y)\n",
    "    \n",
    "    # Set axes limits\n",
    "    axs[0,idx].set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "\n",
    "    # Panel - Active area ---------------------------------------------\n",
    "    \n",
    "    # Plot horizontal zero line for reference\n",
    "    axs[1,idx].axhline(0, color='k', linewidth=1)\n",
    "    \n",
    "    # Plot stationary outline and evolving outlines unary union areas\n",
    "    axs[1,idx].axhline(np.divide(lake_gdf['area (m^2)'], 1e6).values, \n",
    "        color=stationary_outline_color, linestyle='solid', linewidth=2)\n",
    "    axs[1,idx].axhline(np.divide(evolving_union_gdf['area (m^2)'].iloc[0], 1e6), \n",
    "        color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "    # Plot evolving outlines\n",
    "    x = mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime'])\n",
    "    y = np.divide(evolving_geom_calcs_df['evolving_outlines_area (m^2)'], 1e6)\n",
    "    \n",
    "    # Create points and segments for LineCollection\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    \n",
    "    # Create a LineCollection, using the discrete colormap and norm\n",
    "    lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "    lc.set_array(x)\n",
    "    lc.set_linewidth(2)\n",
    "\n",
    "    # Plot multi-colored line and scatter for data points\n",
    "    line = axs[1,idx].add_collection(lc)\n",
    "    # Only plot data points of original evolving outlines (no forward-filled data points)\n",
    "    y = np.divide(evolving_geom_calcs_orig_df['evolving_outlines_area (m^2)'], 1e6)\n",
    "    scatter = axs[1,idx].scatter(x, y, c=x, cmap=cmap, s=9, norm=norm, zorder=2)\n",
    "    \n",
    "    # Get the maximum y value across all data for this lake\n",
    "    if idx == 0:\n",
    "        max_y = max(\n",
    "            np.divide(lake_gdf['area (m^2)'], 1e6).values[0],\n",
    "            np.divide(evolving_union_gdf['area (m^2)'], 1e6).values[0],\n",
    "            np.divide(evolving_geom_calcs_df['evolving_outlines_area (m^2)'], 1e6).max()\n",
    "        )\n",
    "    \n",
    "    # Set y limit with padding above the maximum value to avoid data plotting behind legend\n",
    "    axs[1,idx].set_ylim(bottom=None, top=max_y * 1.3)\n",
    "    \n",
    "    # Panel - Cumulative dh/dt -------------------------------------------------------\n",
    "    \n",
    "    # Plot horizontal zero line for reference\n",
    "    axs[2,idx].axhline(0, color='k', linewidth=1)\n",
    "\n",
    "    # Plot stationary outlines off-lake secular dh\n",
    "    axs[2,idx].plot(mdates.date2num(stationary_geom_calcs_df['mid_pt_datetime']),\n",
    "        np.cumsum(stationary_geom_calcs_df['stationary_outline_region_dh (m)']),\n",
    "        color='lightgray', marker='o', markerfacecolor='w', markersize=3, linewidth=2)\n",
    "\n",
    "    # Plot evolving outlines off-lake secular dh\n",
    "    axs[2,idx].plot(mdates.date2num(evolving_union_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.cumsum(evolving_union_geom_calcs_df['stationary_outline_region_dh (m)']), color='dimgray', \n",
    "            marker='o', markersize=3, linewidth=2)\n",
    "    \n",
    "    # Plot stationary outline time series\n",
    "    axs[2,idx].plot(mdates.date2num(stationary_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.cumsum(stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "            color=stationary_outline_color, marker='o', markerfacecolor='w', markersize=3, linewidth=2)\n",
    "\n",
    "    # Plot evolving outlines time series\n",
    "    x = mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime'])\n",
    "    y = np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'])\n",
    "    \n",
    "    # Create points and segments for LineCollection\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    \n",
    "    # Create a LineCollection, using the discrete colormap and norm\n",
    "    lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "    lc.set_array(x)\n",
    "    lc.set_linewidth(2)\n",
    "\n",
    "    # Plot multi-colored line and scatter for data points\n",
    "    line = axs[2,idx].add_collection(lc)\n",
    "    scatter = axs[2,idx].scatter(x, y, c=x, cmap=cmap, s=9, norm=norm, zorder=2)\n",
    "\n",
    "    # Plot evolving outlines union outline time series\n",
    "    axs[2,idx].plot(mdates.date2num(evolving_union_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.cumsum(evolving_union_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "            color='k', marker='o', markersize=3, linewidth=2)\n",
    "    \n",
    "    # Plot bias (evolving - prior stationary)\n",
    "    axs[2,idx].plot(mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime']),\n",
    "        np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'] - \n",
    "        stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "        color='red', marker='o', markerfacecolor='w', markersize=3, linewidth=2)\n",
    "\n",
    "    # Plot bias (evolving - updated stationary)\n",
    "    axs[2,idx].plot(mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime']),\n",
    "        np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'] - \n",
    "        evolving_union_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "        color='darkred', marker='o', markersize=3, linewidth=2)\n",
    "\n",
    "    axs[2,idx].set_ylim(bottom=-20.5, top=20.5)\n",
    "\n",
    "    # Panel - Cumulative dV/dt --------------------------------------------------\n",
    "    \n",
    "    # Plot horizontal line at zero for reference\n",
    "    axs[3,idx].axhline(0, color='k', linewidth=0.5)\n",
    "    \n",
    "    # Plot stationary outline time series\n",
    "    axs[3,idx].plot(mdates.date2num(stationary_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.divide(np.cumsum(stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "        color=stationary_outline_color, marker='o', markerfacecolor='w', markersize=3, linewidth=2)\n",
    "\n",
    "    # Plot evolving outlines time series\n",
    "    x = mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime'])\n",
    "    y = np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "    \n",
    "    # Create points and segments for LineCollection\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    \n",
    "    # Create a LineCollection, using the discrete colormap and norm\n",
    "    lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "    lc.set_array(x)\n",
    "    lc.set_linewidth(2)\n",
    "\n",
    "    # Plot multi-colored line and scatter for data points\n",
    "    line = axs[3,idx].add_collection(lc)\n",
    "    scatter = axs[3,idx].scatter(x, y, c=x, cmap=cmap, s=9, norm=norm, zorder=2)\n",
    "\n",
    "    # Plot evolving outlines union outline time series\n",
    "    axs[3,idx].plot(mdates.date2num(evolving_union_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.cumsum(np.divide(evolving_union_geom_calcs_df['stationary_outline_dV_corr (m^3)'], 1e9)), \n",
    "            color='k', marker='o', markersize=3, linewidth=2)\n",
    "    \n",
    "    # Plot bias (evolving - prior stationary)\n",
    "    axs[3,idx].plot(mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime']),\n",
    "        np.cumsum(np.divide((evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "            stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9)), \n",
    "            color='red', marker='o', markerfacecolor='w', markersize=3, linewidth=2)\n",
    "\n",
    "    # Plot bias (evolving - updated stationary)\n",
    "    axs[3,idx].plot(mdates.date2num(evolving_union_geom_calcs_df['mid_pt_datetime']),\n",
    "        np.cumsum(np.divide((evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "            evolving_union_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9)), \n",
    "            color='darkred', marker='o', markersize=3, linewidth=2)\n",
    "\n",
    "    axs[3,idx].set_ylim(bottom=-3.5, top=3.5)\n",
    "\n",
    "# Add colorbar, legends, and titles\n",
    "idx=0  # Add colorbar and legends only to first row of plots\n",
    "\n",
    "# Set up colormap\n",
    "min_date = pd.to_datetime(cyc_start_datetimes[0])\n",
    "max_date = pd.to_datetime(cyc_end_datetimes[-1])\n",
    "n_dates = len(cyc_mid_pt_datetimes[:-1])\n",
    "date_range = pd.date_range(min_date, max_date, periods=n_dates)\n",
    "years = date_range.year.unique()\n",
    "years = pd.to_datetime(years, format='%Y')\n",
    "cmap = plt.get_cmap('plasma', n_dates)\n",
    "norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "m.set_array(np.linspace(mdates.date2num(min_date), mdates.date2num(max_date), n_dates))\n",
    "\n",
    "# Add colorbar\n",
    "cax = inset_axes(axs[0,idx],\n",
    "                 width='67%',\n",
    "                 height='3%',\n",
    "                 loc='lower left',\n",
    "                 bbox_to_anchor=[0.31, 0.15, 1, 1],  # [left, bottom, width, height]\n",
    "                 bbox_transform=axs[0,idx].transAxes,\n",
    "                 borderpad=0)\n",
    "cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "cbar.set_label('year', fontsize=16, labelpad=4)\n",
    "\n",
    "# Set ticks for all years but labels only for even years, skipping first year, 2010.0, as it starts before time series\n",
    "tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "tick_labels = [f\"'{date.strftime('%y')}\" if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "cbar.set_ticks(tick_locations)\n",
    "cbar.set_ticklabels(tick_labels)\n",
    "cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Add minor ticks for quarters\n",
    "\n",
    "# Add legends\n",
    "# Define colors and linestyles that will be reused and create lines for legend\n",
    "stationary_outline_color  = 'turquoise'\n",
    "stationary_line = plt.Line2D([], [], color=stationary_outline_color, linestyle='solid', linewidth=2)\n",
    "# Full colormap for evolving outlines with one representative data point\n",
    "mid_idx = len(cyc_datetimes['cyc_start_datetimes']) // 2\n",
    "evolving_scatter_line = []\n",
    "for i, dt in enumerate(cyc_datetimes['cyc_start_datetimes']):\n",
    "    if i == mid_idx:  # only the middle line gets a marker\n",
    "        evolving_scatter_line.append(\n",
    "            Line2D([0], [0],\n",
    "                   color=cmap(norm(mdates.date2num(dt))),\n",
    "                   linestyle='solid',\n",
    "                   marker='o', markersize=5))\n",
    "    else:\n",
    "        evolving_scatter_line.append(\n",
    "            Line2D([0], [0],\n",
    "                   color=cmap(norm(mdates.date2num(dt))),\n",
    "                   linestyle='solid'))\n",
    "evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=2)\n",
    "stationary_scatter_line = plt.Line2D([], [], color=stationary_outline_color, marker='o', markersize=5, markerfacecolor='white', linestyle='solid', linewidth=2)\n",
    "evolving_union_scatter_line = plt.Line2D([], [], color='k', marker='o', markersize=5, linewidth=2)\n",
    "bias = plt.Line2D([], [], color='red', marker='o', markersize=5, markerfacecolor='white', linewidth=2)  # evolving - prior stationary (all lakes)\n",
    "bias2 = plt.Line2D([], [], color='darkred', marker='o', markersize=5, linewidth=2)  # evolving - updated stationary (only evolving lakes)\n",
    "stationary_secular_scatter_line = plt.Line2D([], [], color='lightgray', marker='o', markerfacecolor='w', markersize=5, linewidth=2)\n",
    "evolving_secular_scatter_line = plt.Line2D([], [], color='dimgray', marker='o', markersize=5,  linewidth=2)\n",
    "\n",
    "legend = axs[0,idx].legend(\n",
    "    [tuple(lines), \n",
    "     stationary_line,\n",
    "     evolving_union_line], \n",
    "    ['evolving outlines',\n",
    "     'prior stationary outline',\n",
    "     'updated stationary outline'],\n",
    "     handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "     loc='upper center')\n",
    "\n",
    "legend = axs[1,idx].legend(\n",
    "    [tuple(evolving_scatter_line),\n",
    "     stationary_line, \n",
    "     evolving_union_line],\n",
    "    ['evolving outlines',\n",
    "     'prior stationary outline', \n",
    "     'updated stationary outline'], \n",
    "    handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper center')\n",
    "\n",
    "legend = axs[2,idx].legend(\n",
    "    [stationary_secular_scatter_line,\n",
    "     evolving_secular_scatter_line,\n",
    "     tuple(evolving_scatter_line),   \n",
    "     stationary_scatter_line,  \n",
    "     evolving_union_scatter_line, \n",
    "     bias, bias2],\n",
    "    ['prior stationary off-lake secular',\n",
    "     'updated stationary off-lake secular',\n",
    "     'evolving outlines',\n",
    "     'prior stationary outline', \n",
    "     'updated stationary outline', \n",
    "     'bias (evolving − prior stationary)',\n",
    "     'bias (evolving − updated stationary)'],\n",
    "     handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "     loc='lower center')\n",
    "\n",
    "legend = axs[3,idx].legend(\n",
    "    [tuple(evolving_scatter_line),\n",
    "     stationary_scatter_line,\n",
    "     evolving_union_scatter_line,\n",
    "     bias, bias2],\n",
    "    ['evolving outlines',\n",
    "     'prior stationary outline',\n",
    "     'updated stationary outline', \n",
    "     'bias (evolving − prior stationary)',\n",
    "     'bias (evolving − updated stationary)'], \n",
    "    handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='lower center')\n",
    "\n",
    "# Set common font sizes and axis labels\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        # Set tick sizes for all plots\n",
    "        axs[i,j].tick_params(axis='both')\n",
    "\n",
    "        # Add subplot labels (a, b, c, etc.)\n",
    "        axs[i,j].text(0.03, 0.97, chr(97 + i*ncols + j), transform=axs[i,j].transAxes, \n",
    "                      fontsize=22, va='top', ha='left')\n",
    "\n",
    "        # Configure row-specific settings\n",
    "        if i == 0:\n",
    "            axs[i,j].set_xlabel('x [km]')\n",
    "        if i == 3:\n",
    "            axs[i,j].set_xlabel('year')\n",
    "        if 0 < i < 4:\n",
    "            axs[i,j].xaxis.set_major_formatter(year_interval_formatter())\n",
    "            axs[i,j].xaxis.set_major_locator(mdates.YearLocator())\n",
    "            axs[i,j].xaxis.set_minor_locator(mdates.MonthLocator([1, 4, 7, 10]))\n",
    "        if j == 0:  # Leftmost column labels\n",
    "            y_labels = ['y [km]', 'lakebed active area [km$^2$]', 'cumulative $dh$ [m]', 'cumulative $dV$ [km$^3$]']\n",
    "            axs[i,j].set_ylabel(y_labels[i])\n",
    "        # Do not display redundant tick labels\n",
    "        if 0 < i < 3:\n",
    "            axs[i,j].set_xticklabels([])\n",
    "        if i > 0:\n",
    "            # Set x-axis limits\n",
    "            axs[i,j].set(xlim=(cyc_datetimes['cyc_start_datetimes'].iloc[0],\n",
    "               # Set righthand x-axis limit slightly earlier to prevent tick mark displaying when there is no data point\n",
    "               (cyc_datetimes['cyc_end_datetimes'].iloc[-1] - datetime.timedelta(days=2))))\n",
    "            if j > 0:\n",
    "                axs[i,j].tick_params(axis='y', which='both', labelleft=False)\n",
    "            else:\n",
    "                axs[i,j].tick_params(axis='y', which='both', labelleft=True)\n",
    "\n",
    "# Clear output\n",
    "clear_output()\n",
    "\n",
    "# Save and close plot\n",
    "plt.savefig(OUTPUT_DIR + '/figures/Fig2_lake_reexamination_results.jpg', dpi=500, bbox_inches='tight')\n",
    "\n",
    "# Preview plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868953d7-e5d4-4db5-99af-80f72fce3e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1878d127-ff77-4333-a01c-e13a9da6e8c4",
   "metadata": {},
   "source": [
    "## Fig. 3\n",
    "NOTE: You must run \n",
    "* \"Dissolved inorganic carbon (DIC) export estimates\" section of code earlier in notebook\n",
    "* cell at beginning of Figures section\n",
    "\n",
    "for necessary plotting variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e5106-5ae0-425c-ba11-b7324dd0dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stationary subglacial lake outlines\n",
    "stationary_lakes_gdf = gpd.read_file(os.path.join(os.getcwd(), 'output/lake_outlines/stationary_outlines/stationary_outlines_gdf.geojson'))\n",
    "\n",
    "# Create filtered geodataframes of lakes based on whether they have evolving outlines\n",
    "folder_path = os.path.join ('output/lake_outlines/evolving_outlines')\n",
    "\n",
    "# Lakes with evolving outlines (.geojson)\n",
    "evolving_outlines_lakes = filter_gdf_by_folder_contents(stationary_lakes_gdf, folder_path, file_extension='geojson', exclude=False)\n",
    "\n",
    "# For the evolving_outlines_lakes, we must add the special case of Site_B_Site_C that are now a combined lake\n",
    "include_list = ['Site_B', 'Site_C']\n",
    "included_rows = stationary_lakes_gdf[stationary_lakes_gdf['name'].isin(include_list)]\n",
    "evolving_outlines_lakes = pd.concat([evolving_outlines_lakes, included_rows]).drop_duplicates()\n",
    "print('lakes with evolving outlines:',len(evolving_outlines_lakes))\n",
    "\n",
    "# Lakes with non-dynamic outlines (.txt)\n",
    "no_evolving_outlines_lakes = filter_gdf_by_folder_contents(stationary_lakes_gdf, folder_path, file_extension='txt', exclude=False)\n",
    "print('lakes without evolving outlines:',len(no_evolving_outlines_lakes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0729a16a-928c-45b1-b35b-ca991ad313a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.family': 'DejaVu Sans',\n",
    "    'font.size': 11,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 11,\n",
    "    'ytick.labelsize': 11,\n",
    "    'legend.fontsize':10,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef9c3f8-3167-48db-b44d-ae6b03dd5e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "# Read in continental summation geometric calculation csv files - evolving outlines (only lakes with evolving outlines)\n",
    "base_path = 'output/geometric_calcs/evolving_outlines_geom_calc/forward_fill'\n",
    "filenames = {\n",
    "    'superset_IS2_lakes_sum': 'superset_IS2_lakes_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPreExpansion_sum': 'subset_CS2_IS2_lakes_SARInPreExpansion_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPostExpansion_sum': 'subset_CS2_IS2_lakes_SARInPostExpansion_sum.csv',\n",
    "}\n",
    "\n",
    "dfs = {name: pd.read_csv(f'{base_path}/{filename}') \n",
    "       for name, filename in filenames.items()}\n",
    "\n",
    "# Store dataframes from dfs list for code readability\n",
    "superset_IS2_evolving_sum_df = dfs['superset_IS2_lakes_sum']\n",
    "subset_CS2_IS2_SARInPreExpansion_evolving_sum_df = dfs['subset_CS2_IS2_lakes_SARInPreExpansion_sum']\n",
    "subset_CS2_IS2_SARInPostExpansion_evolving_sum_df = dfs['subset_CS2_IS2_lakes_SARInPostExpansion_sum']\n",
    "\n",
    "# Read in continental summation geometric calculation csv files - stationary outlines (all lakes)\n",
    "base_path = 'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes'\n",
    "filenames = {'superset_IS2_lakes_sum': 'superset_IS2_lakes_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPreExpansion_sum': 'subset_CS2_IS2_lakes_SARInPreExpansion_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPostExpansion_sum': 'subset_CS2_IS2_lakes_SARInPostExpansion_sum.csv'}\n",
    "\n",
    "dfs = {name: pd.read_csv(f'{base_path}/{filename}') \n",
    "       for name, filename in filenames.items()}\n",
    "\n",
    "superset_IS2_stationary_sum_df = dfs['superset_IS2_lakes_sum']\n",
    "subset_CS2_IS2_SARInPreExpansion_stationary_sum_df = dfs['subset_CS2_IS2_lakes_SARInPreExpansion_sum']\n",
    "subset_CS2_IS2_SARInPostExpansion_stationary_sum_df = dfs['subset_CS2_IS2_lakes_SARInPostExpansion_sum']\n",
    "\n",
    "# Read in continental summation geometric calculation csv files - stationary outlines (only at evolving lakes)\n",
    "base_path = 'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes'\n",
    "filenames = {\n",
    "    'superset_IS2_lakes_sum': 'superset_IS2_lakes_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPreExpansion_sum': 'subset_CS2_IS2_lakes_SARInPreExpansion_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPostExpansion_sum': 'subset_CS2_IS2_lakes_SARInPostExpansion_sum.csv',\n",
    "}\n",
    "\n",
    "dfs = {name: pd.read_csv(f'{base_path}/{filename}')\n",
    "       for name, filename in filenames.items()}\n",
    "\n",
    "superset_IS2_stationary_subset_sum_df = dfs['superset_IS2_lakes_sum']\n",
    "subset_CS2_IS2_SARInPreExpansion_stationary_subset_sum_df = dfs['subset_CS2_IS2_lakes_SARInPreExpansion_sum']\n",
    "subset_CS2_IS2_SARInPostExpansion_stationary_subset_sum_df = dfs['subset_CS2_IS2_lakes_SARInPostExpansion_sum']\n",
    "\n",
    "# Read in continental summation geometric calculation csv files - evolving union (only lakes with evolving outlines)\n",
    "base_path = 'output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes'\n",
    "filenames = {'superset_IS2_lakes_sum': 'superset_IS2_lakes_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPreExpansion_sum': 'subset_CS2_IS2_lakes_SARInPreExpansion_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPostExpansion_sum': 'subset_CS2_IS2_lakes_SARInPostExpansion_sum.csv'}\n",
    "\n",
    "dfs = {name: pd.read_csv(f'{base_path}/{filename}')\n",
    "       for name, filename in filenames.items()}\n",
    "\n",
    "superset_IS2_evolving_union_sum_df = dfs['superset_IS2_lakes_sum']\n",
    "subset_CS2_IS2_SARInPreExpansion_evolving_union_sum_df = dfs['subset_CS2_IS2_lakes_SARInPreExpansion_sum']\n",
    "subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df = dfs['subset_CS2_IS2_lakes_SARInPostExpansion_sum']\n",
    "\n",
    "\n",
    "# Setup figure\n",
    "nrows, ncols = 4, 1\n",
    "fig, ax = plt.subplots(nrows, ncols, gridspec_kw={'height_ratios': [2.5, 2.5, 3, 3]}, sharex=True, figsize=(10, 12), constrained_layout=True)\n",
    "\n",
    "# Store dates and time period for satellite coverage eras\n",
    "time_span = mdates.date2num(cyc_datetimes['cyc_end_datetimes'].iloc[-1]) - mdates.date2num(cyc_datetimes['cyc_start_datetimes'].iloc[0])\n",
    "start_date = mdates.date2num(cyc_datetimes['cyc_start_datetimes'].iloc[0])\n",
    "SARIn_expand_date = mdates.date2num(cyc_datetimes[cyc_datetimes['cyc_start_datetimes'] == '2013-10-01T18:00:00.000000000']['cyc_start_datetimes'].iloc[0])\n",
    "CS2_IS2_tie_pt = mdates.date2num(cyc_datetimes[cyc_datetimes['dataset'] == 'ICESat2_ATL15'].iloc[0]['cyc_start_datetimes'])\n",
    "\n",
    "for row in [0,1,2,3]:\n",
    "    # Plot horizontal line at zero for reference\n",
    "    ax[row].axhline(0, color='k', linewidth=0.5)\n",
    "    # Plot vertical lines to indicate CS2 SARIn mode mask moving inland and ICESat-2 era start\n",
    "    ax[row].axvline(SARIn_expand_date, color='dimgray', linestyle='solid', linewidth=0.75, ymin=-1, ymax=1, zorder=0)\n",
    "    ax[row].axvline(CS2_IS2_tie_pt, color='dimgray', linestyle='solid', linewidth=0.75, ymin=-1, ymax=1, zorder=0)\n",
    "\n",
    "# Pick colormap and make continuous cmap discrete for evolving outlines\n",
    "cmap = plt.get_cmap('plasma', len(cyc_start_datetimes[1:]))\n",
    "norm = plt.Normalize(mdates.date2num(cyc_start_datetimes[1]), \n",
    "                    mdates.date2num(cyc_start_datetimes[-1]))\n",
    "\n",
    "# Use for loop to store each time step as line segment to use in legend\n",
    "lines = []  # list of lines to be used for the legend\n",
    "for dt_idx, dt in enumerate(cyc_datetimes['cyc_start_datetimes'][1:]):\n",
    "    x = 1; y = 1\n",
    "    line, = ax[0].plot(x, y, color=cmap(norm(mdates.date2num(cyc_datetimes['cyc_start_datetimes'][dt_idx]))))\n",
    "    lines.append(line)\n",
    "\n",
    "# Define colors and linestyles that will be reused and create lines for legend\n",
    "stationary_outline_color  = 'turquoise'\n",
    "stationary_outline_subset_color  = 'darkcyan'\n",
    "\n",
    "# Panel - Lake active area ---------------------------------------------\n",
    "\n",
    "# Plot evolving outlines time series (CryoSat-2 observation period) \n",
    "x = mdates.date2num(subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['mid_pt_datetime'])\n",
    "y = subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['evolving_outlines_area (m^2)'] / 1e6\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(1)\n",
    "line = ax[0].add_collection(lc)\n",
    "scatter = ax[0].scatter(x, y, c=x, cmap=cmap, norm=norm, s=5)\n",
    "\n",
    "# Plot evolving outlines time series (ICESat-2 era)\n",
    "x = mdates.date2num(superset_IS2_evolving_sum_df['mid_pt_datetime'])\n",
    "y = superset_IS2_evolving_sum_df['evolving_outlines_area (m^2)'] / 1e6\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(2)\n",
    "line = ax[0].add_collection(lc)\n",
    "scatter = ax[0].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "# Plot prior stationary outlines at all lakes\n",
    "ax[0].axhline(subset_CS2_IS2_SARInPreExpansion_stationary_sum_df['stationary_outline_area (m^2)'].iloc[0] / 1e6, \n",
    "              color=stationary_outline_color, linestyle='solid', linewidth=1,\n",
    "              xmin=0, xmax=(SARIn_expand_date-start_date)/time_span, zorder=1)\n",
    "ax[0].axhline(subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['stationary_outline_area (m^2)'].iloc[-1] / 1e6, \n",
    "              color=stationary_outline_color, linestyle='solid', linewidth=1,\n",
    "              xmin=(SARIn_expand_date-start_date)/time_span, xmax=1, zorder=1)\n",
    "ax[0].axhline(superset_IS2_stationary_sum_df['stationary_outline_area (m^2)'].iloc[0] / 1e6, \n",
    "              color=stationary_outline_color, linestyle='solid', linewidth=2, \n",
    "              xmin=(CS2_IS2_tie_pt-start_date)/time_span, xmax=1, zorder=1)\n",
    "\n",
    "# Plot prior stationary outlines at lakes with evolving outlines (subset)\n",
    "ax[0].axhline(subset_CS2_IS2_SARInPreExpansion_stationary_subset_sum_df['stationary_outline_area (m^2)'].iloc[0] / 1e6, \n",
    "              color=stationary_outline_subset_color, linestyle='dashed', linewidth=1,\n",
    "              xmin=0, xmax=(SARIn_expand_date-start_date)/time_span, zorder=1)\n",
    "ax[0].axhline(subset_CS2_IS2_SARInPostExpansion_stationary_subset_sum_df['stationary_outline_area (m^2)'].iloc[-1] / 1e6, \n",
    "              color=stationary_outline_subset_color, linestyle='dashed', linewidth=1,\n",
    "              xmin=(SARIn_expand_date-start_date)/time_span, xmax=1, zorder=1)\n",
    "ax[0].axhline(superset_IS2_stationary_subset_sum_df['stationary_outline_area (m^2)'].iloc[0] / 1e6, \n",
    "              color=stationary_outline_subset_color, linestyle='dashed', linewidth=2, \n",
    "              xmin=(CS2_IS2_tie_pt-start_date)/time_span, xmax=1, zorder=1)\n",
    "\n",
    "# Plot updated stationary outlines\n",
    "ax[0].axhline(subset_CS2_IS2_SARInPreExpansion_evolving_union_sum_df['stationary_outline_area (m^2)'].iloc[0] / 1e6, \n",
    "              color='k', linestyle='dotted', linewidth=1,\n",
    "              xmin=0, xmax=(SARIn_expand_date-start_date)/time_span, zorder=1)\n",
    "ax[0].axhline(subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['stationary_outline_area (m^2)'].iloc[-1] / 1e6, \n",
    "              color='k', linestyle='dotted', linewidth=1,\n",
    "              xmin=(SARIn_expand_date-start_date)/time_span, xmax=1, zorder=1)\n",
    "ax[0].axhline(superset_IS2_evolving_union_sum_df['stationary_outline_area (m^2)'].iloc[0] / 1e6, \n",
    "              color='k', linestyle='dotted', linewidth=2, \n",
    "              xmin=(CS2_IS2_tie_pt-start_date)/time_span, xmax=1, zorder=1)\n",
    "\n",
    "\n",
    "# Carbon export\n",
    "\n",
    "# Define conversion factor for grams\n",
    "# g_conv = 1e6  # Megagrams\n",
    "g_conv = 1e9  # Gigagrams\n",
    "\n",
    "# Plot evolving outlines time series (CryoSat-2 observation period)\n",
    "x = mdates.date2num(subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['mid_pt_datetime'])\n",
    "y = subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['evolving_outlines_area (m^2)'] * SLM_evolving_per_area_per_step_DIC_export_filling_period / g_conv\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(1)\n",
    "line = ax[1].add_collection(lc)\n",
    "scatter = ax[1].scatter(x, y, c=x, cmap=cmap, norm=norm, s=5)\n",
    "\n",
    "# Plot evolving outlines time series (ICESat-2 observation period)\n",
    "x = mdates.date2num(superset_IS2_evolving_sum_df['mid_pt_datetime'])\n",
    "y = superset_IS2_evolving_sum_df['evolving_outlines_area (m^2)'] * SLM_evolving_per_area_per_step_DIC_export_filling_period / g_conv \n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(2)\n",
    "line = ax[1].add_collection(lc)\n",
    "scatter = ax[1].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "# Plot stationary outlines at all lakes\n",
    "ax[1].axhline(subset_CS2_IS2_SARInPreExpansion_stationary_sum_df['stationary_outline_area (m^2)'].iloc[0] * SLM_stationary_per_area_per_step_DIC_export / g_conv,\n",
    "              color=stationary_outline_color, linewidth=1,\n",
    "              xmin=0, xmax=(SARIn_expand_date-start_date)/time_span)\n",
    "ax[1].axhline(subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['stationary_outline_area (m^2)'].iloc[-1] * SLM_stationary_per_area_per_step_DIC_export / g_conv, \n",
    "              color=stationary_outline_color, linewidth=1,\n",
    "              xmin=(SARIn_expand_date-start_date)/time_span, xmax=1)\n",
    "ax[1].axhline(superset_IS2_stationary_sum_df['stationary_outline_area (m^2)'].iloc[0] * SLM_stationary_per_area_per_step_DIC_export / g_conv, \n",
    "              color=stationary_outline_color, linewidth=2, \n",
    "              xmin=(CS2_IS2_tie_pt-start_date)/time_span, xmax=1)\n",
    "\n",
    "# Plot stationary outlines at lakes with evolving outlines (subset)\n",
    "ax[1].axhline(subset_CS2_IS2_SARInPreExpansion_stationary_subset_sum_df['stationary_outline_area (m^2)'].iloc[0] * SLM_stationary_per_area_per_step_DIC_export / g_conv,\n",
    "              color=stationary_outline_subset_color, linestyle='dashed', linewidth=1,\n",
    "              xmin=0, xmax=(SARIn_expand_date-start_date)/time_span)\n",
    "ax[1].axhline(subset_CS2_IS2_SARInPostExpansion_stationary_subset_sum_df['stationary_outline_area (m^2)'].iloc[-1] * SLM_stationary_per_area_per_step_DIC_export / g_conv, \n",
    "              color=stationary_outline_subset_color, linestyle='dashed', linewidth=1,\n",
    "              xmin=(SARIn_expand_date-start_date)/time_span, xmax=1)\n",
    "ax[1].axhline(superset_IS2_stationary_subset_sum_df['stationary_outline_area (m^2)'].iloc[0] * SLM_stationary_per_area_per_step_DIC_export / g_conv, \n",
    "              color=stationary_outline_subset_color, linestyle='dashed', linewidth=2, \n",
    "              xmin=(CS2_IS2_tie_pt-start_date)/time_span, xmax=1)\n",
    "\n",
    "# Plot evolving outlines union\n",
    "ax[1].axhline(subset_CS2_IS2_SARInPreExpansion_evolving_union_sum_df['stationary_outline_area (m^2)'].iloc[0] * SLM_stationary_per_area_per_step_DIC_export / g_conv,\n",
    "              color='k', linestyle='dotted', linewidth=1,\n",
    "              xmin=0, xmax=(SARIn_expand_date-start_date)/time_span)\n",
    "ax[1].axhline(subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['stationary_outline_area (m^2)'].iloc[-1] * SLM_stationary_per_area_per_step_DIC_export / g_conv,\n",
    "              color='k', linestyle='dotted',linewidth=1,\n",
    "              xmin=(SARIn_expand_date-start_date)/time_span, xmax=1)\n",
    "ax[1].axhline(superset_IS2_evolving_union_sum_df['stationary_outline_area (m^2)'].iloc[0] * SLM_stationary_per_area_per_step_DIC_export / g_conv,\n",
    "              color='k', linestyle='dotted', linewidth=2, \n",
    "              xmin=(CS2_IS2_tie_pt-start_date)/time_span, xmax=1)\n",
    "\n",
    "\n",
    "# Panel - cumulative dV/dt --------------------------------------------------\n",
    "\n",
    "# Plot dV time series of evolving outlines using LineCollection from points/segments to plot multi-colored line\n",
    "x = mdates.date2num(subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['mid_pt_datetime'])\n",
    "y = np.cumsum(subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['evolving_outlines_dV_corr (m^3)'] / 1e9)\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(1)\n",
    "line = ax[2].add_collection(lc)\n",
    "scatter = ax[2].scatter(x, y, c=x, cmap=cmap, norm=norm, s=8)\n",
    "\n",
    "CS2_last_cyc_date = str(cyc_datetimes[cyc_datetimes['dataset'] == 'CryoSat2_SARIn']['cyc_end_datetimes'].iloc[-1])\n",
    "cum_sum_last_CS2_midcyc_date = np.cumsum(np.divide(\n",
    "    subset_CS2_IS2_SARInPostExpansion_evolving_sum_df[\n",
    "        subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date]\n",
    "        ['evolving_outlines_dV_corr (m^3)'], 1e9)).iloc[-1]\n",
    "\n",
    "x = mdates.date2num(superset_IS2_evolving_sum_df['mid_pt_datetime'])\n",
    "y = np.cumsum(superset_IS2_evolving_sum_df['evolving_outlines_dV_corr (m^3)'] / 1e9) + cum_sum_last_CS2_midcyc_date\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(2)\n",
    "line = ax[2].add_collection(lc)\n",
    "scatter = ax[2].scatter(x, y, c=x, cmap=cmap, norm=norm, s=14)\n",
    "\n",
    "# Plot dV time series of stationary outline of all lakes\n",
    "ax[2].plot(mdates.date2num(subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum(subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['stationary_outline_dV_corr (m^3)'] / 1e9), \n",
    "    color=stationary_outline_color, marker='o', markerfacecolor='white', markersize=3, linewidth=1)\n",
    "cum_sum_last_CS2_midcyc_date = np.cumsum(\n",
    "    subset_CS2_IS2_SARInPostExpansion_stationary_sum_df[subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date]\n",
    "    ['stationary_outline_dV_corr (m^3)'] / 1e9).iloc[-1]\n",
    "ax[2].plot(mdates.date2num(superset_IS2_stationary_sum_df['mid_pt_datetime']), \n",
    "np.cumsum(superset_IS2_stationary_sum_df['stationary_outline_dV_corr (m^3)'] / 1e9) + cum_sum_last_CS2_midcyc_date, \n",
    "           color=stationary_outline_color, marker='o', markerfacecolor='white', markersize=4, linewidth=2)\n",
    "\n",
    "# Plot dV time series of stationary outline of lakes with evolving outlines (subset)\n",
    "ax[2].plot(mdates.date2num(subset_CS2_IS2_SARInPostExpansion_stationary_subset_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum(subset_CS2_IS2_SARInPostExpansion_stationary_subset_sum_df['stationary_outline_dV_corr (m^3)'] / 1e9), \n",
    "    color=stationary_outline_subset_color, marker='o', markersize=3, markerfacecolor='white', linewidth=1)\n",
    "cum_sum_last_CS2_midcyc_date = np.cumsum(\n",
    "    subset_CS2_IS2_SARInPostExpansion_stationary_subset_sum_df[subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date]\n",
    "    ['stationary_outline_dV_corr (m^3)'] / 1e9).iloc[-1]\n",
    "ax[2].plot(mdates.date2num(superset_IS2_stationary_subset_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum(superset_IS2_stationary_subset_sum_df['stationary_outline_dV_corr (m^3)'] / 1e9) + cum_sum_last_CS2_midcyc_date, \n",
    "    color=stationary_outline_subset_color, marker='o', markersize=4, markerfacecolor='white', linewidth=2)\n",
    "\n",
    "# Plot dV time series of updated stationary outline\n",
    "ax[2].plot(mdates.date2num(subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum(subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['stationary_outline_dV_corr (m^3)']) / 1e9, \n",
    "    color='k', marker='o', markersize=3, linewidth=1)\n",
    "cum_sum_last_CS2_midcyc_date = np.cumsum((\n",
    "    subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df[\n",
    "    subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date]\n",
    "    ['stationary_outline_dV_corr (m^3)']) / 1e9).iloc[-1]\n",
    "ax[2].plot(mdates.date2num(superset_IS2_evolving_union_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum(superset_IS2_evolving_union_sum_df['stationary_outline_dV_corr (m^3)'] / 1e9) + cum_sum_last_CS2_midcyc_date, \n",
    "           color='k', marker='o', markersize=4, linewidth=2)\n",
    "\n",
    "\n",
    "# Panel - cumulative dV/dt bias --------------------------------------------------\n",
    "\n",
    "# Plot bias (evolving - prior stationary (all lakes))\n",
    "ax[3].plot(mdates.date2num(subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum(subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['evolving_outlines_dV_corr (m^3)'] - \n",
    "        subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['stationary_outline_dV_corr (m^3)']) / 1e9, \n",
    "        color='red', marker='o', markerfacecolor='white', markersize=3, linewidth=1)\n",
    "cum_sum_last_CS2_midcyc_date = np.cumsum((\n",
    "    subset_CS2_IS2_SARInPostExpansion_evolving_sum_df[\n",
    "        subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date]\n",
    "        ['evolving_outlines_dV_corr (m^3)'] - \n",
    "        subset_CS2_IS2_SARInPostExpansion_stationary_sum_df[\n",
    "        subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date]\n",
    "        ['stationary_outline_dV_corr (m^3)']) / 1e9).iloc[-1]\n",
    "ax[3].plot(mdates.date2num(superset_IS2_stationary_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum((superset_IS2_evolving_sum_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "        superset_IS2_stationary_sum_df['stationary_outline_dV_corr (m^3)']) / 1e9) + cum_sum_last_CS2_midcyc_date, \n",
    "        color='red', marker='o', markerfacecolor='white', markersize=4, linewidth=2)\n",
    "\n",
    "\n",
    "# Plot bias (evolving - prior stationary (subset at evolving lakes))\n",
    "ax[3].plot(mdates.date2num(subset_CS2_IS2_SARInPostExpansion_stationary_subset_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum(subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['evolving_outlines_dV_corr (m^3)'] - \n",
    "        subset_CS2_IS2_SARInPostExpansion_stationary_subset_sum_df['stationary_outline_dV_corr (m^3)']) / 1e9, \n",
    "        color='lightcoral', marker='o', markersize=3, markerfacecolor='k', linewidth=1)\n",
    "cum_sum_last_CS2_midcyc_date = np.cumsum((\n",
    "    subset_CS2_IS2_SARInPostExpansion_evolving_sum_df[\n",
    "        subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date]\n",
    "        ['evolving_outlines_dV_corr (m^3)'] - \n",
    "        subset_CS2_IS2_SARInPostExpansion_stationary_subset_sum_df[\n",
    "        subset_CS2_IS2_SARInPostExpansion_stationary_subset_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date]\n",
    "        ['stationary_outline_dV_corr (m^3)']) / 1e9).iloc[-1]\n",
    "ax[3].plot(mdates.date2num(superset_IS2_stationary_subset_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum((superset_IS2_evolving_sum_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "        superset_IS2_stationary_subset_sum_df['stationary_outline_dV_corr (m^3)']) / 1e9) + cum_sum_last_CS2_midcyc_date, \n",
    "        color='lightcoral', marker='o', markersize=4, markerfacecolor='k', linewidth=2)\n",
    "\n",
    "# Plot bias (evolving - updated stationary)\n",
    "ax[3].plot(mdates.date2num(subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum(subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['evolving_outlines_dV_corr (m^3)'] - \n",
    "        subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['stationary_outline_dV_corr (m^3)']) / 1e9, \n",
    "        color='darkred', marker='o', markersize=3, linestyle='solid', linewidth=1)\n",
    "cum_sum_last_CS2_midcyc_date = np.cumsum((\n",
    "    subset_CS2_IS2_SARInPostExpansion_evolving_sum_df[\n",
    "        subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date]\n",
    "        ['evolving_outlines_dV_corr (m^3)'] - \n",
    "        subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df[\n",
    "        subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date]\n",
    "        ['stationary_outline_dV_corr (m^3)']) / 1e9).iloc[-1]\n",
    "ax[3].plot(mdates.date2num(superset_IS2_stationary_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum((superset_IS2_evolving_sum_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "        superset_IS2_evolving_union_sum_df['stationary_outline_dV_corr (m^3)']) / 1e9) + cum_sum_last_CS2_midcyc_date, \n",
    "        color='darkred', marker='o', markersize=4, linewidth=2)\n",
    "\n",
    "\n",
    "# Set y axes limits\n",
    "ax0_auto_ymin, ax0_auto_ymax = ax[0].get_ylim()\n",
    "ax1_auto_ymin, ax1_auto_ymax = ax[1].get_ylim()\n",
    "ax[0].set_ylim(-(ax0_auto_ymax-ax0_auto_ymin)*0.1, None) # Prescribe lower limit to accommodate text annotations of satellite eras\n",
    "ax[1].set_ylim(-(ax1_auto_ymax-ax1_auto_ymin)*0.1, None)\n",
    "ax[2].set_ylim(-8.5, 8.5)\n",
    "ax[3].set_ylim(-8.5, 1)\n",
    "del ax0_auto_ymin, ax0_auto_ymax, ax1_auto_ymin, ax1_auto_ymax\n",
    "\n",
    "# Add colorbar, legends, and titles\n",
    "ax[3].set_xlabel('year')\n",
    "\n",
    "# Add text label near the vertical line\n",
    "start_date_text = pd.to_datetime(cyc_datetimes['cyc_start_datetimes'].iloc[0]) + pd.Timedelta(days=15)\n",
    "sarin_expand_date_text = pd.to_datetime(cyc_datetimes[cyc_datetimes['cyc_start_datetimes'] == '2013-10-01T18:00:00.000000000']['cyc_start_datetimes'].iloc[0]) + pd.Timedelta(days=15)\n",
    "is2_start_date_text = pd.to_datetime(cyc_datetimes[cyc_datetimes['dataset'] == 'ICESat2_ATL15'].iloc[0]['cyc_start_datetimes']) + pd.Timedelta(days=15)\n",
    "\n",
    "ax[0].text(start_date_text, -800, 'CryoSat-2 era begins', horizontalalignment='left', verticalalignment='top', color='k')\n",
    "ax[0].text(sarin_expand_date_text, -800, 'CryoSat-2 SARIn mode expands', horizontalalignment='left', verticalalignment='top', color='k')\n",
    "ax[0].text(is2_start_date_text, -800, 'ICESat-2 era begins', horizontalalignment='left', verticalalignment='top', color='k')\n",
    "\n",
    "# Add legends\n",
    "# Create legend handles\n",
    "stationary_line = plt.Line2D([], [], color=stationary_outline_color, linestyle='solid', linewidth=2)\n",
    "stationary_subset_line = plt.Line2D([], [], color=stationary_outline_subset_color, linestyle='dashed', linewidth=2)\n",
    "# Full colormap for evolving outlines with one representative data point\n",
    "mid_idx = len(cyc_datetimes['cyc_start_datetimes']) // 2\n",
    "evolving_scatter_line = []\n",
    "for i, dt in enumerate(cyc_datetimes['cyc_start_datetimes']):\n",
    "    if i == mid_idx:  # only the middle line gets a marker\n",
    "        evolving_scatter_line.append(\n",
    "            Line2D([0], [0],\n",
    "                   color=cmap(norm(mdates.date2num(dt))),\n",
    "                   linestyle='solid',\n",
    "                   marker='o', markersize=5))\n",
    "    else:\n",
    "        evolving_scatter_line.append(\n",
    "            Line2D([0], [0],\n",
    "                   color=cmap(norm(mdates.date2num(dt))),\n",
    "                   linestyle='solid'))\n",
    "evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=2)\n",
    "stationary_scatter_line = plt.Line2D([], [], color=stationary_outline_color, marker='o', markersize=5, markerfacecolor='white', linewidth=2)\n",
    "stationary_subset_scatter_line = plt.Line2D([], [], color=stationary_outline_subset_color, marker='o', markersize=5, markerfacecolor='white', linewidth=2)\n",
    "evolving_union_scatter_line = plt.Line2D([], [], color='k', marker='o', markersize=5, markerfacecolor='k', linewidth=2)\n",
    "bias_line = plt.Line2D([], [], color='red', marker='o', markersize=5, markerfacecolor='white', linewidth=2)  # evolving - prior stationary (all lakes)\n",
    "bias_line2 = plt.Line2D([], [], color='lightcoral', marker='o', markersize=5, markerfacecolor='k', linewidth=2)  # evolving - prior stationary (only at evolving lakes)\n",
    "bias_line3 = plt.Line2D([], [], color='darkred', marker='o', markersize=5, linewidth=2)  # evolving - updated stationary (only evolving lakes)\n",
    "\n",
    "x0 = 0.03\n",
    "legend0 = ax[0].legend([tuple(evolving_scatter_line),\n",
    "                        stationary_line,\n",
    "                        stationary_subset_line,\n",
    "                        evolving_union_line],\n",
    "    [f'evolving outlines (n={len(evolving_outlines_lakes)})',\n",
    "     f'prior stationary outlines at all analyzed lakes (n={len(no_evolving_outlines_lakes) + len(evolving_outlines_lakes)})',\n",
    "     f'prior stationary outlines at evolving lakes subset (n={len(evolving_outlines_lakes)})',\n",
    "     f'updated stationary outlines (n={len(evolving_outlines_lakes)})'],\n",
    "    handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper left', bbox_to_anchor=(x0, 1))\n",
    "\n",
    "legend2 = ax[2].legend([tuple(evolving_scatter_line),\n",
    "                        stationary_scatter_line,\n",
    "                        stationary_subset_scatter_line,\n",
    "                        evolving_union_scatter_line],\n",
    "    [f'evolving outlines (n={len(evolving_outlines_lakes)})',\n",
    "     f'prior stationary outlines at all analyzed lakes (n={len(no_evolving_outlines_lakes) + len(evolving_outlines_lakes)})',\n",
    "     f'prior stationary outlines at evolving lakes subset (n={len(evolving_outlines_lakes)})',\n",
    "     f'updated stationary outlines (n={len(evolving_outlines_lakes)})'],\n",
    "    handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper left', bbox_to_anchor=(x0, 1))\n",
    "\n",
    "legend3 = ax[3].legend([bias_line, bias_line2, bias_line3], \n",
    "                      [f'bias, evolving (n={len(evolving_outlines_lakes)}) − prior stationary at all analyzed lakes (n={len(no_evolving_outlines_lakes) + len(evolving_outlines_lakes)})',\n",
    "                       f'bias, evolving (n={len(evolving_outlines_lakes)}) − prior stationary at evolving lakes subset (n={len(evolving_outlines_lakes)})',\n",
    "                       f'bias, evolving (n={len(evolving_outlines_lakes)}) − updated stationary (n={len(evolving_outlines_lakes)})'\n",
    "                      ],\n",
    "    handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper left', bbox_to_anchor=(x0, 0.27))\n",
    "\n",
    "for row in [0,2]:\n",
    "    # Remove x tick labels\n",
    "    ax[row].set_xticklabels([])\n",
    "\n",
    "    # Format the x-axis to display years only\n",
    "    ax[row].xaxis.set_major_locator(mdates.YearLocator(base=1))  # Major ticks every other year\n",
    "    ax[row].xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Minor ticks every quarter\n",
    "    ax[row].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Display major ticks as years\n",
    "\n",
    "    # Set x-axis limits\n",
    "    ax[row].set(xlim=(cyc_datetimes['cyc_start_datetimes'].iloc[0],\n",
    "        # Set righthand x-axis limit slightly earlier to prevent tick mark displaying when there is no data point\n",
    "        (cyc_datetimes['cyc_end_datetimes'].iloc[-1] - datetime.timedelta(days=2))))\n",
    "\n",
    "# Set axes titles\n",
    "ax[0].set_ylabel('lakebed active area [km$^2$]')\n",
    "ax[1].set_ylabel('DIC export [Gg C]')\n",
    "ax[2].set_ylabel('cumulative $dV$ [km$^3$]')\n",
    "ax[3].set_ylabel('cumulative $dV$ bias [km$^3$]')\n",
    "\n",
    "# Adding annotations at the top left of the subplot\n",
    "ax_array = np.array(ax)  # Convert gridspec list of lists into numpy array to use .flatten() method\n",
    "char_index = 97  # ASCII value for 'a'\n",
    "for i, ax_i in enumerate(ax_array.flatten()):\n",
    "    # `transform=ax.transAxes` makes coordinates relative to the axes (0,0 is bottom left and 1,1 is top right)\n",
    "    ax_i.text(0.01, 0.97, chr(char_index), transform=ax_i.transAxes, fontsize=16, va='top', ha='left')\n",
    "    char_index += 1 # Increment the ASCII index to get the next character\n",
    "    \n",
    "# Save and close plot\n",
    "plt.savefig(OUTPUT_DIR + '/figures/Fig3_lake_reexamination_results_continental_integration.jpg',\n",
    "    dpi=500, bbox_inches='tight')\n",
    "\n",
    "# Preview plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645bf6aa-8c53-491c-b8ed-25740c0354be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d852f2-1e6b-4e71-8d7b-36a1440a630b",
   "metadata": {},
   "source": [
    "## Table S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c3e562-8026-4435-9faa-fc2546acf486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stationary subglacial lake outlines\n",
    "stationary_lakes_gdf = gpd.read_file(os.path.join(os.getcwd(), 'output/lake_outlines/stationary_outlines/stationary_outlines_gdf.geojson'))\n",
    "\n",
    "# Create filtered geodataframes of lakes based on whether they have evolving outlines\n",
    "folder_path = os.path.join ('output/lake_outlines/evolving_outlines')\n",
    "\n",
    "# Lakes with evolving outlines (.geojson)\n",
    "evolving_outlines_lakes = filter_gdf_by_folder_contents(stationary_lakes_gdf, folder_path, file_extension='geojson', exclude=False)\n",
    "\n",
    "# Lakes with no evolving outlines (.txt)\n",
    "no_evolving_outlines_lakes = filter_gdf_by_folder_contents(stationary_lakes_gdf, folder_path, file_extension='txt', exclude=False)\n",
    "\n",
    "# For the evolving_outlines_lakes, we must add the special case of Site_B_Site_C that are now a combined lake\n",
    "include_list = ['Site_B', 'Site_C']\n",
    "included_rows = stationary_lakes_gdf[stationary_lakes_gdf['name'].isin(include_list)]\n",
    "evolving_outlines_lakes = pd.concat([evolving_outlines_lakes, included_rows]).drop_duplicates()\n",
    "print('lakes with evolving outlines:',len(evolving_outlines_lakes))\n",
    "print('lakes with no evolving outlines:',len(no_evolving_outlines_lakes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c1de70-b97d-4a9a-83e8-50dcf7cc6993",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stationary_lakes_gdf[stationary_lakes_gdf['name'] != 'Crane_Glacier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a01894-a961-4438-b794-3282f3190bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = (\n",
    "    stationary_lakes_gdf\n",
    "    .loc[stationary_lakes_gdf['name'] != 'Crane_Glacier', 'CS2_SARIn_start']\n",
    "    .value_counts(dropna=False)\n",
    ")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7056ad81-7a22-4b1f-901a-1e51d9b85a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = (\n",
    "    evolving_outlines_lakes['CS2_SARIn_start']\n",
    "    .value_counts(dropna=False)\n",
    ")\n",
    "print(summary)\n",
    "print(np.sum(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5c044-cb30-4e01-92c7-1d8ab57cb88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = (\n",
    "    no_evolving_outlines_lakes['CS2_SARIn_start']\n",
    "    .value_counts(dropna=False)\n",
    ")\n",
    "print(summary)\n",
    "print(np.sum(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1360be0-91c1-4a7b-ae93-79f1d3b58017",
   "metadata": {},
   "source": [
    "## Fig. S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b440683-c348-43f6-a5a5-1e71a09779d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.family': 'DejaVu Sans',\n",
    "    'font.size': 9,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 13,\n",
    "    'ytick.labelsize': 13,\n",
    "    'axes.titlesize': 14,\n",
    "    'legend.fontsize': 12,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a485c5-0f8d-454d-a90e-1a550927ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the lakes that have evolving outlines during the date range of contemporaneous CryoSat-2-SARIn-ICESat-2 data availability\n",
    "# by counting the number of geometric calculations CSV files that have evolving_outlines_area (m^2) data within \n",
    "directory = \"output/geometric_calcs/evolving_outlines_geom_calc\"\n",
    "df_matches = find_csvs_with_values_in_range(directory)\n",
    "df_matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a939b9-160e-4d61-ba54-d813ae398eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now find the number of those lakes that have CryoSat-2 SARIn coverage \n",
    "# (filters out continental summation CSV files and those without SARIn coverage)\n",
    "\n",
    "# Ensure CS2_SARIn_start column is treated as proper NA values, not strings\n",
    "reexamined_stationary_outlines_gdf = reexamined_stationary_outlines_gdf.copy()\n",
    "reexamined_stationary_outlines_gdf[\"CS2_SARIn_start\"] = reexamined_stationary_outlines_gdf[\"CS2_SARIn_start\"].replace(\"<NA>\", pd.NA)\n",
    "\n",
    "# Merge df_matches with the stationary outlines GeoDataFrame on \"name\"\n",
    "merged = df_matches.merge(\n",
    "    reexamined_stationary_outlines_gdf[[\"name\", \"CS2_SARIn_start\"]],\n",
    "    on=\"name\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "count_evolving_lakes_IS2_during_CS2_comparison = len(merged[merged[\"CS2_SARIn_start\"].notna()])\n",
    "print(count_evolving_lakes_IS2_during_CS2_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5276576-6a98-4434-bab7-0034aad5aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of lakes that were found to have evolving outlines using only CryoSat-2 SARIn data\n",
    "dir = 'output/geometric_calcs/evolving_outlines_geom_calc/CS2_comparison'\n",
    "print(count_files_by_suffix(dir, \".csv\"))\n",
    "# Double check by looking at the number that were successfully merged with the geometric calculation data from \n",
    "# multi-mission time series\n",
    "dir = 'output/geometric_calcs/evolving_outlines_geom_calc/CS2_comparison/merged'\n",
    "count_evolving_lakes_CS2_IS2 = count_files_by_suffix(dir, \".csv\")\n",
    "print(count_evolving_lakes_CS2_IS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd15b28-51a9-470c-aed3-d05b4cbc871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any lakes where CryoSat-2 found evolving outlines that were not found using multi-mission time series?\n",
    "IS2_dir = 'output/geometric_calcs/evolving_outlines_geom_calc/'\n",
    "CS2_dir = 'output/geometric_calcs/evolving_outlines_geom_calc/CS2_comparison/'\n",
    "\n",
    "# Collect CSV filenames without extension\n",
    "IS2_dir_files = {os.path.splitext(f)[0] for f in os.listdir(IS2_dir)\n",
    "              if f.endswith(\".csv\") and \"sum\" not in f}  # Exclude continental summation files\n",
    "CS2_dir_files = {os.path.splitext(f)[0] for f in os.listdir(CS2_dir) if f.endswith(\".csv\")}\n",
    "\n",
    "evolving_lakes_CS2 = sorted(CS2_dir_files - IS2_dir_files)\n",
    "count_evolving_lakes_CS2 = len(evolving_lakes_CS2)\n",
    "print(count_evolving_lakes_CS2)\n",
    "print(\"CSV files in dir2 but not in dir1:\")\n",
    "for f in evolving_lakes_CS2:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b2414c-b896-4956-a436-c9d7b1c6a743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find files in dir2 but not in dir1\n",
    "evolving_lakes_IS2 = sorted(IS2_dir_files - CS2_dir_files)\n",
    "count_evolving_lakes_IS2 = len(evolving_lakes_IS2)\n",
    "print(count_evolving_lakes_IS2)\n",
    "print(\"CSV files in dir1 but not in dir2:\")\n",
    "for f in evolving_lakes_IS2:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eca313-b24a-419f-aff5-c4afc3a7b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define linear model for ODR\n",
    "def linear_model(B, x):\n",
    "    return B[0] * x + B[1]  # B[0]=slope, B[1]=intercept\n",
    "    \n",
    "# Base dir1 list\n",
    "dir1_list = [\n",
    "    \"/home/jovyan/1_evolving_lakes/Sauthoff-2025-GRL/output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes\",\n",
    "    \"/home/jovyan/1_evolving_lakes/Sauthoff-2025-GRL/output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes\",\n",
    "    \"/home/jovyan/1_evolving_lakes/Sauthoff-2025-GRL/output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes\",\n",
    "    \"/home/jovyan/1_evolving_lakes/Sauthoff-2025-GRL/output/geometric_calcs/evolving_outlines_geom_calc\"\n",
    "]\n",
    "\n",
    "rename_map = {\n",
    "    \"area (m^2)\": \"lakebed active area [km$^2$]\",\n",
    "    \"dh (m)\": \"uncorrected on-lake $dh$ [m]\",\n",
    "    \"region_dh (m)\": \"secular off-lake $dh$ [m]\",\n",
    "    \"dh_corr (m)\": \"corrected on-lake $dh$ [m]\",\n",
    "    \"dV_corr (m^3)\": \"non-cumulative $dV$ [km$^3$]\"\n",
    "}\n",
    "\n",
    "# Collect results per dataset\n",
    "datasets = {}\n",
    "for dir1 in dir1_list:\n",
    "    merged_dir = os.path.join(dir1, \"CS2_comparison\", \"merged\")\n",
    "    label = os.path.basename(dir1)  # last folder of dir1\n",
    "\n",
    "    all_dfs = []\n",
    "    if not os.path.exists(merged_dir):\n",
    "        print(f\"Skipping {merged_dir} (not found)\")\n",
    "        continue\n",
    "\n",
    "    for fname in os.listdir(merged_dir):\n",
    "        if fname.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(merged_dir, fname), parse_dates=[\"date\"])\n",
    "            all_dfs.append(df)\n",
    "\n",
    "    if not all_dfs:\n",
    "        continue\n",
    "\n",
    "    df_all = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "    # --- Normalize column names ---\n",
    "    rename_cols = {}\n",
    "    for col in df_all.columns:\n",
    "        if col.startswith(\"evolving_outlines_\"):\n",
    "            rename_cols[col] = col.replace(\"evolving_outlines_\", \"\")\n",
    "        elif col.startswith(\"stationary_outline_\"):\n",
    "            rename_cols[col] = col.replace(\"stationary_outline_\", \"\")\n",
    "    df_all = df_all.rename(columns=rename_cols)\n",
    "\n",
    "    # Apply variable scaling after renaming\n",
    "    for var, scale in {\"area (m^2)\": 1e6, \"dV_corr (m^3)\": 1e9}.items():\n",
    "        if var in df_all.columns:\n",
    "            df_all[var] = df_all[var] / scale\n",
    "            if f\"{var}_CS2\" in df_all.columns:\n",
    "                df_all[f\"{var}_CS2\"] = df_all[f\"{var}_CS2\"] / scale\n",
    "\n",
    "    datasets[label] = df_all\n",
    "\n",
    "# Identify variables to plot (including the new cumulative dV)\n",
    "exclude_cols = {\"mid_pt_datetime\", \"date\", \"name\"}\n",
    "sample_df = next(iter(datasets.values()))\n",
    "vars_base = [c for c in sample_df.columns if c not in exclude_cols and not c.endswith(\"_CS2\")]\n",
    "\n",
    "# Create subplots grid\n",
    "nvars = len(vars_base)\n",
    "ncols = 2\n",
    "nrows = (nvars + ncols - 1) // ncols\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(10, 5*nrows), constrained_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Adjust padding and spacing *within* constrained layout\n",
    "fig.set_constrained_layout_pads(h_pad=0.05)\n",
    "\n",
    "# colors = [\"turquoise\", \"darkcyan\", \"k\"]\n",
    "colors = [\"turquoise\", \"darkcyan\", \"k\", \"purple\"]\n",
    "markers = [\"o\", \"s\", \"D\", \"^\"]\n",
    "# style_map = {label: {\"color\": color, \"marker\": marker} for (label, _), color, marker in zip(datasets.items(), colors, markers)}\n",
    "style_map = {key: {\"color\": color, \"marker\": marker} \n",
    "             for key, color, marker in zip(datasets.keys(), colors, markers)}\n",
    "\n",
    "# Set up colormap matching evolving outlines\n",
    "min_date = pd.to_datetime(cyc_start_datetimes[1])\n",
    "max_date = pd.to_datetime(cyc_start_datetimes[-1])\n",
    "n_dates = len(cyc_start_datetimes[1:])\n",
    "date_range = pd.date_range(min_date, max_date, periods=len(cyc_start_datetimes[1:]))\n",
    "years = date_range.year.unique()\n",
    "years = pd.to_datetime(years, format='%Y')\n",
    "cmap = plt.get_cmap('plasma', n_dates)\n",
    "norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "mappable = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "mappable.set_array(np.linspace(mdates.date2num(min_date), mdates.date2num(max_date), n_dates))\n",
    "\n",
    "# --- Consistent evolving outlines color (midpoint of full dataset) ---\n",
    "df_evolving = datasets[\"evolving_outlines_geom_calc\"]\n",
    "df_evolving[\"mid_pt_datetime\"] = pd.to_datetime(df_evolving[\"mid_pt_datetime\"], errors=\"coerce\")\n",
    "times_all = mdates.date2num(df_evolving[\"mid_pt_datetime\"].dropna())\n",
    "mid_time_global = 0.5 * (times_all.min() + times_all.max())\n",
    "evolving_outline_color = cmap(norm(mid_time_global))\n",
    "\n",
    "# Define regression line styles\n",
    "regression_styles = {\n",
    "    \"stationary_outlines_at_all_lakes\": \"solid\",\n",
    "    \"stationary_outlines_at_evolving_lakes\": \"dashed\",\n",
    "    \"evolving_union_at_evolving_lakes\": \":\",\n",
    "    \"evolving_outlines_geom_calc\": \"solid\"\n",
    "}\n",
    "\n",
    "# Prepare containers for legend handles/labels\n",
    "legend_handles = []\n",
    "legend_labels = []\n",
    "\n",
    "# Scatter plotting loop\n",
    "for i, var in enumerate(vars_base):\n",
    "    ax = axes[i]\n",
    "\n",
    "    all_x = pd.concat([df[var] for df in datasets.values() if var in df])\n",
    "    all_y = pd.concat([df[f\"{var}_CS2\"] for df in datasets.values() if f\"{var}_CS2\" in df])\n",
    "    if all_x.empty or all_y.empty:\n",
    "        continue\n",
    "\n",
    "    data_min = min(all_x.min(skipna=True), all_y.min(skipna=True))\n",
    "    data_max = max(all_x.max(skipna=True), all_y.max(skipna=True))\n",
    "    range_pad = 0.025 * (data_max - data_min) if data_max != data_min else 1\n",
    "    lim_min = data_min - range_pad\n",
    "    lim_max = data_max + range_pad\n",
    "    ax.set_xlim(lim_min, lim_max)\n",
    "    ax.set_ylim(lim_min, lim_max)\n",
    "\n",
    "    for label, df in datasets.items():\n",
    "        if var not in df.columns or f\"{var}_CS2\" not in df.columns:\n",
    "            continue\n",
    "\n",
    "        x = df[var]\n",
    "        y = df[f\"{var}_CS2\"]\n",
    "        mask_both = pd.notna(x) & pd.notna(y)\n",
    "\n",
    "        # Scatter\n",
    "        if label == \"evolving_outlines_geom_calc\":\n",
    "            df[\"mid_pt_datetime\"] = pd.to_datetime(df[\"mid_pt_datetime\"], errors=\"coerce\")\n",
    "            times_num = mdates.date2num(df[\"mid_pt_datetime\"][mask_both])\n",
    "            scatter_color = cmap(norm(0.5 * (times_num.min() + times_num.max())))  # midpoint color\n",
    "        else:\n",
    "            scatter_color = style_map[label][\"color\"]\n",
    "        \n",
    "        ax.scatter(\n",
    "            x[mask_both], y[mask_both],\n",
    "            s=20,\n",
    "            color=scatter_color if label != \"evolving_outlines_geom_calc\" else None,\n",
    "            c=times_num if label == \"evolving_outlines_geom_calc\" else None,\n",
    "            cmap=cmap if label == \"evolving_outlines_geom_calc\" else None,\n",
    "            norm=norm if label == \"evolving_outlines_geom_calc\" else None,\n",
    "            marker=style_map[label][\"marker\"],\n",
    "            alpha=0.8,\n",
    "            label=None  # Legend handle separately below\n",
    "        )\n",
    "\n",
    "        # Regression\n",
    "        if mask_both.sum() > 1:\n",
    "            model = odr.Model(linear_model)\n",
    "            data_odr = odr.RealData(x[mask_both], y[mask_both])\n",
    "            odr_obj = odr.ODR(data_odr, model, beta0=[1., 0.])\n",
    "            out = odr_obj.run()\n",
    "            slope, intercept = out.beta\n",
    "            fit_x = np.linspace(lim_min, lim_max, 100)\n",
    "            fit_y = slope * fit_x + intercept\n",
    "            reg_color = evolving_outline_color if label == \"evolving_outlines_geom_calc\" else style_map[label][\"color\"]\n",
    "            # reg_color = scatter_color if label == \"evolving_outlines_geom_calc\" else style_map[label][\"color\"]\n",
    "            line = ax.plot(fit_x, fit_y,\n",
    "                           color=reg_color,\n",
    "                           linestyle=regression_styles[label],\n",
    "                           linewidth=1.5)[0]\n",
    "\n",
    "        # Y present only\n",
    "        mask_y = pd.notna(y) & pd.isna(x)\n",
    "        if mask_y.any():\n",
    "            ax.scatter(np.full(mask_y.sum(), lim_min), y[mask_y],\n",
    "                       marker=\"_\", color=\"blue\", label=f\"evolving outline\\nfound only using CryoSat-2 SARIn (n={mask_y.sum()})\", alpha=0.8)\n",
    "            \n",
    "        # X present only\n",
    "        mask_x = pd.notna(x) & pd.isna(y) \n",
    "        if mask_x.any():\n",
    "            ax.scatter(x[mask_x], np.full(mask_x.sum(), lim_min),\n",
    "                       marker=\"|\", color=\"green\", label=f\"evolving outline\\nfound only using ICESat-2 (n={mask_x.sum()})\", alpha=0.8)\n",
    "\n",
    "    # Only add colorbar in the first subplot\n",
    "    if i == 0:\n",
    "        # Inset axes for horizontal colorbar\n",
    "        cax = inset_axes(\n",
    "            ax,                          # parent axes\n",
    "            width='67%',                 # width of colorbar\n",
    "            height='3%',                 # height of colorbar\n",
    "            loc='lower left',\n",
    "            bbox_to_anchor=[0.31, 0.15, 1, 1],  # adjust as needed\n",
    "            bbox_transform=ax.transAxes,\n",
    "            borderpad=0\n",
    "        )\n",
    "        # Use ScalarMappable 'mappable' for the colorbar\n",
    "        cbar = fig.colorbar(mappable, cax=cax, orientation='horizontal')\n",
    "        cbar.set_label('evolving outline year', fontsize=12, labelpad=4)\n",
    "\n",
    "        # Set ticks for all years but labels only for even years, skipping first year, 2010.0, as it starts before time series\n",
    "        tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "        tick_labels = [f\"'{date.strftime('%y')}\" if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "        cbar.set_ticks(tick_locations)\n",
    "        cbar.set_ticklabels(tick_labels)\n",
    "        cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Add minor ticks for quarters\n",
    "\n",
    "        # Add a box highlighting the data range\n",
    "        data_min_num = mdates.date2num(df[\"mid_pt_datetime\"].min())\n",
    "        data_max_num = mdates.date2num(df[\"mid_pt_datetime\"].max())\n",
    "        rect = Rectangle(\n",
    "            (data_min_num, 0),             # lower-left corner (x, y)\n",
    "            data_max_num - data_min_num,   # width\n",
    "            0.95,                          # height of colorbar (0–1 in colorbar coordinates)\n",
    "            edgecolor='black',             # outline color\n",
    "            facecolor='none',              # transparent fill\n",
    "            linewidth=1.5\n",
    "        )\n",
    "        cax.add_patch(rect)\n",
    "            \n",
    "    ax.plot([lim_min, lim_max], [lim_min, lim_max], \"k\", lw=0.75, alpha=0.75, zorder=0)\n",
    "    ax.set_xlabel(\"ICESat-2 ATL15\")\n",
    "    ax.set_ylabel(\"CryoSat-2 SARIn\")\n",
    "    ax.set_title(rename_map.get(var, var))\n",
    "\n",
    "# Add subplot labels (a, b, c, etc.) for flattened axes\n",
    "for i, ax in enumerate(axes[:-1]):\n",
    "    label = chr(97 + i)  # 97 = 'a'\n",
    "    ax.text(\n",
    "        0.02, 0.98, f\"{label}\",\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=14,\n",
    "        va='top', ha='left'\n",
    "    )\n",
    "\n",
    "# Build combined legend handles (marker + regression line together)\n",
    "combo_handles = []\n",
    "combo_labels = []\n",
    "\n",
    "for label, df in datasets.items():\n",
    "    x_cols = [c for c in df.columns if not c.endswith(\"_CS2\")]\n",
    "    y_cols = [f\"{c}_CS2\" for c in x_cols]\n",
    "\n",
    "    mask_x_y_total = pd.Series(False, index=df.index)\n",
    "    mask_x_total = pd.Series(False, index=df.index)\n",
    "    mask_y_total = pd.Series(False, index=df.index)\n",
    "\n",
    "    for x_col, y_col in zip(x_cols, y_cols):\n",
    "        if x_col in df.columns and y_col in df.columns:\n",
    "            x = df[x_col]\n",
    "            y = df[y_col]\n",
    "            mask_x_y_total |= pd.notna(x) & pd.notna(y)\n",
    "            mask_x_total |= pd.notna(x) & pd.isna(y)\n",
    "            mask_y_total |= pd.notna(y) & pd.isna(x)\n",
    "\n",
    "    if label == \"evolving_outlines_geom_calc\":\n",
    "        evolving_outlines_CS2_IS2 = mask_x_y_total.sum()\n",
    "        evolving_outlines_CS2 = mask_y_total.sum()\n",
    "        evolving_outlines_IS2 = mask_x_total.sum()\n",
    "\n",
    "        print(f'ICESat-2 evolving lakes found 2019.0-2021.5, n={count_evolving_lakes_IS2_during_CS2_comparison}')\n",
    "        print(f'CryoSat-2 and ICESat-2 evolving lakes found, n={count_evolving_lakes_CS2_IS2}')\n",
    "        print(f'CryoSat-2-only evolving lakes found, n={count_evolving_lakes_CS2}\\n')\n",
    "   \n",
    "        print(f'CryoSat-2 and ICESat-2 evolving outlines found, n={mask_x_y_total.sum()}')\n",
    "        print(f'CryoSat-2-only evolving outlines found, n={mask_y_total.sum()}')\n",
    "        print(f'ICESat-2-only evolving outlines found, n={mask_x_total.sum()}')\n",
    "\n",
    "dir = 'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes/CS2_comparison/merged'\n",
    "stationary_outlines_at_all_lakes_count = count_files_by_suffix(dir, \".csv\")\n",
    "dir = 'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes/CS2_comparison/merged'\n",
    "stationary_outlines_at_evolving_lakes_count = count_files_by_suffix(dir, \".csv\")\n",
    "dir = 'output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/CS2_comparison/merged'\n",
    "evolving_union_at_evolving_lakes_count = count_files_by_suffix(dir, \".csv\")\n",
    "dir = 'output/geometric_calcs/evolving_outlines_geom_calc/CS2_comparison/merged'\n",
    "evolving_outlines_geom_calc_count = count_files_by_suffix(dir, \".csv\")\n",
    "\n",
    "label_map = {\n",
    "    \"stationary_outlines_at_all_lakes\": \n",
    "        f\"prior stationary outlines\\nat analyzed lakes (n={stationary_outlines_at_all_lakes_count} lakes)\",\n",
    "    \"stationary_outlines_at_evolving_lakes\": \n",
    "        f\"prior stationary outlines\\nat evolving lakes subset (n={stationary_outlines_at_evolving_lakes_count} lakes)\",\n",
    "    \"evolving_union_at_evolving_lakes\": \n",
    "        f\"updated stationary outlines\\nat evolving lakes subset (n={evolving_union_at_evolving_lakes_count} lakes)\",\n",
    "    \"evolving_outlines_geom_calc\": \n",
    "        f\"evolving outlines found in both\\nCryoSat-2 and ICESat-2 data\\n(n={evolving_outlines_geom_calc_count} lakes; n={mask_x_y_total.sum()} time steps)\"\n",
    "}\n",
    "\n",
    "for label in datasets.keys():\n",
    "    if label == \"evolving_outlines_geom_calc\":\n",
    "        color = evolving_outline_color\n",
    "    else:\n",
    "        color = style_map[label][\"color\"]\n",
    "\n",
    "    # Marker handle\n",
    "    marker_handle = Line2D([0], [0], marker=style_map[label][\"marker\"],\n",
    "                           color=color, markersize=7, alpha=0.8, linestyle=\"None\")\n",
    "    # Regression line handle\n",
    "    line_handle = Line2D([0], [0], color=color,\n",
    "                         linestyle=regression_styles[label], linewidth=1.5)\n",
    "\n",
    "    # Combine marker + line\n",
    "    combo_handles.append((marker_handle, line_handle))\n",
    "    combo_labels.append(label_map[label])\n",
    "\n",
    "# Add CryoSat-only and ICESat-only markers too\n",
    "cryosat_only_handle = Line2D([0], [0], marker=\"_\", color=\"blue\", markersize=7, alpha=0.8, linestyle=\"None\")\n",
    "icesat_only_handle = Line2D([0], [0], marker=\"|\", color=\"green\", markersize=7, alpha=0.8, linestyle=\"None\")\n",
    "\n",
    "combo_handles.extend([cryosat_only_handle, icesat_only_handle])\n",
    "combo_labels.extend([\n",
    "    f\"evolving outline found only in\\nCryoSat-2 data\\n(n={count_evolving_lakes_CS2} lakes; n={evolving_outlines_CS2} time steps)\",\n",
    "    f\"evolving outline found only in\\nICESat-2 data\\n(n={count_evolving_lakes_IS2_during_CS2_comparison} lakes; n={evolving_outlines_IS2} time steps)\"\n",
    "])\n",
    "\n",
    "# Place legend in last axis\n",
    "legend_ax = axes[-1]\n",
    "legend_ax.axis(\"off\")\n",
    "\n",
    "legend_ax.legend(combo_handles, combo_labels, loc=\"center\", frameon=False, fontsize=13,\n",
    "                 handler_map={tuple: HandlerTuple(ndivide=None)})\n",
    "\n",
    "# Save and show plot\n",
    "plt.savefig(OUTPUT_DIR + '/figures/FigS2_CryoSat2_ICESat2_compare.jpg',\n",
    "    dpi=500, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d99c3a-c3f7-489b-bbe1-d1304c5db5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab2eee-aa1e-4e04-8356-780914839c96",
   "metadata": {},
   "source": [
    "## Fig. S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d8601-7afc-46a4-92ea-d78606a23ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import subglacial lake outlines\n",
    "reexamined_stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/reexamined_stationary_outlines_gdf.geojson')\n",
    "evolving_outlines_union_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/evolving_outlines_union_gdf.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0331ad8-4e32-4c68-8e38-8e0023e95da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.family': 'DejaVu Sans',\n",
    "    'font.size': 8,\n",
    "    'axes.labelsize': 10,\n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 9,\n",
    "    'legend.fontsize': 8,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00847b27-5f18-45fb-b00b-89fa3e87b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add hydropotential field and colorbar\n",
    "\n",
    "# Select lakes to be included in plot\n",
    "selected_lakes = reexamined_stationary_outlines_gdf[reexamined_stationary_outlines_gdf['name'].isin(['Institute_E1', 'Mac2', 'Site_BC'])]\n",
    "desired_order = ['Institute_E1', 'Mac2', 'Site_BC']\n",
    "stationary_outlines_gdf_filtered = gpd.GeoDataFrame(pd.concat([selected_lakes[selected_lakes['name'] == name] for name in desired_order]))\n",
    "\n",
    "# Mapping for prettier lake names\n",
    "lake_label_map = {\n",
    "    'Institute_E1': r'Institute$_{\\text{E1}}$',\n",
    "    'Mac2': r'Mac$_{\\text{2}}$',\n",
    "    'Site_BC': r'Site$_{\\text{BC}}$'\n",
    "}\n",
    "\n",
    "# Create a grid of plots\n",
    "nrows, ncols = 3, 2\n",
    "# fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(8.5, 11), constrained_layout=False)\n",
    "fig = plt.figure(figsize=(8.5, 11))\n",
    "gs = fig.add_gridspec(\n",
    "    nrows=nrows, ncols=ncols,\n",
    "    width_ratios=[1, 1],   # equal column widths\n",
    "    height_ratios=[1, 1, 1],\n",
    "    wspace=-0.2,           # tighter horizontal gap\n",
    "    hspace=0.1\n",
    ")\n",
    "axs = gs.subplots()\n",
    "\n",
    "# Define colors and linestyles for legend\n",
    "stationary_outline_color = 'darkturquoise'\n",
    "stationary_line = plt.Line2D([], [], color=stationary_outline_color, linestyle='solid', linewidth=2)\n",
    "evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "# Set up colormap for temporal evolution\n",
    "cmap = plt.get_cmap('plasma', len(cyc_start_datetimes))\n",
    "norm = plt.Normalize(mdates.date2num(cyc_start_datetimes[0]), \n",
    "                    mdates.date2num(cyc_end_datetimes[-1]))\n",
    "\n",
    "# --- Load subglacial hydropotential dataset ---\n",
    "hydropot_path = DATA_DIR + \"/subglacial_hydropotential_Antarctica.nc\"\n",
    "hydropot_da = xr.open_dataset(hydropot_path)[\"subglacial_hydropotential\"]\n",
    "hydropot_arr = hydropot_da.squeeze().values\n",
    "\n",
    "# Compute global bounds across all subsets\n",
    "global_hydro_vmin = np.nanmin(hydropot_arr)\n",
    "global_hydro_vmax = np.nanmax(hydropot_arr)\n",
    "print(f\"Global hydropotential range: {global_hydro_vmin:.3f} – {global_hydro_vmax:.3f}\")\n",
    "\n",
    "# Initialize lists for min/max\n",
    "hydro_subset_mins, hydro_subset_maxs = [], []\n",
    "\n",
    "for lake_name in desired_order:\n",
    "    lake_gdf = stationary_outlines_gdf_filtered[\n",
    "        stationary_outlines_gdf_filtered['name'] == lake_name\n",
    "    ]\n",
    "    evolving_outlines_path = os.path.join(\n",
    "        os.getcwd(),\n",
    "        f'output/lake_outlines/evolving_outlines/{lake_name}.geojson'\n",
    "    )\n",
    "    try:\n",
    "        evolving_outlines_gdf = gpd.read_file(evolving_outlines_path)\n",
    "    except fiona.errors.DriverError:\n",
    "        continue\n",
    "\n",
    "    # Union of lake geometries to define spatial subset\n",
    "    evolving_stationary_union_gdf = gpd.GeoDataFrame(\n",
    "        geometry=[lake_gdf.geometry.iloc[0].union(evolving_outlines_gdf.geometry.union_all())],\n",
    "        crs=lake_gdf.crs\n",
    "    )\n",
    "    x_min, y_min, x_max, y_max = evolving_stationary_union_gdf.bounds.iloc[0]\n",
    "\n",
    "    if lake_name == 'Institute_E1':\n",
    "        # Smaller buffer to zoom in to this plot because outlines won't be blocked by inset map \n",
    "        buffer_frac = 0.2\n",
    "    elif lake_name == 'Mac2':\n",
    "        # Smaller buffer to zoom in to this plot because outlines won't be blocked by inset map \n",
    "        # and will crop out more of high hydropotential ridge to see hydropotential gradient better\n",
    "        buffer_frac = 0.15\n",
    "    else:\n",
    "        buffer_frac = 0.35\n",
    "\n",
    "    # Make square bounds + buffer\n",
    "    x_mid = (x_min + x_max) / 2\n",
    "    y_mid = (y_min + y_max) / 2\n",
    "    max_span = max(x_max - x_min, y_max - y_min)\n",
    "    x_min = x_mid - max_span / 2\n",
    "    x_max = x_mid + max_span / 2\n",
    "    y_min = y_mid - max_span / 2\n",
    "    y_max = y_mid + max_span / 2\n",
    "    x_buffer = abs(x_max - x_min) * buffer_frac\n",
    "    y_buffer = abs(y_max - y_min) * buffer_frac\n",
    "\n",
    "    # Subset hydropotential to same buffered area\n",
    "    hydropot_subset = hydropot_da.sel(\n",
    "        x=slice(x_min - x_buffer, x_max + x_buffer),\n",
    "        y=slice(y_max + y_buffer, y_min - y_buffer)\n",
    "    )\n",
    "    arr = hydropot_subset.squeeze().values\n",
    "\n",
    "    # Skip entirely masked or empty arrays\n",
    "    if np.all(~np.isfinite(arr)):\n",
    "        continue\n",
    "\n",
    "    hydro_subset_mins.append(np.nanpercentile(arr, 5))\n",
    "    hydro_subset_maxs.append(np.nanpercentile(arr, 95))\n",
    "\n",
    "# Compute global bounds across all subsets\n",
    "subset_hydro_vmin = np.nanmin(hydro_subset_mins)\n",
    "subset_hydro_vmax = np.nanmax(hydro_subset_maxs)\n",
    "print(f\"Geographic subset hydropotential range: {subset_hydro_vmin:.2f} – {subset_hydro_vmax:.2f}\")\n",
    "\n",
    "# Load modeled subglacial water flux raster\n",
    "flux_path = DATA_DIR + \"/SubglacialWaterFlux_Modelled_1km.tif\"\n",
    "\n",
    "# Use rioxarray to open modeled subglacial water flux raster\n",
    "flux_da = rioxarray.open_rasterio(flux_path, masked=True)\n",
    "flux_arr = flux_da.squeeze().values\n",
    "\n",
    "# Compute global bounds across all subsets\n",
    "global_flux_vmin = np.nanmin(flux_arr)\n",
    "global_flux_vmax = np.nanmax(flux_arr)\n",
    "print(f\"Global flux range: {global_flux_vmin:.3f} – {global_flux_vmax:.3f}\")\n",
    "\n",
    "# Initialize lists for min/max\n",
    "subset_mins, subset_maxs = [], []\n",
    "\n",
    "for lake_name in desired_order:\n",
    "    lake_gdf = stationary_outlines_gdf_filtered[\n",
    "        stationary_outlines_gdf_filtered['name'] == lake_name\n",
    "    ]\n",
    "    evolving_outlines_path = os.path.join(\n",
    "        os.getcwd(),\n",
    "        f'output/lake_outlines/evolving_outlines/{lake_name}.geojson'\n",
    "    )\n",
    "    try:\n",
    "        evolving_outlines_gdf = gpd.read_file(evolving_outlines_path)\n",
    "    except fiona.errors.DriverError:\n",
    "        continue\n",
    "\n",
    "    # Union to get bounds of all relevant geometries\n",
    "    evolving_stationary_union_gdf = gpd.GeoDataFrame(\n",
    "        geometry=[lake_gdf.geometry.iloc[0].union(evolving_outlines_gdf.geometry.union_all())],\n",
    "        crs=lake_gdf.crs\n",
    "    )\n",
    "    x_min, y_min, x_max, y_max = evolving_stationary_union_gdf.bounds.iloc[0]\n",
    "\n",
    "    if lake_name == 'Institute_E1':\n",
    "        buffer_frac = 0.2\n",
    "    elif lake_name == 'Mac2':\n",
    "        buffer_frac = 0.15\n",
    "    else:\n",
    "        buffer_frac = 0.35\n",
    "\n",
    "    # Make square bounds + buffer (same logic as your plot)\n",
    "    x_mid = (x_min + x_max) / 2\n",
    "    y_mid = (y_min + y_max) / 2\n",
    "    max_span = max(x_max - x_min, y_max - y_min)\n",
    "    x_min = x_mid - max_span / 2\n",
    "    x_max = x_mid + max_span / 2\n",
    "    y_min = y_mid - max_span / 2\n",
    "    y_max = y_mid + max_span / 2\n",
    "    x_buffer = abs(x_max - x_min) * buffer_frac\n",
    "    y_buffer = abs(y_max - y_min) * buffer_frac\n",
    "\n",
    "    # Subset flux raster to lake extent\n",
    "    flux_subset = flux_da.sel(\n",
    "        x=slice(x_min - x_buffer, x_max + x_buffer),\n",
    "        y=slice(y_max + y_buffer, y_min - y_buffer)\n",
    "    )\n",
    "\n",
    "    arr = flux_subset.squeeze().values\n",
    "    subset_mins.append(np.nanmin(arr))\n",
    "    subset_maxs.append(np.nanmax(arr))\n",
    "\n",
    "# Compute global bounds across all subsets\n",
    "subset_flux_vmin = np.nanmin(subset_mins)\n",
    "subset_flux_vmax = np.nanmax(subset_maxs)\n",
    "print(f\"Geographic subset flux range: {subset_flux_vmin:.3f} – {subset_flux_vmax:.3f}\")\n",
    "\n",
    "# Manually set vmax to show lower flux pathways at Site_BC\n",
    "flux_vmin_selected = np.nanpercentile(flux_arr, 75)\n",
    "flux_vmax_selected = 10\n",
    "\n",
    "for row in range(1, nrows):\n",
    "    # Share y-axis within each row but not between rows\n",
    "    for col in range(ncols):\n",
    "        axs[row, col].sharey(axs[row, 0])\n",
    "\n",
    "subset_hydro_vmins, subset_hydro_vmaxs = [], []\n",
    "\n",
    "# Process each lake sequentially\n",
    "for row, lake_name in enumerate(desired_order):\n",
    "    print('working on {}'.format(lake_name))\n",
    "\n",
    "    # Get the lake data for the current lake\n",
    "    lake_gdf = stationary_outlines_gdf_filtered[stationary_outlines_gdf_filtered['name'] == lake_name]\n",
    "    stationary_outline = lake_gdf['geometry']\n",
    "    \n",
    "    # Load evolving outlines\n",
    "    try:\n",
    "        evolving_outlines_gdf = gpd.read_file(os.path.join(\n",
    "            os.getcwd(), 'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)))\n",
    "    except fiona.errors.DriverError:\n",
    "        print(f\"File for {lake_name} not found. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Find evolving and stationary outlines union for plotting extent\n",
    "    lake_union_gdf = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]\n",
    "    x_min, y_min, x_max, y_max = lake_union_gdf.bounds.iloc[0]\n",
    "    \n",
    "    # Make plots uniform size and square\n",
    "    x_mid = (x_min + x_max) / 2\n",
    "    y_mid = (y_min + y_max) / 2\n",
    "    x_span = x_max - x_min\n",
    "    y_span = y_max - y_min\n",
    "    max_span = max(x_span, y_span)\n",
    "    \n",
    "    # Update bounds to ensure square dimensions\n",
    "    x_min = x_mid - max_span / 2\n",
    "    x_max = x_mid + max_span / 2\n",
    "    y_min = y_mid - max_span / 2\n",
    "    y_max = y_mid + max_span / 2\n",
    "    \n",
    "    # Add buffer around the plot\n",
    "    if lake_name == 'Institute_E1':\n",
    "        buffer_frac = 0.2\n",
    "    elif lake_name == 'Mac2':\n",
    "        buffer_frac = 0.15  \n",
    "    else:\n",
    "        buffer_frac = 0.35\n",
    "    x_buffer = abs(x_max-x_min) * buffer_frac\n",
    "    y_buffer = abs(y_max-y_min) * buffer_frac\n",
    "    \n",
    "    # Create empty lists to store centroid coordinates\n",
    "    centroids_x = []\n",
    "    centroids_y = []\n",
    "    centroid_dates = []\n",
    "    \n",
    "    # Plot both outline and centroid views\n",
    "    for col in [0, 1]:\n",
    "        # Plot MOA surface imagery for both columns\n",
    "        mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "        mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "        moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "        axs[row, col].imshow(moa_highres_da_subset[0,:,:], cmap='gray', clim=[14000, 17000], \n",
    "                             extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer], zorder=0)\n",
    "\n",
    "\n",
    "        # Plot grounding line\n",
    "        Scripps_landice.boundary.plot(ax=axs[row, col], color='k', linewidth=1)#, zorder=3)\n",
    "        Scripps_landice.boundary.plot(ax=axs[row, col], color='k', linewidth=1)#, zorder=3)\n",
    "\n",
    "        \n",
    "        # --- Plot subglacial hydropotential layer (between MOA and flux) ---\n",
    "        # Subset hydropotential to same extent as MOA subset\n",
    "        hydropot_subset = hydropot_da.sel(\n",
    "            x=slice(x_min - x_buffer, x_max + x_buffer),\n",
    "            y=slice(y_max + y_buffer, y_min - y_buffer)\n",
    "        )\n",
    "        \n",
    "        # Convert to array for plotting\n",
    "        hydropot_arr = hydropot_subset.squeeze().values\n",
    "        \n",
    "        # Mask invalid or extreme values\n",
    "        hydropot_arr = np.where(np.isfinite(hydropot_arr), hydropot_arr, np.nan)\n",
    "        \n",
    "        # Choose a subtle colormap (to keep flux visible)\n",
    "        cmap_hydro = plt.cm.cividis\n",
    "        vmin_hydro = np.nanmin(hydropot_arr)\n",
    "        vmax_hydro = np.nanmax(hydropot_arr)\n",
    "\n",
    "        if col==0:\n",
    "            subset_hydro_vmins.append(vmin_hydro)\n",
    "            subset_hydro_vmaxs.append(vmax_hydro)\n",
    "\n",
    "        # Plot hydropotential (below flux, above MOA)\n",
    "        im_hydro = axs[row, col].imshow(\n",
    "            hydropot_arr,\n",
    "            cmap=cmap_hydro,\n",
    "            vmin=vmin_hydro,\n",
    "            vmax=vmax_hydro,\n",
    "            extent=[x_min - x_buffer, x_max + x_buffer, y_min - y_buffer, y_max + y_buffer],\n",
    "            alpha=0.6,\n",
    "            zorder=0.5   # between MOA (z=0) and flux (zorder=1)\n",
    "        )\n",
    "\n",
    "        # Clip modeled subglacial water flux raster to extent\n",
    "        flux_subset = flux_da.sel(\n",
    "            x=slice(x_min-x_buffer, x_max+x_buffer),\n",
    "            y=slice(y_max+y_buffer, y_min-y_buffer)\n",
    "        )\n",
    "        \n",
    "        # Get your flux data as a numpy array\n",
    "        flux_arr = flux_subset.squeeze().values\n",
    "        \n",
    "        # Define scaling range for alpha\n",
    "        vmin, vmax = flux_vmin_selected, flux_vmax_selected\n",
    "        \n",
    "        # Normalize flux to [0,1]\n",
    "        normed = (flux_arr - vmin) / (vmax - vmin)\n",
    "        normed = np.clip(normed, 0, 1)\n",
    "        \n",
    "        # Get colormap (this returns RGBA with fixed alpha=1)\n",
    "        cmap_flux = plt.cm.Blues\n",
    "        rgba = cmap_flux(normed)\n",
    "        \n",
    "        # Replace alpha channel with normalized flux values\n",
    "        rgba[..., -1] = normed**0.5 # zero -> fully transparent, large -> opaque\n",
    "        \n",
    "        # Transparent overlay of modeled subglacial water flux raster\n",
    "        im_flux = axs[row, col].imshow(\n",
    "            rgba,\n",
    "            extent=[x_min - x_buffer, x_max + x_buffer, y_min - y_buffer, y_max + y_buffer],\n",
    "            zorder=1\n",
    "        )\n",
    "\n",
    "        # Plot stationary outline in both columns\n",
    "        if lake_name == 'Site_BC':\n",
    "            # Plot both Site_B and Site_C prior stationary outlines\n",
    "            for site in ['Site_B', 'Site_C']:\n",
    "                stationary_outlines_gdf[stationary_outlines_gdf['name'] == site]['geometry'].boundary.plot(\n",
    "                    ax=axs[row, col], color=stationary_outline_color, linewidth=2)\n",
    "\n",
    "                # Calculate centroid\n",
    "                centroid = stationary_outlines_gdf[stationary_outlines_gdf['name'] == site]['geometry'].iloc[0].centroid\n",
    "        \n",
    "                # Plot centroids\n",
    "                axs[row, col].scatter(centroid.x, centroid.y, \n",
    "                                   c=stationary_outline_color, marker='.', s=50, linewidth=1, zorder=2)\n",
    "\n",
    "            # Plot both Site_BC updated stationary outlines\n",
    "            evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]['geometry'].boundary.plot(\n",
    "                ax=axs[row, col], color='k', linestyle='dotted', linewidth=2, zorder=2)\n",
    "            \n",
    "\n",
    "        else:\n",
    "            # Original code for other lakes\n",
    "            stationary_outlines_gdf[stationary_outlines_gdf['name'] == lake_name]['geometry'].boundary.plot(\n",
    "                ax=axs[row, col], color=stationary_outline_color, linewidth=2)\n",
    "\n",
    "            evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]['geometry'].boundary.plot(\n",
    "                ax=axs[row, col], color='k', linestyle='dotted', linewidth=2, zorder=2)\n",
    "    \n",
    "    # Plot evolving outlines with colors based on date (left column only)\n",
    "    lines = []\n",
    "    for dt_idx, dt in enumerate(cyc_start_datetimes[1:]):\n",
    "        # Create line for legend\n",
    "        x, y = 1, 1\n",
    "        line, = axs[row, 0].plot(x, y, color=cmap(norm(mdates.date2num(cyc_start_datetimes[dt_idx]))))\n",
    "        lines.append(line)\n",
    "        \n",
    "        # Plot evolving outlines for this time step\n",
    "        evolving_outlines_gdf_dt_sub = evolving_outlines_gdf[evolving_outlines_gdf['mid_pt_datetime'] == dt]\n",
    "        if not evolving_outlines_gdf_dt_sub.empty:\n",
    "            # Plot outline in left column\n",
    "            evolving_outlines_gdf_dt_sub.boundary.plot(\n",
    "                ax=axs[row, 0], \n",
    "                color=cmap(norm(mdates.date2num(cyc_start_datetimes[dt_idx]))), \n",
    "                linewidth=1, zorder=1\n",
    "            )\n",
    "            \n",
    "            # Calculate and store centroid\n",
    "            centroid = evolving_outlines_gdf_dt_sub.geometry.iloc[0].centroid\n",
    "            centroids_x.append(centroid.x)\n",
    "            centroids_y.append(centroid.y)\n",
    "            centroid_dates.append(dt)\n",
    "    \n",
    "    # Plot centroids in right column\n",
    "    axs[row, 1].scatter(centroids_x, centroids_y, \n",
    "                       c=[cmap(norm(mdates.date2num(dt))) for dt in centroid_dates],\n",
    "                       marker='+', s=100, linewidth=1, zorder=2)\n",
    "    \n",
    "    # Set the same limits and formatting for both plots\n",
    "    for col in [0, 1]:\n",
    "        # Format axis ticks to show kilometers\n",
    "        km_scale = 1e3\n",
    "        ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "        ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "        axs[row, col].xaxis.set_major_formatter(ticks_x)\n",
    "        axs[row, col].yaxis.set_major_formatter(ticks_y)\n",
    "        \n",
    "        # Set axes limits\n",
    "        axs[row, col].set(xlim=(x_min-x_buffer, x_max+x_buffer), \n",
    "                         ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "        \n",
    "        # Remove y-tick labels for right column only\n",
    "        if col == 1:\n",
    "            plt.setp(axs[row, col].get_yticklabels(), visible=False)\n",
    "        \n",
    "        axs[2, col].set_xlabel('x [km]')        \n",
    "        axs[row, 0].set_ylabel('y [km]')\n",
    "\n",
    "        # axs[row, 1].set_ylabel(f'{lake_name}')\n",
    "        axs[row, 1].set_ylabel(lake_label_map.get(lake_name, lake_name), fontsize=12)\n",
    "        axs[row, 1].yaxis.set_label_position('right')\n",
    "\n",
    "    # Create and style inset map (only for left column)\n",
    "    axIns = axs[row, 0].inset_axes([0.01, -0.01, 0.3, 0.3])\n",
    "    axIns.set_aspect('equal')\n",
    "    moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    axIns.axis('off')\n",
    "    \n",
    "    # Add location marker to inset map\n",
    "    axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "                 linewidth=1, color='k', s=15, zorder=3)\n",
    "\n",
    "for i in range(axs.shape[0]):\n",
    "    for j in range(axs.shape[1]):\n",
    "        # Add subplot labels (a, b, c, etc.)\n",
    "        axs[i,j].text(0.02, 0.98, chr(97 + i*2 + j), transform=axs[i,j].transAxes, \n",
    "                     fontsize=14, va='top', ha='left')\n",
    "\n",
    "# Set up colormap\n",
    "min_date = pd.to_datetime(cyc_start_datetimes[0])\n",
    "max_date = pd.to_datetime(cyc_end_datetimes[-1])\n",
    "date_range = pd.date_range(min_date, max_date, periods=len(cyc_start_datetimes[1:]))\n",
    "years = date_range.year.unique()\n",
    "years = pd.to_datetime(years, format='%Y')\n",
    "n_dates = len(cyc_start_datetimes[1:])\n",
    "cmap = plt.get_cmap('plasma', n_dates)\n",
    "norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "m.set_array(np.linspace(mdates.date2num(min_date), mdates.date2num(max_date), n_dates))\n",
    "\n",
    "legend = axs[0,0].legend([tuple(lines), stationary_line, evolving_union_line], \n",
    "    ['evolving outlines', 'prior stationary outline', 'updated stationary outline'],\n",
    "    handlelength=2.5, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper right')\n",
    "\n",
    "# Add combined legend for evolving outline centroids in [0,1]\n",
    "ax_legend = axs[0, 1]\n",
    "# Choose start, middle, and end colors from the plasma colormap\n",
    "colors = [cmap(0.25), cmap(0.5), cmap(0.75)]\n",
    "# Create three + markers with different colors\n",
    "markers = [\n",
    "    plt.Line2D([], [], color=c, marker='+', linestyle='None', markersize=7, linewidth=1)\n",
    "    for c in colors\n",
    "]\n",
    "# Combine them into one legend entry\n",
    "combined_handle = tuple(markers)\n",
    "\n",
    "# Add legend with one label\n",
    "ax_legend.legend(\n",
    "    [combined_handle],\n",
    "    ['evolving outline centroids'],\n",
    "    loc='lower center',\n",
    "    handlelength=2.5,\n",
    "    handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    frameon=True,\n",
    "    fontsize=8,\n",
    "    title_fontsize=9,\n",
    "    borderpad=0.4,\n",
    ")\n",
    "\n",
    "\n",
    "lake_point = plt.Line2D([], [], color=stationary_outline_color, ls='none', marker=\".\")\n",
    "grounding_line = plt.Line2D([], [], color='k', lw=0.75)\n",
    "prior_stationary_outline = plt.Line2D([], [], \n",
    "    color=stationary_outline_color, marker='o', ls='none',\n",
    "    mfc='none', mec=stationary_outline_color, mew=1.5)\n",
    "\n",
    "# Create legend\n",
    "legend = axs[2,0].legend(\n",
    "    [lake_point, prior_stationary_outline, grounding_line],\n",
    "    ['lake point', 'prior stationary outline estimate', 'grounding line'],\n",
    "    handlelength=2.5,\n",
    "    handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper right'\n",
    ")\n",
    "\n",
    "\n",
    "# Loop through each row in the first column and add its own colorbar\n",
    "for row in range(nrows):\n",
    "    # Skip if hydropotential not plotted (e.g., empty)\n",
    "    if 'subset_hydro_vmins' not in locals() or 'subset_hydro_vmaxs' not in locals():\n",
    "        continue\n",
    "\n",
    "    # Retrieve per-row vmin/vmax computed during plotting\n",
    "    sm_hydro = ScalarMappable(\n",
    "        norm=Normalize(vmin=subset_hydro_vmins[row]/1e3, vmax=subset_hydro_vmaxs[row]/1e3),\n",
    "        cmap=cmap_hydro\n",
    "    )\n",
    "    sm_hydro.set_array([])\n",
    "\n",
    "    # Create small horizontal inset colorbar inside the subplot\n",
    "    cax_hydro = inset_axes(\n",
    "        axs[row, 0],\n",
    "        width='67%',\n",
    "        height='3%',\n",
    "        loc='lower left',\n",
    "        bbox_to_anchor=[0.3, 0.15, 1, 1],\n",
    "        bbox_transform=axs[row, 0].transAxes,\n",
    "        borderpad=0\n",
    "    )\n",
    "\n",
    "    cbar_hydro = fig.colorbar(\n",
    "        sm_hydro,\n",
    "        cax=cax_hydro,\n",
    "        orientation='horizontal',\n",
    "        extend='both'\n",
    "    )\n",
    "\n",
    "    # Make tick labels white and show only whole numbers\n",
    "    cbar_hydro.ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "    cbar_hydro.ax.tick_params(\n",
    "        labelsize=7,\n",
    "        direction='out',\n",
    "        length=2,\n",
    "        colors='white'  # ticks and tick labels\n",
    "    )\n",
    "\n",
    "    # Set white color for colorbar label\n",
    "    cbar_hydro.set_label(\n",
    "        \"subglacial hydropotential (MPa)\",\n",
    "        labelpad=4,\n",
    "        fontsize=8,\n",
    "        color='white'\n",
    "    )\n",
    "\n",
    "    # Set white color for the colorbar outline (spine)\n",
    "    for spine in cbar_hydro.ax.spines.values():\n",
    "        spine.set_color('white')\n",
    "\n",
    "# Create colorbar for flux using the same global normalization and cmap\n",
    "sm = ScalarMappable(norm=Normalize(vmin=flux_vmin_selected, vmax=flux_vmax_selected), cmap=plt.cm.Blues)\n",
    "sm.set_array([])\n",
    "# Add colorbar at the top (horizontal)\n",
    "cax_flux = fig.add_axes([0.185, 0.94, 0.655, 0.01])\n",
    "cbar_flux = plt.colorbar(sm, cax=cax_flux, orientation='horizontal', extend='both')\n",
    "cbar_flux.ax.xaxis.set_label_position('top')\n",
    "cbar_flux.ax.xaxis.set_ticks_position('top')\n",
    "cbar_flux.ax.xaxis.tick_top()\n",
    "# # cbar_flux.ax.tick_params(bottom=False, labelbottom=False)\n",
    "cbar_flux.set_label(\"modeled subglacial water flux (m$^3$ s$^{-1}$)\", labelpad=5)\n",
    "\n",
    "# Add colorbar \n",
    "cax = fig.add_axes([0.185, 0.1, 0.655, 0.01]) # [left, bottom, width, height] # cbar = fig.colorbar(m, cax=cax, orientation='horizontal') \n",
    "cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "cbar.set_label('evolving outline/centroid year', size=11, labelpad=5) \n",
    "# Set ticks for all years but labels only for even years, skipping first year, 2010.0, as it starts before time series\n",
    "tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "tick_labels = [f\"'{date.strftime('%Y')}\" if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "cbar.set_ticks(tick_locations)\n",
    "cbar.set_ticklabels(tick_labels)\n",
    "cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Add minor ticks for quarters\n",
    "\n",
    "# Adjust the layout to make room for the colorbar\n",
    "plt.subplots_adjust(\n",
    "    top=0.93,       # Reduce top margin (default is usually 0.9)\n",
    "    bottom=0.16,    # Increase bottom margin for colorbar (up from 0.1)\n",
    ")\n",
    "\n",
    "# Save and close plot\n",
    "plt.savefig(OUTPUT_DIR + '/figures/FigS3_lake_migration.jpg',\n",
    "    dpi=500, bbox_inches='tight')\n",
    "\n",
    "# Preview plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e96e995-abd6-4cac-8096-71f169b5730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480791ea-059e-46ee-9d50-f9bcddeda78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View citation for each of plotted lakes for figure caption\n",
    "selected_lakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e5dd0-d26c-405a-982f-ca7537320e7f",
   "metadata": {},
   "source": [
    "## Fig. S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637016fb-36b5-423c-872e-d27fab6bc8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import reexamined stationary subglacial lake outlines\n",
    "reexamined_stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/reexamined_stationary_outlines_gdf.geojson')\n",
    "\n",
    "# Create filtered geodataframes of lakes based on whether they have evolving outlines\n",
    "folder_path = os.path.join ('output/lake_outlines/evolving_outlines')\n",
    "\n",
    "# Lakes with evolving outlines (.geojson)\n",
    "evolving_outlines_lakes = filter_gdf_by_folder_contents(reexamined_stationary_outlines_gdf, folder_path, file_extension='geojson', exclude=False)\n",
    "\n",
    "# For the evolving_outlines_lakes, we must add the special case of Site_B_Site_C that are now a combined lake\n",
    "# include_list = ['Site_B', 'Site_C']\n",
    "# included_rows = reexamined_stationary_outlines_gdf[reexamined_stationary_outlines_gdf['name'].isin(include_list)]\n",
    "# evolving_outlines_lakes = pd.concat([evolving_outlines_lakes, included_rows]).drop_duplicates()\n",
    "print('lakes with evolving outlines:',len(evolving_outlines_lakes))\n",
    "\n",
    "# Lakes with non-dynamic outlines (.txt)\n",
    "no_evolving_outlines_lakes = filter_gdf_by_folder_contents(reexamined_stationary_outlines_gdf, folder_path, file_extension='txt', exclude=False)\n",
    "print('lakes without evolving outlines:',len(no_evolving_outlines_lakes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14376899-3327-4f94-83bc-6c8385c6a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.family': 'DejaVu Sans',\n",
    "    'font.size': 14,\n",
    "    'axes.labelsize': 13,\n",
    "    'xtick.labelsize': 14,\n",
    "    'ytick.labelsize': 14,\n",
    "    'legend.fontsize': 12,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed423b8-7916-4553-a676-394c108155bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "def process_lake_data(directory, is_evolving=True):\n",
    "    dfs = {}\n",
    "    for file in os.listdir(directory):\n",
    "        # Exclude continental summation files\n",
    "        if \"subset\" in file or \"superset\" in file:\n",
    "            continue\n",
    "        # Ignore files that are not csv's\n",
    "        if not file.endswith('.csv'):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(directory, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        lake_name = os.path.splitext(file)[0]\n",
    "\n",
    "        df['lake_name'] = lake_name\n",
    "        df['datetime'] = pd.to_datetime(df['mid_pt_datetime'])\n",
    "\n",
    "        if is_evolving:\n",
    "            df['cumsum_vol'] = np.cumsum(df['evolving_outlines_dV_corr (m^3)']) / 1e9\n",
    "        else:\n",
    "            df['cumsum_vol'] = np.cumsum(df['stationary_outline_dV_corr (m^3)']) / 1e9\n",
    "\n",
    "        dfs[lake_name] = df\n",
    "    return dfs\n",
    "\n",
    "# Load data\n",
    "prior_stationary_dfs = process_lake_data(\"output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes\", is_evolving=False)\n",
    "prior_stationary_subset_dfs = process_lake_data(\"output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes\", is_evolving=False)\n",
    "evolving_dfs = process_lake_data(\"output/geometric_calcs/evolving_outlines_geom_calc/forward_fill\", is_evolving=True)\n",
    "updated_stationary_dfs = process_lake_data(\"output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes\", is_evolving=False)\n",
    "\n",
    "# Setup figure\n",
    "fig, axs = plt.subplots(5, 1, figsize=(11, 16), sharex=True, constrained_layout=True)\n",
    "\n",
    "# Store dates and time period for satellite coverage eras\n",
    "time_span = mdates.date2num(cyc_datetimes['cyc_end_datetimes'].iloc[-1]) - mdates.date2num(cyc_datetimes['cyc_start_datetimes'].iloc[0])\n",
    "start_date = mdates.date2num(cyc_datetimes['cyc_start_datetimes'].iloc[0])\n",
    "SARIn_expand_date = mdates.date2num(cyc_datetimes[cyc_datetimes['cyc_start_datetimes'] == '2013-10-01T18:00:00.000000000']['cyc_start_datetimes'].iloc[0])\n",
    "CS2_IS2_tie_pt = mdates.date2num(cyc_datetimes[cyc_datetimes['dataset'] == 'ICESat2_ATL15'].iloc[0]['cyc_start_datetimes'])\n",
    "\n",
    "# Add annotations for satellite/mode dates\n",
    "start_date_text = pd.to_datetime(cyc_datetimes['cyc_start_datetimes'].iloc[0]) + pd.Timedelta(days=15)\n",
    "sarin_expand_date_text = pd.to_datetime(cyc_datetimes[cyc_datetimes['cyc_start_datetimes'] == '2013-10-01T18:00:00.000000000']['cyc_start_datetimes'].iloc[0]) + pd.Timedelta(days=15)\n",
    "is2_start_date_text = pd.to_datetime(cyc_datetimes[cyc_datetimes['dataset'] == 'ICESat2_ATL15'].iloc[0]['cyc_start_datetimes']) + pd.Timedelta(days=15)\n",
    "\n",
    "axs[0].text(start_date_text, -3.9, 'CryoSat-2 era begins', horizontalalignment='left', verticalalignment='top', color='k')\n",
    "axs[0].text(sarin_expand_date_text, -3.9, 'CryoSat-2 SARIn mode expands', horizontalalignment='left', verticalalignment='top', color='k')\n",
    "axs[0].text(is2_start_date_text, -3.9, 'ICESat-2 era begins', horizontalalignment='left', verticalalignment='top', color='k')\n",
    "\n",
    "for row in range(len(axs)):\n",
    "    # Plot horizontal line at zero for reference\n",
    "    axs[row].axhline(0, color='k', linewidth=0.5, zorder=0)\n",
    "    # Plot vertical lines to indicate CS2 SARIn mode mask moving inland and ICESat-2 era start\n",
    "    axs[row].axvline(SARIn_expand_date, color='dimgray', linestyle='solid', linewidth=0.75, ymin=-1, ymax=1, zorder=0)\n",
    "    axs[row].axvline(CS2_IS2_tie_pt, color='dimgray', linestyle='solid', linewidth=0.75, ymin=-1, ymax=1, zorder=0)\n",
    "\n",
    "# Settings\n",
    "special_lakes = [\"Byrd_2\", \"EngelhardtSubglacialLake\", \"Site_BC\", \"Slessor_23\", \"Thw_124\"]\n",
    "special_colors = dict(zip(special_lakes, [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:purple\", \"tab:brown\"]))\n",
    "\n",
    "# Panel 1: evolving (subset)\n",
    "for lake, df in evolving_dfs.items():\n",
    "    if lake in special_lakes:\n",
    "        axs[0].plot(df[\"datetime\"], df[\"cumsum_vol\"], color=special_colors[lake], \n",
    "            marker='o', markersize=2, lw=1.5, label=lake, zorder=2)\n",
    "    else:\n",
    "        axs[0].plot(df[\"datetime\"], df[\"cumsum_vol\"], color=\"lightgray\", marker='o', markersize=2, lw=0.8, zorder=1)\n",
    "\n",
    "# Legend labels\n",
    "special_labels = {\n",
    "    \"Byrd_2\": r\"Byrd$_{2}$\",\n",
    "    \"EngelhardtSubglacialLake\": \"Engelhardt Subglacial Lake\",\n",
    "    \"Site_BC\": r\"Site$_{\\text{BC}}$\",\n",
    "    \"Slessor_23\": r\"Slessor$_{23}$\",\n",
    "    \"Thw_124\": r\"Thw$_{124}$\",\n",
    "}\n",
    "\n",
    "handles = []\n",
    "for key, clr in special_colors.items():\n",
    "    handles.append(Line2D([0], [0], color=clr, lw=1.25, marker='o', markersize=3, label=special_labels[key]))\n",
    "\n",
    "# Append “other lakes” at the end\n",
    "handles.append(Line2D([0], [0], color=\"gray\", lw=1.25, marker='o', markersize=3, label=f\"other lakes (n={len(evolving_outlines_lakes) - len(special_lakes)})\"))\n",
    "\n",
    "axs[0].legend(handles=handles, loc=\"upper center\", ncol=2, bbox_to_anchor=(0.3, 1.0))\n",
    "\n",
    "# Panel 2: prior stationary (all lakes)\n",
    "for lake, df in prior_stationary_dfs.items():\n",
    "    color = \"turquoise\" if lake in evolving_dfs else \"red\"\n",
    "    axs[1].plot(df[\"datetime\"], df[\"cumsum_vol\"],  marker='o', markersize=2, color=color, alpha=0.5, lw=1)\n",
    "\n",
    "# Add custom legend handles and legend\n",
    "handles = [\n",
    "    Line2D([0], [0], color=\"turquoise\", lw=1.25, marker='o', markersize=3, label=f\"evolving outlines found (n={len(evolving_outlines_lakes)})\"),\n",
    "    Line2D([0], [0], color=\"red\", lw=1.25, marker='o', markersize=3, label=f\"evolving outlines not found (n={len(no_evolving_outlines_lakes)})\")\n",
    "]\n",
    "axs[1].legend(handles=handles, loc=\"upper center\", bbox_to_anchor=(0.79, 1.0))\n",
    "\n",
    "# Panel 3: updated stationary (subset)\n",
    "for lake, df in updated_stationary_dfs.items():\n",
    "    if lake in special_lakes:\n",
    "        axs[2].plot(df[\"datetime\"], df[\"cumsum_vol\"], color=special_colors[lake], marker='o', markersize=2, lw=1.5, zorder=1)\n",
    "    else:\n",
    "        axs[2].plot(df[\"datetime\"], df[\"cumsum_vol\"], color=\"lightgray\", marker='o', markersize=2, lw=0.8, zorder=0)\n",
    "\n",
    "# Panel 4: bias (evolving – prior stationary)\n",
    "for lake, df in evolving_dfs.items():\n",
    "    df_evo = evolving_dfs[lake][[\"datetime\", \"cumsum_vol\"]].rename(columns={\"cumsum_vol\": \"evolving\"})\n",
    "    df_stat = prior_stationary_dfs[lake][[\"datetime\", \"cumsum_vol\"]].rename(columns={\"cumsum_vol\": \"stationary\"})\n",
    "    merged = pd.merge(df_evo, df_stat, on=\"datetime\", how=\"inner\")\n",
    "    merged[\"bias\"] = merged[\"evolving\"] - merged[\"stationary\"]\n",
    "    if lake in special_colors:\n",
    "        color = special_colors[lake]\n",
    "        lw = 1.5\n",
    "        zorder = 2\n",
    "    else:\n",
    "        color = \"lightgray\"\n",
    "        lw = 0.8\n",
    "        zorder = 1\n",
    "    axs[3].plot(merged[\"datetime\"], merged[\"bias\"], color=color, marker='o', markersize=2, zorder=zorder, lw=lw)\n",
    "\n",
    "# Panel 5: bias (evolving – updated stationary)\n",
    "for lake, df in evolving_dfs.items():\n",
    "    df_evo = evolving_dfs[lake][[\"datetime\", \"cumsum_vol\"]].rename(columns={\"cumsum_vol\": \"evolving\"})\n",
    "    df_stat = updated_stationary_dfs[lake][[\"datetime\", \"cumsum_vol\"]].rename(columns={\"cumsum_vol\": \"stationary\"})\n",
    "    merged = pd.merge(df_evo, df_stat, on=\"datetime\", how=\"inner\")\n",
    "    merged[\"bias\"] = merged[\"evolving\"] - merged[\"stationary\"]\n",
    "    color = \"darkblue\" if lake in special_lakes else \"lightgray\"\n",
    "    merged[\"bias\"] = merged[\"evolving\"] - merged[\"stationary\"]\n",
    "    if lake in special_colors:\n",
    "        color = special_colors[lake]\n",
    "        lw = 1.5\n",
    "        zorder = 2\n",
    "    else:\n",
    "        color = \"lightgray\"\n",
    "        lw = 0.8\n",
    "        zorder = 1\n",
    "    axs[4].plot(merged[\"datetime\"], merged[\"bias\"], color=color, marker='o', markersize=2, zorder=zorder, lw=lw)\n",
    "\n",
    "# Axis formatting\n",
    "for ax in axs:\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator(base=1))  # Major ticks every other year\n",
    "    ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Minor ticks every quarter\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "    ax.tick_params(axis=\"x\")\n",
    "\n",
    "    # Set x-axis limits\n",
    "    axs[row].set(xlim=(cyc_datetimes['cyc_start_datetimes'].iloc[0],\n",
    "        # Set righthand x-axis limit slightly earlier to prevent tick mark displaying when there is no data point\n",
    "        (cyc_datetimes['cyc_end_datetimes'].iloc[-1] - datetime.timedelta(days=2))))\n",
    "\n",
    "# Set axes titles\n",
    "axs[0].set_ylabel('cumulative $dV$ [km$^3$]\\nusing evolving outlines')\n",
    "axs[1].set_ylabel('cumulative $dV$ [km$^3$] using\\nprior stationary outlines')\n",
    "axs[2].set_ylabel('cumulative $dV$ [km$^3$] using\\nupdated stationary outlines')\n",
    "axs[3].set_ylabel('cumulative $dV$ bias [km$^3$],\\nevolving – prior stationary')\n",
    "axs[4].set_ylabel('cumulative $dV$ bias [km$^3$],\\nevolving – updated stationary')\n",
    "\n",
    "# Set y axes limits\n",
    "for ax in axs:\n",
    "    ax.set_ylim(-4.5, 4.5)\n",
    "\n",
    "# Adding annotations at the top left of the subplot\n",
    "ax_array = np.array(axs)  # Convert gridspec list of lists into numpy array to use .flatten() method\n",
    "char_index = 97  # ASCII value for 'a'\n",
    "for i, ax in enumerate(ax_array.flatten()):\n",
    "    # `transform=ax.transAxes` makes coordinates relative to the axes (0,0 is bottom left and 1,1 is top right)\n",
    "    ax.text(0.01, 0.98, chr(char_index), transform=ax.transAxes, fontsize=14, va='top', ha='left')\n",
    "    char_index += 1 # Increment the ASCII index to get the next character\n",
    "    \n",
    "\n",
    "axs[-1].set_xlabel(\"year\", fontsize=16)\n",
    "\n",
    "# Save and close plot\n",
    "plt.savefig(OUTPUT_DIR + '/figures/FigS4_individual_lakes_dV.jpg',\n",
    "    dpi=500, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa62bb3-2608-4f96-93ba-2dad8bf9f87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924412bc-1bdb-451b-82e5-abd920a66b2c",
   "metadata": {},
   "source": [
    "## Fig. S5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98128240-3fcd-4804-abae-c9cb9d18884c",
   "metadata": {},
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3adfdd-693c-45a6-a980-9a50d753fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore different lake groups to find one to highlight in publication\n",
    "\n",
    "# Example lake groups\n",
    "lake_groups = [\n",
    "    ('Bindschadler', ['Bindschadler_1', 'Bindschadler_2', 'Bindschadler_3', 'Bindschadler_4', 'Bindschadler_5', 'Bindschadler_6']),\n",
    "    ('Byrd', ['Byrd_1', 'Byrd_2', 'Byrd_s1', 'Byrd_s2', 'Byrd_s3', 'Byrd_s4', 'Byrd_s5', 'Byrd_s6', 'Byrd_s7', 'Byrd_s8',\n",
    "              'Byrd_s9', 'Byrd_s10', 'Byrd_s11', 'Byrd_s12', 'Byrd_s13', 'Byrd_s14', 'Byrd_s15']),\n",
    "    ('Cook', ['Cook_E1', 'Cook_E2']),\n",
    "    ('David', ['David_1', 'David_s1', 'David_s2', 'David_s3', 'David_s4', 'David_s5']),\n",
    "    ('DML', ['M1', 'M2', 'R1', 'R2', 'R3', 'V1']),\n",
    "    ('EAP', ['EAP_1', 'EAP_2', 'EAP_3', 'EAP_4', 'EAP_5', 'EAP_6', 'EAP_7', 'EAP_8', 'EAP_9']),\n",
    "    ('Foundation_N', ['Foundation_N1', 'Foundation_N2', 'Foundation_N3']),\n",
    "    ('Foundation', ['Foundation_1', 'Foundation_2', 'Foundation_3', 'Foundation_4', 'Foundation_5', 'Foundation_6', 'Foundation_7', 'Foundation_8',\n",
    "                    'Foundation_9', 'Foundation_10', 'Foundation_11', 'Foundation_12', 'Foundation_13', 'Foundation_14', 'Foundation_15', 'Foundation_16']),\n",
    "    ('Institute', ['Institute_E1', 'Institute_E2', 'Institute_W1', 'Institute_W2']),\n",
    "    ('Jutulstraumen', ['JG_Combined_D2_b_E1', 'JG_Combined_E2_F2', 'JG_D1_a', 'JG_D1_a', 'JG_F1']),\n",
    "    ('KambTrunk', ['KT3', 'KT2', 'KT1']),\n",
    "    ('Kamb', ['Kamb_1', 'Kamb_2', 'Kamb_3', 'Kamb_4', 'Kamb_5', 'Kamb_6', 'Kamb_7', 'Kamb_8', 'Kamb_9', 'Kamb_10', 'Kamb_11', 'Kamb_12']),\n",
    "    ('Lambert', ['Lambert_1']),\n",
    "    ('MacAyeal', ['Mac1', 'Mac2', 'Mac3', 'Mac4', 'Mac5', 'Mac6']),\n",
    "    ('Nimrod', ['Nimrod_1', 'Nimrod_2']),\n",
    "    ('Ninnis', ['Ninnis_1', 'Ninnis_2']),\n",
    "    ('Recovery', ['Rec1', 'Rec2', 'Rec3', 'Rec4', 'Rec5', 'Rec6', 'Rec7', 'Recovery_8', 'Rec8', 'Rec9']),\n",
    "    ('Rutford', ['Rutford_1']),\n",
    "    ('Site_ABC', ['Site_A', 'Site_BC']),\n",
    "    ('Slessor', ['Slessor_1', 'Slessor_23', 'Slessor_4', 'Slessor_5', 'Slessor_6', 'Slessor_7']),\n",
    "    ('Thwaites', ['Thw_70', 'Thw_124', 'Thw_142', 'Thw_170', 'TL96', 'WT']),\n",
    "    ('Totten', ['Totten_1', 'Totten_2']),\n",
    "    ('Wilkes', ['Wilkes_1', 'Wilkes_2']),\n",
    "    ('Mercer Whillans', ['EngelhardtSubglacialLake', 'UpperEngelhardtSubglacialLake', 'Lake12', 'Lake10', 'Lake78', 'WhillansSubglacialLake', \n",
    "                         'LowerMercerSubglacialLake', 'MercerSubglacialLake', 'LowerConwaySubglacialLake', 'ConwaySubglacialLake', 'UpperSubglacialLakeConway', \n",
    "                         'Whillans_6', 'Whillans_7', 'Whillans_8'])\n",
    "]\n",
    "\n",
    "# Call the function\n",
    "plot_lake_groups_dV(lake_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35512b7-564d-4ded-b074-38e0ad1f7d6b",
   "metadata": {},
   "source": [
    "### Fig. S5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5370774f-a5f3-45dd-a902-68fce46b47f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.family': 'DejaVu Sans',\n",
    "    'font.size': 14,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 11,\n",
    "    'ytick.labelsize': 11,\n",
    "    'legend.fontsize': 10,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4735ee90-2857-4e3d-b6ce-539c8005fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select lakes to highlight in figure\n",
    "lake_groups = [('Thwaites', ['Thw_70', 'Thw_124', 'Thw_142', 'Thw_170'])]\n",
    "\n",
    "# Initialize lists to store valid lake data\n",
    "valid_lakes = []\n",
    "evolving_outlines_gdfs = []\n",
    "lake_gdfs = []\n",
    "evolving_geom_calcs_dfs = []\n",
    "stationary_geom_calcs_dfs = []\n",
    "evolving_union_geom_calcs_dfs = []\n",
    "\n",
    "# Process lakes and populate the lists\n",
    "for lake_name in lake_groups[0][1]:  # Access the lake list from the first group\n",
    "    print(f\"Processing data for {lake_name}...\")\n",
    "    \n",
    "    # Get lake data from stationary outlines\n",
    "    lake_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == lake_name]\n",
    "    if lake_gdf.empty:\n",
    "        print(f\"Skipping {lake_name}: not found in stationary outlines\")\n",
    "        continue\n",
    "    \n",
    "    # Try loading evolving outlines\n",
    "    try:\n",
    "        evolving_outlines_gdf = gpd.read_file(os.path.join(\n",
    "            'output/lake_outlines/evolving_outlines',\n",
    "            f'{lake_name}.geojson'))\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {lake_name}: no evolving outlines file - {str(e)}\")\n",
    "        continue\n",
    "    \n",
    "    # Try loading geometric calculations\n",
    "    try:\n",
    "        evolving_geom_calcs_df = pd.read_csv(os.path.join(\n",
    "            'output/geometric_calcs/evolving_outlines_geom_calc/forward_fill/',\n",
    "            f'{lake_name}.csv'))\n",
    "        evolving_geom_calcs_df['mid_pt_datetime'] = pd.to_datetime(evolving_geom_calcs_df['mid_pt_datetime'])\n",
    "\n",
    "        evolving_union_geom_calcs_df = pd.read_csv(os.path.join(\n",
    "            'output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/',\n",
    "            f'{lake_name}.csv'))\n",
    "        evolving_union_geom_calcs_df['mid_pt_datetime'] = pd.to_datetime(evolving_union_geom_calcs_df['mid_pt_datetime'])\n",
    "\n",
    "        stationary_geom_calcs_df = pd.read_csv(os.path.join(\n",
    "            'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes/',\n",
    "            f'{lake_name}.csv'))\n",
    "        stationary_geom_calcs_df['mid_pt_datetime'] = pd.to_datetime(stationary_geom_calcs_df['mid_pt_datetime'])\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {lake_name}: error loading geometric calculations - {str(e)}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Valid data found for {lake_name}\")\n",
    "    valid_lakes.append(lake_name)\n",
    "    lake_gdfs.append(lake_gdf)\n",
    "    evolving_outlines_gdfs.append(evolving_outlines_gdf)\n",
    "    evolving_geom_calcs_dfs.append(evolving_geom_calcs_df)\n",
    "    stationary_geom_calcs_dfs.append(stationary_geom_calcs_df)\n",
    "    evolving_union_geom_calcs_dfs.append(evolving_union_geom_calcs_df)\n",
    "\n",
    "if not valid_lakes:\n",
    "    raise ValueError(\"No valid lakes found to process\")\n",
    "    \n",
    "# Create figure\n",
    "fig = plt.figure(figsize=(10, 15))\n",
    "\n",
    "# Create a 3x2 gridspec\n",
    "gs = fig.add_gridspec(3, 2)\n",
    "\n",
    "# Main spatial overview panel in first cell\n",
    "ax_main = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "# Get combined extent for all valid lakes\n",
    "x_mins, x_maxs, y_mins, y_maxs = [], [], [], []\n",
    "\n",
    "for lake_gdf, evolving_outlines_gdf in zip(lake_gdfs, evolving_outlines_gdfs):\n",
    "    # Find evolving and stationary outlines union for plotting extent\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    evolving_stationary_union_gdf = gpd.GeoDataFrame(\n",
    "        geometry=[lake_gdf.geometry.iloc[0].union(evolving_outlines_gdf.geometry.union_all())],\n",
    "        crs=lake_gdf.crs)\n",
    "    \n",
    "    # Get extent\n",
    "    x_min, y_min, x_max, y_max = evolving_stationary_union_gdf['geometry'].bounds.iloc[0]\n",
    "    buffer_dist = max(x_max - x_min, y_max - y_min) * 0.05\n",
    "    x_mins.append(x_min - buffer_dist)\n",
    "    x_maxs.append(x_max + buffer_dist)\n",
    "    y_mins.append(y_min - buffer_dist)\n",
    "    y_maxs.append(y_max + buffer_dist)\n",
    "\n",
    "# Set plot extent\n",
    "x_min, x_max = min(x_mins), max(x_maxs)\n",
    "y_min, y_max = min(y_mins), max(y_maxs)\n",
    "\n",
    "# Plot MOA background\n",
    "mask_x = (moa_highres_da.x >= x_min) & (moa_highres_da.x <= x_max)\n",
    "mask_y = (moa_highres_da.y >= y_min) & (moa_highres_da.y <= y_max)\n",
    "moa_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "ax_main.imshow(moa_subset[0,:,:], cmap='gray', clim=[14000, 17000],\n",
    "              extent=[x_min, x_max, y_min, y_max])\n",
    "\n",
    "# Plot stationary outlines\n",
    "stationary_color = 'darkturquoise'\n",
    "for lake_gdf in lake_gdfs:\n",
    "    lake_gdf.boundary.plot(ax=ax_main, color=stationary_color, linewidth=2)\n",
    "\n",
    "# Define custom offsets and display names for each lake\n",
    "# Format: 'lake_name': {'offset': (x_offset, y_offset), 'display': 'custom_name'}\n",
    "label_configs = {\n",
    "    'Thw_70': {\n",
    "        'offset': (-9e3, 8e3),\n",
    "        'display_bold': r'$\\mathbf{Thw}_{\\mathbf{70}}$',  # used in ax_main\n",
    "        'display': r'Thw$_{70}$'  # used in subplot titles\n",
    "    },\n",
    "    'Thw_124': {\n",
    "        'offset': (-23e3, 21e3),\n",
    "        'display_bold': r'$\\mathbf{Thw}_{\\mathbf{124}}$',\n",
    "        'display': r'Thw$_{124}$'\n",
    "    },\n",
    "    'Thw_142': {\n",
    "        'offset': (-26e3, 23e3),\n",
    "        'display_bold': r'$\\mathbf{Thw}_{\\mathbf{142}}$',\n",
    "        'display': r'Thw$_{142}$'\n",
    "    },\n",
    "    'Thw_170': {\n",
    "        'offset': (-18e3, 12e3),\n",
    "        'display_bold': r'$\\mathbf{Thw}_{\\mathbf{170}}$',\n",
    "        'display': r'Thw$_{170}$'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add lake labels\n",
    "for lake_gdf in lake_gdfs:\n",
    "    # Get the centroid of the lake geometry\n",
    "    centroid = lake_gdf.geometry.iloc[0].centroid\n",
    "    # Get the lake name\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    # Get custom offset and display name for this lake (or use defaults)\n",
    "    config = label_configs.get(lake_name, {'offset': (0, 0), 'display': lake_name})\n",
    "    x_offset, y_offset = config['offset']\n",
    "    # display_name = config['display']\n",
    "    display_name = config.get('display_bold', config.get('display', lake_name))\n",
    "    # Add label\n",
    "    ax_main.annotate(display_name, \n",
    "                    xy=(centroid.x + x_offset, centroid.y + y_offset),\n",
    "                    color='white',\n",
    "                    fontweight='bold',\n",
    "                    ha='center', va='center',\n",
    "                    path_effects=[PathEffects.withStroke(linewidth=3, foreground='k')])\n",
    "\n",
    "# Plot evolving outlines with time-based coloring\n",
    "# cmap = plt.get_cmap('plasma')\n",
    "# norm = plt.Normalize(mdates.date2num(cyc_start_datetimes[1]), \n",
    "#                    mdates.date2num(cyc_start_datetimes[-1]))\n",
    "# min_date = pd.to_datetime(cyc_start_datetimes[0])\n",
    "# max_date = pd.to_datetime(cyc_end_datetimes[-1])\n",
    "# cmap = plt.get_cmap('plasma')\n",
    "# norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "\n",
    "# Set up colormap\n",
    "min_date = pd.to_datetime(cyc_start_datetimes[0])\n",
    "max_date = pd.to_datetime(cyc_end_datetimes[-1])\n",
    "date_range = pd.date_range(min_date, max_date, periods=len(cyc_mid_pt_datetimes))\n",
    "years = date_range.year.unique()\n",
    "years = pd.to_datetime(years, format='%Y')\n",
    "n_dates = len(cyc_mid_pt_datetimes)\n",
    "cmap = plt.get_cmap('plasma', n_dates)\n",
    "norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "m.set_array(np.linspace(mdates.date2num(min_date), mdates.date2num(max_date), n_dates))\n",
    "\n",
    "for evolving_outlines_gdf in evolving_outlines_gdfs:\n",
    "    for idx, row in evolving_outlines_gdf.iterrows():\n",
    "        color = cmap(norm(mdates.date2num(pd.to_datetime(row['mid_pt_datetime']))))\n",
    "        gpd.GeoSeries(row['geometry']).boundary.plot(\n",
    "            ax=ax_main, color=color, linewidth=1)\n",
    "\n",
    "    # Plot inset map\n",
    "    axIns = ax_main.inset_axes([0.7, 0.02, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "    axIns.set_aspect('equal')\n",
    "    moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1)\n",
    "    moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1)\n",
    "    axIns.axis('off')\n",
    "    # Plot star to indicate location\n",
    "    axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "        linewidth=1, color='k', s=75)\n",
    "\n",
    "# Plot evolving outlines union\n",
    "for lake_gdf in lake_gdfs:\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    evolving_union_gdf = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]\n",
    "    evolving_union_gdf.boundary.plot(ax=ax_main, color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "# Format overview axes\n",
    "km_scale = 1e3\n",
    "ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "ax_main.xaxis.set_major_formatter(ticks_x)\n",
    "ax_main.yaxis.set_major_formatter(ticks_y)\n",
    "ax_main.set_xlabel('x [km]')\n",
    "ax_main.set_ylabel('y [km]')\n",
    "\n",
    "# Add colorbar\n",
    "divider = make_axes_locatable(ax_main)\n",
    "cax = divider.append_axes('bottom', size='2.5%', pad=0.55)\n",
    "cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "\n",
    "# Set colorbar ticks\n",
    "cbar.ax.xaxis.set_major_formatter(year_interval_formatter())\n",
    "cbar.ax.xaxis.set_major_locator(mdates.YearLocator())  # Every year\n",
    "cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator([1, 4, 7, 10]))  # Quarter year ticks\n",
    "\n",
    "# cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Quarter intervals only\n",
    "cbar.set_label('year', size=12)\n",
    "\n",
    "# Get y axis limits for volume plots\n",
    "y_min, y_max = get_overall_y_limits(evolving_geom_calcs_dfs, \n",
    "                                  stationary_geom_calcs_dfs,\n",
    "                                  evolving_union_geom_calcs_dfs)        \n",
    "# Calculate limits with buffer\n",
    "y_range = y_max - y_min\n",
    "buffer = y_range * 0.05\n",
    "y_limits = (y_min - buffer, y_max + buffer)\n",
    "\n",
    "# Create axes for all plots (excluding the overview plot position)\n",
    "axes = []\n",
    "plot_positions = [(0,1), (1,0), (1,1), (2,0), (2,1)]  # Row, Col positions for dV plots\n",
    "\n",
    "for pos in plot_positions:\n",
    "    ax = fig.add_subplot(gs[pos])\n",
    "    axes.append(ax)\n",
    "\n",
    "# Plot individual lakes\n",
    "for idx, (lake_name, evolving_df, stationary_df, union_df) in enumerate(zip(\n",
    "        valid_lakes, evolving_geom_calcs_dfs, stationary_geom_calcs_dfs, evolving_union_geom_calcs_dfs)):\n",
    "    ax = axes[idx]\n",
    "    ax.axhline(0, color='k', linewidth=0.5)\n",
    "    \n",
    "    dates = mdates.date2num(evolving_df['mid_pt_datetime'])\n",
    "    \n",
    "    # Plot stationary outline\n",
    "    stationary_cumsum = np.cumsum(np.divide(stationary_df['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "    ax.plot(dates, stationary_cumsum, color=stationary_color, marker='o', markerfacecolor='w', markersize=3, label='Stationary', linewidth=2)\n",
    "    ax.scatter(dates, stationary_cumsum, color=stationary_color, s=5)\n",
    "\n",
    "    # Store line segments for multi-colored line in legend\n",
    "    lines = []\n",
    "    for i, dt in enumerate(dates):\n",
    "        line = ax.plot(1, 1, color=cmap(norm(mdates.date2num(cyc_start_datetimes[i]))), linewidth=2)[0]\n",
    "        lines.append(line)\n",
    "        line.remove()  # Remove the dummy lines after creating them\n",
    "\n",
    "    # Plot evolving outlines (multi-colored line)\n",
    "    x = dates\n",
    "    y = np.cumsum(np.divide(evolving_df['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "    lc.set_array(x)\n",
    "    lc.set_linewidth(2)\n",
    "    ax.add_collection(lc)\n",
    "    ax.scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "    # Plot evolving outlines union\n",
    "    union_cumsum = np.cumsum(np.divide(union_df['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "    ax.plot(dates, union_cumsum, color='k', marker='o', markersize=3, linewidth=2)\n",
    "\n",
    "    # Plot bias\n",
    "    bias = np.cumsum(np.divide(evolving_df['evolving_outlines_dV_corr (m^3)'] - stationary_df['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "    ax.plot(dates, bias, color='r', marker='o', markerfacecolor='white', markersize=3, linewidth=2)\n",
    "\n",
    "    bias2 = np.cumsum(np.divide(evolving_df['evolving_outlines_dV_corr (m^3)'] - union_df['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "    ax.plot(dates, bias2, color='darkred', marker='o', markersize=3, linewidth=2)\n",
    "\n",
    "    # Format axes\n",
    "    ax.xaxis.set_major_formatter(year_interval_formatter())\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax.xaxis.set_minor_locator(mdates.MonthLocator([1, 4, 7, 10]))  # Quarter year ticks (Jan, Apr, Jul, Oct)\n",
    "\n",
    "    # Set x and y axes limit\n",
    "    ax.set_xlim(cyc_datetimes['cyc_start_datetimes'].iloc[0],\n",
    "        (cyc_datetimes['cyc_end_datetimes'].iloc[-1] - datetime.timedelta(days=2)))\n",
    "    ax.set_ylim(y_min, y_max)    \n",
    "\n",
    "    # Handle y-axis labels\n",
    "    row = plot_positions[idx][0]\n",
    "    col = plot_positions[idx][1]\n",
    "    if (col == 1 and row == 0) or (col == 0 and (row == 1 or row == 2)):  # First dV plot (0,1) and left column of rows 1 and 2\n",
    "        ax.set_ylabel('cumulative $dV$ [km$^3$]')\n",
    "    else:  # All other plots\n",
    "        ax.set_yticklabels([])\n",
    "        \n",
    "    # Handle x-axis labels\n",
    "    if row == 2:  # Bottom row\n",
    "        ax.set_xlabel('year')\n",
    "    else:  # Top row\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_xlabel('')\n",
    "    \n",
    "    # Get the display name from label_configs, fallback to lake_name if not found\n",
    "    display_name = label_configs.get(lake_name, {}).get('display', lake_name)\n",
    "    \n",
    "    # Use display_name instead of lake_name for the title\n",
    "    ax.set_title(display_name, fontsize=16)\n",
    "\n",
    "# Add legends\n",
    "stationary_line = plt.Line2D([], [], color=stationary_color, linestyle='solid', linewidth=2)\n",
    "stationary_dV_line = plt.Line2D([], [], color=stationary_color, marker='o', markersize=5, markerfacecolor='white', linestyle='solid', linewidth=2)\n",
    "# Full colormap for evolving outlines with one representative data point\n",
    "mid_idx = len(cyc_datetimes['cyc_start_datetimes']) // 2\n",
    "evolving_scatter_line = []\n",
    "for i, dt in enumerate(cyc_datetimes['cyc_start_datetimes']):\n",
    "    if i == mid_idx:  # only the middle line gets a marker\n",
    "        evolving_scatter_line.append(\n",
    "            Line2D([0], [0],\n",
    "                   color=cmap(norm(mdates.date2num(dt))),\n",
    "                   linestyle='solid',\n",
    "                   marker='o', markersize=5))\n",
    "    else:\n",
    "        evolving_scatter_line.append(\n",
    "            Line2D([0], [0],\n",
    "                   color=cmap(norm(mdates.date2num(dt))),\n",
    "                   linestyle='solid'))\n",
    "evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=2)\n",
    "evolving_union_dV_line = plt.Line2D([], [], color='k', marker='o', markersize=5, linestyle='solid', linewidth=2)\n",
    "bias_line = plt.Line2D([], [], color='red', marker='o', markersize=5, markerfacecolor='white', linestyle='solid', linewidth=2)  # evolving - prior stationary (all lakes)\n",
    "bias_line2 = plt.Line2D([], [], color='darkred', marker='o', markersize=5, linestyle='solid', linewidth=2)  # evolving - updated stationary (only evolving lakes)\n",
    "\n",
    "legend = ax_main.legend(\n",
    "    [tuple(lines),\n",
    "     stationary_line,\n",
    "     evolving_union_line,],\n",
    "    ['evolving outlines',\n",
    "     'prior stationary outline',\n",
    "     'updated stationary outline',],\n",
    "    handlelength=3,\n",
    "    handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    fontsize=12,\n",
    "    loc='upper center', bbox_to_anchor=(0.5, 1.3),\n",
    ")\n",
    "\n",
    "legend = axes[0].legend(\n",
    "    [tuple(evolving_scatter_line), \n",
    "     stationary_dV_line,\n",
    "     evolving_union_dV_line,\n",
    "     bias_line,\n",
    "     bias_line2],\n",
    "    ['evolving outlines',\n",
    "     'prior stationary outline',\n",
    "     'updated stationary outline',\n",
    "     'bias (evolving − prior stationary)',\n",
    "     'bias (evolving − updated stationary)'],\n",
    "    handlelength=3,\n",
    "    handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    fontsize=12,\n",
    "    loc='lower center'\n",
    ")\n",
    "    \n",
    "# Plot combined data in the last position\n",
    "last_ax = axes[-1]\n",
    "\n",
    "# Combine all dataframes by summing values for each timestamp\n",
    "combined_evolving = pd.concat(evolving_geom_calcs_dfs).groupby('mid_pt_datetime').sum().reset_index()\n",
    "combined_stationary = pd.concat(stationary_geom_calcs_dfs).groupby('mid_pt_datetime').sum().reset_index()\n",
    "combined_union = pd.concat(evolving_union_geom_calcs_dfs).groupby('mid_pt_datetime').sum().reset_index()\n",
    "\n",
    "dates = mdates.date2num(combined_evolving['mid_pt_datetime'])\n",
    "\n",
    "# Plot stationary outline\n",
    "stationary_cumsum = np.cumsum(np.divide(combined_stationary['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "last_ax.plot(dates, stationary_cumsum, color=stationary_color,  marker='o', markersize=3, markerfacecolor='w', linewidth=2)\n",
    "# last_ax.scatter(dates, stationary_cumsum, color=stationary_color, s=5)\n",
    "\n",
    "# Plot evolving outlines (multi-colored line)\n",
    "evolving_cumsum = np.cumsum(np.divide(combined_evolving['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "points = np.array([dates, evolving_cumsum]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(dates)\n",
    "lc.set_linewidth(2)\n",
    "last_ax.add_collection(lc)\n",
    "last_ax.scatter(dates, evolving_cumsum, c=dates, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "# Plot evolving outlines union\n",
    "union_cumsum = np.cumsum(np.divide(combined_union['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "last_ax.plot(dates, union_cumsum, color='k', linestyle='solid', marker='o', markersize=3, linewidth=2)\n",
    "# last_ax.scatter(dates, union_cumsum, color='k', s=5)\n",
    "\n",
    "# Plot bias\n",
    "bias_cumsum = np.cumsum(np.divide(\n",
    "    combined_evolving['evolving_outlines_dV_corr (m^3)'] - \n",
    "    combined_stationary['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "last_ax.plot(dates, bias_cumsum, color='r', marker='o', markersize=3, markerfacecolor='w', linewidth=2)\n",
    "# last_ax.scatter(dates, bias_cumsum, color='r', s=5)\n",
    "\n",
    "bias_cumsum2 = np.cumsum(np.divide(\n",
    "    combined_evolving['evolving_outlines_dV_corr (m^3)'] - \n",
    "    combined_union['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "last_ax.plot(dates, bias_cumsum2, color='darkred', marker='o', markersize=3, linewidth=2)\n",
    "# last_ax.scatter(dates, bias_cumsum, color='r', s=5)\n",
    "\n",
    "# Set axes limits and format\n",
    "last_ax.set_xlim(cyc_datetimes['cyc_start_datetimes'].iloc[0],\n",
    "                 (cyc_datetimes['cyc_end_datetimes'].iloc[-1] - datetime.timedelta(days=2)))\n",
    "last_ax.set_ylim(y_min, y_max)\n",
    "last_ax.set_yticklabels([])\n",
    "last_ax.axhline(0, color='k', linewidth=0.5)\n",
    "last_ax.set_xlabel('year', size=14) # TEMP ensure same font size as other 'year'\n",
    "last_ax.xaxis.set_major_formatter(year_interval_formatter())\n",
    "last_ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "last_ax.xaxis.set_minor_locator(mdates.MonthLocator([1, 4, 7, 10]))\n",
    "\n",
    "last_ax.set_title('total', fontsize=16)\n",
    "\n",
    "# Add subplot annotations ('a'-'f') to all plots\n",
    "char_index = 97  # ASCII value for 'a'\n",
    "for ax in [ax_main] + axes:\n",
    "    ax.text(0.03, 0.97, chr(char_index), transform=ax.transAxes, \n",
    "            fontsize=20, va='top', ha='left')\n",
    "    char_index += 1\n",
    "\n",
    "# Adjust layout to prevent overlapping\n",
    "plt.tight_layout(h_pad=0.0)\n",
    "\n",
    "# Save and close plot\n",
    "plt.savefig(OUTPUT_DIR + '/figures/FigS5_Thw_lakes_dV.jpg',\n",
    "    dpi=400, bbox_inches='tight')\n",
    "\n",
    "# Preview plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac93c6a2-0c9c-4c3c-9746-c72cb186ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3882b7dd-4ae2-494b-9b53-77219b3e5704",
   "metadata": {},
   "source": [
    "## Fig. S6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c5b60-f600-42ba-8a85-372e2d93b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_first_rows(folder_path):\n",
    "    '''\n",
    "    Reads all CSV files in the specified folder and combines their first rows into a single DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    folder_path (str): Path to the folder containing CSV files\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame containing the first row from each CSV file\n",
    "    '''\n",
    "    # List to store first rows\n",
    "    first_rows = []\n",
    "    \n",
    "    # Iterate through all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Get the first row and add filename as a column\n",
    "                if not df.empty:\n",
    "                    first_row = df.iloc[0:1].copy()\n",
    "                    \n",
    "                    # Add full filename and filename without extension as columns\n",
    "                    first_row['name'] = os.path.splitext(filename)[0]\n",
    "                    \n",
    "                    first_rows.append(first_row)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f'Error processing {filename}: {str(e)}')\n",
    "    \n",
    "    # Combine all first rows into a single DataFrame\n",
    "    if first_rows:\n",
    "        result = pd.concat(first_rows, ignore_index=True)\n",
    "        return result\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f5001-64d6-4d76-a1d6-96fe1708e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/home/jovyan/1_evolving_lakes/output/FigS1_lake_reexamination_methods/levels'\n",
    "result_df = combine_first_rows(folder_path)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7753db1f-ec09-4a17-b9a2-fc083fc68f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove lake rows where we found through manual inspection that the lake's evolving outlines were similar to off-lake, background activity\n",
    "evolving_lake_names = [f.replace('.geojson', '') for f in os.listdir('output/lake_outlines/evolving_outlines') if f.endswith('.geojson')]\n",
    "filtered_df = result_df[result_df['name'].isin(evolving_lake_names)]\n",
    "print(len(filtered_df))  # =98 because Site_B and Site_C saved as output file Site_BC.geojson (Fig. S4e,f)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefeeef3-cdc9-473e-bf48-d1cbc807d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_level_histogram(df, column='level', bins=10):\n",
    "    '''\n",
    "    Creates a histogram of a specified column with enhanced styling\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the data\n",
    "    column (str): Name of the column to create histogram for\n",
    "    bins (int): Number of bins for the histogram\n",
    "    '''\n",
    "    # Create figure and axis objects with larger size\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create histogram\n",
    "    plt.hist(df[column], bins=bins, color='skyblue', edgecolor='black')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title(f'Distribution of {column}', pad=15, fontsize=14)\n",
    "    plt.xlabel(column, fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Rotate x-axis labels if needed\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_level_histogram(filtered_df, column='level', bins=35)\n",
    "# plot_level_histogram(filtered_df, column='within_area_multiple', bins=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fcc31f-3d50-4acc-85b5-eae9205b05a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_level_histogram(filtered_df, column='level', bins=35)\n",
    "plot_level_histogram(filtered_df, column='within_area_multiple', bins=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e67d48c-a357-4398-9766-e543e293b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[filtered_df['level'] > 0.5].sort_values('level', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d66abb-bde5-4ae4-b4fc-80432b8846c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_level_histogram(df, column='level', bins=10):\n",
    "#     \"\"\"\n",
    "#     Histogram with:\n",
    "#     - values >0.5 highlighted per lake (alphabetized)\n",
    "#     - values <=0.5 grouped as 'other lakes' (gray)\n",
    "#     - same bar widths\n",
    "#     - x-axis capped at 1.75\n",
    "#     - gray bars drawn first so highlights appear on top\n",
    "#     - legend correctly filtered to avoid '_nolegend_' warnings\n",
    "#     \"\"\"\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     df_other = df[df[column] <= 0.5]\n",
    "#     df_highlight = df[df[column] > 0.5]\n",
    "#     # Create bin edges aligned with 0.01 intervals\n",
    "#     bin_edges = np.arange(0, 1.8, 0.01)\n",
    "#     bin_width = bin_edges[1] - bin_edges[0]\n",
    "#     cmap = plt.get_cmap(\"tab20\")\n",
    "#     unique_names = sorted(df_highlight['name'].unique()) if not df_highlight.empty else []\n",
    "    \n",
    "#     # --- Plot gray \"other lakes\" first ---\n",
    "#     counts_other, _ = np.histogram(df_other[column], bins=bin_edges)\n",
    "    \n",
    "#     # Only plot bars where counts > 0 to avoid black lines along x-axis\n",
    "#     non_zero_mask = counts_other > 0\n",
    "#     if non_zero_mask.any():\n",
    "#         bars_other = plt.bar(bin_edges[:-1][non_zero_mask], counts_other[non_zero_mask], \n",
    "#                            width=bin_width, color='gray', edgecolor='black', alpha=0.6,\n",
    "#                            align='edge')\n",
    "        \n",
    "#         # Force the label after creation\n",
    "#         if len(df_other) > 0 and counts_other.sum() > 0:\n",
    "#             bars_other[0].set_label(f'other lakes (n={len(df_other)})')\n",
    "#     else:\n",
    "#         bars_other = None\n",
    "    \n",
    "#     # --- Plot highlighted lakes (stacked) ---\n",
    "#     highlight_bars = []\n",
    "#     cumulative_counts = np.zeros(len(bin_edges) - 1)  # Track cumulative height for stacking\n",
    "    \n",
    "#     for i, name in enumerate(unique_names):\n",
    "#         subset = df_highlight[df_highlight['name'] == name]\n",
    "#         counts, _ = np.histogram(subset[column], bins=bin_edges)\n",
    "        \n",
    "#         bars = plt.bar(bin_edges[:-1], counts, width=bin_width,\n",
    "#                        color=cmap(i % cmap.N), edgecolor='black', alpha=0.8,\n",
    "#                        align='edge', bottom=cumulative_counts)\n",
    "        \n",
    "#         # Update cumulative counts for next lake\n",
    "#         cumulative_counts += counts\n",
    "        \n",
    "#         # Force the label after creation\n",
    "#         if counts.sum() > 0:\n",
    "#             bars[0].set_label(name)\n",
    "#             highlight_bars.append(bars)\n",
    "    \n",
    "#     # --- Build legend handles and labels safely ---\n",
    "#     legend_handles = []\n",
    "#     legend_labels = []\n",
    "    \n",
    "#     # Add colored lakes alphabetically\n",
    "#     if highlight_bars:\n",
    "#         valid_bars = [(b[0], b[0].get_label()) for b in highlight_bars \n",
    "#                       if not b[0].get_label().startswith('_') and b[0].get_label()]\n",
    "#         sorted_bars = sorted(valid_bars, key=lambda x: x[1].lower())\n",
    "        \n",
    "#         for h, l in sorted_bars:\n",
    "#             legend_handles.append(h)\n",
    "#             legend_labels.append(l)\n",
    "    \n",
    "#     # Add \"other lakes\" last if valid\n",
    "#     if (bars_other is not None and len(df_other) > 0 and counts_other.sum() > 0 and \n",
    "#         not bars_other[0].get_label().startswith('_') and bars_other[0].get_label()):\n",
    "#         legend_handles.append(bars_other[0])\n",
    "#         legend_labels.append(bars_other[0].get_label())\n",
    "    \n",
    "#     plt.xlabel('$dh$ threshold [m]')\n",
    "#     plt.ylabel('Count')\n",
    "#     plt.xlim(0, 1.7)\n",
    "    \n",
    "#     # Set major x-axis ticks at 0.1 intervals\n",
    "#     plt.xticks(np.arange(0, 1.7, 0.1))\n",
    "    \n",
    "#     # Set minor x-axis ticks at 0.01 intervals\n",
    "#     ax = plt.gca()\n",
    "#     ax.set_xticks(np.arange(0, 1.7, 0.01), minor=True)\n",
    "    \n",
    "#     # Create legend if we have handles\n",
    "#     if legend_handles:\n",
    "#         plt.legend(legend_handles, legend_labels)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "\n",
    "#     # Save and show plot\n",
    "#     plt.savefig(OUTPUT_DIR + '/figures/FigS6_dh_threshold_distribution.jpg',\n",
    "#         dpi=500, bbox_inches='tight')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d0418-c0da-4757-bb1e-fe21bafb6689",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.family': 'DejaVu Sans',\n",
    "    'font.size': 9,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'legend.fontsize': 12,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec0605b-8a5f-4de1-b883-035c5d201f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_level_histogram(df, column='level', bins=10):\n",
    "    \"\"\"\n",
    "    Histogram with:\n",
    "    - values >0.5 highlighted per lake (alphabetized)\n",
    "    - values <=0.5 grouped as 'other lakes' (gray)\n",
    "    - same bar widths\n",
    "    - x-axis capped at 1.75\n",
    "    - gray bars drawn first so highlights appear on top\n",
    "    - legend correctly filtered and labeled using lake_label_map\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Lake label map with upright subscripts ---\n",
    "    lake_label_map = {\n",
    "        'Lambert_1': r'Lambert$_{1}$',\n",
    "        'Site_A': r'Site$_{\\text{A}}$',\n",
    "        'Site_BC': r'Site$_{\\text{BC}}$',\n",
    "        'Slessor_1': r'Slessor$_{1}$',\n",
    "        'Slessor_23': r'Slessor$_{23}$',\n",
    "        'Thw_70': r'Thw$_{70}$',\n",
    "        'Thw_124': r'Thw$_{124}$',\n",
    "        'Thw_142': r'Thw$_{142}$',\n",
    "        'Thw_170': r'Thw$_{170}$'\n",
    "    }\n",
    "\n",
    "    # --- Custom legend ordering ---\n",
    "    custom_order = [\n",
    "        'Lambert_1', 'Site_A', 'Site_BC', \n",
    "        'Slessor_1', 'Slessor_23', \n",
    "        'Thw_70', 'Thw_124', 'Thw_142', 'Thw_170'\n",
    "    ]\n",
    "\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df_other = df[df[column] <= 0.5]\n",
    "    df_highlight = df[df[column] > 0.5]\n",
    "    \n",
    "    # Create bin edges aligned with 0.01 intervals\n",
    "    bin_edges = np.arange(0, 1.8, 0.01)\n",
    "    bin_width = bin_edges[1] - bin_edges[0]\n",
    "    cmap = plt.get_cmap(\"tab20\")\n",
    "    unique_names = sorted(df_highlight['name'].unique()) if not df_highlight.empty else []\n",
    "    \n",
    "    # Plot gray \"other lakes\" first (stacked vertical bars)\n",
    "    counts_other, _ = np.histogram(df_other[column], bins=bin_edges)\n",
    "    \n",
    "    # Only plot bars where counts > 0 to avoid black lines along x-axis\n",
    "    non_zero_mask = counts_other > 0\n",
    "    if non_zero_mask.any():\n",
    "        bars_other = plt.bar(bin_edges[:-1][non_zero_mask], counts_other[non_zero_mask], \n",
    "                           width=bin_width, color='gray', edgecolor='black', alpha=0.6,\n",
    "                           align='edge')\n",
    "        \n",
    "        # Force the label after creation\n",
    "        if len(df_other) > 0 and counts_other.sum() > 0:\n",
    "            bars_other[0].set_label(f'other lakes (n={len(df_other)})')\n",
    "    else:\n",
    "        bars_other = None\n",
    "    \n",
    "    # Plot highlighted lakes (stacked vertical bars)\n",
    "    highlight_bars = []\n",
    "    cumulative_counts = np.zeros(len(bin_edges) - 1)  # Track cumulative height for stacking\n",
    "    \n",
    "    for i, name in enumerate(unique_names):\n",
    "        subset = df_highlight[df_highlight['name'] == name]\n",
    "        counts, _ = np.histogram(subset[column], bins=bin_edges)\n",
    "        \n",
    "        bars = plt.bar(bin_edges[:-1], counts, width=bin_width,\n",
    "                       color=cmap(i % cmap.N), edgecolor='black', alpha=0.8,\n",
    "                       align='edge', bottom=cumulative_counts)\n",
    "        \n",
    "        # Update cumulative counts for next lake\n",
    "        cumulative_counts += counts\n",
    "        \n",
    "        # Force the label after creation\n",
    "        if counts.sum() > 0:\n",
    "            bars[0].set_label(name)\n",
    "            highlight_bars.append(bars)\n",
    "    \n",
    "    # Build legend handles and labels\n",
    "    legend_handles = []\n",
    "    legend_labels = []\n",
    "\n",
    "    if highlight_bars:\n",
    "        valid_bars = [(b[0], b[0].get_label()) for b in highlight_bars \n",
    "                      if not b[0].get_label().startswith('_') and b[0].get_label()]\n",
    "\n",
    "        # Use custom order where possible, fallback to alphabetical\n",
    "        def custom_sort_key(name):\n",
    "            return custom_order.index(name) if name in custom_order else len(custom_order) + ord(name[0])\n",
    "        \n",
    "        sorted_bars = sorted(valid_bars, key=lambda x: custom_sort_key(x[1]))\n",
    "        \n",
    "        for h, l in sorted_bars:\n",
    "            label_pretty = lake_label_map.get(l, l)\n",
    "            legend_handles.append(h)\n",
    "            legend_labels.append(label_pretty)\n",
    "    \n",
    "    # Add \"other lakes\" last if valid\n",
    "    if (bars_other is not None and len(df_other) > 0 and counts_other.sum() > 0 and \n",
    "        not bars_other[0].get_label().startswith('_') and bars_other[0].get_label()):\n",
    "        legend_handles.append(bars_other[0])\n",
    "        legend_labels.append(bars_other[0].get_label())\n",
    "    \n",
    "    plt.xlabel('$dh$ threshold [m]')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlim(0, 1.7)\n",
    "    \n",
    "    # Set major x-axis ticks at 0.1 intervals\n",
    "    plt.xticks(np.arange(0, 1.7, 0.1))\n",
    "    \n",
    "    # Set minor x-axis ticks at 0.01 intervals\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0, 1.7, 0.01), minor=True)\n",
    "    \n",
    "    # Create legend if we have handles\n",
    "    if legend_handles:\n",
    "        plt.legend(legend_handles, legend_labels)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and show plot\n",
    "    plt.savefig(OUTPUT_DIR + '/figures/FigS6_dh_threshold_distribution.jpg',\n",
    "        dpi=500, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526f9f0b-3231-44fd-b87a-a7001c5746dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_level_histogram(filtered_df, column='level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9440823b-c4bf-4e9b-9e85-e2c8e9a69867",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c4c89d-0818-435a-a868-e8a778ef4a29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
