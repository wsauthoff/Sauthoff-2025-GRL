{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f816f210-8934-4276-a4bb-775da6475a4c",
   "metadata": {},
   "source": [
    "Notebook generates Figs. 2, 3, S2, and S3.\n",
    "\n",
    "Written 2023-11-11 by W. Sauthoff (wsauthoff.github.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63862714-5c5b-41ad-9f67-8e8fe91b4c52",
   "metadata": {},
   "source": [
    "# Set up computing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0490b8-0bb2-4bf8-8ea1-d93c28a1610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import datetime\n",
    "import fiona\n",
    "import functools\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "import hvplot.pandas\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "from math import radians\n",
    "import matplotlib\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.legend_handler import HandlerPatch, HandlerTuple\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pyproj import CRS, Geod, Transformer\n",
    "import rioxarray\n",
    "from shapely.geometry import MultiPolygon, Point, Polygon\n",
    "from shapely.ops import transform, unary_union\n",
    "\n",
    "# Magic functions\n",
    "%matplotlib widget\n",
    "\n",
    "# Define data directories dependent on home environment\n",
    "# Replace with your directory file paths\n",
    "if os.getenv('HOME') == '/home/jovyan':\n",
    "    DATA_DIR = '/home/jovyan/data'\n",
    "    OUTPUT_DIR = '/home/jovyan/1_evolving_lakes/output/Figs23_S23_lake_reexamination_results'\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Define constants and coordinate transforms for the geodesic area calculation\n",
    "CRS_LL = \"EPSG:4326\" # wgs84 in lon,lat\n",
    "GEOD = CRS(CRS_LL).get_geod() # geod object for calculating geodesic area on defined ellipsoid\n",
    "CRS_XY = \"EPSG:3031\" # Antarctic Polar Stereographic in x, y\n",
    "XY_TO_LL = Transformer.from_crs(CRS_XY, CRS_LL, always_xy = True) # make coord transformer\n",
    "geod = Geod(ellps=\"WGS84\") # Create a Geod object for calculating area on the WGS84 ellipsoid\n",
    "\n",
    "# Define utility functions\n",
    "def ll2ps(lon, lat):\n",
    "    \"\"\"\n",
    "    Transform coordinates from geodetic coordinates (lon, lat)\n",
    "    to Antarctic Polar Stereograph coordinates (x, y)\n",
    "    x, y = ll2ps(lon, lat)\n",
    "    \"\"\"\n",
    "    crs_ll = CRS(\"EPSG:4326\")\n",
    "    crs_xy = CRS(\"EPSG:3031\")\n",
    "    ll_to_xy = Transformer.from_crs(crs_ll, crs_xy, always_xy = True)\n",
    "    x, y = ll_to_xy.transform(lon, lat)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6a7d91-6d40-45b7-8ff8-0e29f2e55c5f",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a605abb-9415-47a6-926a-e55cb4807b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantify_lake_extensions(lakes_gdf, evolving_outlines_union_gdf, area_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Quantify lakes with extensions beyond their original stationary outlines using geodesic area calculations.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lakes_gdf : GeoDataFrame\n",
    "        GeoDataFrame containing stationary lake outlines with 'name' column\n",
    "    evolving_outlines_union_gdf : GeoDataFrame\n",
    "        GeoDataFrame containing evolving union outlines with 'name' column\n",
    "    area_threshold : float, default=0.05\n",
    "        Minimum fraction of area increase to consider as a reportable extension\n",
    "        (e.g., 0.05 means 5% area increase)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (lakes_with_extensions, extension_results)\n",
    "        - lakes_with_extensions: count of lakes with reportable extensions\n",
    "        - extension_results: DataFrame with detailed extension metrics for each lake\n",
    "    \"\"\"\n",
    "    # Define the geodesic object for Earth calculations\n",
    "    geod = Geod(ellps=\"WGS84\")\n",
    "    \n",
    "    # Create a transformer to convert from EPSG:3031 to EPSG:4326\n",
    "    project = Transformer.from_crs(\"EPSG:3031\", \"EPSG:4326\", always_xy=True).transform\n",
    "\n",
    "    def transform_to_4326(geometry):\n",
    "        '''\n",
    "        Transform geometry from EPSG:3031 to EPSG:4326\n",
    "        '''\n",
    "        if geometry is None or not geometry.is_valid:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Use functools.partial to create a function that can be used with shapely's transform\n",
    "            project_func = functools.partial(project)\n",
    "            transformed_geom = transform(project_func, geometry)\n",
    "            return transformed_geom\n",
    "        except Exception as e:\n",
    "            print(f\"Error transforming geometry: {e}\")\n",
    "            return None\n",
    "\n",
    "    def calculate_geodesic_area_and_perimeter(geometry):\n",
    "        '''\n",
    "        Calculate geodesic area and perimeter of a polygon or multipolygon.\n",
    "        First transforms geometry from EPSG:3031 to EPSG:4326, then performs calculations.\n",
    "        '''\n",
    "        \n",
    "        # Ensure geometry exists and is valid\n",
    "        if geometry is None or not geometry.is_valid:\n",
    "            return None, None\n",
    "        \n",
    "        # Transform geometry to EPSG:4326\n",
    "        geom_4326 = transform_to_4326(geometry)\n",
    "        if geom_4326 is None:\n",
    "            return None, None\n",
    "            \n",
    "        if isinstance(geom_4326, Polygon):\n",
    "            # Calculate area and perimeter for a single polygon\n",
    "            area, perimeter = geod.polygon_area_perimeter(geom_4326.exterior.coords.xy[0], \n",
    "                                                          geom_4326.exterior.coords.xy[1])\n",
    "            # Subtract areas of holes if any exist\n",
    "            for interior in geom_4326.interiors:\n",
    "                hole_area, _ = geod.polygon_area_perimeter(interior.coords.xy[0], \n",
    "                                                           interior.coords.xy[1])\n",
    "                area -= hole_area\n",
    "                \n",
    "            return abs(area), abs(perimeter)\n",
    "            \n",
    "        elif isinstance(geom_4326, MultiPolygon):\n",
    "            # Calculate combined area and perimeter for multipolygons\n",
    "            total_area = 0\n",
    "            total_perimeter = 0\n",
    "            for part in geom_4326.geoms:\n",
    "                # Add the part's area\n",
    "                part_area, part_perimeter = geod.polygon_area_perimeter(part.exterior.coords.xy[0], \n",
    "                                                                        part.exterior.coords.xy[1])\n",
    "                total_area += abs(part_area)\n",
    "                total_perimeter += abs(part_perimeter)\n",
    "                \n",
    "                # Subtract areas of holes if any exist\n",
    "                for interior in part.interiors:\n",
    "                    hole_area, _ = geod.polygon_area_perimeter(interior.coords.xy[0], \n",
    "                                                              interior.coords.xy[1])\n",
    "                    total_area -= abs(hole_area)\n",
    "            \n",
    "            return total_area, total_perimeter\n",
    "        else:\n",
    "            return None, None\n",
    "            \n",
    "    results = []\n",
    "    \n",
    "    # Process each lake\n",
    "    for lake_name in lakes_gdf['name'].unique():\n",
    "        try:\n",
    "            # Get stationary outline\n",
    "            stationary = lakes_gdf[lakes_gdf['name'] == lake_name]['geometry'].iloc[0]\n",
    "            \n",
    "            # Get evolving union outline\n",
    "            evolving = evolving_outlines_union_gdf[\n",
    "                evolving_outlines_union_gdf['name'] == lake_name]['geometry'].iloc[0]\n",
    "            \n",
    "            # Skip if either geometry is missing\n",
    "            if stationary is None or evolving is None:\n",
    "                continue\n",
    "                \n",
    "            # Calculate geodesic areas\n",
    "            stationary_area, _ = calculate_geodesic_area_and_perimeter(stationary)\n",
    "            evolving_area, _ = calculate_geodesic_area_and_perimeter(evolving)\n",
    "            \n",
    "            # Calculate extension area (area in evolving that's not in stationary)\n",
    "            if evolving.contains(stationary):\n",
    "                # Simple case: evolving completely contains stationary\n",
    "                extension = evolving.difference(stationary)\n",
    "            else:\n",
    "                # More complex case: find areas in evolving that aren't in stationary\n",
    "                extension = evolving.difference(stationary)\n",
    "            \n",
    "            # Get geodesic area of the extension\n",
    "            extension_area, _ = calculate_geodesic_area_and_perimeter(extension)\n",
    "            \n",
    "            # If any area calculation failed, skip this lake\n",
    "            if stationary_area is None or evolving_area is None or extension_area is None:\n",
    "                print(f\"Warning: Could not calculate geodesic area for {lake_name}, skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate relative extension (as percentage of original area)\n",
    "            relative_extension = extension_area / stationary_area if stationary_area > 0 else 0\n",
    "            \n",
    "            # Calculate metrics to determine if this is a reportable extension\n",
    "            has_reportable_extension = relative_extension >= area_threshold\n",
    "            \n",
    "            results.append({\n",
    "                'lake_name': lake_name,\n",
    "                'stationary_area': stationary_area,\n",
    "                'evolving_area': evolving_area,\n",
    "                'extension_area': extension_area,\n",
    "                'relative_extension': relative_extension,\n",
    "                'has_reportable_extension': has_reportable_extension\n",
    "            })\n",
    "\n",
    "            # Clear output\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {lake_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create DataFrame with results\n",
    "    extension_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Count lakes with extensions\n",
    "    lakes_with_extensions = extension_df['has_reportable_extension'].sum()\n",
    "    \n",
    "    return lakes_with_extensions, extension_df\n",
    "\n",
    "def generate_extension_summary(extension_df, km2=True):\n",
    "    \"\"\"\n",
    "    Generate a summary report about lake extensions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    extension_df : DataFrame\n",
    "        DataFrame with lake extension results from quantify_lake_extensions\n",
    "    km2 : bool, default=True\n",
    "        If True, displays area in square kilometers, otherwise in square meters\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Summary text with key findings\n",
    "    \"\"\"\n",
    "    total_lakes = len(extension_df)\n",
    "    lakes_with_extensions = extension_df['has_reportable_extension'].sum()\n",
    "    \n",
    "    # Calculate average extension for lakes that have extensions\n",
    "    extended_lakes = extension_df[extension_df['has_reportable_extension']]\n",
    "    avg_extension = extended_lakes['relative_extension'].mean() if len(extended_lakes) > 0 else 0\n",
    "    \n",
    "    # Calculate total area of extensions\n",
    "    total_extension_area = extended_lakes['extension_area'].sum() if len(extended_lakes) > 0 else 0\n",
    "    area_unit = \"km²\" if km2 else \"m²\"\n",
    "    area_divisor = 1_000_000 if km2 else 1  # Convert to km² if requested\n",
    "    \n",
    "    # Find lake with maximum extension\n",
    "    if len(extended_lakes) > 0:\n",
    "        max_extension_lake = extended_lakes.loc[extended_lakes['relative_extension'].idxmax()]\n",
    "        max_extension_pct = max_extension_lake['relative_extension'] * 100\n",
    "        max_lake_name = max_extension_lake['lake_name']\n",
    "        \n",
    "        # Find lake with largest absolute extension area\n",
    "        max_area_lake = extended_lakes.loc[extended_lakes['extension_area'].idxmax()]\n",
    "        max_area_value = max_area_lake['extension_area'] / area_divisor\n",
    "        max_area_lake_name = max_area_lake['lake_name']\n",
    "    else:\n",
    "        max_extension_pct = 0\n",
    "        max_lake_name = \"None\"\n",
    "        max_area_value = 0\n",
    "        max_area_lake_name = \"None\"\n",
    "    \n",
    "    # Generate summary\n",
    "    summary = f\"\"\"Lake Extension Analysis Summary:\n",
    "    ---------------------------------\n",
    "    We found {lakes_with_extensions} lakes ({lakes_with_extensions/total_lakes*100:.1f}% of {total_lakes} analyzed) \n",
    "    with previously unidentified lake extensions beyond their original stationary outlines.\n",
    "    \n",
    "    For lakes with extensions:\n",
    "    - Total extension area: {total_extension_area/area_divisor:.2f} {area_unit}\n",
    "    - Average extension: {avg_extension*100-100:.1f}% beyond original outline\n",
    "    - Largest relative extension: {max_extension_pct:.1f}% beyond original outline (Lake {max_lake_name})\n",
    "    - Largest absolute extension: {max_area_value:.2f} {area_unit} (Lake {max_area_lake_name})\n",
    "    \"\"\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example usage:\n",
    "# lakes_with_extensions, extension_results = quantify_lake_extensions(\n",
    "#     lakes_gdf, \n",
    "#     evolving_outlines_union_gdf, \n",
    "# )\n",
    "# summary = generate_extension_summary(extension_results, km2=True)\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd84727-a5b2-415a-98bb-f9702819979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def muliple_area_buffer(polygon, area_multiple, precision=100):\n",
    "    \"\"\"\n",
    "    This function takes a polygon and returns a polygon with a buffer such that the area of the buffered polygon\n",
    "    is approximately the specified multiple the area of the original polygon.\n",
    "\n",
    "    :param polygon: Shapely Polygon object\n",
    "    :param area_multiple: the multiple of the original polygon area you wish the buffered polygon to be\n",
    "    :param precision: Precision for the iterative process to find the buffer distance\n",
    "    :return: Buffered Polygon\n",
    "\n",
    "    # Example usage\n",
    "    # Define a simple square polygon\n",
    "    square = Polygon([(0, 0), (1, 0), (1, 1), (0, 1)])\n",
    "    # Apply the function to find the buffered polygon area and bounds\n",
    "    buffered_poly = muliple_area_buffer(square, 2)\n",
    "    \"\"\"\n",
    "    original_area = polygon.area\n",
    "    target_area = area_multiple * original_area\n",
    "    buffer_distance = 0\n",
    "    buffered_polygon = polygon\n",
    "\n",
    "    while True:\n",
    "        buffered_polygon = polygon.buffer(buffer_distance)\n",
    "        if buffered_polygon.area >= target_area:\n",
    "            break\n",
    "        buffer_distance += precision\n",
    "    \n",
    "    # Convert to geodataframe\n",
    "    buffered_polygon_gdf = gpd.GeoDataFrame({'geometry': [buffered_polygon]})\n",
    "\n",
    "    return buffered_polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a766aa3d-cc07-4af3-a19d-e61e7750b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR + '/lake_group_dV_plots', exist_ok=True)\n",
    "\n",
    "def plot_lake_groups_dV(lake_groups):\n",
    "    \"\"\"\n",
    "    Create multi-panel plots for groups of lakes showing spatial overview and volume changes.\n",
    "    Lakes are arranged in rows of three plots of equal size, with valid data checking.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lake_groups : list of tuples\n",
    "        Each tuple contains (group_name, lake_list) where:\n",
    "        - group_name: str, name of the lake group for file naming and identification\n",
    "        - lake_list: list of str, names of lakes to be analyzed together\n",
    "    \"\"\"\n",
    "    \n",
    "    for group_idx, (group_name, lake_list) in enumerate(lake_groups):\n",
    "        print(f\"\\nProcessing lake group: {group_name}\")\n",
    "        \n",
    "        # Lists to store valid lake data\n",
    "        valid_lakes = []\n",
    "        evolving_outlines_gdfs = []\n",
    "        evolving_geom_calcs_dfs = []\n",
    "        stationary_geom_calcs_dfs = []\n",
    "        evolving_union_geom_calcs_dfs = []\n",
    "        lake_gdfs = []\n",
    "        \n",
    "        # First pass: collect all valid lake data\n",
    "        for lake_name in lake_list:\n",
    "            print(f\"Checking data for {lake_name}...\")\n",
    "            \n",
    "            # Get lake data from stationary outlines\n",
    "            lake_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == lake_name]\n",
    "            if lake_gdf.empty:\n",
    "                print(f\"Skipping {lake_name}: not found in stationary outlines\")\n",
    "                continue\n",
    "                \n",
    "            # Try loading evolving outlines gdf\n",
    "            try:\n",
    "                evolving_outlines_gdf = gpd.read_file(os.path.join(\n",
    "                    'output/lake_outlines/evolving_outlines',\n",
    "                    f'{lake_name}.geojson'))\n",
    "            except Exception as e:\n",
    "                print(f\"  Skipping {lake_name}: no evolving outlines file\")\n",
    "                continue\n",
    "\n",
    "            # Attempt to open the geometric calculations CSV files\n",
    "            try:\n",
    "                evolving_geom_calcs_df = pd.read_csv('output/geometric_calcs/evolving_outlines_geom_calc/forward_fill/{}.csv'.format(lake_name))\n",
    "                evolving_union_geom_calcs_df = pd.read_csv('output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/{}.csv'.format(lake_name))\n",
    "                stationary_geom_calcs_df = pd.read_csv('output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes/{}.csv'.format(lake_name))\n",
    "            except FileNotFoundError:\n",
    "                print(f\"At least one of the geometric calculations CSV files for {lake_name} not found. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Convert strings to datetime\n",
    "            evolving_geom_calcs_df['mid_pt_datetime'] = pd.to_datetime(evolving_geom_calcs_df['mid_pt_datetime'])\n",
    "            evolving_union_geom_calcs_df['mid_pt_datetime'] = pd.to_datetime(evolving_union_geom_calcs_df['mid_pt_datetime'])\n",
    "            stationary_geom_calcs_df['mid_pt_datetime'] = pd.to_datetime(stationary_geom_calcs_df['mid_pt_datetime'])\n",
    "\n",
    "            # If we got here, all data is valid\n",
    "            print(f\"Valid data found for {lake_name}\")\n",
    "            valid_lakes.append(lake_name)\n",
    "            lake_gdfs.append(lake_gdf)\n",
    "            evolving_outlines_gdfs.append(evolving_outlines_gdf)\n",
    "            evolving_geom_calcs_dfs.append(evolving_geom_calcs_df)\n",
    "            stationary_geom_calcs_dfs.append(stationary_geom_calcs_df)\n",
    "            evolving_union_geom_calcs_dfs.append(evolving_union_geom_calcs_df)\n",
    "        \n",
    "        # Skip this group if no valid lakes found\n",
    "        if not valid_lakes:\n",
    "            print(f\"Skipping group {group_name}: no valid lakes found\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nCreating plots for valid lakes in group {group_name}: {valid_lakes}\")\n",
    "\n",
    "        # Calculate plot layout (including space for combined plot)\n",
    "        n_lakes = len(valid_lakes)\n",
    "        n_plots = n_lakes + 1  # Add 1 for the combined plot\n",
    "        n_rows = (n_plots + 2) // 3  # Integer division rounded up\n",
    "        \n",
    "        # Create figure\n",
    "        fig = plt.figure(figsize=(15, 5*n_rows + 3))\n",
    "        gs = fig.add_gridspec(n_rows + 1, 3, height_ratios=[1] + [1]*n_rows)\n",
    "        \n",
    "        # Main spatial overview panel\n",
    "        ax_main = fig.add_subplot(gs[0, :])\n",
    "        \n",
    "        # Get combined extent for all valid lakes\n",
    "        x_mins, x_maxs, y_mins, y_maxs = [], [], [], []\n",
    "        \n",
    "        for lake_gdf, evolving_outlines_gdf in zip(lake_gdfs, evolving_outlines_gdfs):\n",
    "            # Find evolving and stationary outlines union for plotting extent\n",
    "            lake_name = lake_gdf['name'].iloc[0]\n",
    "            evolving_stationary_union_gdf = gpd.GeoDataFrame(\n",
    "                geometry=[lake_gdf.geometry.iloc[0].union(evolving_outlines_gdf.geometry.union_all())],\n",
    "                crs=lake_gdf.crs)\n",
    "\n",
    "            # Get extent\n",
    "            x_min, y_min, x_max, y_max = evolving_stationary_union_gdf['geometry'].bounds.iloc[0]\n",
    "            buffer_dist = max(x_max - x_min, y_max - y_min) * 0.05\n",
    "            x_mins.append(x_min - buffer_dist)\n",
    "            x_maxs.append(x_max + buffer_dist)\n",
    "            y_mins.append(y_min - buffer_dist)\n",
    "            y_maxs.append(y_max + buffer_dist)\n",
    "        \n",
    "        # Set plot extent\n",
    "        x_min, x_max = min(x_mins), max(x_maxs)\n",
    "        y_min, y_max = min(y_mins), max(y_maxs)\n",
    "        \n",
    "        # Plot MOA background\n",
    "        mask_x = (moa_highres_da.x >= x_min) & (moa_highres_da.x <= x_max)\n",
    "        mask_y = (moa_highres_da.y >= y_min) & (moa_highres_da.y <= y_max)\n",
    "        moa_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "        ax_main.imshow(moa_subset[0,:,:], cmap='gray', clim=[14000, 17000],\n",
    "                      extent=[x_min, x_max, y_min, y_max])\n",
    "        \n",
    "        # Plot stationary outlines\n",
    "        stationary_color = 'darkturquoise'\n",
    "        for lake_gdf in lake_gdfs:\n",
    "            lake_gdf.boundary.plot(ax=ax_main, color=stationary_color, linewidth=2)\n",
    "\n",
    "        # Plot evolving outlines union\n",
    "        for lake_gdf in lake_gdfs:\n",
    "            lake_name = lake_gdf['name'].iloc[0]\n",
    "            evolving_union_gdf = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]\n",
    "            evolving_union_gdf.boundary.plot(ax=ax_main, color='k', linestyle='dotted', linewidth=2)\n",
    "        \n",
    "        # Plot evolving outlines with time-based coloring\n",
    "        cmap = plt.get_cmap('plasma')\n",
    "        norm = plt.Normalize(mdates.date2num(cyc_start_datetimes[1]), \n",
    "                           mdates.date2num(cyc_start_datetimes[-1]))\n",
    "        \n",
    "        for evolving_outlines_gdf in evolving_outlines_gdfs:\n",
    "            for idx, row in evolving_outlines_gdf.iterrows():\n",
    "                color = cmap(norm(mdates.date2num(pd.to_datetime(row['mid_pt_datetime']))))\n",
    "                gpd.GeoSeries(row['geometry']).boundary.plot(\n",
    "                    ax=ax_main, color=color, linewidth=1)\n",
    "\n",
    "        # Format overview axes\n",
    "        km_scale = 1e3\n",
    "        ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "        ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "        ax_main.xaxis.set_major_formatter(ticks_x)\n",
    "        ax_main.yaxis.set_major_formatter(ticks_y)\n",
    "        ax_main.set_xlabel('x [km]')\n",
    "        ax_main.set_ylabel('y [km]')\n",
    "\n",
    "        # Set up colormap\n",
    "        min_date = pd.to_datetime(cyc_start_datetimes[1])\n",
    "        max_date = pd.to_datetime(cyc_start_datetimes[-1])\n",
    "        date_range = pd.date_range(min_date, max_date, periods=len(cyc_start_datetimes[1:]))\n",
    "        years = date_range.year.unique()\n",
    "        years = pd.to_datetime(years, format='%Y')\n",
    "        n_dates = len(cyc_start_datetimes[1:])\n",
    "        cmap = plt.get_cmap('plasma', n_dates)\n",
    "        norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "        m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        m.set_array(np.linspace(mdates.date2num(min_date), mdates.date2num(max_date), n_dates))\n",
    "        \n",
    "        # Add colorbar\n",
    "        divider = make_axes_locatable(ax_main)\n",
    "        cax = divider.append_axes('bottom', size='2.5%', pad=0.5)\n",
    "        cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "\n",
    "        # Set colorbar ticks\n",
    "        cbar.ax.xaxis.set_major_formatter(year_interval_formatter(interval=4))\n",
    "        cbar.ax.xaxis.set_major_locator(mdates.YearLocator())  # Every year\n",
    "        cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator([1, 4, 7, 10]))  # Quarter year ticks\n",
    "        cbar.set_label('Year')\n",
    "\n",
    "        # # Get y axis limits for volume plots\n",
    "        y_min, y_max = get_overall_y_limits(evolving_geom_calcs_dfs, \n",
    "                                          stationary_geom_calcs_dfs,\n",
    "                                          evolving_union_geom_calcs_dfs)        \n",
    "        # Calculate limits with buffer\n",
    "        y_range = y_max - y_min\n",
    "        buffer = y_range * 0.05\n",
    "        y_limits = (y_min - buffer, y_max + buffer)\n",
    "        \n",
    "        # Create axes for all plots\n",
    "        axes = []\n",
    "        for idx in range(n_plots):\n",
    "            row = (idx // 3) + 1\n",
    "            col = idx % 3\n",
    "            ax = fig.add_subplot(gs[row, col])\n",
    "            axes.append(ax)\n",
    "        \n",
    "        # Plot individual lakes\n",
    "        for idx, (lake_name, evolving_df, stationary_df, union_df) in enumerate(zip(\n",
    "                valid_lakes, evolving_geom_calcs_dfs, stationary_geom_calcs_dfs, evolving_union_geom_calcs_dfs)):\n",
    "            ax = axes[idx]\n",
    "            ax.axhline(0, color='k', linestyle='--')\n",
    "            \n",
    "            dates = mdates.date2num(evolving_df['mid_pt_datetime'])\n",
    "            \n",
    "            # Plot stationary outline\n",
    "            stationary_cumsum = np.cumsum(np.divide(stationary_df['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "            ax.plot(dates, stationary_cumsum, color=stationary_color, label='Stationary', linewidth=2)\n",
    "            ax.scatter(dates, stationary_cumsum, color=stationary_color, s=5)\n",
    "\n",
    "            # Plot evolving outlines union\n",
    "            union_cumsum = np.cumsum(np.divide(union_df['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "            ax.plot(dates, union_cumsum, color='k', linestyle='dotted', label='Union', linewidth=2)\n",
    "            ax.scatter(dates, union_cumsum, color='k', s=5)\n",
    "\n",
    "            # Store line segments for multi-colored line in legend\n",
    "            lines = []\n",
    "            for i, dt in enumerate(dates):\n",
    "                line = ax.plot(1, 1, color=cmap(norm(mdates.date2num(cyc_start_datetimes[i]))), linewidth=2)[0]\n",
    "                lines.append(line)\n",
    "                line.remove()  # Remove the dummy lines after creating them\n",
    "\n",
    "            # Store line segments for multi-colored line in legend\n",
    "            onlake_lines = []\n",
    "            for i, dt in enumerate(dates):\n",
    "                x, y = 1, 1\n",
    "                onlake_line, = ax.plot(x, y, color=cmap(norm(mdates.date2num(cyc_start_datetimes[i]))), linewidth=2)\n",
    "                onlake_lines.append(onlake_line)\n",
    "\n",
    "            # Plot evolving outlines (multi-colored line)\n",
    "            x = dates\n",
    "            y = np.cumsum(np.divide(evolving_df['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "            points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "            segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "            lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "            lc.set_array(x)\n",
    "            lc.set_linewidth(2)\n",
    "            ax.add_collection(lc)\n",
    "            ax.scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "            # Plot bias\n",
    "            bias = np.cumsum(np.divide(evolving_df['evolving_outlines_dV_corr (m^3)'] - \n",
    "                                     stationary_df['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "            ax.plot(dates, bias, color='r', label='Bias', linewidth=2)\n",
    "            ax.scatter(dates, bias, color='r', linewidth=2, s=5)\n",
    "\n",
    "            # Add legend only to the first plot\n",
    "            if idx == 0:\n",
    "                stationary_line = plt.Line2D([], [], color=stationary_color, linestyle='solid', linewidth=2)\n",
    "                evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=2)\n",
    "                bias_line = plt.Line2D([], [], color='red', linestyle='solid', linewidth=2)\n",
    "                legend = ax.legend(\n",
    "                    [tuple(lines), \n",
    "                     evolving_union_line,\n",
    "                     stationary_line,\n",
    "                     bias_line],\n",
    "                    ['evolving outlines',\n",
    "                     'evolving outlines union',\n",
    "                     'stationary outline',\n",
    "                     'bias (evolving − stationary)'],\n",
    "                    handlelength=3,\n",
    "                    handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "                    fontsize=12,\n",
    "                    loc='upper center')\n",
    "\n",
    "            # Format axes\n",
    "            ax.xaxis.set_major_formatter(year_interval_formatter())\n",
    "            ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "            ax.xaxis.set_minor_locator(mdates.MonthLocator([1, 4, 7, 10]))\n",
    "            ax.set_xlabel('Year')\n",
    "            ax.set_title(lake_name)\n",
    "\n",
    "            # Set x and y axes limit\n",
    "            ax.set_xlim(cyc_start_datetimes[0], cyc_end_datetimes[-1])\n",
    "            ax.set_ylim(y_min, y_max)    \n",
    "\n",
    "            # Handle y-axis labels and ticks\n",
    "            if idx % 3 == 0:  # Leftmost column\n",
    "                ax.set_ylabel('cumulative dV [km$^3$]')\n",
    "            else:  # Middle and right columns\n",
    "                ax.set_yticklabels([])\n",
    "            \n",
    "            # Handle x-axis labels\n",
    "            # Calculate if this is the last plot in its column\n",
    "            current_row = (idx // 3) + 1\n",
    "            current_col = idx % 3\n",
    "            is_last_in_column = True\n",
    "            for next_idx in range(idx + 1, n_plots):\n",
    "                if next_idx % 3 == current_col:  # Same column\n",
    "                    is_last_in_column = False\n",
    "                    break\n",
    "            \n",
    "            if not is_last_in_column:\n",
    "                ax.set_xticklabels([])\n",
    "                ax.set_xlabel('')\n",
    "            else:\n",
    "                ax.set_xlabel('Year')\n",
    "            \n",
    "            ax.set_title(lake_name)\n",
    "        \n",
    "        # Format last plot (combined data)\n",
    "        last_ax = axes[-1]\n",
    "        last_col = (n_plots - 1) % 3\n",
    "\n",
    "        # Plot combined data\n",
    "        # Combine all dataframes by summing values for each timestamp\n",
    "        combined_evolving = pd.concat(evolving_geom_calcs_dfs).groupby('mid_pt_datetime').sum().reset_index()\n",
    "        combined_stationary = pd.concat(stationary_geom_calcs_dfs).groupby('mid_pt_datetime').sum().reset_index()\n",
    "        combined_union = pd.concat(evolving_union_geom_calcs_dfs).groupby('mid_pt_datetime').sum().reset_index()\n",
    "        \n",
    "        dates = mdates.date2num(combined_evolving['mid_pt_datetime'])\n",
    "        \n",
    "        # Plot stationary outline\n",
    "        stationary_cumsum = np.cumsum(np.divide(combined_stationary['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "        last_ax.plot(dates, stationary_cumsum, color=stationary_color, label='Stationary', linewidth=2)\n",
    "        last_ax.scatter(dates, stationary_cumsum, color=stationary_color, s=5)\n",
    "\n",
    "        # Plot evolving outlines union\n",
    "        union_cumsum = np.cumsum(np.divide(combined_union['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "        last_ax.plot(dates, union_cumsum, color='k', linestyle='dotted', label='Union', linewidth=2)\n",
    "        last_ax.scatter(dates, union_cumsum, color='k', s=5)\n",
    "\n",
    "        # Plot evolving outlines (multi-colored line)\n",
    "        evolving_cumsum = np.cumsum(np.divide(combined_evolving['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "        points = np.array([dates, evolving_cumsum]).T.reshape(-1, 1, 2)\n",
    "        segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "        lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "        lc.set_array(dates)\n",
    "        lc.set_linewidth(2)\n",
    "        last_ax.add_collection(lc)\n",
    "        last_ax.scatter(dates, evolving_cumsum, c=dates, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "        # Plot bias\n",
    "        bias_cumsum = np.cumsum(np.divide(\n",
    "            combined_evolving['evolving_outlines_dV_corr (m^3)'] - \n",
    "            combined_stationary['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "        last_ax.plot(dates, bias_cumsum, color='r', label='Bias', linewidth=2)\n",
    "        last_ax.scatter(dates, bias_cumsum, color='r', s=5)\n",
    "\n",
    "        # Set axes limits for combined plot\n",
    "        last_ax.set_xlim(cyc_start_datetimes[0], cyc_end_datetimes[-1])\n",
    "        last_ax.set_ylim(y_min, y_max)\n",
    "        last_ax.axhline(0, color='k', linestyle='--')\n",
    "        \n",
    "        # Set y-axis formatting for combined plot\n",
    "        if last_col == 0:  # Leftmost column\n",
    "            last_ax.set_ylabel('Cumulative dV [km$^3$]')\n",
    "        else:\n",
    "            last_ax.set_yticklabels([])\n",
    "        \n",
    "        # Always show x-axis labels for the combined plot as it's the last one\n",
    "        last_ax.set_xlabel('Year')\n",
    "        last_ax.xaxis.set_major_formatter(year_interval_formatter())\n",
    "        last_ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "        last_ax.xaxis.set_minor_locator(mdates.MonthLocator([1, 4, 7, 10]))\n",
    "        last_ax.set_title('Summed')\n",
    "\n",
    "        # Save the figure using the group name\n",
    "        sanitized_group_name = group_name.replace(' ', '_').replace('/', '_')\n",
    "        plt.savefig(f'{OUTPUT_DIR}/lake_group_dV_plots/{sanitized_group_name}.jpg', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Clear output\n",
    "        clear_output(wait=True)\n",
    "\n",
    "def get_overall_y_limits(evolving_geom_calcs_dfs, stationary_geom_calcs_dfs, evolving_union_geom_calcs_dfs):\n",
    "    \"\"\"\n",
    "    Calculate overall y-axis limits for all lake volume plots based on three types of geometric calculations.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    evolving_geom_calcs_dfs : list of pandas.DataFrame\n",
    "        List of dataframes containing evolving outline calculations\n",
    "    stationary_geom_calcs_dfs : list of pandas.DataFrame\n",
    "        List of dataframes containing stationary outline calculations\n",
    "    evolving_union_geom_calcs_dfs : list of pandas.DataFrame\n",
    "        List of dataframes containing evolving union calculations\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (y_min, y_max)\n",
    "        The minimum and maximum y-axis values with a 5% buffer\n",
    "    \"\"\"\n",
    "    all_y_values = []\n",
    "    \n",
    "    # Process each lake's data\n",
    "    for evolving_df, stationary_df, union_df in zip(evolving_geom_calcs_dfs, \n",
    "                                                   stationary_geom_calcs_dfs,\n",
    "                                                   evolving_union_geom_calcs_dfs):\n",
    "        # Calculate cumulative values for all time series\n",
    "        stationary_cumsum = np.cumsum(np.divide(stationary_df['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "        evolving_cumsum = np.cumsum(np.divide(evolving_df['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "        union_cumsum = np.cumsum(np.divide(union_df['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "        bias_cumsum = np.cumsum(np.divide(evolving_df['evolving_outlines_dV_corr (m^3)'] - \n",
    "                                        stationary_df['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "        \n",
    "        # Extend list with all values\n",
    "        all_y_values.extend(stationary_cumsum)\n",
    "        all_y_values.extend(evolving_cumsum)\n",
    "        all_y_values.extend(union_cumsum)\n",
    "        all_y_values.extend(bias_cumsum)\n",
    "    \n",
    "    # Also include the combined plot values if there are any lakes\n",
    "    if evolving_geom_calcs_dfs:\n",
    "        # Combine all dataframes by summing values for each timestamp\n",
    "        combined_evolving = pd.concat(evolving_geom_calcs_dfs).groupby('mid_pt_datetime').sum().reset_index()\n",
    "        combined_stationary = pd.concat(stationary_geom_calcs_dfs).groupby('mid_pt_datetime').sum().reset_index()\n",
    "        combined_union = pd.concat(evolving_union_geom_calcs_dfs).groupby('mid_pt_datetime').sum().reset_index()\n",
    "        \n",
    "        # Calculate cumulative sums for combined data\n",
    "        stationary_cumsum = np.cumsum(np.divide(combined_stationary['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "        evolving_cumsum = np.cumsum(np.divide(combined_evolving['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "        union_cumsum = np.cumsum(np.divide(combined_union['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "        bias_cumsum = np.cumsum(np.divide(combined_evolving['evolving_outlines_dV_corr (m^3)'] - \n",
    "                                        combined_stationary['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "        \n",
    "        all_y_values.extend(stationary_cumsum)\n",
    "        all_y_values.extend(evolving_cumsum)\n",
    "        all_y_values.extend(union_cumsum)\n",
    "        all_y_values.extend(bias_cumsum)\n",
    "    \n",
    "    # Calculate limits with a small buffer (5% of range)\n",
    "    y_min = min(all_y_values)\n",
    "    y_max = max(all_y_values)\n",
    "    y_range = y_max - y_min\n",
    "    buffer = y_range * 0.05\n",
    "    \n",
    "    return y_min - buffer, y_max + buffer\n",
    "\n",
    "def year_interval_formatter(interval=2, start_year=2012):\n",
    "    '''\n",
    "    Create custom formatter that labels years at specified intervals\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    interval : int, default=2\n",
    "        Interval between labeled years (e.g., 2 for every 2 years, 4 for every 4 years)\n",
    "    start_year : int, optional\n",
    "        Starting year for the interval. If None, uses modulo arithmetic.\n",
    "        If provided, labels years that are start_year + n*interval\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    function : formatter function for matplotlib\n",
    "    \n",
    "    Examples:\n",
    "    ---------\n",
    "    # Every 2 years (even years): 2012, 2014, 2016, 2018, 2020, 2022, 2024\n",
    "    formatter = year_interval_formatter(interval=2)\n",
    "    \n",
    "    # Every 4 years starting from 2012: 2012, 2016, 2020, 2024\n",
    "    formatter = year_interval_formatter(interval=4, start_year=2012)\n",
    "    \n",
    "    # Every 4 years using modulo (years divisible by 4): 2012, 2016, 2020, 2024\n",
    "    formatter = year_interval_formatter(interval=4)\n",
    "\n",
    "    \n",
    "    # Usage examples:\n",
    "    # For specific case (2012, 2016, 2020, 2024):\n",
    "    formatter = year_interval_formatter(interval=4, start_year=2012)\n",
    "    \n",
    "    # For every 4 years using modulo:\n",
    "    formatter = year_interval_formatter(interval=4)\n",
    "    \n",
    "    # For every 2 years using modulo:\n",
    "    formatter = year_interval_formatter(interval=2)\n",
    "    '''\n",
    "    def formatter_func(x, pos):\n",
    "        date = mdates.num2date(x)\n",
    "        year = date.year\n",
    "        \n",
    "        if start_year is not None:\n",
    "            # Use specific starting year and interval\n",
    "            if (year - start_year) % interval == 0 and year >= start_year:\n",
    "                return date.strftime('%Y')\n",
    "        else:\n",
    "            # Use modulo arithmetic\n",
    "            if year % interval == 0:\n",
    "                return date.strftime('%Y')\n",
    "        \n",
    "        return ''\n",
    "    \n",
    "    return formatter_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d1083-ac7e-4bc0-be61-e069e7848354",
   "metadata": {},
   "source": [
    "# Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae841541-da2b-4fbf-b92c-6ec5c7e9221b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import subglacial lake outlines \n",
    "stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/stationary_outlines_gdf.geojson')\n",
    "reexamined_stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/reexamined_stationary_outlines_gdf.geojson')\n",
    "evolving_outlines_union_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/evolving_outlines_union_gdf.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a901efcd-04b5-468b-ac6e-f820147c67b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CryoSat-2 SARIn mode mask\n",
    "# See 0_preprocess_data.ipynb for data source and pre-processing steps\n",
    "gdf_SARIn_3_1 = gpd.read_file('output/CryoSat2_SARIn_mode_masks/gdf_SARIn_3_1.geojson')\n",
    "gdf_SARIn_3_1_3_6_diff= gpd.read_file('output/CryoSat2_SARIn_mode_masks/gdf_SARIn_3_1_3_6_diff.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cf07dd-b864-4517-bbc9-60048ea9878a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import cyc_dates\n",
    "cyc_dates = pd.read_csv('output/cycle_dates.csv', parse_dates=['cyc_start_datetimes', 'cyc_end_datetimes'])\n",
    "\n",
    "# Store the cyc_dates columns as a np array with datetime64[ns] data type\n",
    "cyc_start_datetimes = [np.datetime64(ts) for ts in cyc_dates['cyc_start_datetimes']]\n",
    "cyc_end_datetimes = [np.datetime64(ts) for ts in cyc_dates['cyc_end_datetimes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe18173c-0ef2-4897-b7a7-11852526e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MODIS Mosaic of Antarctica (MOA) surface imagery\n",
    "# https://nsidc.org/data/nsidc-0730/versions/1\n",
    "moa_highres = DATA_DIR + '/moa125_2014_hp1_v01.tif' \n",
    "moa_highres_da = rioxarray.open_rasterio(moa_highres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9477ce8b-819f-4bc9-b1aa-94d444759f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIS MOA 2014 coastline and grounding line\n",
    "# https://nsidc.org/data/nsidc-0730/versions/1\n",
    "shp = DATA_DIR + '/moa2014_coastline_v01.shp' \n",
    "moa_2014_coastline = gpd.read_file(shp)\n",
    "shp = DATA_DIR + '/moa2014_grounding_line_v01.shp' \n",
    "moa_2014_groundingline = gpd.read_file(shp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff157c2-2af5-45eb-b735-215d19ad1074",
   "metadata": {},
   "source": [
    "# Results and discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8498282-b806-459a-a654-1d21ee8340e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many previously identified lakes were analyzed?\n",
    "print(len(stationary_outlines_gdf), 'lakes reanalyzed')\n",
    "print(len(reexamined_stationary_outlines_gdf), \n",
    "    'lakes analyzed in revised inventory due to Site_B and Site_C being combined into Site_BC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6dc095-814b-4b70-987e-7d886b44ca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many lakes are missing CryoSat-2 SARIn coverage?\n",
    "print(stationary_outlines_gdf[stationary_outlines_gdf['CS2_SARIn_start'] == '<NA>'].shape[0])\n",
    "print(reexamined_stationary_outlines_gdf[reexamined_stationary_outlines_gdf['CS2_SARIn_start'] == '<NA>'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f5b237-d443-4dc4-ba68-2c855352751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many lakes have CryoSat-2 SARIn coverage?\n",
    "print(stationary_outlines_gdf[stationary_outlines_gdf['CS2_SARIn_start'] != '<NA>'].shape[0])\n",
    "print(reexamined_stationary_outlines_gdf[reexamined_stationary_outlines_gdf['CS2_SARIn_start'] != '<NA>'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe66e07-5d47-409a-bc73-6d4c280caa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many lakes have CryoSat-2 SARIn coverage from the start of the mission?\n",
    "print(stationary_outlines_gdf[stationary_outlines_gdf['CS2_SARIn_start'] == '2010.5'].shape[0])\n",
    "print(reexamined_stationary_outlines_gdf[reexamined_stationary_outlines_gdf['CS2_SARIn_start'] == '2010.5'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21245f4a-daf0-405f-b9fd-2b1d40fb2682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many lakes have CryoSat-2 SARIn coverage starting when LRM/SARIn boundary moved inland?\n",
    "print(stationary_outlines_gdf[stationary_outlines_gdf['CS2_SARIn_start'] == '2013.75'].shape[0])\n",
    "print(reexamined_stationary_outlines_gdf[reexamined_stationary_outlines_gdf['CS2_SARIn_start'] == '2013.75'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d0795a-f3f7-4627-a64b-869a37a8401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many lakes exhibit evolving outlines?\n",
    "print(len([f for f in os.listdir('output/lake_outlines/evolving_outlines') if f.endswith('.geojson')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaf861b-de0c-4455-9d60-f657e0e124e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many lakes exhibit no evolving outlines?\n",
    "print(len([f for f in os.listdir('output/lake_outlines/evolving_outlines') if f.endswith('.txt')]))\n",
    "print(len(stationary_outlines_gdf) - len([f for f in os.listdir('output/lake_outlines/evolving_outlines') if f.endswith('.geojson')]))\n",
    "print(len(reexamined_stationary_outlines_gdf) - len([f for f in os.listdir('output/lake_outlines/evolving_outlines') if f.endswith('.geojson')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99f729f-b2a4-42ce-9e11-412c016c60f9",
   "metadata": {},
   "source": [
    "## Evolving outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b15ab-ecd9-420b-aa27-c0ed354e1336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze lake extensions\n",
    "lakes_with_extensions, extension_results = quantify_lake_extensions(\n",
    "    reexamined_stationary_outlines_gdf, \n",
    "    evolving_outlines_union_gdf, \n",
    "    area_threshold=0.25)\n",
    "summary = generate_extension_summary(extension_results, km2=True)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942789f1-1d9d-40f7-a45c-8b7536e503dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many lakes have no extension beyond stationary outline?\n",
    "len(extension_results[extension_results['relative_extension'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10727648-a236-407c-a393-686ab3cf6707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View lakes that exceed the area threshold\n",
    "extension_results[extension_results['relative_extension'] > 0.25].sort_values('relative_extension', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc12128-2aab-44d3-a071-2460f9873c0c",
   "metadata": {},
   "source": [
    "## Active area and carbon export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af05cae5-1947-49f3-81a1-d74b8babb278",
   "metadata": {},
   "source": [
    "### Active area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9584261-bb9b-490b-a03c-2b0483b0c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in continental summation geometric calculation csv files - evolving outlines (only lakes with evolving outlines)\n",
    "base_path = 'output/geometric_calcs/evolving_outlines_geom_calc/forward_fill'\n",
    "filenames = {\n",
    "    'superset_IS2_lakes_sum': 'superset_IS2_lakes_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPreExpansion_sum': 'subset_CS2_IS2_lakes_SARInPreExpansion_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPostExpansion_sum': 'subset_CS2_IS2_lakes_SARInPostExpansion_sum.csv',\n",
    "}\n",
    "\n",
    "dfs = {name: pd.read_csv(f'{base_path}/{filename}') \n",
    "       for name, filename in filenames.items()}\n",
    "\n",
    "# Store dataframes from dfs list for code readability\n",
    "superset_IS2_evolving_sum_df = dfs['superset_IS2_lakes_sum']\n",
    "subset_CS2_IS2_SARInPreExpansion_evolving_sum_df = dfs['subset_CS2_IS2_lakes_SARInPreExpansion_sum']\n",
    "subset_CS2_IS2_SARInPostExpansion_evolving_sum_df = dfs['subset_CS2_IS2_lakes_SARInPostExpansion_sum']\n",
    "\n",
    "# Read in continental summation geometric calculation csv files - stationary outlines (all lakes)\n",
    "base_path = 'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes'\n",
    "filenames = {'superset_IS2_lakes_sum': 'superset_IS2_lakes_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPreExpansion_sum': 'subset_CS2_IS2_lakes_SARInPreExpansion_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPostExpansion_sum': 'subset_CS2_IS2_lakes_SARInPostExpansion_sum.csv'}\n",
    "\n",
    "dfs = {name: pd.read_csv(f'{base_path}/{filename}') \n",
    "       for name, filename in filenames.items()}\n",
    "\n",
    "superset_IS2_stationary_sum_df = dfs['superset_IS2_lakes_sum']\n",
    "subset_CS2_IS2_SARInPreExpansion_stationary_sum_df = dfs['subset_CS2_IS2_lakes_SARInPreExpansion_sum']\n",
    "subset_CS2_IS2_SARInPostExpansion_stationary_sum_df = dfs['subset_CS2_IS2_lakes_SARInPostExpansion_sum']\n",
    "\n",
    "# Read in continental summation geometric calculation csv files - evolving union (only lakes with evolving outlines)\n",
    "base_path = 'output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes'\n",
    "filenames = {'superset_IS2_lakes_sum': 'superset_IS2_lakes_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPreExpansion_sum': 'subset_CS2_IS2_lakes_SARInPreExpansion_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPostExpansion_sum': 'subset_CS2_IS2_lakes_SARInPostExpansion_sum.csv'}\n",
    "\n",
    "dfs = {name: pd.read_csv(f'{base_path}/{filename}')\n",
    "       for name, filename in filenames.items()}\n",
    "\n",
    "superset_IS2_evolving_union_sum_df = dfs['superset_IS2_lakes_sum']\n",
    "subset_CS2_IS2_SARInPreExpansion_evolving_union_sum_df = dfs['subset_CS2_IS2_lakes_SARInPreExpansion_sum']\n",
    "subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df = dfs['subset_CS2_IS2_lakes_SARInPostExpansion_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8914cee9-a042-43a6-8ede-7b8d3160e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot evolving outlines time series as multi-colored line using LineCollection from points/segments \n",
    "print(np.round(superset_IS2_evolving_sum_df['evolving_outlines_area (m^2)'].max() / 1e6, 0))\n",
    "\n",
    "# Plot stationary outlines\n",
    "print(np.round(superset_IS2_evolving_sum_df['evolving_outlines_area (m^2)'].max() / \n",
    "               superset_IS2_stationary_sum_df['stationary_outline_area (m^2)'].iloc[0] * 100, 0), '%')\n",
    "\n",
    "# \n",
    "print(np.round(superset_IS2_stationary_sum_df['stationary_outline_area (m^2)'].iloc[0] / 1e6, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202d821b-5f5b-468c-bf5e-60919788eefc",
   "metadata": {},
   "source": [
    "### Dissolved inorganic carbon (DIC) export estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d24baf6-6b93-4aeb-aa80-3f2db19b02ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the microbial respiration rate  of 1.4 x 10^4 g C d-1 measured directly in Mercer Subglacial Lake (SLM; Venturelli and others, 2023) \n",
    "# as an estimate of dissolved inorganic carbon (DIC) flux from saturated sediment to the subglacial water column via respiration\n",
    "# doi.org/10.1029/2022AV000846\n",
    "resp_rate = 1.4 * 10**4  # g C d-1 in SLM\n",
    "print('respiration rate:', resp_rate, 'g C d-1')\n",
    "\n",
    "# Estimate DIC export from SLM per CryoSat-2/ICESat-2 satellite repeat cycle (91 days) time step\n",
    "SLM_DIC_export_per_step = resp_rate * 91\n",
    "print('DIC export per time step at SLM:', SLM_DIC_export_per_step, 'g C / 91 d')  # g C / 91 d in SLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05392b4e-a184-4af4-ace8-3439b5d9afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the stationary areas\n",
    "SLM_stationary_area = reexamined_stationary_outlines_gdf[reexamined_stationary_outlines_gdf['name'] == 'MercerSubglacialLake']['area (m^2)'].values[0]\n",
    "SLM_evolving_union_area = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == 'MercerSubglacialLake']['area (m^2)'].values[0]\n",
    "print('SLM stationary outline area:', np.round(SLM_stationary_area/1e6,1), 'km^2')\n",
    "print('SLM updated stationary area:', np.round(SLM_evolving_union_area/1e6,1), 'km^2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3faee2-fd1c-402c-8c98-3e4c6a11198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the per area DIC export values using stationary outline\n",
    "SLM_stationary_per_area_DIC_export = resp_rate / SLM_stationary_area\n",
    "SLM_stationary_per_area_per_step_DIC_export = resp_rate / SLM_stationary_area * 91\n",
    "\n",
    "print('SLM stationary outline per area DIC export:', np.round(SLM_stationary_per_area_DIC_export,6), 'g C d-1 m-2')\n",
    "print('SLM stationary outline per area per time step DIC export:', np.round(SLM_stationary_per_area_per_step_DIC_export,6), 'g C step-1 m-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1408e64-3648-4829-8ced-b173dfb2a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the per area per time step DIC export values using evolving union outline\n",
    "SLM_evolving_union_per_area_DIC_export = resp_rate / SLM_evolving_union_area\n",
    "SLM_evolving_union_per_area_per_step_DIC_export = resp_rate / SLM_evolving_union_area * 91\n",
    "\n",
    "print('SLM updated stationary outline per area DIC export:', np.round(SLM_evolving_union_per_area_DIC_export,6), 'g C d-1 m-2')\n",
    "print('SLM updated stationary outline per area per time step DIC export:', np.round(SLM_evolving_union_per_area_per_step_DIC_export,6), 'g C step-1 m-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f244d-ccaf-4eb3-af65-ca48f03b02bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we estimate the per area respiration using evolving outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64781e19-e9b5-423a-aced-103fd2690729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dV time series to view filling/draining history\n",
    "\n",
    "# Load SLM geometries dataframe\n",
    "SLM_evolving_outlines_geom = pd.read_csv('/home/jovyan/1_evolving_lakes/Sauthoff-2025-GRL/output/geometric_calcs/evolving_outlines_geom_calc/forward_fill/MercerSubglacialLake.csv')\n",
    "SLM_stationary_outline_geom = pd.read_csv('/home/jovyan/1_evolving_lakes/Sauthoff-2025-GRL/output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes/MercerSubglacialLake.csv')\n",
    "SLM_updated_stationary_outline_geom = pd.read_csv('/home/jovyan/1_evolving_lakes/Sauthoff-2025-GRL/output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/MercerSubglacialLake.csv')\n",
    "\n",
    "# Convert to datetimes\n",
    "SLM_evolving_outlines_geom['mid_pt_datetime'] = pd.to_datetime(SLM_evolving_outlines_geom['mid_pt_datetime'])\n",
    "\n",
    "# Define x and y for plotting\n",
    "x = SLM_evolving_outlines_geom['mid_pt_datetime']\n",
    "y = (SLM_evolving_outlines_geom['evolving_outlines_dV_corr (m^3)'] / 1e9)  # km^3\n",
    "y2 = np.cumsum(SLM_evolving_outlines_geom['evolving_outlines_dV_corr (m^3)'] / 1e9)  # km^3\n",
    "y3 = (SLM_stationary_outline_geom['stationary_outline_dV_corr (m^3)'] / 1e9)  # km^3\n",
    "y4 = np.cumsum(SLM_stationary_outline_geom['stationary_outline_dV_corr (m^3)'] / 1e9)  # km^3\n",
    "y5 = (SLM_updated_stationary_outline_geom['stationary_outline_dV_corr (m^3)'] / 1e9)  # km^3\n",
    "y6 = np.cumsum(SLM_updated_stationary_outline_geom['stationary_outline_dV_corr (m^3)'] / 1e9)  # km^3\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(15, 6))  # Set figure size\n",
    "\n",
    "# Get current axis\n",
    "ax = plt.gca()\n",
    "\n",
    "# Define masks for shading\n",
    "positive_mask = y > 0\n",
    "negative_mask = y < 0\n",
    "\n",
    "# +1 = positive, -1 = negative, 0 = zero\n",
    "sign_series = positive_mask.astype(int) - negative_mask.astype(int)\n",
    "\n",
    "# Find change points\n",
    "changes = np.diff(sign_series, prepend=sign_series.iloc[0])\n",
    "change_points = np.where(changes != 0)[0]\n",
    "change_points = np.concatenate(([0], change_points, [len(x)]))\n",
    "\n",
    "# Compute half time step for proper discrete alignment\n",
    "if len(x) > 1:\n",
    "    dt = (x.iloc[1] - x.iloc[0]) / 2\n",
    "else:\n",
    "    dt = pd.Timedelta(days=0)  # fallback\n",
    "\n",
    "# Shaded regions\n",
    "for i in range(len(change_points) - 1):\n",
    "    start_idx = change_points[i]\n",
    "    end_idx = change_points[i + 1]\n",
    "\n",
    "    # Determine color based on start of interval\n",
    "    val = sign_series.iloc[start_idx]\n",
    "    if val > 0:\n",
    "        color = \"blue\"\n",
    "    elif val < 0:\n",
    "        color = \"red\"\n",
    "    else:\n",
    "        color = \"gray\"\n",
    "\n",
    "    # Use start_idx-1 to fully cover previous regime for perfect alignment\n",
    "    plt.axvspan(x.iloc[start_idx-1], x.iloc[end_idx-1],\n",
    "        alpha=0.2, color=color, zorder=0)\n",
    "    \n",
    "# Plot both lines (with fixed colors)\n",
    "plt.plot(x, y, 'k-', linewidth=2, label='evolving outlines $dV$ ($km^3$)')\n",
    "plt.plot(x, y2, 'k--', linewidth=2, label='evolving outlines cumulative $dV$ ($km^3$)')\n",
    "plt.plot(x, y3, 'b-', linewidth=2, label='stationary outline $dV$ ($km^3$)')\n",
    "plt.plot(x, y4, 'b--', linewidth=2, label='stationary outline cumulative $dV$ ($km^3$)')\n",
    "plt.plot(x, y5, 'g-', linewidth=2, label='updated stationary outline $dV$ ($km^3$)')\n",
    "plt.plot(x, y6, 'g--', linewidth=2, label='updated stationary outline cumulative $dV$ ($km^3$)')\n",
    "\n",
    "# Format the x-axis to show years as major ticks and quarters as minor ticks\n",
    "# Set major ticks to years\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Format as year only\n",
    "\n",
    "# Set minor ticks to quarters\n",
    "ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1, 4, 7, 10]))  # Jan, Apr, Jul, Oct\n",
    "\n",
    "# Add gridlines for better readability\n",
    "ax.grid(True, which='major', linestyle='-', alpha=0.7)\n",
    "ax.grid(True, which='minor', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Add reference line at y=0\n",
    "plt.axhline(y=0, color='black', linestyle='-', lw=0.5)\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Year', fontsize=12)\n",
    "plt.ylabel('$dV$ ($km^3$)', fontsize=12)\n",
    "ax.set_title('Mercer Subglacial Lake $dV$ time series')\n",
    "\n",
    "ax.set_xlim(x.iloc[0], x.iloc[-1])\n",
    "\n",
    "# Add proxy patches for shaded regions\n",
    "pos_patch = mpatches.Patch(color='blue', alpha=0.2, label='filling')\n",
    "neg_patch = mpatches.Patch(color='red', alpha=0.2, label='draining')\n",
    "zero_patch = mpatches.Patch(color='gray', alpha=0.2, label='zero cumulative $dV$')\n",
    "\n",
    "# Add legend with both line plots and shaded patches\n",
    "plt.legend(handles=[pos_patch, neg_patch, zero_patch] + ax.get_legend_handles_labels()[0],\n",
    "           loc='best', bbox_to_anchor=(1, 0.5, 0.25, 0.25))  # (x, y, width, height)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a22009-3a7c-4229-9205-d1ddce767029",
   "metadata": {},
   "source": [
    "We observed a 3.75 year filling period prior to the 2018-2019 subglacial lake access campaign (2014.5 - 2018.25) cf. 4.5 years reported by Venturelli and others (2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8eebc4-411e-4ff9-a236-6fcafde23e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for date range 2014.75 to 2018.25\n",
    "# Filter for SLM fill period prior to Venturelli et al., 2023 sampling (2014.5 to 2018.25)\n",
    "start_date = pd.to_datetime('2014-07-01')  # ~2014.5\n",
    "end_date = pd.to_datetime('2018-04-01')    # ~2018.25\n",
    "SLM_evolving_outlines_geom_filling_period = SLM_evolving_outlines_geom[(SLM_evolving_outlines_geom['mid_pt_datetime'] >= start_date) & (SLM_evolving_outlines_geom['mid_pt_datetime'] <= end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1224a38-d07b-4897-a4a0-a785031941a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Conservation-based integration\n",
    "\n",
    "# Respiration rate [g C d-1]\n",
    "resp_rate = resp_rate  \n",
    "\n",
    "# Duration of each step [days/time step]\n",
    "time_step_days = 91  \n",
    "\n",
    "# Total filling period length [days = (years in filling period) * (days/year)]\n",
    "T_days = 3.75 * 365.25  \n",
    "\n",
    "# Total carbon respired over 3.75-year filling period [g C]\n",
    "C_total = resp_rate * T_days \n",
    "\n",
    "# Area at each time step [m^2]\n",
    "areas = SLM_evolving_outlines_geom_filling_period['evolving_outlines_area (m^2)'].values\n",
    "\n",
    "# Total area–time exposure [m^2 d]\n",
    "A_time_total = np.sum(areas * time_step_days)\n",
    "\n",
    "# Conservation-based per-area per-day rate [g C d-1 m-2]\n",
    "rate_method1 = C_total / A_time_total\n",
    "\n",
    "print('Method 1:', np.round(rate_method1, 6), 'g C d-1 m-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c4991b-c254-4f4e-a188-99b6d697e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2 - Mean of per-step rates\n",
    "rate_method2 = np.average(resp_rate / SLM_evolving_outlines_geom_filling_period['evolving_outlines_area (m^2)'])\n",
    "print('Method 2:', np.round(rate_method2,6), 'g C d-1 m-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd0fb8e-f993-4174-952e-ea52477d9c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3 - Mean area approach\n",
    "rate_method3 = resp_rate / np.average(SLM_evolving_outlines_geom_filling_period['evolving_outlines_area (m^2)'])\n",
    "print('Method 3:', np.round(rate_method3,6), 'g C d-1 m-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f2ab5f-77d8-494a-92ce-e73dc5bbfede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert per-area respiration rate to per-area, per-time-step respiration rate to be compatible \n",
    "# [g C m-2 time step-1 = (g C d-1 m-2) * (days/time step)]\n",
    "SLM_evolving_per_area_per_step_DIC_export_filling_period = rate_method1 * time_step_days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d56ec-96cc-4136-aa0f-f09bcd3003bf",
   "metadata": {},
   "source": [
    "## Volume change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469cdb3b-0b96-4848-af0b-6477d6c42706",
   "metadata": {},
   "source": [
    "### Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3799a4e-5417-4d71-9149-0ca4633af348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory paths for the new folder structure\n",
    "evolving_folder = 'output/geometric_calcs/evolving_outlines_geom_calc/forward_fill/'\n",
    "stationary_folder = 'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes/'\n",
    "\n",
    "# Files to ignore in our analysis\n",
    "files_to_ignore = [\n",
    "    'superset_IS2_lakes_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPostExpansion_sum.csv', \n",
    "    'subset_CS2_IS2_lakes_SARInPreExpansion_sum.csv',\n",
    "    'subset_noCS2_IS2_lakes_sum.csv'\n",
    "]\n",
    "\n",
    "# Get lists of CSV files in each directory, excluding the files to ignore\n",
    "evolving_csv_files = [f for f in os.listdir(evolving_folder) \n",
    "                     if f.endswith('.csv') and f not in files_to_ignore]\n",
    "stationary_csv_files = [f for f in os.listdir(stationary_folder) \n",
    "                       if f.endswith('.csv') and f not in files_to_ignore]\n",
    "\n",
    "# Find common lake names (files that exist in both folders)\n",
    "evolving_lake_names = {os.path.splitext(f)[0] for f in evolving_csv_files}\n",
    "stationary_lake_names = {os.path.splitext(f)[0] for f in stationary_csv_files}\n",
    "common_lake_names = evolving_lake_names.intersection(stationary_lake_names)\n",
    "\n",
    "# Initialize DataFrame to store lake-level results\n",
    "lake_results_df = pd.DataFrame({\n",
    "    'lake_name': list(common_lake_names),\n",
    "    'greater_than_125_percent': False,\n",
    "    'less_than_75_percent': False,\n",
    "    'both_conditions': False,\n",
    "    'either_condition': False,\n",
    "    'total_time_steps': 0,\n",
    "    'valid_data_found': False\n",
    "})\n",
    "\n",
    "# Create a list to store the combined data for all lakes (for time step analysis)\n",
    "all_combined_data = []\n",
    "\n",
    "# Loop through each lake and process data\n",
    "for idx, lake_name in enumerate(lake_results_df['lake_name']):\n",
    "    try:\n",
    "        # Read the evolving lake data\n",
    "        evolving_file_path = os.path.join(evolving_folder, f\"{lake_name}.csv\")\n",
    "        evolving_df = pd.read_csv(evolving_file_path)\n",
    "        \n",
    "        # Read the stationary lake data\n",
    "        stationary_file_path = os.path.join(stationary_folder, f\"{lake_name}.csv\")\n",
    "        stationary_df = pd.read_csv(stationary_file_path)\n",
    "        \n",
    "        # Identify the date column and volume column in each dataframe\n",
    "        evolving_date_col = None\n",
    "        evolving_vol_col = None\n",
    "        \n",
    "        for col in evolving_df.columns:\n",
    "            if any(date_indicator in col.lower() for date_indicator in ['date', 'time', 'day']):\n",
    "                evolving_date_col = col\n",
    "            if 'dv_corr' in col.lower():\n",
    "                evolving_vol_col = col\n",
    "        \n",
    "        # For stationary dataframe\n",
    "        stationary_date_col = None\n",
    "        stationary_vol_col = None\n",
    "        \n",
    "        for col in stationary_df.columns:\n",
    "            if any(date_indicator in col.lower() for date_indicator in ['date', 'time', 'day']):\n",
    "                stationary_date_col = col\n",
    "            if 'dv_corr' in col.lower():\n",
    "                stationary_vol_col = col\n",
    "        \n",
    "        # Skip if we couldn't identify the necessary columns\n",
    "        if not all([evolving_date_col, evolving_vol_col, stationary_date_col, stationary_vol_col]):\n",
    "            print(f\"Skipping {lake_name} - could not identify all required columns\")\n",
    "            continue\n",
    "        \n",
    "        # Create standardized dataframes for merging\n",
    "        evolving_std_df = pd.DataFrame({\n",
    "            'lake_name': lake_name,\n",
    "            'date': evolving_df[evolving_date_col],\n",
    "            'evolving_outlines_dV_corr (m^3)': evolving_df[evolving_vol_col]\n",
    "        })\n",
    "        \n",
    "        stationary_std_df = pd.DataFrame({\n",
    "            'lake_name': lake_name,\n",
    "            'date': stationary_df[stationary_date_col],\n",
    "            'stationary_outline_dV_corr (m^3)': stationary_df[stationary_vol_col]\n",
    "        })\n",
    "        \n",
    "        # Merge the dataframes on lake_name and date\n",
    "        merged_df = pd.merge(evolving_std_df, stationary_std_df, on=['lake_name', 'date'])\n",
    "        \n",
    "        # Filter out rows where evolving volume is 0\n",
    "        filtered_df = merged_df[merged_df['evolving_outlines_dV_corr (m^3)'] != 0]\n",
    "        \n",
    "        # Skip lakes with no valid data\n",
    "        if filtered_df.empty:\n",
    "            print(f\"Skipping {lake_name} - no valid data after filtering\")\n",
    "            continue\n",
    "        \n",
    "        # Update lake-level results\n",
    "        lake_results_df.loc[idx, 'valid_data_found'] = True\n",
    "        lake_results_df.loc[idx, 'total_time_steps'] = len(filtered_df)\n",
    "        \n",
    "        # Check conditions for lake-level analysis\n",
    "        condition_greater_than = (filtered_df['evolving_outlines_dV_corr (m^3)'] > \n",
    "                                1.25 * filtered_df['stationary_outline_dV_corr (m^3)'])\n",
    "        condition_less_than = (filtered_df['evolving_outlines_dV_corr (m^3)'] < \n",
    "                             0.75 * filtered_df['stationary_outline_dV_corr (m^3)'])\n",
    "        \n",
    "        # Update DataFrame with lake-level results\n",
    "        lake_results_df.loc[idx, 'greater_than_125_percent'] = condition_greater_than.any()\n",
    "        lake_results_df.loc[idx, 'less_than_75_percent'] = condition_less_than.any()\n",
    "        lake_results_df.loc[idx, 'both_conditions'] = condition_greater_than.any() and condition_less_than.any()\n",
    "        \n",
    "        # Calculate either condition\n",
    "        condition_either = condition_greater_than | condition_less_than\n",
    "        lake_results_df.loc[idx, 'either_condition'] = condition_either.any()\n",
    "        \n",
    "        # Add to combined data for time step analysis\n",
    "        all_combined_data.append(filtered_df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {lake_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# ==================== LAKE-LEVEL ANALYSIS ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LAKE-LEVEL ANALYSIS RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Filter out lakes with no valid data for final analysis\n",
    "valid_lake_results = lake_results_df[lake_results_df['valid_data_found']]\n",
    "\n",
    "# Calculate proportions using vectorized operations\n",
    "total_valid_lakes = len(valid_lake_results)\n",
    "print(f\"Valid lakes analyzed: {total_valid_lakes}\")\n",
    "\n",
    "if total_valid_lakes > 0:\n",
    "    lake_proportions = {\n",
    "        'greater_than_125_percent': valid_lake_results['greater_than_125_percent'].sum() / total_valid_lakes,\n",
    "        'less_than_75_percent': valid_lake_results['less_than_75_percent'].sum() / total_valid_lakes,\n",
    "        'both_conditions': valid_lake_results['both_conditions'].sum() / total_valid_lakes,\n",
    "        'either_condition': valid_lake_results['either_condition'].sum() / total_valid_lakes\n",
    "    }\n",
    "    \n",
    "    # Print lake-level results\n",
    "    print(f\"Lakes meeting condition 1 (greater than 125%): {valid_lake_results['greater_than_125_percent'].sum()} ({lake_proportions['greater_than_125_percent']:.2f})\")\n",
    "    print(f\"Lakes meeting condition 2 (less than 75%): {valid_lake_results['less_than_75_percent'].sum()} ({lake_proportions['less_than_75_percent']:.2f})\")\n",
    "    print(f\"Lakes meeting either condition: {valid_lake_results['either_condition'].sum()} ({lake_proportions['either_condition']:.2f})\")\n",
    "    print(f\"Lakes meeting both conditions: {valid_lake_results['both_conditions'].sum()} ({lake_proportions['both_conditions']:.2f})\")\n",
    "    \n",
    "else:\n",
    "    print(\"No valid lakes found for analysis\")\n",
    "\n",
    "# ==================== TIME STEP ANALYSIS ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TIME STEP ANALYSIS RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Concatenate all the combined data into a single dataframe\n",
    "if all_combined_data:\n",
    "    time_step_df = pd.concat(all_combined_data, ignore_index=True)\n",
    "    \n",
    "    # Define the separate conditions, using absolute values\n",
    "    condition1 = (time_step_df['evolving_outlines_dV_corr (m^3)'].abs() > \n",
    "                  1.25 * time_step_df['stationary_outline_dV_corr (m^3)'].abs())\n",
    "    condition2 = (time_step_df['evolving_outlines_dV_corr (m^3)'].abs() < \n",
    "                  0.75 * time_step_df['stationary_outline_dV_corr (m^3)'].abs())\n",
    "    \n",
    "    # Condition where either condition is met\n",
    "    either_condition = (condition1 | condition2)\n",
    "    \n",
    "    # Condition where neither condition is met\n",
    "    neither_condition = ~(condition1 | condition2)\n",
    "    \n",
    "    # Condition where both conditions are met\n",
    "    both_conditions = condition1 & condition2\n",
    "    \n",
    "    # Count the number of rows meeting each condition\n",
    "    num_rows_condition1 = condition1.sum()\n",
    "    num_rows_condition2 = condition2.sum()\n",
    "    num_rows_either_condition = either_condition.sum()\n",
    "    num_rows_neither_condition = neither_condition.sum()\n",
    "    num_rows_both_conditions = both_conditions.sum()\n",
    "    \n",
    "    # Calculate proportions based on the length of time_step_df\n",
    "    total_time_steps = len(time_step_df)\n",
    "    proportion_condition1 = num_rows_condition1 / total_time_steps if total_time_steps > 0 else 0\n",
    "    proportion_condition2 = num_rows_condition2 / total_time_steps if total_time_steps > 0 else 0\n",
    "    proportion_either_condition = num_rows_either_condition / total_time_steps if total_time_steps > 0 else 0\n",
    "    proportion_neither_condition = num_rows_neither_condition / total_time_steps if total_time_steps > 0 else 0\n",
    "    proportion_both_conditions = num_rows_both_conditions / total_time_steps if total_time_steps > 0 else 0\n",
    "    \n",
    "    # Calculate sum of proportions as a sanity check\n",
    "    sum_of_proportions = np.sum([proportion_condition1, proportion_condition2, proportion_both_conditions, proportion_neither_condition])\n",
    "    \n",
    "    # Print out the time step results\n",
    "    print(f\"Valid time steps analyzed: {total_time_steps}\")\n",
    "    print(f\"Proportion of time steps meeting condition 1 (greater than 125%): {np.round(proportion_condition1, 2)}\")\n",
    "    print(f\"Proportion of time steps meeting condition 2 (less than 75%): {np.round(proportion_condition2, 2)}\")\n",
    "    print(f\"Proportion of time steps meeting either conditions: {np.round(proportion_either_condition, 2)}\")\n",
    "    print(f\"Proportion of time steps meeting neither condition: {np.round(proportion_neither_condition, 2)}\")\n",
    "    print(f\"Proportion of time steps meeting both conditions: {np.round(proportion_both_conditions, 2)}\")\n",
    "    print(f\"Sum of proportions: {np.round(sum_of_proportions, 2)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Failed to create any valid combined data for time step analysis.\")\n",
    "\n",
    "# ==================== SUMMARY ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if total_valid_lakes > 0 and all_combined_data:\n",
    "    print(f\"Analysis completed successfully:\")\n",
    "    print(f\"  • {total_valid_lakes} lakes analyzed\")\n",
    "    print(f\"  • {len(time_step_df)} total time steps analyzed\")\n",
    "    print(f\"\\nKey findings:\")\n",
    "    print(f\"  • {lake_proportions['either_condition']:.1%} of lakes have at least one time step with dV differences ±25%\")\n",
    "    print(f\"  • {proportion_either_condition:.1%} of time steps show dV differences ±25%\")\n",
    "else:\n",
    "    print(\"Analysis could not be completed due to data issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35266a7-4cf8-48a7-acf9-de6ffb1e8586",
   "metadata": {},
   "source": [
    "### Explaining continental sum trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8225abee-a0cc-44dd-9750-f6f62583d650",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'output/geometric_calcs/evolving_outlines_geom_calc/forward_fill'\n",
    "\n",
    "def create_interactive_plot(directory, stationary_outlines_gdf):\n",
    "    # Load and process all lake data\n",
    "    dfs = []\n",
    "    lake_names = []\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        # Exclude continental summation files\n",
    "        if \"subset\" in file or \"superset\" in file:\n",
    "            continue\n",
    "\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            lake_name = os.path.splitext(file)[0]\n",
    "            \n",
    "            # Process each lake's data\n",
    "            df['lake_name'] = lake_name\n",
    "            df['datetime'] = pd.to_datetime(df['mid_pt_datetime'])\n",
    "            df['cumsum_vol'] = np.cumsum(df['evolving_outlines_dV_corr (m^3)']/1e9)\n",
    "            dfs.append(df)\n",
    "            lake_names.append(lake_name)\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Create plot using hvplot\n",
    "    plot = combined_df.hvplot.line(\n",
    "        x='datetime',\n",
    "        y='cumsum_vol',\n",
    "        by='lake_name',\n",
    "        width=800,\n",
    "        height=400,\n",
    "        title='Lake Volume Changes Over Time',\n",
    "        xlabel='Date',\n",
    "        ylabel='Cumulative Volume Change (km³)',\n",
    "        grid=True,\n",
    "        legend=False\n",
    "    )\n",
    "    \n",
    "    return plot\n",
    "\n",
    "# Usage:\n",
    "plot = create_interactive_plot(directory, stationary_outlines_gdf)\n",
    "plot  # Display in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345d7bfc-019d-4570-919e-175a1dde2e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'output/geometric_calcs/evolving_outlines_geom_calc/forward_fill'\n",
    "\n",
    "def create_interactive_plot(directory, stationary_outlines_gdf):\n",
    "    # Load and process all lake data\n",
    "    dfs_subset_CS2_IS2_lakes = []\n",
    "    lake_names = []\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            lake_name = os.path.splitext(file)[0]\n",
    "            \n",
    "            # Process each lake's data\n",
    "            df['lake_name'] = lake_name\n",
    "            df['datetime'] = pd.to_datetime(df['mid_pt_datetime'])\n",
    "            df['cumsum_vol'] = np.cumsum(np.divide(df['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "            dfs_subset_CS2_IS2_lakes.append(df)\n",
    "            lake_names.append(lake_name)\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(dfs_subset_CS2_IS2_lakes, ignore_index=True)\n",
    "    \n",
    "    # Create the plot using Dataset and Curve\n",
    "    dataset = hv.Dataset(combined_df)\n",
    "    curves = dataset.to(hv.Curve, \n",
    "                       kdims=['datetime'], \n",
    "                       vdims=['cumsum_vol', 'lake_name'],\n",
    "                       groupby='lake_name')\n",
    "    \n",
    "    # Apply options to the plot\n",
    "    plot = curves.opts(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        tools=['hover'],\n",
    "        title='Lake volume changes over time',\n",
    "        xlabel='Date',\n",
    "        ylabel='Cumulative volume change (km³)',\n",
    "        show_grid=True,\n",
    "        toolbar='above'\n",
    "    )\n",
    "    \n",
    "    return plot\n",
    "\n",
    "# Usage:\n",
    "plot = create_interactive_plot(directory, stationary_outlines_gdf)\n",
    "plot  # Display in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f265cfb6-ce0b-4107-bfb0-e772dcf1e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the lakes driving the deviation of evolving and stationary \n",
    "\n",
    "evolving_directory = 'output/geometric_calcs/evolving_outlines_geom_calc'\n",
    "stationary_directory = 'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes'\n",
    "\n",
    "def process_lake_data(directory, is_evolving=True):\n",
    "    dfs = {}\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        # Exclude continental summation files\n",
    "        if \"subset\" in file or \"superset\" in file:\n",
    "            continue\n",
    "\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            lake_name = os.path.splitext(file)[0]\n",
    "            \n",
    "            # Process each lake's data\n",
    "            df['lake_name'] = lake_name\n",
    "            df['datetime'] = pd.to_datetime(df['mid_pt_datetime'])\n",
    "            \n",
    "            # Calculate cumulative volume based on directory type\n",
    "            if is_evolving:\n",
    "                df['cumsum_vol'] = np.cumsum(df['evolving_outlines_dV_corr (m^3)'])\n",
    "            else:\n",
    "                df['cumsum_vol'] = np.cumsum(df['stationary_outline_dV_corr (m^3)'])\n",
    "            \n",
    "            dfs[lake_name] = df\n",
    "    \n",
    "    return dfs\n",
    "\n",
    "def create_interactive_plot(evolving_directory, stationary_directory):\n",
    "    # Load data from both directories\n",
    "    evolving_dfs = process_lake_data(evolving_directory, is_evolving=True)\n",
    "    stationary_dfs = process_lake_data(stationary_directory, is_evolving=False)\n",
    "    \n",
    "    # Initialize lists to store processed dataframes\n",
    "    plot_dfs = []\n",
    "    \n",
    "    # Process common lakes\n",
    "    common_lakes = set(evolving_dfs.keys()) & set(stationary_dfs.keys())\n",
    "    for lake_name in common_lakes:\n",
    "        evolving_df = evolving_dfs[lake_name].copy()\n",
    "        stationary_df = stationary_dfs[lake_name].copy()\n",
    "        \n",
    "        # Calculate difference (evolving - stationary)\n",
    "        merged_df = pd.merge(\n",
    "            evolving_df[['datetime', 'cumsum_vol']], \n",
    "            stationary_df[['datetime', 'cumsum_vol']], \n",
    "            on='datetime', \n",
    "            suffixes=('_evolving', '_stationary')\n",
    "        )\n",
    "        merged_df['cumsum_vol'] = merged_df['cumsum_vol_evolving'] - merged_df['cumsum_vol_stationary']\n",
    "        merged_df['lake_name'] = lake_name + '_difference'\n",
    "        plot_dfs.append(merged_df[['datetime', 'cumsum_vol', 'lake_name']])\n",
    "    \n",
    "    # Process lakes only in stationary directory\n",
    "    stationary_only = set(stationary_dfs.keys()) - set(evolving_dfs.keys())\n",
    "    for lake_name in stationary_only:\n",
    "        df = stationary_dfs[lake_name].copy()\n",
    "        df['lake_name'] = lake_name + '_stationary'\n",
    "        plot_dfs.append(df[['datetime', 'cumsum_vol', 'lake_name']])\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(plot_dfs, ignore_index=True)\n",
    "    \n",
    "    # Create plot using hvplot\n",
    "    plot = combined_df.hvplot.line(\n",
    "        x='datetime',\n",
    "        y='cumsum_vol',\n",
    "        by='lake_name',\n",
    "        width=800,\n",
    "        height=400,\n",
    "        title='Lake volume changes over time',\n",
    "        xlabel='Date',\n",
    "        ylabel='Cumulative volume change (km³)',\n",
    "        grid=True,\n",
    "        legend=False\n",
    "    )\n",
    "    \n",
    "    return plot\n",
    "\n",
    "# Usage:\n",
    "plot = create_interactive_plot(evolving_directory, stationary_directory)\n",
    "plot  # Display in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef1f7ba-529b-44f2-a6ea-db473f951313",
   "metadata": {},
   "source": [
    "# Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e4e7e-6175-4eda-b85b-944f4d970fde",
   "metadata": {},
   "source": [
    "## Fig. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d07985-5934-4208-a4df-1d4bc1c73b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.family': 'DejaVu Sans',\n",
    "    'font.size': 9,\n",
    "    'axes.labelsize': 10,\n",
    "    'axes.titlesize': 9,\n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 9,\n",
    "    'legend.fontsize': 9,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d325c8-bf5f-4d59-8d01-3dbb9099f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolen arg on whether to use the forward filled version of the evolving outlines\n",
    "forward_fill = True\n",
    "# forward_fill = False\n",
    "\n",
    "# Select lakes to be included in plot\n",
    "selected_lakes = stationary_outlines_gdf[stationary_outlines_gdf['name'].isin(['ConwaySubglacialLake', 'David_s1', 'Slessor_23'])]\n",
    "desired_order = ['ConwaySubglacialLake', 'David_s1', 'Slessor_23']\n",
    "stationary_outlines_gdf_filtered = gpd.GeoDataFrame(pd.concat([selected_lakes[selected_lakes['name'] == name] for name in desired_order]))\n",
    "\n",
    "# Number of rows and columns\n",
    "nrows, ncols = 4, 3\n",
    "\n",
    "# Create a 4x3 grid of plots (4 metrics, 3 lakes per metric)\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 20), constrained_layout=True)\n",
    "\n",
    "# Define the display names for lakes\n",
    "lake_names = ['Conway Subglacial Lake', 'David$_{s1}$', 'Slessor$_{23}$']\n",
    "\n",
    "# Add titles to the top row of subplots\n",
    "for col, title in enumerate(lake_names):\n",
    "    axs[0, col].set_title(title, fontsize=18, pad=12)\n",
    "\n",
    "# Define color that will be reused\n",
    "stationary_outline_color  = 'darkturquoise'\n",
    "\n",
    "for row in range(1, nrows):\n",
    "    # Share y-axis within each row but not between rows\n",
    "    for col in range(ncols):\n",
    "        axs[row, col].sharey(axs[row, 0])\n",
    "\n",
    "# Pick colormap and normalize to cyc_start_datetimes\n",
    "cmap = plt.get_cmap('plasma', len(cyc_start_datetimes[1:]))\n",
    "norm = plt.Normalize(mdates.date2num(cyc_start_datetimes[1]), \n",
    "                    mdates.date2num(cyc_start_datetimes[-1]))\n",
    "\n",
    "for idx, (lake_idx, lake) in enumerate(stationary_outlines_gdf_filtered.iterrows()):\n",
    "    # Select the row by index and convert it to a GeoDataFrame\n",
    "    lake_gdf = stationary_outlines_gdf_filtered.loc[[lake_idx]]\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    stationary_outline = lake_gdf['geometry']\n",
    "    print(f\"\\nProcessing lake: {lake_name}\")\n",
    "    \n",
    "    # Attempt to open the evolving outlines GeoJSON file\n",
    "    try:\n",
    "        evolving_outlines_gdf = gpd.read_file('output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name))\n",
    "    except fiona.errors.DriverError:\n",
    "        print(f\"File for {lake_name} not found. Skipping...\")\n",
    "        continue  # Skip the rest of the function if the file doesn't exist\n",
    "    \n",
    "    # Attempt to open the geometric calculations CSV files\n",
    "    try:\n",
    "        if forward_fill==True: \n",
    "            evolving_geom_calcs_df = pd.read_csv('output/geometric_calcs/evolving_outlines_geom_calc/forward_fill/{}.csv'.format(lake_name))\n",
    "        elif forward_fill==False:\n",
    "            print('using not forward fill')\n",
    "            evolving_geom_calcs_df = pd.read_csv('output/geometric_calcs/evolving_outlines_geom_calc/{}.csv'.format(lake_name))\n",
    "        evolving_union_geom_calcs_df = pd.read_csv('output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/{}.csv'.format(lake_name))\n",
    "        stationary_geom_calcs_df = pd.read_csv('output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes/{}.csv'.format(lake_name))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"At least of of the geometric calculations CSV files for {lake_name} not found. Skipping...\")\n",
    "        continue  # Skip the rest of the function if the file doesn't exist\n",
    "    \n",
    "    # Convert of strings to datetime\n",
    "    evolving_geom_calcs_df['mid_pt_datetime'] = pd.to_datetime(evolving_geom_calcs_df['mid_pt_datetime'])\n",
    "\n",
    "    \n",
    "    # Panel - evolving outlines ------------------------------------------------------\n",
    "    \n",
    "    # Find evolving and stationary outlines union for plotting extent\n",
    "    evolving_stationary_union_gdf = gpd.GeoDataFrame(\n",
    "        geometry=[lake_gdf.geometry.iloc[0].union(evolving_outlines_gdf.geometry.union_all())],\n",
    "        crs=lake_gdf.crs)\n",
    "    x_min, y_min, x_max, y_max = evolving_stationary_union_gdf.bounds.iloc[0]\n",
    "\n",
    "    # Make plots a uniform size\n",
    "    # Make x_min, y_min, x_max, and y_max define a square area centered at the original midpoints\n",
    "    # Calculate the midpoints of the current bounds\n",
    "    x_mid = (x_min + x_max) / 2\n",
    "    y_mid = (y_min + y_max) / 2\n",
    "    \n",
    "    # Calculate the current spans of the x and y dimensions\n",
    "    x_span = x_max - x_min\n",
    "    y_span = y_max - y_min\n",
    "    \n",
    "    # Determine the maximum span to ensure square dimensions\n",
    "    max_span = max(x_span, y_span)\n",
    "    \n",
    "    # Update the min and max values to match the new span, keeping the midpoint the same\n",
    "    x_min = x_mid - max_span / 2\n",
    "    x_max = x_mid + max_span / 2\n",
    "    y_min = y_mid - max_span / 2\n",
    "    y_max = y_mid + max_span / 2\n",
    "    \n",
    "    buffer_frac = 0.35\n",
    "    x_buffer = abs(x_max-x_min)*buffer_frac\n",
    "    y_buffer = abs(y_max-y_min)*buffer_frac\n",
    "\n",
    "    # Plot MOA surface imagery\n",
    "    mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "    mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "    moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "    axs[0,idx].imshow(moa_highres_da_subset[0,:,:], cmap='gray', clim=[14000, 17000], extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "    \n",
    "    # Use for loop to store each time slice as line segment to use in legend\n",
    "    # And plot each evolving outline in the geodataframe color by date\n",
    "    lines = []  # list of lines to be used for the legend\n",
    "    for dt_idx, dt in enumerate(cyc_start_datetimes[1:]):\n",
    "        x = 1; y = 1\n",
    "        line, = axs[0,idx].plot(x, y, color=cmap(norm(mdates.date2num(cyc_start_datetimes[dt_idx]))))\n",
    "        lines.append(line)\n",
    "        \n",
    "        # Filter rows that match the current time step\n",
    "        evolving_outlines_gdf_dt_sub = evolving_outlines_gdf[evolving_outlines_gdf['mid_pt_datetime'] == dt]\n",
    "    \n",
    "        # Plotting the subset if not empty\n",
    "        if not evolving_outlines_gdf_dt_sub.empty:\n",
    "            evolving_outlines_gdf_dt_sub.boundary.plot(ax=axs[0,idx], \n",
    "                color=cmap(norm(mdates.date2num(cyc_start_datetimes[dt_idx]))), linewidth=1)\n",
    "    \n",
    "    # Plot stationary outline\n",
    "    stationary_outlines_gdf['geometry'].boundary.plot(ax=axs[0,idx], color=stationary_outline_color, linewidth=2)\n",
    "\n",
    "    # Import evolving_outlines_union_gdf and plot\n",
    "    evolving_union_gdf = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]\n",
    "    evolving_union_gdf.boundary.plot(ax=axs[0,idx], color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "    # Plot inset map\n",
    "    axIns = axs[0,idx].inset_axes([0.01, -0.01, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "    axIns.set_aspect('equal')\n",
    "    moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    axIns.axis('off')\n",
    "    # Plot star to indicate location\n",
    "    axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "        linewidth=2, color='k', s=30, zorder=3)\n",
    "\n",
    "    # Change polar stereographic m to km\n",
    "    km_scale = 1e3\n",
    "    ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    axs[0,idx].xaxis.set_major_formatter(ticks_x)\n",
    "    ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    axs[0,idx].yaxis.set_major_formatter(ticks_y)\n",
    "    \n",
    "    # Set axes limits\n",
    "    axs[0,idx].set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "\n",
    "    # Panel - Active area ---------------------------------------------\n",
    "    \n",
    "    # Plot horizontal zero line for reference\n",
    "    axs[1,idx].axhline(0, color='k', linewidth=1)\n",
    "    \n",
    "    # Plot stationary outline and evolving outlines unary union areas\n",
    "    axs[1,idx].axhline(np.divide(lake_gdf['area (m^2)'], 1e6).values, \n",
    "        color=stationary_outline_color, linestyle='solid', linewidth=2)\n",
    "    axs[1,idx].axhline(np.divide(evolving_union_gdf['area (m^2)'].iloc[0], 1e6), \n",
    "        color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "    # Plot evolving outlines\n",
    "    x = mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime'])\n",
    "    y = np.divide(evolving_geom_calcs_df['evolving_outlines_area (m^2)'], 1e6)\n",
    "    \n",
    "    # Create points and segments for LineCollection\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    \n",
    "    # Create a LineCollection, using the discrete colormap and norm\n",
    "    lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "    lc.set_array(x)\n",
    "    lc.set_linewidth(2)\n",
    "\n",
    "    # Plot multi-colored line and scatter for data points\n",
    "    line = axs[1,idx].add_collection(lc)\n",
    "    scatter = axs[1,idx].scatter(x, y, c=x, cmap=cmap, s=9, norm=norm, zorder=2)\n",
    "    \n",
    "    # Get the maximum y value across all data for this lake\n",
    "    if idx == 0:\n",
    "        max_y = max(\n",
    "            np.divide(lake_gdf['area (m^2)'], 1e6).values[0],\n",
    "            np.divide(evolving_union_gdf['area (m^2)'], 1e6).values[0],\n",
    "            np.divide(evolving_geom_calcs_df['evolving_outlines_area (m^2)'], 1e6).max()\n",
    "        )\n",
    "    \n",
    "    # Set y limit with padding above the maximum value to avoid data plotting behind legend\n",
    "    axs[1,idx].set_ylim(bottom=None, top=max_y * 1.3)\n",
    "    \n",
    "    # Panel - Cumulative dh/dt -------------------------------------------------------\n",
    "    \n",
    "    # Plot horizontal zero line for reference\n",
    "    axs[2,idx].axhline(0, color='k', linewidth=1)\n",
    "\n",
    "    # Plot stationary outlines off-lake secular dh\n",
    "    axs[2,idx].plot(mdates.date2num(stationary_geom_calcs_df['mid_pt_datetime']),\n",
    "        np.cumsum(stationary_geom_calcs_df['stationary_outline_region_dh (m)']),\n",
    "        color='lightgray', linestyle='solid', linewidth=2)\n",
    "    axs[2,idx].scatter(mdates.date2num(stationary_geom_calcs_df['mid_pt_datetime']),\n",
    "        np.cumsum(stationary_geom_calcs_df['stationary_outline_region_dh (m)']),\n",
    "        color='lightgray', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "    # Plot evolving outlines off-lake secular dh\n",
    "    axs[2,idx].plot(mdates.date2num(evolving_union_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.cumsum(evolving_union_geom_calcs_df['stationary_outline_region_dh (m)']), color='dimgray', \n",
    "            linestyle='solid', linewidth=2)\n",
    "    axs[2,idx].scatter(mdates.date2num(evolving_union_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.cumsum(evolving_union_geom_calcs_df['stationary_outline_region_dh (m)']), color='dimgray', \n",
    "            linestyle='solid', linewidth=2, s=5)\n",
    "    \n",
    "    # Plot stationary outline time series\n",
    "    axs[2,idx].plot(mdates.date2num(stationary_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.cumsum(stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "            color=stationary_outline_color, linestyle='solid', linewidth=2)\n",
    "    axs[2,idx].scatter(mdates.date2num(stationary_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.cumsum(stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "            color=stationary_outline_color, linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "    # Plot evolving outlines time series\n",
    "    x = mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime'])\n",
    "    y = np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'])\n",
    "    \n",
    "    # Create points and segments for LineCollection\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    \n",
    "    # Create a LineCollection, using the discrete colormap and norm\n",
    "    lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "    lc.set_array(x)\n",
    "    lc.set_linewidth(2)\n",
    "\n",
    "    # Plot multi-colored line and scatter for data points\n",
    "    line = axs[2,idx].add_collection(lc)\n",
    "    scatter = axs[2,idx].scatter(x, y, c=x, cmap=cmap, s=9, norm=norm, zorder=2)\n",
    "\n",
    "    # Plot evolving outlines union outline time series\n",
    "    axs[2,idx].plot(mdates.date2num(evolving_union_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.cumsum(evolving_union_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "            color='k', linestyle='dotted', linewidth=2)\n",
    "    axs[2,idx].scatter(mdates.date2num(evolving_union_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.cumsum(evolving_union_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "            color='k', linestyle='dotted', linewidth=2, s=5)\n",
    "    \n",
    "    # Plot bias (evolving - prior stationary)\n",
    "    axs[2,idx].plot(mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime']),\n",
    "        np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'] - \n",
    "        stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "        color='red', linestyle='solid', linewidth=2)\n",
    "    axs[2,idx].scatter(mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime']),\n",
    "        np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'] - \n",
    "        stationary_geom_calcs_df['stationary_outline_dh_corr (m)']),\n",
    "        color='red', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "    # Plot bias (evolving - updated stationary)\n",
    "    axs[2,idx].plot(mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime']),\n",
    "        np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'] - \n",
    "        evolving_union_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "        color='darkred', linestyle='solid', linewidth=2)\n",
    "    axs[2,idx].scatter(mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime']),\n",
    "        np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'] - \n",
    "        evolving_union_geom_calcs_df['stationary_outline_dh_corr (m)']),\n",
    "        color='darkred', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "    axs[2,idx].set_ylim(bottom=-17.5, top=17.5)\n",
    "\n",
    "    # Panel - Cumulative dV/dt --------------------------------------------------\n",
    "    \n",
    "    # Plot horizontal line at zero for reference\n",
    "    axs[3,idx].axhline(0, color='k', linewidth=1)\n",
    "    \n",
    "    # Plot stationary outline time series\n",
    "    axs[3,idx].plot(mdates.date2num(stationary_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.divide(np.cumsum(stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "        color=stationary_outline_color, linestyle='solid', linewidth=2)\n",
    "    axs[3,idx].scatter(mdates.date2num(stationary_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.divide(np.cumsum(stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "        color=stationary_outline_color, linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "    # Plot evolving outlines time series\n",
    "    x = mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime'])\n",
    "    y = np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "    \n",
    "    # Create points and segments for LineCollection\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    \n",
    "    # Create a LineCollection, using the discrete colormap and norm\n",
    "    lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "    lc.set_array(x)\n",
    "    lc.set_linewidth(2)\n",
    "\n",
    "    # Plot multi-colored line and scatter for data points\n",
    "    line = axs[3,idx].add_collection(lc)\n",
    "    scatter = axs[3,idx].scatter(x, y, c=x, cmap=cmap, s=9, norm=norm, zorder=2)\n",
    "\n",
    "    # Plot evolving outlines union outline time series\n",
    "    axs[3,idx].plot(mdates.date2num(evolving_union_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.cumsum(np.divide(evolving_union_geom_calcs_df['stationary_outline_dV_corr (m^3)'], 1e9)), \n",
    "            color='k', linestyle='dotted', linewidth=2)\n",
    "    axs[3,idx].scatter(mdates.date2num(evolving_union_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.cumsum(np.divide(evolving_union_geom_calcs_df['stationary_outline_dV_corr (m^3)'], 1e9)), \n",
    "            color='k', linestyle='dotted', linewidth=2, s=5)\n",
    "    \n",
    "    # Plot bias (evolving - prior stationary)\n",
    "    axs[3,idx].plot(mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime']),\n",
    "        np.cumsum(np.divide((evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "            stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9)), \n",
    "            color='red', linestyle='solid', linewidth=2)\n",
    "    axs[3,idx].scatter(mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime']),\n",
    "        np.cumsum(np.divide((evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "            stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9)), \n",
    "            color='red', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "    # Plot bias (evolving - updated stationary)\n",
    "    axs[3,idx].plot(mdates.date2num(evolving_union_geom_calcs_df['mid_pt_datetime']),\n",
    "        np.cumsum(np.divide((evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "            evolving_union_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9)), \n",
    "            color='darkred', linestyle='solid', linewidth=2)\n",
    "    axs[3,idx].scatter(mdates.date2num(evolving_union_geom_calcs_df['mid_pt_datetime']),\n",
    "        np.cumsum(np.divide((evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "            evolving_union_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9)), \n",
    "            color='darkred', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "    axs[3,idx].set_ylim(bottom=-3.5, top=3.5)\n",
    "\n",
    "# Add colorbar, legends, and titles\n",
    "idx=0  # Add colorbar and legends only to first row of plots\n",
    "\n",
    "# Set up colormap\n",
    "min_date = pd.to_datetime(cyc_start_datetimes[1])\n",
    "max_date = pd.to_datetime(cyc_start_datetimes[-1])\n",
    "date_range = pd.date_range(min_date, max_date, periods=len(cyc_start_datetimes[1:]))\n",
    "years = date_range.year.unique()\n",
    "years = pd.to_datetime(years, format='%Y')\n",
    "n_dates = len(cyc_start_datetimes[:-1])\n",
    "cmap = plt.get_cmap('plasma', n_dates)\n",
    "norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "m.set_array(np.linspace(mdates.date2num(min_date), mdates.date2num(max_date), n_dates))\n",
    "\n",
    "# Add colorbar\n",
    "cax = inset_axes(axs[0,idx],\n",
    "                 width='67%',\n",
    "                 height='3%',\n",
    "                 loc='lower left',\n",
    "                 bbox_to_anchor=[0.31, 0.12, 1, 1],  # [left, bottom, width, height]\n",
    "                 bbox_transform=axs[0,idx].transAxes,\n",
    "                 borderpad=0)\n",
    "cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "cbar.set_label('year', size=10, labelpad=5)\n",
    "\n",
    "# Set ticks for all years but labels only for odd years\n",
    "tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "tick_labels = [date.strftime('%Y') if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "cbar.set_ticks(tick_locations)\n",
    "cbar.set_ticklabels(tick_labels)\n",
    "\n",
    "# Add minor ticks for quarters\n",
    "cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Quarter intervals only\n",
    "\n",
    "# Add legends\n",
    "stationary_line = plt.Line2D([], [], color=stationary_outline_color, linestyle='solid', linewidth=2)\n",
    "evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "legend = axs[0,idx].legend(\n",
    "    [stationary_line,\n",
    "     tuple(lines), \n",
    "     evolving_union_line], \n",
    "    ['stationary outline',\n",
    "     'evolving outlines',\n",
    "     'updated stationary outline'],\n",
    "     handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    fontsize=12, loc='upper center')\n",
    "\n",
    "legend = axs[1,idx].legend(\n",
    "    [stationary_line,\n",
    "     tuple(lines), \n",
    "     evolving_union_line],\n",
    "    ['stationary outline',\n",
    "     'evolving outlines', \n",
    "     'updated stationary outline'], \n",
    "    handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    fontsize=12, loc='upper center')\n",
    "\n",
    "evolving_region = plt.Line2D([], [], color='dimgray', linestyle='solid', linewidth=2)\n",
    "stationary_region = plt.Line2D([], [], color='lightgray', linestyle='solid', linewidth=2)\n",
    "bias = plt.Line2D([], [], color='red', linestyle='solid', linewidth=2)\n",
    "bias2 = plt.Line2D([], [], color='darkred', linestyle='solid', linewidth=2)\n",
    "\n",
    "legend = axs[2,idx].legend(\n",
    "    [stationary_region,\n",
    "     evolving_region,\n",
    "     stationary_line,  \n",
    "     tuple(lines),\n",
    "     evolving_union_line, \n",
    "     bias, bias2],\n",
    "    ['stationary outline off-lake secular',\n",
    "     'updated stationary outline off-lake secular',\n",
    "     'stationary outline', \n",
    "     'evolving outlines',\n",
    "     'updated stationary outline', \n",
    "     'bias (evolving − prior stationary)',\n",
    "     'bias (evolving − updated stationary)'],\n",
    "     handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    fontsize=11, loc='lower center')\n",
    "\n",
    "legend = axs[3,idx].legend(\n",
    "    [stationary_line,\n",
    "     tuple(lines),\n",
    "     evolving_union_line,\n",
    "     bias, bias2],\n",
    "    ['stationary outline',\n",
    "     'evolving outlines',\n",
    "     'updated stationary outline', \n",
    "     'bias (evolving − prior stationary)',\n",
    "     'bias (evolving − updated stationary)'], \n",
    "    handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    fontsize=12, loc='lower center')\n",
    "\n",
    "# Set font sizes for all plots\n",
    "TICK_SIZE = 14\n",
    "LABEL_SIZE = 18\n",
    "\n",
    "# Set common font sizes and axis labels\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        # Set tick sizes for all plots\n",
    "        axs[i,j].tick_params(axis='both', labelsize=TICK_SIZE)\n",
    "\n",
    "        # Add subplot labels (a, b, c, etc.)\n",
    "        axs[i,j].text(0.02, 0.98, chr(97 + i*ncols + j), transform=axs[i,j].transAxes, \n",
    "                      fontsize=20, va='top', ha='left')\n",
    "\n",
    "        # Configure row-specific settings\n",
    "        if i == 0:\n",
    "            axs[i,j].set_xlabel('x [km]', size=LABEL_SIZE)\n",
    "        if i == 3:\n",
    "            axs[i,j].set_xlabel('year', size=LABEL_SIZE)\n",
    "        if 0 < i < 4:\n",
    "            axs[i,j].xaxis.set_major_formatter(year_interval_formatter())\n",
    "            axs[i,j].xaxis.set_major_locator(mdates.YearLocator())\n",
    "            axs[i,j].xaxis.set_minor_locator(mdates.MonthLocator([1, 4, 7, 10]))\n",
    "        if j == 0:  # Leftmost column labels\n",
    "            y_labels = ['y [km]', 'active area [km$^2$]', 'cumulative $dh$ [m]', 'cumulative $dV$ [km$^3$]']\n",
    "            axs[i,j].set_ylabel(y_labels[i], size=LABEL_SIZE)\n",
    "        # Do not display redundant tick labels\n",
    "        if 0 < i < 3:\n",
    "            axs[i,j].set_xticklabels([])\n",
    "        if i > 0:\n",
    "            # Set x-axis limits\n",
    "            axs[i,j].set(xlim=(cyc_dates['cyc_start_datetimes'].iloc[0],\n",
    "               # Set righthand x-axis limit slightly earlier to prevent tick mark displaying when there is no data point\n",
    "               (cyc_dates['cyc_end_datetimes'].iloc[-1] - datetime.timedelta(days=2))))\n",
    "            if j > 0:\n",
    "                axs[i,j].tick_params(axis='y', which='both', labelleft=False)\n",
    "            else:\n",
    "                axs[i,j].tick_params(axis='y', which='both', labelleft=True)\n",
    "\n",
    "# Clear output\n",
    "clear_output()\n",
    "\n",
    "# Save and close plot\n",
    "plt.savefig(OUTPUT_DIR + '/figures/Fig2_lake_reexamination_results.jpg', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Preview plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868953d7-e5d4-4db5-99af-80f72fce3e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1878d127-ff77-4333-a01c-e13a9da6e8c4",
   "metadata": {},
   "source": [
    "## Fig. 3\n",
    "NOTE: You must run \"Dissolved inorganic carbon (DIC) export estimates\" section of code earlier in notebook for necessary plotting variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da48d013-6a8f-4159-b722-456e5951f60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filtered geodataframes of lakes based on whether they have evolving outlines\n",
    "folder_path = os.path.join ('output/lake_outlines/evolving_outlines')\n",
    "\n",
    "# Lakes with non-dynamic outlines (.txt)\n",
    "no_evolving_outlines_lakes = filter_gdf_by_folder_contents(stationary_lakes_gdf, folder_path, file_extension='txt', exclude=False)\n",
    "print('non-dynamic:',len(no_evolving_outlines_lakes))\n",
    "\n",
    "# Lakes with evolving outlines (.geojson)\n",
    "evolving_outlines_lakes = filter_gdf_by_folder_contents(stationary_lakes_gdf, folder_path, file_extension='geojson', exclude=False)\n",
    "\n",
    "# For the evolving_outlines_lakes, we must add the special case of Site_B_Site_C that are now a combined lake\n",
    "include_list = ['Site_B', 'Site_C']\n",
    "included_rows = stationary_lakes_gdf[stationary_lakes_gdf['name'].isin(include_list)]\n",
    "evolving_outlines_lakes = pd.concat([evolving_outlines_lakes, included_rows]).drop_duplicates()\n",
    "print('dynamic:',len(evolving_outlines_lakes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09281cc-b147-45f7-859e-76ecaef9f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3x1 grid of plots\n",
    "nrows, ncols = 3, 1\n",
    "\n",
    "# Setup figure\n",
    "fig, ax = plt.subplots(nrows, ncols, gridspec_kw={'height_ratios': [2.5, 2.5, 5]}, sharex=True, figsize=(10, 12), constrained_layout=True)\n",
    "\n",
    "# Define colors and linestyles that will be reused and create lines for legend\n",
    "stationary_outline_color  = 'darkturquoise'\n",
    "stationary_line = plt.Line2D([], [], color=stationary_outline_color, linestyle='dashed', linewidth=2)\n",
    "stationary_all_lakes_line = plt.Line2D([], [], color=stationary_outline_color, linestyle='solid', linewidth=2)\n",
    "evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=2)\n",
    "evolving_stationary_union_all_lakes_line = plt.Line2D([], [], color='teal', linestyle='solid', linewidth=2)\n",
    "evolving_stationary_union_evolving_lakes_line = plt.Line2D([], [], color='teal', linestyle='dashed', linewidth=2)\n",
    "bias = plt.Line2D([], [], color='red', linestyle='solid', linewidth=2)\n",
    "bias2 = plt.Line2D([], [], color='darkred', linestyle='solid', linewidth=2)\n",
    "\n",
    "# Pick colormap and make continuous cmap discrete for evolving outlines\n",
    "cmap = plt.get_cmap('plasma', len(cyc_start_datetimes[1:]))\n",
    "norm = plt.Normalize(mdates.date2num(cyc_start_datetimes[1]), \n",
    "                    mdates.date2num(cyc_start_datetimes[-1]))\n",
    "\n",
    "# Use for loop to store each time step as line segment to use in legend\n",
    "lines = []  # list of lines to be used for the legend\n",
    "for dt_idx, dt in enumerate(cyc_dates['cyc_start_datetimes'][1:]):\n",
    "    x = 1; y = 1\n",
    "    line, = ax[0].plot(x, y, color=cmap(norm(mdates.date2num(cyc_dates['cyc_start_datetimes'][dt_idx]))))\n",
    "    lines.append(line)\n",
    "\n",
    "# Read in continental summation geometric calculation csv files - evolving outlines (only lakes with evolving outlines)\n",
    "base_path = 'output/geometric_calcs/evolving_outlines_geom_calc/forward_fill'\n",
    "filenames = {\n",
    "    'superset_IS2_lakes_sum': 'superset_IS2_lakes_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPreExpansion_sum': 'subset_CS2_IS2_lakes_SARInPreExpansion_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPostExpansion_sum': 'subset_CS2_IS2_lakes_SARInPostExpansion_sum.csv',\n",
    "}\n",
    "\n",
    "dfs = {name: pd.read_csv(f'{base_path}/{filename}') \n",
    "       for name, filename in filenames.items()}\n",
    "\n",
    "# Store dataframes from dfs list for code readability\n",
    "superset_IS2_evolving_sum_df = dfs['superset_IS2_lakes_sum']\n",
    "subset_CS2_IS2_SARInPreExpansion_evolving_sum_df = dfs['subset_CS2_IS2_lakes_SARInPreExpansion_sum']\n",
    "subset_CS2_IS2_SARInPostExpansion_evolving_sum_df = dfs['subset_CS2_IS2_lakes_SARInPostExpansion_sum']\n",
    "\n",
    "# Read in continental summation geometric calculation csv files - stationary outlines (all lakes)\n",
    "base_path = 'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes'\n",
    "filenames = {'superset_IS2_lakes_sum': 'superset_IS2_lakes_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPreExpansion_sum': 'subset_CS2_IS2_lakes_SARInPreExpansion_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPostExpansion_sum': 'subset_CS2_IS2_lakes_SARInPostExpansion_sum.csv'}\n",
    "\n",
    "dfs = {name: pd.read_csv(f'{base_path}/{filename}') \n",
    "       for name, filename in filenames.items()}\n",
    "\n",
    "superset_IS2_stationary_sum_df = dfs['superset_IS2_lakes_sum']\n",
    "subset_CS2_IS2_SARInPreExpansion_stationary_sum_df = dfs['subset_CS2_IS2_lakes_SARInPreExpansion_sum']\n",
    "subset_CS2_IS2_SARInPostExpansion_stationary_sum_df = dfs['subset_CS2_IS2_lakes_SARInPostExpansion_sum']\n",
    "\n",
    "# Read in continental summation geometric calculation csv files - evolving union (only lakes with evolving outlines)\n",
    "base_path = 'output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes'\n",
    "filenames = {'superset_IS2_lakes_sum': 'superset_IS2_lakes_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPreExpansion_sum': 'subset_CS2_IS2_lakes_SARInPreExpansion_sum.csv',\n",
    "    'subset_CS2_IS2_lakes_SARInPostExpansion_sum': 'subset_CS2_IS2_lakes_SARInPostExpansion_sum.csv'}\n",
    "\n",
    "dfs = {name: pd.read_csv(f'{base_path}/{filename}')\n",
    "       for name, filename in filenames.items()}\n",
    "\n",
    "superset_IS2_evolving_union_sum_df = dfs['superset_IS2_lakes_sum']\n",
    "subset_CS2_IS2_SARInPreExpansion_evolving_union_sum_df = dfs['subset_CS2_IS2_lakes_SARInPreExpansion_sum']\n",
    "subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df = dfs['subset_CS2_IS2_lakes_SARInPostExpansion_sum']\n",
    "\n",
    "\n",
    "# Panel - Lake active area ---------------------------------------------\n",
    "# Plot horizontal line at zero for reference\n",
    "ax[0].axhline(0, color='k', linewidth=1)\n",
    "\n",
    "# Plot evolving outlines time series as multi-colored line using LineCollection from points/segments \n",
    "x = mdates.date2num(subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['mid_pt_datetime'])\n",
    "y = subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['evolving_outlines_area (m^2)'] / 1e6\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(1)\n",
    "line = ax[0].add_collection(lc)\n",
    "scatter = ax[0].scatter(x, y, c=x, cmap=cmap, norm=norm, s=5)\n",
    "\n",
    "# Plot evolving outlines time series (ICESat-2 era)\n",
    "x = mdates.date2num(superset_IS2_evolving_sum_df['mid_pt_datetime'])\n",
    "y = superset_IS2_evolving_sum_df['evolving_outlines_area (m^2)'] / 1e6\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(2)\n",
    "line = ax[0].add_collection(lc)\n",
    "scatter = ax[0].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "# Store dates and time period for satellite coverage eras\n",
    "time_span = mdates.date2num(cyc_dates['cyc_end_datetimes'].iloc[-1]) - mdates.date2num(cyc_dates['cyc_start_datetimes'].iloc[0])\n",
    "start_date = mdates.date2num(cyc_dates['cyc_start_datetimes'].iloc[0])\n",
    "SARIn_expand_date = mdates.date2num(cyc_dates[cyc_dates['cyc_start_datetimes'] == '2014-10-01T18:00:00.000000000']['cyc_start_datetimes'].iloc[0])\n",
    "CS2_IS2_tie_pt = mdates.date2num(cyc_dates[cyc_dates['dataset'] == 'IceSat2_ATL15'].iloc[0]['cyc_start_datetimes'])\n",
    "\n",
    "# Plot prior stationary outlines\n",
    "ax[0].axhline(subset_CS2_IS2_SARInPreExpansion_stationary_sum_df['stationary_outline_area (m^2)'].iloc[0] / 1e6, \n",
    "              color=stationary_outline_color, linestyle='solid', linewidth=1,\n",
    "              xmin=0, xmax=(SARIn_expand_date-start_date)/time_span)\n",
    "ax[0].axhline(subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['stationary_outline_area (m^2)'].iloc[-1] / 1e6, \n",
    "              color=stationary_outline_color, linestyle='solid', linewidth=1,\n",
    "              xmin=(SARIn_expand_date-start_date)/time_span, xmax=1)\n",
    "ax[0].axhline(superset_IS2_stationary_sum_df['stationary_outline_area (m^2)'].iloc[0] / 1e6, \n",
    "              color=stationary_outline_color, linestyle='solid', linewidth=2, \n",
    "              xmin=(CS2_IS2_tie_pt-start_date)/time_span, xmax=1)\n",
    "\n",
    "# Plot updated stationary outlines\n",
    "ax[0].axhline(subset_CS2_IS2_SARInPreExpansion_evolving_union_sum_df['stationary_outline_area (m^2)'].iloc[0] / 1e6, \n",
    "              color='k', linestyle='dotted', linewidth=1,\n",
    "              xmin=0, xmax=(SARIn_expand_date-start_date)/time_span)\n",
    "ax[0].axhline(subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['stationary_outline_area (m^2)'].iloc[-1] / 1e6, \n",
    "              color='k', linestyle='dotted', linewidth=1,\n",
    "              xmin=(SARIn_expand_date-start_date)/time_span, xmax=1)\n",
    "ax[0].axhline(superset_IS2_evolving_union_sum_df['stationary_outline_area (m^2)'].iloc[0] / 1e6, \n",
    "              color='k', linestyle='dotted', linewidth=2, \n",
    "              xmin=(CS2_IS2_tie_pt-start_date)/time_span, xmax=1)\n",
    "\n",
    "\n",
    "# Carbon export\n",
    "\n",
    "# Define conversion factor for grams\n",
    "# g_conv = 1e6  # Megagrams\n",
    "g_conv = 1e9  # Gigagrams\n",
    "\n",
    "# Plot horizontal line at zero for reference\n",
    "ax[1].axhline(0, color='k', linewidth=1)\n",
    "\n",
    "# Plot evolving outlines time series as multi-colored line using LineCollection from points/segments \n",
    "x = mdates.date2num(subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['mid_pt_datetime'])\n",
    "y = subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['evolving_outlines_area (m^2)'] * SLM_evolving_per_area_per_step_DIC_export_filling_period / g_conv\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(1)\n",
    "line = ax[1].add_collection(lc)\n",
    "scatter = ax[1].scatter(x, y, c=x, cmap=cmap, norm=norm, s=5)\n",
    "\n",
    "# Plot evolving outlines time series (ICESat-2 era)\n",
    "x = mdates.date2num(superset_IS2_evolving_sum_df['mid_pt_datetime'])\n",
    "y = superset_IS2_evolving_sum_df['evolving_outlines_area (m^2)'] * SLM_evolving_per_area_per_step_DIC_export_filling_period / g_conv \n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(2)\n",
    "line = ax[1].add_collection(lc)\n",
    "scatter = ax[1].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "# Plot stationary outlines\n",
    "ax[1].axhline(subset_CS2_IS2_SARInPreExpansion_stationary_sum_df['stationary_outline_area (m^2)'].iloc[0] * SLM_stationary_per_area_per_step_DIC_export / g_conv,\n",
    "              color=stationary_outline_color, linestyle='solid', linewidth=1,\n",
    "              xmin=0, xmax=(SARIn_expand_date-start_date)/time_span)\n",
    "ax[1].axhline(subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['stationary_outline_area (m^2)'].iloc[-1] * SLM_stationary_per_area_per_step_DIC_export / g_conv, \n",
    "              color=stationary_outline_color, linestyle='solid', linewidth=1,\n",
    "              xmin=(SARIn_expand_date-start_date)/time_span, xmax=1)\n",
    "ax[1].axhline(superset_IS2_stationary_sum_df['stationary_outline_area (m^2)'].iloc[0] * SLM_stationary_per_area_per_step_DIC_export / g_conv, \n",
    "              color=stationary_outline_color, linestyle='solid', linewidth=2, \n",
    "              xmin=(CS2_IS2_tie_pt-start_date)/time_span, xmax=1)\n",
    "\n",
    "# Plot evolving outlines union\n",
    "ax[1].axhline(subset_CS2_IS2_SARInPreExpansion_evolving_union_sum_df['stationary_outline_area (m^2)'].iloc[0] * SLM_stationary_per_area_per_step_DIC_export / g_conv,\n",
    "              color='k', linestyle='dotted', linewidth=1,\n",
    "              xmin=0, xmax=(SARIn_expand_date-start_date)/time_span)\n",
    "ax[1].axhline(subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['stationary_outline_area (m^2)'].iloc[-1] * SLM_stationary_per_area_per_step_DIC_export / g_conv,\n",
    "              color='k', linestyle='dotted', linewidth=1,\n",
    "              xmin=(SARIn_expand_date-start_date)/time_span, xmax=1)\n",
    "ax[1].axhline(superset_IS2_evolving_union_sum_df['stationary_outline_area (m^2)'].iloc[0] * SLM_stationary_per_area_per_step_DIC_export / g_conv,\n",
    "              color='k', linestyle='dotted', linewidth=2, \n",
    "              xmin=(CS2_IS2_tie_pt-start_date)/time_span, xmax=1)\n",
    "\n",
    "\n",
    "# Panel - cumulative dV/dt --------------------------------------------------\n",
    "# Plot horizontal line at zero for reference\n",
    "ax[2].axhline(0, color='k', linewidth=1)\n",
    "\n",
    "# Plot dV time series of evolving outlines using LineCollection from points/segments to plot multi-colored line\n",
    "x = mdates.date2num(subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['mid_pt_datetime'])\n",
    "y = np.cumsum(subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['evolving_outlines_dV_corr (m^3)'] / 1e9)\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(1)\n",
    "line = ax[2].add_collection(lc)\n",
    "scatter = ax[2].scatter(x, y, c=x, cmap=cmap, norm=norm, s=5)\n",
    "\n",
    "CS2_last_cyc_date = str(cyc_dates[cyc_dates['dataset'] == 'CryoSat2_SARIn']['cyc_start_datetimes'].iloc[-1])\n",
    "cum_sum_last_CS2_midcyc_date = np.cumsum(np.divide(\n",
    "    subset_CS2_IS2_SARInPostExpansion_evolving_sum_df[\n",
    "        subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date]\n",
    "        ['evolving_outlines_dV_corr (m^3)'], 1e9)).iloc[-1]\n",
    "\n",
    "x = mdates.date2num(superset_IS2_evolving_sum_df['mid_pt_datetime'])\n",
    "y = np.cumsum(superset_IS2_evolving_sum_df['evolving_outlines_dV_corr (m^3)'] / 1e9) + cum_sum_last_CS2_midcyc_date\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(2)\n",
    "line = ax[2].add_collection(lc)\n",
    "scatter = ax[2].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "# Plot dV time series of stationary outline of all lakes\n",
    "ax[2].plot(mdates.date2num(subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum(subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['stationary_outline_dV_corr (m^3)'] / 1e9), \n",
    "    color=stationary_outline_color, linestyle='solid', linewidth=1)\n",
    "ax[2].scatter(mdates.date2num(subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum(subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['stationary_outline_dV_corr (m^3)'] / 1e9), \n",
    "           color=stationary_outline_color, linestyle='solid', linewidth=1, s=3)\n",
    "\n",
    "cum_sum_last_CS2_midcyc_date = np.cumsum(\n",
    "    subset_CS2_IS2_SARInPostExpansion_stationary_sum_df[subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date]\n",
    "    ['stationary_outline_dV_corr (m^3)'] / 1e9).iloc[-1]\n",
    "\n",
    "ax[2].plot(mdates.date2num(superset_IS2_stationary_sum_df['mid_pt_datetime']), \n",
    "np.cumsum(superset_IS2_stationary_sum_df['stationary_outline_dV_corr (m^3)'] / 1e9) + cum_sum_last_CS2_midcyc_date, \n",
    "           color=stationary_outline_color, linestyle='solid', linewidth=2)\n",
    "ax[2].scatter(mdates.date2num(superset_IS2_stationary_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum(superset_IS2_stationary_sum_df['stationary_outline_dV_corr (m^3)'] / 1e9) + cum_sum_last_CS2_midcyc_date, \n",
    "           color=stationary_outline_color, linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# Plot bias (evolving - prior stationary)\n",
    "ax[2].plot(mdates.date2num(subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum(subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['evolving_outlines_dV_corr (m^3)'] - \n",
    "        subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['stationary_outline_dV_corr (m^3)']) / 1e9, \n",
    "        color='red', linestyle='solid', linewidth=1)\n",
    "ax[2].scatter(mdates.date2num(subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum(subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "        subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['stationary_outline_dV_corr (m^3)']) / 1e9, \n",
    "        color='red', linestyle='solid', linewidth=1, s=3)\n",
    "\n",
    "cum_sum_last_CS2_midcyc_date = np.cumsum((\n",
    "    subset_CS2_IS2_SARInPostExpansion_evolving_sum_df[\n",
    "        subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date]\n",
    "        ['evolving_outlines_dV_corr (m^3)'] - \n",
    "        subset_CS2_IS2_SARInPostExpansion_stationary_sum_df[\n",
    "        subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date]\n",
    "        ['stationary_outline_dV_corr (m^3)']) / 1e9).iloc[-1]\n",
    "\n",
    "ax[2].plot(mdates.date2num(superset_IS2_stationary_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum((superset_IS2_evolving_sum_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "        superset_IS2_stationary_sum_df['stationary_outline_dV_corr (m^3)']) / 1e9) + cum_sum_last_CS2_midcyc_date, \n",
    "        color='red', linestyle='solid', linewidth=2)\n",
    "ax[2].scatter(mdates.date2num(superset_IS2_stationary_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum((superset_IS2_evolving_sum_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "        superset_IS2_stationary_sum_df['stationary_outline_dV_corr (m^3)']) / 1e9) + cum_sum_last_CS2_midcyc_date, \n",
    "        color='red', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# Plot bias (evolving - updated stationary)\n",
    "ax[2].plot(mdates.date2num(subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum(subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['evolving_outlines_dV_corr (m^3)'] - \n",
    "        subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['stationary_outline_dV_corr (m^3)']) / 1e9, \n",
    "        color='darkred', linestyle='solid', linewidth=1)\n",
    "ax[2].scatter(mdates.date2num(subset_CS2_IS2_SARInPostExpansion_stationary_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum(subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "        subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['stationary_outline_dV_corr (m^3)']) / 1e9, \n",
    "        color='darkred', linestyle='solid', linewidth=1, s=3)\n",
    "\n",
    "cum_sum_last_CS2_midcyc_date = np.cumsum((\n",
    "    subset_CS2_IS2_SARInPostExpansion_evolving_sum_df[\n",
    "        subset_CS2_IS2_SARInPostExpansion_evolving_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date]\n",
    "        ['evolving_outlines_dV_corr (m^3)'] - \n",
    "        subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df[\n",
    "        subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date]\n",
    "        ['stationary_outline_dV_corr (m^3)']) / 1e9).iloc[-1]\n",
    "\n",
    "ax[2].plot(mdates.date2num(superset_IS2_stationary_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum((superset_IS2_evolving_sum_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "        superset_IS2_evolving_union_sum_df['stationary_outline_dV_corr (m^3)']) / 1e9) + cum_sum_last_CS2_midcyc_date, \n",
    "        color='darkred', linestyle='solid', linewidth=2)\n",
    "ax[2].scatter(mdates.date2num(superset_IS2_stationary_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum((superset_IS2_evolving_sum_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "        superset_IS2_evolving_union_sum_df['stationary_outline_dV_corr (m^3)']) / 1e9) + cum_sum_last_CS2_midcyc_date, \n",
    "        color='darkred', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# Plot dV time series of updated stationary outline\n",
    "ax[2].plot(mdates.date2num(subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum(subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['stationary_outline_dV_corr (m^3)']) / 1e9, \n",
    "    color='k', linestyle='dotted', linewidth=1)\n",
    "ax[2].scatter(mdates.date2num(subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum(subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['stationary_outline_dV_corr (m^3)']) / 1e9, \n",
    "    color='k', linestyle='solid', linewidth=1, s=5)\n",
    "\n",
    "cum_sum_last_CS2_midcyc_date = np.cumsum((\n",
    "    subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df[subset_CS2_IS2_SARInPostExpansion_evolving_union_sum_df['mid_pt_datetime'] <= CS2_last_cyc_date]\n",
    "    ['stationary_outline_dV_corr (m^3)']) / 1e9).iloc[-1]\n",
    "\n",
    "ax[2].plot(mdates.date2num(superset_IS2_evolving_union_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum(superset_IS2_evolving_union_sum_df['stationary_outline_dV_corr (m^3)'] / 1e9) + cum_sum_last_CS2_midcyc_date, \n",
    "           color='k', linestyle='dotted', linewidth=2)\n",
    "ax[2].scatter(mdates.date2num(superset_IS2_evolving_union_sum_df['mid_pt_datetime']), \n",
    "    np.cumsum(superset_IS2_evolving_union_sum_df['stationary_outline_dV_corr (m^3)'] / 1e9) + cum_sum_last_CS2_midcyc_date, \n",
    "           color='k', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# Set y axes limits\n",
    "ax0_auto_ymin, ax0_auto_ymax = ax[0].get_ylim()\n",
    "ax1_auto_ymin, ax1_auto_ymax = ax[1].get_ylim()\n",
    "ax[0].set_ylim(-(ax0_auto_ymax-ax0_auto_ymin)*0.1, None) # Prescribe lower limit to accommodate text annotations of satellite eras\n",
    "ax[1].set_ylim(-(ax1_auto_ymax-ax1_auto_ymin)*0.1, None)\n",
    "ax[2].set_ylim(-8.5, 8.5)\n",
    "del ax0_auto_ymin, ax0_auto_ymax, ax1_auto_ymin, ax1_auto_ymax\n",
    "\n",
    "# Add colorbar, legends, and titles\n",
    "ax[2].set_xlabel('year', size=14)\n",
    "\n",
    "# Plot vertical lines to indicate CS2 SARIn mode mask moving inland and ICESat-2 era start\n",
    "for row in [0,1,2]:\n",
    "    ax[row].axvline(SARIn_expand_date, color='dimgray', linestyle='solid', linewidth=1, ymin=-1, ymax=1)\n",
    "    ax[row].axvline(CS2_IS2_tie_pt, color='dimgray', linestyle='solid', linewidth=1, ymin=-1, ymax=1)\n",
    "\n",
    "# Add text label near the vertical line\n",
    "start_date_text = pd.to_datetime(cyc_dates['cyc_start_datetimes'].iloc[0]) + pd.Timedelta(days=15)\n",
    "sarin_expand_date_text = pd.to_datetime(cyc_dates[cyc_dates['cyc_start_datetimes'] == '2014-10-01T18:00:00.000000000']['cyc_start_datetimes'].iloc[0]) + pd.Timedelta(days=15)\n",
    "is2_start_date_text = pd.to_datetime(cyc_dates[cyc_dates['dataset'] == 'IceSat2_ATL15'].iloc[0]['cyc_start_datetimes']) + pd.Timedelta(days=15)\n",
    "\n",
    "ax[0].text(start_date_text, \n",
    "    -800, 'CryoSat-2 era begins', horizontalalignment='left', verticalalignment='top', color='k')\n",
    "ax[0].text(sarin_expand_date_text, \n",
    "    -800, 'SARIn mode expands', horizontalalignment='left', verticalalignment='top', color='k')\n",
    "ax[0].text(is2_start_date_text, \n",
    "    -800, 'ICESat-2 era begins', horizontalalignment='left', verticalalignment='top', color='k')\n",
    "\n",
    "# Add legends\n",
    "x0 = 0.44\n",
    "legend = ax[0].legend([stationary_all_lakes_line,\n",
    "                       tuple(lines),\n",
    "                       evolving_union_line],\n",
    "    [f'stationary outlines (n={len(no_evolving_outlines_lakes)})',\n",
    "     f'evolving outlines (n={len(evolving_outlines_lakes)})',\n",
    "     f'updated stationary outline (n={len(no_evolving_outlines_lakes)})'],\n",
    "    handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper center', bbox_to_anchor=(x0, 1))\n",
    "\n",
    "legend = ax[2].legend([bias,\n",
    "                       bias2], \n",
    "                      ['bias (evolving − prior stationary)',\n",
    "                       'bias (evolving − updated stationary)'],\n",
    "    handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper center', bbox_to_anchor=(x0, 1))\n",
    "\n",
    "for row in [0,1]:\n",
    "    # Remove x tick labels\n",
    "    ax[row].set_xticklabels([])\n",
    "\n",
    "    # Format the x-axis to display years only\n",
    "    ax[row].xaxis.set_major_locator(mdates.YearLocator(base=1))  # Major ticks every other year\n",
    "    ax[row].xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Minor ticks every quarter\n",
    "    ax[row].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Display major ticks as years\n",
    "\n",
    "    # Set x-axis limits\n",
    "    ax[row].set(xlim=(cyc_dates['cyc_start_datetimes'].iloc[0],\n",
    "        # Set righthand x-axis limit slightly earlier to prevent tick mark displaying when there is no data point\n",
    "        (cyc_dates['cyc_end_datetimes'].iloc[-1] - datetime.timedelta(days=2))))\n",
    "\n",
    "# Set axes titles\n",
    "ax[0].set_ylabel('active area [km$^2$]', size=14)\n",
    "ax[1].set_ylabel('DIC export [Gg C]', size=14)\n",
    "ax[2].set_ylabel('cumulative $dV$ [km$^3$]', size=14)\n",
    "\n",
    "# Adding annotations 'a', 'b', 'c' at the top left of the subplot\n",
    "ax_array = np.array(ax)  # Convert gridspec list of lists into numpy array to use .flatten() method\n",
    "char_index = 97  # ASCII value for 'a'\n",
    "for i, ax_i in enumerate(ax_array.flatten()):\n",
    "    # `transform=ax.transAxes` makes coordinates relative to the axes (0,0 is bottom left and 1,1 is top right)\n",
    "    ax_i.text(0.01, 0.98, chr(char_index), transform=ax_i.transAxes, fontsize=14, va='top', ha='left')\n",
    "    char_index += 1 # Increment the ASCII index to get the next character\n",
    "    \n",
    "# Save and close plot\n",
    "plt.savefig(OUTPUT_DIR + '/figures/Fig3_lake_reexamination_results_continental_integration.jpg',\n",
    "    dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Preview plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645bf6aa-8c53-491c-b8ed-25740c0354be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab2eee-aa1e-4e04-8356-780914839c96",
   "metadata": {},
   "source": [
    "## Fig. S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0331ad8-4e32-4c68-8e38-8e0023e95da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.family': 'DejaVu Sans',\n",
    "    'font.size': 9,\n",
    "    'axes.labelsize': 10,\n",
    "    'axes.titlesize': 9,\n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 9,\n",
    "    'legend.fontsize': 9,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac862d-4ec7-454d-beea-f329d9ab65c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select lakes to be included in plot\n",
    "selected_lakes = reexamined_stationary_outlines_gdf[reexamined_stationary_outlines_gdf['name'].isin(['Institute_E1', 'Mac2', 'Site_BC'])]\n",
    "desired_order = ['Institute_E1', 'Mac2', 'Site_BC']\n",
    "stationary_outlines_gdf_filtered = gpd.GeoDataFrame(pd.concat([selected_lakes[selected_lakes['name'] == name] for name in desired_order]))\n",
    "\n",
    "# Create a grid of plots\n",
    "nrows, ncols = 3, 2\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(6, 9), constrained_layout=False)\n",
    "\n",
    "# Define colors and linestyles for legend\n",
    "stationary_outline_color = 'darkturquoise'\n",
    "stationary_line = plt.Line2D([], [], color=stationary_outline_color, linestyle='solid', linewidth=2)\n",
    "evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "# Set up colormap for temporal evolution\n",
    "cmap = plt.get_cmap('plasma', len(cyc_start_datetimes[1:]))\n",
    "norm = plt.Normalize(mdates.date2num(cyc_start_datetimes[1]), \n",
    "                    mdates.date2num(cyc_start_datetimes[-1]))\n",
    "\n",
    "for row in range(1, nrows):\n",
    "    # Share y-axis within each row but not between rows\n",
    "    for col in range(ncols):\n",
    "        axs[row, col].sharey(axs[row, 0])\n",
    "\n",
    "# Process each lake sequentially\n",
    "for row, lake_name in enumerate(desired_order):\n",
    "    print('working on {}'.format(lake_name))\n",
    "    \n",
    "    # Get the lake data for the current lake\n",
    "    lake_gdf = stationary_outlines_gdf_filtered[stationary_outlines_gdf_filtered['name'] == lake_name]\n",
    "    stationary_outline = lake_gdf['geometry']\n",
    "    \n",
    "    # Load evolving outlines\n",
    "    try:\n",
    "        evolving_outlines_gdf = gpd.read_file(os.path.join(\n",
    "            os.getcwd(), 'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)))\n",
    "    except fiona.errors.DriverError:\n",
    "        print(f\"File for {lake_name} not found. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Find evolving and stationary outlines union for plotting extent\n",
    "    evolving_stationary_union_gdf = gpd.GeoDataFrame(\n",
    "        geometry=[lake_gdf.geometry.iloc[0].union(evolving_outlines_gdf.geometry.union_all())],\n",
    "        crs=lake_gdf.crs)\n",
    "    x_min, y_min, x_max, y_max = evolving_stationary_union_gdf.bounds.iloc[0]\n",
    "    \n",
    "    # Make plots uniform size and square\n",
    "    x_mid = (x_min + x_max) / 2\n",
    "    y_mid = (y_min + y_max) / 2\n",
    "    x_span = x_max - x_min\n",
    "    y_span = y_max - y_min\n",
    "    max_span = max(x_span, y_span)\n",
    "    \n",
    "    # Update bounds to ensure square dimensions\n",
    "    x_min = x_mid - max_span / 2\n",
    "    x_max = x_mid + max_span / 2\n",
    "    y_min = y_mid - max_span / 2\n",
    "    y_max = y_mid + max_span / 2\n",
    "    \n",
    "    # Add buffer around the plot\n",
    "    buffer_frac = 0.35\n",
    "    x_buffer = abs(x_max-x_min) * buffer_frac\n",
    "    y_buffer = abs(y_max-y_min) * buffer_frac\n",
    "    \n",
    "    # Create empty lists to store centroid coordinates\n",
    "    centroids_x = []\n",
    "    centroids_y = []\n",
    "    centroid_dates = []\n",
    "    \n",
    "    # Plot both outline and centroid views\n",
    "    for col in [0, 1]:\n",
    "        # Plot MOA surface imagery for both columns\n",
    "        mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "        mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "        moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "        axs[row, col].imshow(moa_highres_da_subset[0,:,:], cmap='gray', clim=[14000, 17000], \n",
    "                           extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "\n",
    "        # Plot stationary outline in both columns\n",
    "        if lake_name == 'Site_BC':\n",
    "            # Plot both Site_B and Site_C outlines\n",
    "            for site in ['Site_B', 'Site_C']:\n",
    "                stationary_outlines_gdf[stationary_outlines_gdf['name'] == site]['geometry'].boundary.plot(\n",
    "                    ax=axs[row, col], \n",
    "                    color=stationary_outline_color, \n",
    "                    linewidth=2\n",
    "                )\n",
    "\n",
    "                # Calculate centroid\n",
    "                centroid = stationary_outlines_gdf[stationary_outlines_gdf['name'] == site]['geometry'].iloc[0].centroid\n",
    "        \n",
    "                # Plot centroids\n",
    "                axs[row, col].scatter(centroid.x, centroid.y, \n",
    "                                   c=stationary_outline_color,\n",
    "                                   marker='.', s=50, linewidth=1, zorder=2)\n",
    "\n",
    "        else:\n",
    "            # Original code for other lakes\n",
    "            stationary_outlines_gdf[stationary_outlines_gdf['name'] == lake_name]['geometry'].boundary.plot(\n",
    "                ax=axs[row, col], \n",
    "                color=stationary_outline_color, \n",
    "                linewidth=2)\n",
    "    \n",
    "    # Plot evolving outlines with colors based on date (left column only)\n",
    "    lines = []\n",
    "    for dt_idx, dt in enumerate(cyc_start_datetimes[1:]):\n",
    "        # Create line for legend\n",
    "        x, y = 1, 1\n",
    "        line, = axs[row, 0].plot(x, y, color=cmap(norm(mdates.date2num(cyc_start_datetimes[dt_idx]))))\n",
    "        lines.append(line)\n",
    "        \n",
    "        # Plot evolving outlines for this time step\n",
    "        evolving_outlines_gdf_dt_sub = evolving_outlines_gdf[evolving_outlines_gdf['mid_pt_datetime'] == dt]\n",
    "        if not evolving_outlines_gdf_dt_sub.empty:\n",
    "            # Plot outline in left column\n",
    "            evolving_outlines_gdf_dt_sub.boundary.plot(\n",
    "                ax=axs[row, 0], \n",
    "                color=cmap(norm(mdates.date2num(cyc_start_datetimes[dt_idx]))), \n",
    "                linewidth=1\n",
    "            )\n",
    "            \n",
    "            # Calculate and store centroid\n",
    "            centroid = evolving_outlines_gdf_dt_sub.geometry.iloc[0].centroid\n",
    "            centroids_x.append(centroid.x)\n",
    "            centroids_y.append(centroid.y)\n",
    "            centroid_dates.append(dt)\n",
    "    \n",
    "    # Plot centroids in right column\n",
    "    axs[row, 1].scatter(centroids_x, centroids_y, \n",
    "                       c=[cmap(norm(mdates.date2num(dt))) for dt in centroid_dates],\n",
    "                       marker='+', s=100, linewidth=1, zorder=2)\n",
    "    \n",
    "    # Set the same limits and formatting for both plots\n",
    "    for col in [0, 1]:\n",
    "        # Format axis ticks to show kilometers\n",
    "        km_scale = 1e3\n",
    "        ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "        ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "        axs[row, col].xaxis.set_major_formatter(ticks_x)\n",
    "        axs[row, col].yaxis.set_major_formatter(ticks_y)\n",
    "        \n",
    "        # Set axes limits\n",
    "        axs[row, col].set(xlim=(x_min-x_buffer, x_max+x_buffer), \n",
    "                         ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "        \n",
    "        # Remove y-tick labels for right column only\n",
    "        if col == 1:\n",
    "            plt.setp(axs[row, col].get_yticklabels(), visible=False)\n",
    "        \n",
    "        axs[2, col].set_xlabel('x [km]')        \n",
    "        axs[row, 0].set_ylabel('y [km]')\n",
    "        # axs[row, 0].set_ylabel(f'{lake_name}\\n\\ny [km]')\n",
    "\n",
    "    # Create and style inset map (only for left column)\n",
    "    axIns = axs[row, 0].inset_axes([0.01, -0.01, 0.3, 0.3])\n",
    "    axIns.set_aspect('equal')\n",
    "    moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    axIns.axis('off')\n",
    "    \n",
    "    # Add location marker to inset map\n",
    "    axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "                 linewidth=2, color='k', s=15, zorder=3)\n",
    "\n",
    "for i in range(axs.shape[0]):\n",
    "    for j in range(axs.shape[1]):\n",
    "        # Add subplot labels (a, b, c, etc.)\n",
    "        axs[i,j].text(0.02, 0.98, chr(97 + i*2 + j), transform=axs[i,j].transAxes, \n",
    "                     fontsize=14, va='top', ha='left')\n",
    "\n",
    "# Set up colormap\n",
    "min_date = pd.to_datetime(cyc_start_datetimes[1])\n",
    "max_date = pd.to_datetime(cyc_start_datetimes[-1])\n",
    "date_range = pd.date_range(min_date, max_date, periods=len(cyc_start_datetimes[1:]))\n",
    "years = date_range.year.unique()\n",
    "years = pd.to_datetime(years, format='%Y')\n",
    "n_dates = len(cyc_start_datetimes[1:])\n",
    "cmap = plt.get_cmap('plasma', n_dates)\n",
    "norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "m.set_array(np.linspace(mdates.date2num(min_date), mdates.date2num(max_date), n_dates))\n",
    "\n",
    "# Add colorbar\n",
    "cax = fig.add_axes([0.15, 0.08, 0.73, 0.01])  # [left, bottom, width, height]\n",
    "cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "cbar.set_label('year', size=12, labelpad=5)\n",
    "\n",
    "# Set ticks for all years but labels only for odd years\n",
    "tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "tick_labels = [date.strftime('%Y') if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "cbar.set_ticks(tick_locations)\n",
    "cbar.set_ticklabels(tick_labels)\n",
    "\n",
    "# Add minor ticks for quarters\n",
    "cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Quarter intervals only\n",
    "\n",
    "# Adjust the layout to make room for the colorbar\n",
    "plt.subplots_adjust(\n",
    "    top=0.95,      # Reduce top margin (default is usually 0.9)\n",
    "    bottom=0.15,   # Increase bottom margin for colorbar (up from 0.1)\n",
    "    wspace=0.01,   # Keep the same horizontal spacing\n",
    "    hspace=0.2     # Control vertical spacing between subplots\n",
    ")\n",
    "\n",
    "# Save and close plot\n",
    "plt.savefig(OUTPUT_DIR + '/figures/FigS2_lake_migration.jpg',\n",
    "    dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Preview plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e96e995-abd6-4cac-8096-71f169b5730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480791ea-059e-46ee-9d50-f9bcddeda78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View citation for each of plotted lakes for figure caption\n",
    "selected_lakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924412bc-1bdb-451b-82e5-abd920a66b2c",
   "metadata": {},
   "source": [
    "## Fig. S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3adfdd-693c-45a6-a980-9a50d753fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore different lake groups to find one to highlight in publication\n",
    "\n",
    "# Example lake groups\n",
    "lake_groups = [\n",
    "    ('Bindschadler', ['Bindschadler_1', 'Bindschadler_2', 'Bindschadler_3', 'Bindschadler_4', 'Bindschadler_5', 'Bindschadler_6']),\n",
    "    ('Byrd', ['Byrd_1', 'Byrd_2', 'Byrd_s1', 'Byrd_s2', 'Byrd_s3', 'Byrd_s4', 'Byrd_s5', 'Byrd_s6', 'Byrd_s7', 'Byrd_s8',\n",
    "     'Byrd_s9', 'Byrd_s10', 'Byrd_s11', 'Byrd_s12', 'Byrd_s13', 'Byrd_s14', 'Byrd_s15']),\n",
    "    ('Cook', ['Cook_E1', 'Cook_E2']),\n",
    "    ('David', ['David_1', 'David_s1', 'David_s2', 'David_s3', 'David_s4', 'David_s5']),\n",
    "    ('EAP', ['EAP_1', 'EAP_2', 'EAP_3', 'EAP_4', 'EAP_5', 'EAP_6', 'EAP_7', 'EAP_8', 'EAP_9']),\n",
    "    ('Foundation_N', ['Foundation_N1', 'Foundation_N2', 'Foundation_N3']),\n",
    "    ('Foundation', ['Foundation_1', 'Foundation_2', 'Foundation_3', 'Foundation_4', 'Foundation_5', 'Foundation_6', 'Foundation_7', 'Foundation_8',\n",
    "     'Foundation_9', 'Foundation_10', 'Foundation_11', 'Foundation_12', 'Foundation_13', 'Foundation_14', 'Foundation_15', 'Foundation_16']),\n",
    "    ('Institute', ['Institute_E1', 'Institute_E2', 'Institute_W1', 'Institute_W2']),\n",
    "    ('KambTrunk', ['KT3', 'KT2', 'KT1']),\n",
    "    ('Kamb', ['Kamb_1', 'Kamb_2', 'Kamb_3', 'Kamb_4', 'Kamb_5', 'Kamb_6', 'Kamb_7', 'Kamb_8', 'Kamb_9', 'Kamb_10', 'Kamb_11', 'Kamb_12']),\n",
    "    ('MacAyeal', ['Mac1', 'Mac2', 'Mac3', 'Mac4', 'Mac5', 'Mac6']),\n",
    "    ('Nimrod', ['Nimrod_1', 'Nimrod_2']),\n",
    "    ('Ninnis', ['Ninnis_1', 'Ninnis_2']),\n",
    "    ('Recovery', ['Rec1', 'Rec2', 'Rec3', 'Rec4', 'Rec5', 'Rec6', 'Rec7', 'Rec8', 'Rec9']),\n",
    "    ('Slessor', ['Slessor_1', 'Slessor_23', 'Slessor_4', 'Slessor_5', 'Slessor_6', 'Slessor_7']),\n",
    "    ('Thwaites', ['Thw_70', 'Thw_124', 'Thw_142', 'Thw_170']),\n",
    "    ('Totten', ['Totten_1', 'Totten_2']),\n",
    "    ('Wilkes', ['Wilkes_1', 'Wilkes_2']),\n",
    "    ('Mercer_Whillans', ['EngelhardtSubglacialLake', 'UpperEngelhardtSubglacialLake', 'Lake12', 'Lake10', 'Lake78', 'WhillansSubglacialLake', \n",
    "     'LowerMercerSubglacialLake', 'MercerSubglacialLake', 'LowerConwaySubglacialLake', 'ConwaySubglacialLake', 'UpperSubglacialLakeConway', \n",
    "    'Whillans_6', 'Whillans_7', 'Whillans_8'])\n",
    "]\n",
    "\n",
    "# Call the function\n",
    "plot_lake_groups_dV(lake_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5370774f-a5f3-45dd-a902-68fce46b47f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.family': 'DejaVu Sans',\n",
    "    'font.size': 9,\n",
    "    'axes.labelsize': 10,\n",
    "    'axes.titlesize': 9,\n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 9,\n",
    "    'legend.fontsize': 9,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4735ee90-2857-4e3d-b6ce-539c8005fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure S3\n",
    "\n",
    "# Select lakes to highlight in figure\n",
    "lake_groups = [('Thwaites', ['Thw_70', 'Thw_124', 'Thw_142', 'Thw_170'])]\n",
    "\n",
    "# Initialize lists to store valid lake data\n",
    "valid_lakes = []\n",
    "evolving_outlines_gdfs = []\n",
    "lake_gdfs = []\n",
    "evolving_geom_calcs_dfs = []\n",
    "stationary_geom_calcs_dfs = []\n",
    "evolving_union_geom_calcs_dfs = []\n",
    "\n",
    "# Process lakes and populate the lists\n",
    "for lake_name in lake_groups[0][1]:  # Access the lake list from the first group\n",
    "    print(f\"Processing data for {lake_name}...\")\n",
    "    \n",
    "    # Get lake data from stationary outlines\n",
    "    lake_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == lake_name]\n",
    "    if lake_gdf.empty:\n",
    "        print(f\"Skipping {lake_name}: not found in stationary outlines\")\n",
    "        continue\n",
    "    \n",
    "    # Try loading evolving outlines\n",
    "    try:\n",
    "        evolving_outlines_gdf = gpd.read_file(os.path.join(\n",
    "            'output/lake_outlines/evolving_outlines',\n",
    "            f'{lake_name}.geojson'))\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {lake_name}: no evolving outlines file - {str(e)}\")\n",
    "        continue\n",
    "    \n",
    "    # Try loading geometric calculations\n",
    "    try:\n",
    "        evolving_geom_calcs_df = pd.read_csv(os.path.join(\n",
    "            'output/geometric_calcs/evolving_outlines_geom_calc/',\n",
    "            f'{lake_name}.csv'))\n",
    "        evolving_geom_calcs_df['mid_pt_datetime'] = pd.to_datetime(evolving_geom_calcs_df['mid_pt_datetime'])\n",
    "\n",
    "        evolving_union_geom_calcs_df = pd.read_csv(os.path.join(\n",
    "            'output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/',\n",
    "            f'{lake_name}.csv'))\n",
    "        evolving_union_geom_calcs_df['mid_pt_datetime'] = pd.to_datetime(evolving_union_geom_calcs_df['mid_pt_datetime'])\n",
    "\n",
    "        stationary_geom_calcs_df = pd.read_csv(os.path.join(\n",
    "            'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes/',\n",
    "            f'{lake_name}.csv'))\n",
    "        stationary_geom_calcs_df['mid_pt_datetime'] = pd.to_datetime(stationary_geom_calcs_df['mid_pt_datetime'])\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {lake_name}: error loading geometric calculations - {str(e)}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Valid data found for {lake_name}\")\n",
    "    valid_lakes.append(lake_name)\n",
    "    lake_gdfs.append(lake_gdf)\n",
    "    evolving_outlines_gdfs.append(evolving_outlines_gdf)\n",
    "    evolving_geom_calcs_dfs.append(evolving_geom_calcs_df)\n",
    "    stationary_geom_calcs_dfs.append(stationary_geom_calcs_df)\n",
    "    evolving_union_geom_calcs_dfs.append(evolving_union_geom_calcs_df)\n",
    "\n",
    "if not valid_lakes:\n",
    "    raise ValueError(\"No valid lakes found to process\")\n",
    "    \n",
    "# Create figure\n",
    "fig = plt.figure(figsize=(10, 15))\n",
    "\n",
    "# Create a 3x2 gridspec\n",
    "gs = fig.add_gridspec(3, 2)\n",
    "\n",
    "# Main spatial overview panel in first cell\n",
    "ax_main = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "# Get combined extent for all valid lakes\n",
    "x_mins, x_maxs, y_mins, y_maxs = [], [], [], []\n",
    "\n",
    "for lake_gdf, evolving_outlines_gdf in zip(lake_gdfs, evolving_outlines_gdfs):\n",
    "    # Find evolving and stationary outlines union for plotting extent\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    evolving_stationary_union_gdf = gpd.GeoDataFrame(\n",
    "        geometry=[lake_gdf.geometry.iloc[0].union(evolving_outlines_gdf.geometry.union_all())],\n",
    "        crs=lake_gdf.crs)\n",
    "    \n",
    "    # Get extent\n",
    "    x_min, y_min, x_max, y_max = evolving_stationary_union_gdf['geometry'].bounds.iloc[0]\n",
    "    buffer_dist = max(x_max - x_min, y_max - y_min) * 0.05\n",
    "    x_mins.append(x_min - buffer_dist)\n",
    "    x_maxs.append(x_max + buffer_dist)\n",
    "    y_mins.append(y_min - buffer_dist)\n",
    "    y_maxs.append(y_max + buffer_dist)\n",
    "\n",
    "# Set plot extent\n",
    "x_min, x_max = min(x_mins), max(x_maxs)\n",
    "y_min, y_max = min(y_mins), max(y_maxs)\n",
    "\n",
    "# Plot MOA background\n",
    "mask_x = (moa_highres_da.x >= x_min) & (moa_highres_da.x <= x_max)\n",
    "mask_y = (moa_highres_da.y >= y_min) & (moa_highres_da.y <= y_max)\n",
    "moa_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "ax_main.imshow(moa_subset[0,:,:], cmap='gray', clim=[14000, 17000],\n",
    "              extent=[x_min, x_max, y_min, y_max])\n",
    "\n",
    "# Plot stationary outlines\n",
    "stationary_color = 'darkturquoise'\n",
    "for lake_gdf in lake_gdfs:\n",
    "    lake_gdf.boundary.plot(ax=ax_main, color=stationary_color, linewidth=2)\n",
    "\n",
    "# Define custom offsets and display names for each lake\n",
    "# Format: 'lake_name': {'offset': (x_offset, y_offset), 'display': 'custom_name'}\n",
    "label_configs = {\n",
    "    'Thw_70': {\n",
    "        'offset': (-8e3, 7e3),\n",
    "        'display': 'Thw$_{70}$'\n",
    "    },\n",
    "    'Thw_124': {\n",
    "        'offset': (-20e3, 20e3),\n",
    "        'display': 'Thw$_{124}$'\n",
    "    },\n",
    "    'Thw_142': {\n",
    "        'offset': (-22e3, 22e3),\n",
    "        'display': 'Thw$_{142}$'\n",
    "    },\n",
    "    'Thw_170': {\n",
    "        'offset': (-16e3, 11e3),\n",
    "        'display': 'Thw$_{170}$'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add lake labels\n",
    "for lake_gdf in lake_gdfs:\n",
    "    # Get the centroid of the lake geometry\n",
    "    centroid = lake_gdf.geometry.iloc[0].centroid\n",
    "    # Get the lake name\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    # Get custom offset and display name for this lake (or use defaults)\n",
    "    config = label_configs.get(lake_name, {'offset': (0, 0), 'display': lake_name})\n",
    "    x_offset, y_offset = config['offset']\n",
    "    display_name = config['display']\n",
    "    # Add label\n",
    "    ax_main.annotate(display_name, \n",
    "                    xy=(centroid.x + x_offset, centroid.y + y_offset),\n",
    "                    color='white',\n",
    "                    fontweight='bold',\n",
    "                    ha='center', va='center',\n",
    "                    path_effects=[PathEffects.withStroke(linewidth=3, foreground='black')])\n",
    "\n",
    "# Plot evolving outlines with time-based coloring\n",
    "cmap = plt.get_cmap('plasma')\n",
    "norm = plt.Normalize(mdates.date2num(cyc_start_datetimes[1]), \n",
    "                   mdates.date2num(cyc_start_datetimes[-1]))\n",
    "\n",
    "for evolving_outlines_gdf in evolving_outlines_gdfs:\n",
    "    for idx, row in evolving_outlines_gdf.iterrows():\n",
    "        color = cmap(norm(mdates.date2num(pd.to_datetime(row['mid_pt_datetime']))))\n",
    "        gpd.GeoSeries(row['geometry']).boundary.plot(\n",
    "            ax=ax_main, color=color, linewidth=1)\n",
    "\n",
    "    # Plot inset map\n",
    "    axIns = ax_main.inset_axes([0.7, 0.02, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "    axIns.set_aspect('equal')\n",
    "    moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1)\n",
    "    moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1)\n",
    "    axIns.axis('off')\n",
    "    # Plot star to indicate location\n",
    "    axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "        linewidth=1, color='k', s=75)\n",
    "\n",
    "# Plot evolving outlines union\n",
    "for lake_gdf in lake_gdfs:\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    evolving_union_gdf = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]\n",
    "    evolving_union_gdf.boundary.plot(ax=ax_main, color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "# Format overview axes\n",
    "km_scale = 1e3\n",
    "ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "ax_main.xaxis.set_major_formatter(ticks_x)\n",
    "ax_main.yaxis.set_major_formatter(ticks_y)\n",
    "ax_main.set_xlabel('x [km]')\n",
    "ax_main.set_ylabel('y [km]')\n",
    "\n",
    "# Set up colormap\n",
    "min_date = pd.to_datetime(cyc_start_datetimes[1])\n",
    "max_date = pd.to_datetime(cyc_start_datetimes[-1])\n",
    "date_range = pd.date_range(min_date, max_date, periods=len(cyc_start_datetimes[1:]))\n",
    "years = date_range.year.unique()\n",
    "years = pd.to_datetime(years, format='%Y')\n",
    "n_dates = len(cyc_start_datetimes[1:])\n",
    "cmap = plt.get_cmap('plasma', n_dates)\n",
    "norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "m.set_array(np.linspace(mdates.date2num(min_date), mdates.date2num(max_date), n_dates))\n",
    "\n",
    "# Add colorbar\n",
    "divider = make_axes_locatable(ax_main)\n",
    "cax = divider.append_axes('bottom', size='2.5%', pad=0.5)\n",
    "cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "\n",
    "# Set colorbar ticks\n",
    "cbar.ax.xaxis.set_major_formatter(year_interval_formatter())\n",
    "cbar.ax.xaxis.set_major_locator(mdates.YearLocator())  # Every year\n",
    "cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator([1, 4, 7, 10]))  # Quarter year ticks\n",
    "\n",
    "# cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Quarter intervals only\n",
    "cbar.set_label('year', size=12)\n",
    "\n",
    "# Get y axis limits for volume plots\n",
    "y_min, y_max = get_overall_y_limits(evolving_geom_calcs_dfs, \n",
    "                                  stationary_geom_calcs_dfs,\n",
    "                                  evolving_union_geom_calcs_dfs)        \n",
    "# Calculate limits with buffer\n",
    "y_range = y_max - y_min\n",
    "buffer = y_range * 0.05\n",
    "y_limits = (y_min - buffer, y_max + buffer)\n",
    "\n",
    "# Create axes for all plots (excluding the overview plot position)\n",
    "axes = []\n",
    "plot_positions = [(0,1), (1,0), (1,1), (2,0), (2,1)]  # Row, Col positions for dV plots\n",
    "\n",
    "for pos in plot_positions:\n",
    "    ax = fig.add_subplot(gs[pos])\n",
    "    axes.append(ax)\n",
    "\n",
    "# Plot individual lakes\n",
    "for idx, (lake_name, evolving_df, stationary_df, union_df) in enumerate(zip(\n",
    "        valid_lakes, evolving_geom_calcs_dfs, stationary_geom_calcs_dfs, evolving_union_geom_calcs_dfs)):\n",
    "    ax = axes[idx]\n",
    "    ax.axhline(0, color='k', linestyle='--')\n",
    "    \n",
    "    dates = mdates.date2num(evolving_df['mid_pt_datetime'])\n",
    "    \n",
    "    # Plot stationary outline\n",
    "    stationary_cumsum = np.cumsum(np.divide(stationary_df['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "    ax.plot(dates, stationary_cumsum, color=stationary_color, label='Stationary', linewidth=2)\n",
    "    ax.scatter(dates, stationary_cumsum, color=stationary_color, s=5)\n",
    "\n",
    "    # Store line segments for multi-colored line in legend\n",
    "    lines = []\n",
    "    for i, dt in enumerate(dates):\n",
    "        line = ax.plot(1, 1, color=cmap(norm(mdates.date2num(cyc_start_datetimes[i]))), linewidth=2)[0]\n",
    "        lines.append(line)\n",
    "        line.remove()  # Remove the dummy lines after creating them\n",
    "\n",
    "    # Plot evolving outlines (multi-colored line)\n",
    "    x = dates\n",
    "    y = np.cumsum(np.divide(evolving_df['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "    lc.set_array(x)\n",
    "    lc.set_linewidth(2)\n",
    "    ax.add_collection(lc)\n",
    "    ax.scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "    # Plot evolving outlines union\n",
    "    union_cumsum = np.cumsum(np.divide(union_df['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "    ax.plot(dates, union_cumsum, color='k', linestyle='dotted', label='Union', linewidth=2)\n",
    "    ax.scatter(dates, union_cumsum, color='k', s=5)\n",
    "\n",
    "    # Plot bias\n",
    "    bias = np.cumsum(np.divide(evolving_df['evolving_outlines_dV_corr (m^3)'] - \n",
    "                             stationary_df['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "    ax.plot(dates, bias, color='r', label='Bias', linewidth=2)\n",
    "    ax.scatter(dates, bias, color='r', linewidth=2, s=5)\n",
    "\n",
    "    # Add legend only to the first plot\n",
    "    if idx == 0:\n",
    "        stationary_line = plt.Line2D([], [], color=stationary_color, linestyle='solid', linewidth=2)\n",
    "        evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=2)\n",
    "        bias_line = plt.Line2D([], [], color='red', linestyle='solid', linewidth=2)\n",
    "        legend = ax.legend(\n",
    "            [tuple(lines), \n",
    "             evolving_union_line,\n",
    "             stationary_line,\n",
    "             bias_line],\n",
    "            ['evolving outlines',\n",
    "             'evolving outlines union',\n",
    "             'stationary outline',\n",
    "             'bias (evolving − stationary)'],\n",
    "            handlelength=3,\n",
    "            handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "            fontsize=12,\n",
    "            loc='lower center'\n",
    "        )\n",
    "\n",
    "    # Format axes\n",
    "    ax.xaxis.set_major_formatter(year_interval_formatter())\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax.xaxis.set_minor_locator(mdates.MonthLocator([1, 4, 7, 10]))  # Quarter year ticks (Jan, Apr, Jul, Oct)\n",
    "\n",
    "    # Set x and y axes limit\n",
    "    ax.set_xlim(cyc_dates['cyc_start_datetimes'].iloc[0],\n",
    "        (cyc_dates['cyc_end_datetimes'].iloc[-1] - datetime.timedelta(days=2)))\n",
    "    ax.set_ylim(y_min, y_max)    \n",
    "\n",
    "    # Handle y-axis labels\n",
    "    row = plot_positions[idx][0]\n",
    "    col = plot_positions[idx][1]\n",
    "    if (col == 1 and row == 0) or (col == 0 and (row == 1 or row == 2)):  # First dV plot (0,1) and left column of rows 1 and 2\n",
    "        ax.set_ylabel('cumulative $dV$ [km$^3$]', fontsize=12)\n",
    "    else:  # All other plots\n",
    "        ax.set_yticklabels([])\n",
    "        \n",
    "    # Handle x-axis labels\n",
    "    if row == 2:  # Bottom row\n",
    "        ax.set_xlabel('Year', fontsize=12)\n",
    "    else:  # Top row\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_xlabel('')\n",
    "    \n",
    "    # Get the display name from label_configs, fallback to lake_name if not found\n",
    "    display_name = label_configs.get(lake_name, {}).get('display', lake_name)\n",
    "\n",
    "    # Use display_name instead of lake_name for the title\n",
    "    ax.set_title(display_name, fontsize=16)\n",
    "\n",
    "# Plot combined data in the last position\n",
    "last_ax = axes[-1]\n",
    "\n",
    "# Combine all dataframes by summing values for each timestamp\n",
    "combined_evolving = pd.concat(evolving_geom_calcs_dfs).groupby('mid_pt_datetime').sum().reset_index()\n",
    "combined_stationary = pd.concat(stationary_geom_calcs_dfs).groupby('mid_pt_datetime').sum().reset_index()\n",
    "combined_union = pd.concat(evolving_union_geom_calcs_dfs).groupby('mid_pt_datetime').sum().reset_index()\n",
    "\n",
    "dates = mdates.date2num(combined_evolving['mid_pt_datetime'])\n",
    "\n",
    "# Plot stationary outline\n",
    "stationary_cumsum = np.cumsum(np.divide(combined_stationary['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "last_ax.plot(dates, stationary_cumsum, color=stationary_color, label='Stationary', linewidth=2)\n",
    "last_ax.scatter(dates, stationary_cumsum, color=stationary_color, s=5)\n",
    "\n",
    "# Plot evolving outlines (multi-colored line)\n",
    "evolving_cumsum = np.cumsum(np.divide(combined_evolving['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "points = np.array([dates, evolving_cumsum]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(dates)\n",
    "lc.set_linewidth(2)\n",
    "last_ax.add_collection(lc)\n",
    "last_ax.scatter(dates, evolving_cumsum, c=dates, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "# Plot bias\n",
    "bias_cumsum = np.cumsum(np.divide(\n",
    "    combined_evolving['evolving_outlines_dV_corr (m^3)'] - \n",
    "    combined_stationary['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "last_ax.plot(dates, bias_cumsum, color='r', label='Bias', linewidth=2)\n",
    "last_ax.scatter(dates, bias_cumsum, color='r', s=5)\n",
    "\n",
    "# Plot evolving outlines union\n",
    "union_cumsum = np.cumsum(np.divide(combined_union['stationary_outline_dV_corr (m^3)'], 1e9))\n",
    "last_ax.plot(dates, union_cumsum, color='k', linestyle='dotted', label='Union', linewidth=2)\n",
    "last_ax.scatter(dates, union_cumsum, color='k', s=5)\n",
    "\n",
    "# Set axes limits and format\n",
    "last_ax.set_xlim(cyc_dates['cyc_start_datetimes'].iloc[0],\n",
    "                 (cyc_dates['cyc_end_datetimes'].iloc[-1] - datetime.timedelta(days=2)))\n",
    "last_ax.set_ylim(y_min, y_max)\n",
    "last_ax.set_yticklabels([])\n",
    "last_ax.axhline(0, color='k', linestyle='--')\n",
    "last_ax.set_xlabel('Year', fontsize=12)\n",
    "last_ax.xaxis.set_major_formatter(year_interval_formatter())\n",
    "last_ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "last_ax.xaxis.set_minor_locator(mdates.MonthLocator([1, 4, 7, 10]))\n",
    "\n",
    "last_ax.set_title('Summed', fontsize=16)\n",
    "\n",
    "# Add subplot annotations ('a'-'f') to all plots\n",
    "char_index = 97  # ASCII value for 'a'\n",
    "\n",
    "# Add annotation to main spatial overview panel\n",
    "ax_main.text(0.02, 0.98, chr(char_index), transform=ax_main.transAxes, \n",
    "             fontsize=14, va='top', ha='left')\n",
    "char_index += 1\n",
    "\n",
    "# Add annotations to volume plots\n",
    "for ax in axes:\n",
    "    ax.text(0.01, 0.98, chr(char_index), transform=ax.transAxes, \n",
    "            fontsize=14, va='top', ha='left')\n",
    "    char_index += 1\n",
    "\n",
    "# Adjust layout to prevent overlapping\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and close plot\n",
    "plt.savefig(OUTPUT_DIR + '/figures/FigS3_Thw_lakes_dV.jpg',\n",
    "    dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Preview plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac93c6a2-0c9c-4c3c-9746-c72cb186ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451de470-09c7-447b-9076-200cf613722a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
