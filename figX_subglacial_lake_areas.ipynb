{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# write psuedo code into real code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Computing environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Set up computing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement import (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for import\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [113], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Code to create fig. X in Sauthoff and others, 2023\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Written 2023-09-06 by W. Sauthoff (sauthoff@mines.edu)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Import packages\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# Code to create fig. X in Sauthoff and others, 2023\n",
    "#\n",
    "# Written 2023-09-06 by W. Sauthoff (sauthoff@mines.edu)\n",
    "\n",
    "# Import packages\n",
    "import cv2\n",
    "import datetime\n",
    "import math\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors  # delete when no longer needed\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import openpyxl\n",
    "from pathlib import Path\n",
    "from pyproj import CRS, Transformer\n",
    "import os\n",
    "import rioxarray\n",
    "from skimage import measure\n",
    "import xarray as xr\n",
    "\n",
    "# import geojson\n",
    "# import numpy as np\n",
    "# import geopandas as gpd\n",
    "# from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "# from matplotlib.patches import Rectangle\n",
    "# import shapely\n",
    "# from pyproj import CRS, Transformer\n",
    "# import matplotlib.transforms as mtransforms\n",
    "\n",
    "# Define data directories dependent on home environment\n",
    "# Replace with your directory file paths\n",
    "if os.getenv('HOME') == '/home/jovyan':\n",
    "    # DATA_DIR = '/home/jovyan/data_dir'\n",
    "    DATA_DIR = '/home/jovyan/data'\n",
    "    SCRIPT_DIR = '/home/jovyan/repos_my/script_dir'\n",
    "    OUTPUT_DIR = '/home/jovyan/1_outlines_candidates/output'\n",
    "elif os.getenv('HOME') == '/Users/Wilson': \n",
    "    DATA_DIR = '/Users/Wilson/Documents/data'\n",
    "    # DATA_DIR = '/Volumes/ExtremeSSD/data'\n",
    "    SCRIPT_DIR = '/Users/Wilson/Documents/0-code/repos_my/script_dir'\n",
    "    OUTPUT_DIR = '/Users/Wilson/Documents/0-code/1_outlines_candidates/output'\n",
    "\n",
    "# Define constants and coordinate transforms for the geodesic area calculation\n",
    "CRS_LL = \"EPSG:4326\" # wgs84 in lon,lat\n",
    "GEOD = CRS(CRS_LL).get_geod() # geod object for calculating geodesic area on defined ellipsoid\n",
    "CRS_XY = \"EPSG:3031\" # Antarctic Polar Stereographic in x, y\n",
    "XY_TO_LL = Transformer.from_crs(CRS_XY, CRS_LL, always_xy = True) # make coord transformer\n",
    "\n",
    "# Define utility functions\n",
    "def datetime2fracyear(date):\n",
    "    start = datetime.date(date.year, 1, 1).toordinal()\n",
    "    year_length = datetime.date(date.year+1, 1, 1).toordinal() - start\n",
    "    return date.year + float(date.toordinal() - start) / year_length\n",
    "\n",
    "def ll2ps(lon, lat):\n",
    "    \"\"\"\n",
    "    Transform coordinates from geodetic coordinates (lon, lat)\n",
    "    to Antarctic Polar Stereograph coordinates (x, y)\n",
    "    x, y = ll2ps(lon, lat)\n",
    "    Inputs\n",
    "    * lon, lat in decimal degrees (lon: W is negative; lat: S is negative)\n",
    "    Outputs\n",
    "    * x, y polar stereographic 71 in m\n",
    "    \"\"\"\n",
    "    crs_ll = CRS(\"EPSG:4326\")\n",
    "    crs_xy = CRS(\"EPSG:3031\")\n",
    "    ll_to_xy = Transformer.from_crs(crs_ll, crs_xy, always_xy = True)\n",
    "    x, y = ll_to_xy.transform(lon, lat)\n",
    "    return x, y\n",
    "\n",
    "# Change default font to Arial and increase font size\n",
    "# plt.rcParams[\"font.family\"] = 'Arial'\n",
    "# plt.rcParams.update({'font.size': 22})\n",
    "# plt.rcParams[\"font.family\"] = 'sans-serif' # set back to default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "io.py\n",
    "Written by Tyler Sutterley (08/2023)\n",
    "Utilities for reading gridded ICESat-2 files using rasterio and xarray\n",
    "\n",
    "PYTHON DEPENDENCIES:\n",
    "    h5netcdf: Pythonic interface to netCDF4 via h5py\n",
    "        https://h5netcdf.org/\n",
    "    numpy: Scientific Computing Tools For Python\n",
    "        https://numpy.org\n",
    "        https://numpy.org/doc/stable/user/numpy-for-matlab-users.html\n",
    "    rasterio: Access to geospatial raster data\n",
    "        https://github.com/rasterio/rasterio\n",
    "        https://rasterio.readthedocs.io\n",
    "    rioxarray: geospatial xarray extension powered by rasterio\n",
    "        https://github.com/corteva/rioxarray\n",
    "    xarray: N-D labeled arrays and datasets in Python\n",
    "        https://docs.xarray.dev/en/stable/\n",
    "\n",
    "UPDATE HISTORY:\n",
    "    Updated 08/2023: use xarray h5netcdf to read files streaming from s3\n",
    "        add open_dataset function for opening multiple granules\n",
    "        add merging of datasets in preparation for Release-3 data\n",
    "    Updated 07/2023: use logging instead of warnings for import attempts\n",
    "    Written 11/2022\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# attempt imports\n",
    "try:\n",
    "    import rioxarray\n",
    "    import rioxarray.merge\n",
    "except (AttributeError, ImportError, ModuleNotFoundError) as exc:\n",
    "    logging.critical(\"rioxarray not available\")\n",
    "try:\n",
    "    import xarray as xr\n",
    "except (AttributeError, ImportError, ModuleNotFoundError) as exc:\n",
    "    logging.critical(\"xarray not available\")\n",
    "\n",
    "# set environmental variable for anonymous s3 access\n",
    "os.environ['AWS_NO_SIGN_REQUEST'] = 'YES'\n",
    "\n",
    "# default engine for xarray\n",
    "_default_engine = dict(nc='h5netcdf', zarr='zarr')\n",
    "\n",
    "def open_dataset(granule,\n",
    "        group: str | None = None,\n",
    "        format: str = 'nc',\n",
    "        **kwargs\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Reads and optionally merges gridded ICESat-2 files\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    granule: str or list\n",
    "        presigned url or path for granule(s) as a s3fs object\n",
    "    group: str or NoneType, default None\n",
    "        Data group to read\n",
    "    format: str, default 'nc'\n",
    "        Data format to read\n",
    "    kwargs: dict\n",
    "        Keyword arguments to pass to ``xarray`` reader\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ds: object\n",
    "        ``xarray`` dataset\n",
    "    \"\"\"\n",
    "    # check if merging multiple granules\n",
    "    if isinstance(granule, list):\n",
    "        # merge multiple granules\n",
    "        datasets = []\n",
    "        # read each granule and append to list\n",
    "        for g in granule:\n",
    "            datasets.append(from_file(g,\n",
    "                group=group,\n",
    "                format=format,\n",
    "                **kwargs)\n",
    "            )\n",
    "        # merge datasets\n",
    "        ds = rioxarray.merge.merge_datasets(datasets)\n",
    "    else:\n",
    "        # read a single granule\n",
    "        ds = from_file(granule,\n",
    "            group=group,\n",
    "            format=format,\n",
    "            **kwargs\n",
    "        )\n",
    "    # return the dataset\n",
    "    return ds\n",
    "\n",
    "def from_file(granule,\n",
    "        group: str | None = None,\n",
    "        format: str = 'nc',\n",
    "        **kwargs\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Reads a gridded ICESat-2 file using ``rioxarray`` or ``xarray``\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    granule: str\n",
    "        presigned url or path for granule\n",
    "    group: str or NoneType, default None\n",
    "        Data group to read\n",
    "    format: str, default 'nc'\n",
    "        Data format to read\n",
    "    kwargs: dict\n",
    "        Keyword arguments to pass to ``xarray`` reader\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ds: object\n",
    "        ``xarray`` dataset\n",
    "    \"\"\"\n",
    "    # set default engine\n",
    "    kwargs.setdefault('engine', _default_engine[format])\n",
    "    if isinstance(granule, str) and format in ('nc',):\n",
    "        ds = from_rasterio(granule,\n",
    "            group=group,\n",
    "            **kwargs\n",
    "        )\n",
    "    else:\n",
    "        # read a single granule\n",
    "        ds = from_xarray(granule,\n",
    "            group=group,\n",
    "            **kwargs\n",
    "        )\n",
    "    # return the dataset\n",
    "    return ds\n",
    "\n",
    "def from_rasterio(granule,\n",
    "        group: str | None = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Reads a gridded ICESat-2 file using ``rioxarray``\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    granule: str\n",
    "        presigned url or path for granule\n",
    "    group: str or NoneType, default None\n",
    "        Data group to read\n",
    "    kwargs: dict\n",
    "        Keyword arguments to pass to ``rioxarray``\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ds: object\n",
    "        ``xarray`` dataset\n",
    "    \"\"\"\n",
    "    ds = rioxarray.open_rasterio(granule,\n",
    "        group=group,\n",
    "        masked=True,\n",
    "        **kwargs\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "\n",
    "def from_xarray(granule,\n",
    "        group: str | None = None,\n",
    "        engine: str = 'h5netcdf',\n",
    "        **kwargs\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Reads a gridded ICESat-2 file using ``xarray``\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    granule: str\n",
    "        presigned url or path for granule\n",
    "    group: str or NoneType, default None\n",
    "        Data group to read\n",
    "    engine: str, default 'h5netcdf'\n",
    "        Engine to use when reading files\n",
    "    kwargs: dict\n",
    "        Keyword arguments to pass to ``xarray``\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ds: object\n",
    "        ``xarray`` dataset\n",
    "    \"\"\"\n",
    "    kwargs.setdefault('variable', [])\n",
    "    variable = kwargs.pop('variable')\n",
    "    # read xarray dataset\n",
    "    ds = xr.open_dataset(granule,\n",
    "        group=group,\n",
    "        engine=engine,\n",
    "        chunks='auto',\n",
    "        decode_cf=True,\n",
    "        mask_and_scale=True,\n",
    "        decode_times=False,\n",
    "        concat_characters=True,\n",
    "        decode_coords=True,\n",
    "        overwrite_encoded_chunks=False,\n",
    "        **kwargs\n",
    "    )\n",
    "    # set the coordinate reference system\n",
    "    ds.rio.write_crs(ds.Polar_Stereographic.attrs['crs_wkt'], inplace=True)\n",
    "    # reduce xarray dataset to specific variables\n",
    "    if any(variable):\n",
    "        ds = ds[variable]\n",
    "    # flip orientation of y dimension\n",
    "    ds = ds.isel(y=slice(None, None, -1))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CS2_dh = xr.open_dataset(DATA_DIR + '/altimetry/CryoSat2/CS2_SARIn_Smith2017method/mos_2010.5_2021.5.nc')\n",
    "CS2_dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import ICESat-2 ATL15 Gridded Antarctic and Arctic Land Ice Height Change data product \n",
    "# https://doi.org/10.5067/ATLAS/ATL15.002\n",
    "if os.getenv('HOME') == '/home/jovyan':\n",
    "    # FIXME: temporary workaround  \n",
    "    # TODO: Grab ATL15 data using earthaccess or icepyx\n",
    "    os.environ['AWS_NO_SIGN_REQUEST'] = 'YES'\n",
    "    granules = ['s3://is2view/ATLAS/ATL15/003/2019/ATL15_A1_0318_01km_003_01.nc',\n",
    "        's3://is2view/ATLAS/ATL15/003/2019/ATL15_A2_0318_01km_003_02.nc',\n",
    "        's3://is2view/ATLAS/ATL15/003/2019/ATL15_A3_0318_01km_003_01.nc',\n",
    "        's3://is2view/ATLAS/ATL15/003/2019/ATL15_A4_0318_01km_003_03.nc']\n",
    "    ATL15_dh = open_dataset(granules, \n",
    "        group='delta_h', engine='h5netcdf')\n",
    "    ATL15_dh\n",
    "    \n",
    "elif os.getenv('HOME') == '/Users/Wilson': \n",
    "# if os.getenv('HOME') == '/Users/Wilson':\n",
    "    # TODO: Grab ATL15 data using earthaccess or icepyx\n",
    "    # s3url = 's3://nsidc-cumulus-prod-protected/ATLAS/ATL15/002/2019/ATL15_GL_0314_01km_002_01.nc'\n",
    "    # FIXME: temporary workaround\n",
    "    os.environ['AWS_NO_SIGN_REQUEST'] = 'YES'\n",
    "    # granule = 's3://is2view/ATLAS/ATL15/003/2019/ATL15_A1_0318_01km_003_01.nc'\n",
    "    # group = 'delta_h'\n",
    "    # ATL15_dh_A1 = rioxarray.open_rasterio(granule, group=group, masked=True)\n",
    "    # granule = 's3://is2view/ATLAS/ATL15/003/2019/ATL15_A2_0318_01km_003_02.nc'\n",
    "    # group = 'delta_h'\n",
    "    # ATL15_dh_A2 = rioxarray.open_rasterio(granule, group=group, masked=True)\n",
    "    granule = 's3://is2view/ATLAS/ATL15/003/2019/ATL15_A3_0318_01km_003_01.nc'\n",
    "    group = 'delta_h'\n",
    "    ATL15_dh_A3 = rioxarray.open_rasterio(granule, group=group, masked=True)\n",
    "    # granule = 's3://is2view/ATLAS/ATL15/003/2019/ATL15_A4_0318_01km_003_03.nc'\n",
    "    # group = 'delta_h'\n",
    "    # ATL15_dh_A4 = rioxarray.open_rasterio(granule, group=group, masked=True)\n",
    "\n",
    "    # Add a new attribute\n",
    "    # ATL15_dh.attrs['title'] = 'Gridded Antarctic Land Ice Height Change'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ATL15_dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure dates acquisition working\n",
    "dates = []\n",
    "dataset1 = CS2_dh\n",
    "dataset2 = ATL15_dh\n",
    "for idx in range(len(dataset1.delta_h[:-12])):\n",
    "    dhdt = dataset1.delta_h[idx+1,:,:]-dataset1.delta_h[idx,:,:]\n",
    "    # calculate mid-cycle dates for plotting\n",
    "    newdate = datetime.datetime(int(dataset1.time.values[idx]), 1, 1) + datetime.timedelta(days = (dataset1.time.values[idx] % 1) * 365.25)\n",
    "    newdate1 = datetime.datetime(int(dataset1.time.values[idx+1]), 1, 1) + datetime.timedelta(days = (dataset1.time.values[idx+1] % 1) * 365.25)\n",
    "    midcycdays = newdate1 - newdate\n",
    "    midcycdate = newdate + midcycdays/2\n",
    "    dates += [midcycdate]\n",
    "for idx in range(len(dataset2.delta_h)-1): \n",
    "    dhdt = dataset2.delta_h[idx+1,:,:]-dataset2.delta_h[idx,:,:]\n",
    "    date_time_str = '18-01-01'\n",
    "    date_time_obj = datetime.datetime.strptime(date_time_str, '%y-%m-%d')\n",
    "    newdate = date_time_obj + datetime.timedelta(days=dataset2.time.values[idx])\n",
    "    newdate1 = date_time_obj + datetime.timedelta(days=dataset2.time.values[idx+1])\n",
    "    midcycdays = newdate1 - newdate\n",
    "    midcycdate = newdate + midcycdays/2\n",
    "    dates += [midcycdate]\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import subglacial lake outline inventories\n",
    "# exec(open(SCRIPT_DIR + '/Smith2009_outlines.py').read())\n",
    "# exec(open(SCRIPT_DIR + '/SiegfriedFricker2018_outlines.py').read())\n",
    "# Will earlier two inventories load using the 2023 py script\n",
    "exec(open(SCRIPT_DIR + '/Sauthoff2023_outlines.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import MODIS MOA 2009 and 2014 coastline and grounding line for plotting inset maps\n",
    "# https://nsidc.org/data/nsidc-0593/versions/1\n",
    "# shp = DATA_DIR + '/boundaries/MODIS_MOA/2009/moa_2009_coastline_v02.0.shp' \n",
    "# moa_2009_coastline = gpd.read_file(shp)\n",
    "# shp = DATA_DIR + '/boundaries/MODIS_MOA/2009/moa_2009_groundingline_v02.0.shp' \n",
    "# moa_2009_groundingline = gpd.read_file(shp)\n",
    "# https://nsidc.org/data/nsidc-0730/versions/1\n",
    "shp = DATA_DIR + '/boundaries/MODIS_MOA/2014/moa2014_coastline_v01.shp' \n",
    "moa_2014_coastline = gpd.read_file(shp)\n",
    "shp = DATA_DIR + '/boundaries/MODIS_MOA/2014/moa2014_grounding_line_v01.shp' \n",
    "moa_2014_groundingline = gpd.read_file(shp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clip CS2_dh to grounding line (first set crs)\n",
    "# CS2_dh.rio.write_crs(3031, inplace=True)\n",
    "# CS2_dh_clipped = CS2_dh.rio.clip(moa_2014_groundingline.geometry.values, moa_2014_groundingline.crs, drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clip ATL15_dh to grounding line (first set crs)\n",
    "# ATL15_dh.rio.write_crs(3031, inplace=True)\n",
    "# ATL15_dh_clipped = ATL15_dh.rio.clip(moa_2014_groundingline.geometry.values, moa_2014_groundingline.crs, drop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The following functions enable a workflow to define region of interest, find evolving contours of delta height, plot results, refine those results to manually remove signal that is likely not related to subglacial lake activity, and export the evolving outlines and delta area time series.\n",
    "* func for finding bbox coords for region of interest\n",
    "* func to make directories to store output of latter func's\n",
    "* func for contouring creating plots for each differenced time slice including saving plots and a gdf of the evolving outlines\n",
    "* func to plot evolving outlines in aggregate\n",
    "* func to remove non-overlapping outlines\n",
    "* func to remove most extreme polygon to clean up outlines that are off-lake\n",
    "* func to take modified gdf and calculated delta area per time slice and export as csv\n",
    "* func to plot evolving outlines in aggregate and the dVol time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_bbox_coords(ROI, buffer):\n",
    "    '''\n",
    "    Func to find the bounding box coordinates of a region of interest\n",
    "    \n",
    "    Inputs\n",
    "    * ROI: Region of interest in form of a lake name from Sauthoff and others, 2024 inventory (Sauthoff2024_outlines),\n",
    "    IMBIE basin or refinded basins, or list of x_min, x_max, y_min, y_max bbox coords\n",
    "    * buffer: horizontal distance in meters that will be added to each edge of the ROI to include in the bbox\n",
    "    \n",
    "    Outputs:\n",
    "    * x_min, x_max, y_min, y_max bbox coords\n",
    "    * datasets clipped to ROI ????\n",
    "    '''\n",
    "    if Sauthoff2023_outlines['name'].str.contains(ROI).any():\n",
    "        # Isolate individual lake using gpd buffer\n",
    "        lake_gpd = Sauthoff2023_outlines.loc[Sauthoff2023_outlines['name'] == lake]  \n",
    "        lake_buffer = lake_gpd.buffer(buffer)\n",
    "        # Define lake bounding box\n",
    "        x_min = lake_buffer.bounds.values[0,0]; x_max = lake_buffer.bounds.values[0,2]\n",
    "        y_min = lake_buffer.bounds.values[0,1]; y_max = lake_buffer.bounds.values[0,3]\n",
    "        \n",
    "        # # Clip dataset to region of interest\n",
    "        \n",
    "    elif IMBIE_basins['NAME'].str.contains(ROI).any():\n",
    "        # Isolate individual basin as geodf\n",
    "        basin = IMBIE_basins[IMBIE_basins['NAME'] == region_or_lake]\n",
    "\n",
    "        # Define region bounding box\n",
    "        x_min = basin.geometry.bounds.minx.values[0]; x_max = basin.geometry.bounds.maxx.values[0]\n",
    "        y_min = basin.geometry.bounds.miny.values[0]; y_max = basin.geometry.bounds.maxy.values[0]\n",
    "        \n",
    "        # # Clip dataset to IMBIE basin of interest\n",
    "        # ds_clipped = dataset.rio.clip(basin.geometry.values, basin.crs)\n",
    "        \n",
    "    elif len(ROI)==2:\n",
    "        x_min = ROI[0]-buffer; x_max = ROI[0]+buffer\n",
    "        y_min = ROI[1]-buffer; y_max = ROI[1]+buffer     \n",
    "    \n",
    "    elif len(ROI)==4: \n",
    "        x_min = ROI[0]; x_max = ROI[1]\n",
    "        y_min = ROI[2]; y_max = ROI[3]\n",
    "        \n",
    "    return x_min, x_max, y_min, y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs_file_paths(func_name):\n",
    "    '''\n",
    "    Make appropriate directories if they don't already exist and\n",
    "    create Path objects for the given file paths\n",
    "    \n",
    "    Inputs\n",
    "    * func name (str) for where you'd like to store output and\n",
    "    store the Path objects\n",
    "    \n",
    "    Outputs\n",
    "    * tuple: A tuple containing two Path objects representing the file paths.\n",
    "    '''\n",
    "    png_path = OUTPUT_DIR + '/figX_subglacial_lake_areas/{}/png'.format(func_name)\n",
    "    avi_path = OUTPUT_DIR + '/figX_subglacial_lake_areas/{}/avi'.format(func_name)\n",
    "    paths = [png_path, avi_path]\n",
    "    for i in paths: \n",
    "        if not os.path.exists(i):\n",
    "            os.makedirs(i)\n",
    "\n",
    "    return Path(png_path), Path(avi_path)\n",
    "\n",
    "# Example usage\n",
    "# (png_path, avi_path) = make_dirs_file_paths('find_evolving_contours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_evolving_outlines(x_min, x_max, y_min, y_max, threshold, dataset1, dataset2): \n",
    "    '''\n",
    "    Create planview dh/dt plots of ice surface height changes \n",
    "    Create time-variable outlines using skimage contour to plot evolving outlines as polygons.\n",
    "    \n",
    "    Inputs: \n",
    "    * x_min, x_max, y_min, y_max: polar stereographic coords\n",
    "    * threshold: vertical distance in meters to delineate ice surface deformation contour\n",
    "    * dataset1: dataset1 to be analyzed\n",
    "    * dataset2: dataset2 to be analyzed in conjunction with dataset1; currently configured to \n",
    "    splice CryoSat-2 and ICESat-2 eras\n",
    "    \n",
    "    Outputs: \n",
    "    * Sequence of planview delta height visuals of ICESat-2 ATL15 with variable ice surface \n",
    "    deformation contours plotted to delineate evolving lake boundaries.\n",
    "    * geopandas geodataframe of polygons created at each step (would need to modify to collect all polygons \n",
    "    at all time steps)\n",
    "    '''\n",
    "\n",
    "    # Subset CryoSat-2 SARIn data set to region of interest\n",
    "    mask_x = (dataset1.x >= x_min) & (dataset1.x <= x_max)\n",
    "    mask_y = (dataset1.y >= y_min) & (dataset1.y <= y_max)\n",
    "    ds1_clipped = dataset1.where(mask_x & mask_y, drop=True)\n",
    "    # Subset ATL15 data set to region of interest\n",
    "    mask_x = (dataset2.x >= x_min) & (dataset2.x <= x_max)\n",
    "    mask_y = (dataset2.y >= y_min) & (dataset2.y <= y_max)\n",
    "    ds2_clipped = dataset2.where(mask_x & mask_y, drop=True)\n",
    "\n",
    "    # Create lines for legend\n",
    "    uplift = plt.Line2D((0, 1), (0, 0), color='mediumblue', linestyle='dashdot', linewidth=2)\n",
    "    subsidence = plt.Line2D((0, 1), (0, 0), color='maroon', linestyle=(0, (3, 1, 1, 1)), linewidth=2)\n",
    "\n",
    "    # Find start, end, and mid-cycle dates of combined CryoSat-2 and ICESat-2 data\n",
    "    cyc_start_dates = []\n",
    "    cyc_end_dates = []\n",
    "    midcyc_dates = []\n",
    "    for idx in range(len(dataset1.delta_h[:-12])):\n",
    "        # Smith and others, 2017 method CryoSat-2 SARIn data\n",
    "        if dataset1.identifier_product_DOI == 'doi:10.5194/tc-11-451-2017':\n",
    "            cyc_start_date = datetime.datetime(int(dataset1.time.values[idx]), 1, 1) + datetime.timedelta(days = (dataset1.time.values[idx] % 1) * 365.25)\n",
    "            cyc_end_date = datetime.datetime(int(dataset1.time.values[idx+1]), 1, 1) + datetime.timedelta(days = (dataset1.time.values[idx+1] % 1) * 365.25)\n",
    "            midcyc_days = cyc_end_date - cyc_start_date\n",
    "            midcyc_date = cyc_start_date + midcyc_days/2\n",
    "            cyc_start_dates += [cyc_start_date]\n",
    "            cyc_end_dates += [cyc_end_date]\n",
    "            midcyc_dates += [midcyc_date]\n",
    "        # Cryo-TEMPO-EOLIS Swath Thematic Gridded Product \n",
    "        elif dataset1.Title == 'Land Ice Elevation Thematic Gridded Product':\n",
    "            date_time_str = '70-01-01'\n",
    "            date_time_obj = datetime.datetime.strptime(date_time_str, '%y-%m-%d')\n",
    "            newdate = date_time_obj + datetime.timedelta(seconds = ds_sub.time.values[idx].astype(float))\n",
    "            newdate1 = date_time_obj + datetime.timedelta(seconds = ds_sub.time.values[idx+1].astype(float))\n",
    "            midcycdays = newdate1 - newdate\n",
    "            midcycdate = newdate + midcycdays/2\n",
    "            midcycdates += [midcycdate]\n",
    "    for idx in range(len(dataset2.delta_h)-1): \n",
    "        # ICESat-2 ATL15 r003\n",
    "        if dataset2.identifier_product_DOI == 'NOT_SET':    \n",
    "            date_time_str = '18-01-01'\n",
    "            date_time_obj = datetime.datetime.strptime(date_time_str, '%y-%m-%d')\n",
    "            cyc_start_date = date_time_obj + datetime.timedelta(days=dataset2.time.values[idx])\n",
    "            cyc_end_date = date_time_obj + datetime.timedelta(days=dataset2.time.values[idx+1])\n",
    "            midcyc_days = cyc_end_date - cyc_start_date\n",
    "            midcyc_date = cyc_start_date + midcyc_days/2\n",
    "            cyc_start_dates += [cyc_start_date]\n",
    "            cyc_end_dates += [cyc_end_date]\n",
    "            midcyc_dates += [midcyc_date]\n",
    "        # ICESat-2 ATL15 r002\n",
    "        if dataset2.identifier_product_DOI == 'doi:10.5067/ATLAS/ATL15.002':    \n",
    "            date_time_str = '18-01-01'\n",
    "            date_time_obj = datetime.datetime.strptime(date_time_str, '%y-%m-%d')\n",
    "            newdate = date_time_obj + datetime.timedelta(days=dataset2.time.values[idx])\n",
    "            newdate1 = date_time_obj + datetime.timedelta(days=dataset2.time.values[idx+1])\n",
    "            midcycdays = newdate1 - newdate\n",
    "            midcycdate = newdate + midcycdays/2\n",
    "            midcycdates += [midcycdate]\n",
    "        # ICESat-2 ATL15 r001\n",
    "        elif dataset2.identifier_product_DOI == '10.5067/ATLAS/ATL15.001': \n",
    "            date_time_str = '18-01-01'\n",
    "            date_time_obj = datetime.datetime.strptime(date_time_str, '%y-%m-%d')\n",
    "            newdate = date_time_obj + datetime.timedelta(days=182.625 + 91.3125 * float(dataset2.band.values[idx]))\n",
    "            newdate1 = date_time_obj + datetime.timedelta(days=182.625 + 91.3125 * float(dataset2.band.values[idx+1]))\n",
    "            midcycdays = newdate1 - newdate\n",
    "            midcycdate = newdate + midcycdays/2\n",
    "            midcycdates += [midcycdate]\n",
    "\n",
    "    # Create empty list to store polygons, areas, perimeters and dates\n",
    "    polys = []\n",
    "    areas = []\n",
    "    perims = []\n",
    "    datetimes = []\n",
    "\n",
    "    # Calculate cycle-to-cycle dh/dt at each cycle of the spliced data sets\n",
    "    for idx in range(len(midcyc_dates)-1): \n",
    "        if idx < 35:\n",
    "            dhdt = ds1_clipped.delta_h[idx+1,:,:]-ds1_clipped.delta_h[idx,:,:]\n",
    "            cyc_start_date = cyc_start_dates[idx]\n",
    "            cyc_end_date = cyc_end_dates[idx]\n",
    "            midcycdate = midcyc_dates[idx]\n",
    "\n",
    "        elif idx > 34:\n",
    "            dhdt = ds2_clipped.delta_h[(idx-40)+1,:,:]-ds2_clipped.delta_h[idx-40,:,:]\n",
    "            cyc_start_date = cyc_start_dates[idx]\n",
    "            cyc_end_date = cyc_end_dates[idx]\n",
    "            midcycdate = midcyc_dates[idx]\n",
    "\n",
    "        # Create mapping conversion factor to map array location to polar stereographic x,y\n",
    "        x_conv = (x_max-x_min)/dhdt.shape[1]\n",
    "        y_conv = (y_max-y_min)/dhdt.shape[0]\n",
    "\n",
    "        # Create fig, ax\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        # Plot figure\n",
    "        img = ax.imshow(dhdt, extent=[x_min, x_max, y_min, y_max], \n",
    "            origin='upper', cmap='coolwarm_r', \n",
    "            norm=colors.CenteredNorm())\n",
    "        \n",
    "        # Create empty lists to store contours \n",
    "        contours_pos = []\n",
    "        contours_neg = []\n",
    "\n",
    "        # For loop to create contours of each threshold given \n",
    "        contour = measure.find_contours(dhdt.values, threshold)\n",
    "        if len(contour) > 0: \n",
    "            contours_pos += [contour]\n",
    "        contour = measure.find_contours(dhdt.values, -threshold)\n",
    "        if len(contour) > 0: \n",
    "            contours_neg += [contour]\n",
    "\n",
    "        # Plot contours and make into polygons\n",
    "        for i in range(len(contours_pos)): \n",
    "            for j in range(len(contours_pos[i])):\n",
    "                x = x_min+contours_pos[i][j][:,1]*x_conv\n",
    "                y = y_max-contours_pos[i][j][:,0]*y_conv\n",
    "                ax.plot(x, y, color='mediumblue', linestyle='dashdot', linewidth=1, label=threshold)\n",
    "                \n",
    "                # Make polygons from variable outlines and store to list\n",
    "                if len(contours_pos[i][j][:,1]) > 2: \n",
    "                    poly = Polygon(list(zip(x, y))) \n",
    "                    polys += [poly]\n",
    "                    \n",
    "                    # Calc polygon area and perimeter and store to lists\n",
    "                    lon, lat = XY_TO_LL.transform(x,y)\n",
    "                    poly_area = abs(GEOD.polygon_area_perimeter(lon, lat)[0])/1e6\n",
    "                    poly_perim = abs(GEOD.polygon_area_perimeter(lon, lat)[1])/1e3\n",
    "                    areas += [poly_area]\n",
    "                    perims += [poly_perim]\n",
    "                    datetimes += [midcycdate]\n",
    "\n",
    "        for i in range(len(contours_neg)): \n",
    "            for j in range(len(contours_neg[i])):\n",
    "                x = x_min+contours_neg[i][j][:,1]*x_conv\n",
    "                y = y_max-contours_neg[i][j][:,0]*y_conv\n",
    "                ax.plot(x, y, color='maroon', linestyle=(0, (3, 1, 1, 1)), linewidth=1, label=-threshold)\n",
    "                \n",
    "                # Make polygons from variable outlines\n",
    "                if len(contours_neg[i][j][:,1]) > 2: \n",
    "                    poly = Polygon(list(zip(x, y)))\n",
    "                    polys += [poly]\n",
    "                    \n",
    "                    # Calc polygon area and perimeter and store to lists\n",
    "                    lon, lat = XY_TO_LL.transform(x,y)\n",
    "                    poly_area = abs(GEOD.polygon_area_perimeter(lon, lat)[0])/1e6\n",
    "                    poly_perim = abs(GEOD.polygon_area_perimeter(lon, lat)[1])/1e3\n",
    "                    areas += [poly_area]\n",
    "                    perims += [poly_perim]\n",
    "                    datetimes += [midcycdate]\n",
    "\n",
    "        # Overlay published active lake outlines for visual comparison and grounding line\n",
    "        S09_color = 'cyan'\n",
    "        SF18_color  = 'darkcyan'\n",
    "        S23_color = 'deepskyblue'\n",
    "        Smith2009_outlines.boundary.plot(ax=ax, facecolor=S09_color, linestyle=(0, (1, 10)), linewidth=2, alpha=0.25)\n",
    "        Smith2009_outlines.boundary.plot(ax=ax, edgecolor=S09_color, linestyle=(0, (1, 10)), linewidth=2)\n",
    "        SiegfriedFricker2018_SF18outlines.boundary.plot(ax=ax, facecolor=SF18_color, linestyle=(0, (1, 5)), linewidth=2, alpha=0.25)\n",
    "        SiegfriedFricker2018_SF18outlines.boundary.plot(ax=ax, edgecolor=SF18_color, linestyle=(0, (1, 5)), linewidth=2)\n",
    "        Sauthoff2023_S23outlines.boundary.plot(ax=ax, facecolor=S23_color, linestyle=(0, (1, 1)), linewidth=2, alpha=0.25)\n",
    "        Sauthoff2023_S23outlines.boundary.plot(ax=ax, edgecolor=S23_color, linestyle=(0, (1, 1)), linewidth=2)\n",
    "        \n",
    "        # Change polar stereographic m to km\n",
    "        km_scale = 1e3\n",
    "        ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "        ax.xaxis.set_major_formatter(ticks_x)\n",
    "        ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "        ax.yaxis.set_major_formatter(ticks_y)  \n",
    "\n",
    "        # Label axes, set limits, and set title\n",
    "        ax.set_xlabel('x [m]', size=15)\n",
    "        ax.set_ylabel('y [m]', size=15) \n",
    "        ax.set(xlim=(x_min, x_max), ylim=(y_min, y_max))\n",
    "        ax.set_title('dh between {} and {}'.format(cyc_start_date.date(), \n",
    "            cyc_end_date.date()), size=15)\n",
    "\n",
    "        # Plot inset map\n",
    "        axIns = ax.inset_axes([-0.01, 0.01, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "        axIns.set_aspect('equal')\n",
    "        moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        \n",
    "        # Plot black rectangle to indicate location\n",
    "        # rect = Rectangle((x_min, y_min), (x_max-x_min), (y_max-y_min), fill=False, linewidth=2, color='k', zorder=3)\n",
    "        # axIns.add_artist(rect)\n",
    "        \n",
    "        # Plot red star to indicate location\n",
    "        axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "            linewidth=1, edgecolor='k', facecolor='r', s=100, zorder=3)\n",
    "        axIns.axis('off')\n",
    "\n",
    "        # Add colorbar \n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "        fig.colorbar(img, cax=cax).set_label('height change (dh) [m]', size=15)\n",
    "        \n",
    "        # Add legend\n",
    "        ax.legend([uplift, subsidence],\n",
    "            [('+ '+str(threshold)+' m uplift (filling) evolving outline'), \n",
    "             ('– '+str(threshold)+' m subsidence (draining) evolving outline')], \n",
    "            loc='upper left')\n",
    "        \n",
    "        # Save and close fig\n",
    "        plt.savefig(OUTPUT_DIR + \n",
    "                    '/figX_subglacial_lake_areas/find_evolving_outlines/png/evolving_contours_{}_{}km-buffer_{}m-threshold_{}-{}.png'\n",
    "                    .format(ROI, buffer/1e3, threshold, cyc_start_date.date(), cyc_end_date.date()), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # # Create avi video from figs created of each differenced time slice\n",
    "    # # Create video name\n",
    "    # video_name = '/evolving_contours_{}_{}km-buffer_{}m-threshold.avi'.format(ROI, buffer/1e3, threshold)\n",
    "    # # Store individual frames\n",
    "    # images = [img for img in os.listdir(png_path) if img.endswith(\".png\")]\n",
    "    # # Create frame dimension\n",
    "    # frame = cv2.imread(os.path.join(png_path, images[0]))\n",
    "    # height, width, layers = frame.shape\n",
    "    # # VideoWriter object will create a frame \n",
    "    # video = cv2.VideoWriter(avi_path + video_name, 0, 1, (width,height))\n",
    "    # for image in images:\n",
    "    #     video.write(cv2.imread(os.path.join(png_path, image)))\n",
    "    # # Close all the frames\n",
    "    # cv2.destroyAllWindows()\n",
    "    # # Release the video write object\n",
    "    # video.release()\n",
    "\n",
    "    # # Store polygons in geopandas geodataframe for further analysis\n",
    "    # d = {'geometry': polys, 'evolving area (km^2)': areas, 'perim (km)': perims, 'datetime': datetimes}\n",
    "    # gdf = gpd.GeoDataFrame(d, crs=\"EPSG:3031\")\n",
    "    # gdf['centroid'] = gdf['geometry'].centroid\n",
    "    # gdf['date'] = pd.to_datetime(gdf['datetime']).apply(lambda x: x.date())\n",
    "\n",
    "    return polys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_evolving_contours(polys): \n",
    "    '''\n",
    "    Func to make gdf of outlines once appropriate \n",
    "    height threshold selected\n",
    "    \n",
    "    Inputs: \n",
    "    * list of polygons created in find_evolving_contours func\n",
    "    \n",
    "    Outputs: \n",
    "    * geopandas geodataframe of polygons created at each step (would need to modify to collect all polygons \n",
    "    at all time steps)\n",
    "    '''\n",
    "    \n",
    "    # Store polygons in geopandas geodataframe for further analysis\n",
    "    d = {'geometry': polys, 'evolving area (km^2)': areas, 'perim (km)': perims, 'datetime': datetimes}\n",
    "    gdf = gpd.GeoDataFrame(d, crs=\"EPSG:3031\")\n",
    "    gdf['centroid'] = gdf['geometry'].centroid\n",
    "    gdf['date'] = pd.to_datetime(gdf['datetime']).apply(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evolving_outlines_time_series(gdf):\n",
    "    '''\n",
    "    Func to plot evolving outlines in aggregate\n",
    "    '''\n",
    "    # Set up dataset\n",
    "    dataset1 = CS2_dh\n",
    "    dataset2 = ATL15_dh\n",
    "\n",
    "    # Find mid-cycle dates of combined CS2 and IS2 data\n",
    "    dates = []\n",
    "    for idx in range(len(dataset1.delta_h[:-12])):\n",
    "        if dataset1.identifier_product_DOI == 'doi:10.5194/tc-11-451-2017':\n",
    "            newdate = datetime.datetime(int(dataset1.time.values[idx]), 1, 1) + datetime.timedelta(days = (dataset1.time.values[idx] % 1) * 365.25)\n",
    "            newdate1 = datetime.datetime(int(dataset1.time.values[idx+1]), 1, 1) + datetime.timedelta(days = (dataset1.time.values[idx+1] % 1) * 365.25)\n",
    "            midcycdays = newdate1 - newdate\n",
    "            midcycdate = newdate + midcycdays/2\n",
    "            dates += [midcycdate]\n",
    "        elif dataset1.Title == 'Land Ice Elevation Thematic Gridded Product':\n",
    "            date_time_str = '70-01-01'\n",
    "            date_time_obj = datetime.datetime.strptime(date_time_str, '%y-%m-%d')\n",
    "            newdate = date_time_obj + datetime.timedelta(seconds = ds_sub.time.values[idx].astype(float))\n",
    "            newdate1 = date_time_obj + datetime.timedelta(seconds = ds_sub.time.values[idx+1].astype(float))\n",
    "            midcycdays = newdate1 - newdate\n",
    "            midcycdate = newdate + midcycdays/2\n",
    "    for idx in range(len(dataset2.delta_h)-1): \n",
    "        if dataset2.identifier_product_DOI == 'doi:10.5067/ATLAS/ATL15.002':    \n",
    "            date_time_str = '18-01-01'\n",
    "            date_time_obj = datetime.datetime.strptime(date_time_str, '%y-%m-%d')\n",
    "            newdate = date_time_obj + datetime.timedelta(days=dataset2.time.values[idx])\n",
    "            newdate1 = date_time_obj + datetime.timedelta(days=dataset2.time.values[idx+1])\n",
    "            midcycdays = newdate1 - newdate\n",
    "            midcycdate = newdate + midcycdays/2\n",
    "            dates += [midcycdate]\n",
    "\n",
    "    # create fig, ax\n",
    "    fig, ax = plt.subplots(figsize=(15,15))\n",
    "\n",
    "    # Set colormap and normalize to date values\n",
    "    cmap = plt.get_cmap('plasma', len(dates)-1)\n",
    "    norm = plt.Normalize(datetime2fracyear(dates[0]), datetime2fracyear(dates[-1]))\n",
    "    m = plt.cm.ScalarMappable(cmap=cmap)\n",
    "    m.set_array(np.array([datetime2fracyear(date) for date in dates[0:-1]]))\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"bottom\", size=\"2.5%\", pad=0.9)\n",
    "    fig.colorbar(m, cax=cax, orientation='horizontal').set_label('evolving outline year', size=25)\n",
    "\n",
    "    # Add black background to see outlines better\n",
    "    rect = Rectangle((x_min, y_min), (x_max-x_min), (y_max-y_min), fill=True, linewidth=2, color='k', zorder=1)\n",
    "    ax.add_artist(rect) \n",
    "\n",
    "    # Overlay published active lake outlines for visual comparison and grounding line\n",
    "    S09_color = 'cyan'\n",
    "    SF18_color  = 'darkcyan'\n",
    "    S23_color = 'deepskyblue'\n",
    "    Smith2009_outlines.boundary.plot(ax=ax, facecolor=S09_color, linestyle=(0, (1, 10)), linewidth=2, alpha=0.25)\n",
    "    Smith2009_outlines.boundary.plot(ax=ax, edgecolor=S09_color, linestyle=(0, (1, 10)), linewidth=2)\n",
    "    SiegfriedFricker2018_SF18outlines.boundary.plot(ax=ax, facecolor=SF18_color, linestyle=(0, (1, 5)), linewidth=2, alpha=0.25)\n",
    "    SiegfriedFricker2018_SF18outlines.boundary.plot(ax=ax, edgecolor=SF18_color, linestyle=(0, (1, 5)), linewidth=2)\n",
    "    Sauthoff2023_S23outlines.boundary.plot(ax=ax, facecolor=S23_color, linestyle=(0, (1, 1)), linewidth=2, alpha=0.25)\n",
    "    Sauthoff2023_S23outlines.boundary.plot(ax=ax, edgecolor=S23_color, linestyle=(0, (1, 1)), linewidth=2)\n",
    "    moa_2014_groundingline.boundary.plot(ax=ax, edgecolor='w', linewidth=1)\n",
    "\n",
    "    # Use for loop to plot each outline in the geopandas dataframe and color by date\n",
    "    lines = []  # list of lines to be used for the legend\n",
    "    for idx, dt in enumerate(dates[0:-1], 0):\n",
    "        x = 1; y = 1\n",
    "        line, = ax.plot(x, y, color=cmap(norm(datetime2fracyear(dates[idx]))), linewidth=3)\n",
    "        lines.append(line)\n",
    "        gdf[gdf['datetime'] == dt].boundary.plot(ax=ax, \n",
    "            color=cmap(norm(datetime2fracyear(dates[idx])))\n",
    "            )\n",
    "\n",
    "    # Change polar stereographic m to km for cleaner-looking axes labels\n",
    "    km_scale = 1e3\n",
    "    ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax.xaxis.set_major_formatter(ticks_x)\n",
    "    ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax.yaxis.set_major_formatter(ticks_y)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Label axes and set limits\n",
    "    ax.set_xlabel('x [km]', size=25)\n",
    "    ax.set_ylabel('y [km]', size=25)\n",
    "    ax.set(xlim=(x_min, x_max), ylim=(y_min, y_max))\n",
    "\n",
    "    # Create lines for legend\n",
    "    Smith2009 = plt.Line2D((0, 1), (0, 0), color=S09_color, linestyle=(0, (1, 10)), linewidth=3)\n",
    "    SiegfriedFricker2018 = plt.Line2D((0, 1), (0, 0), color=SF18_color, linestyle=(0, (1, 5)), linewidth=3)\n",
    "    \n",
    "    # Create legend\n",
    "    ax.legend(handles=[Smith2009, \n",
    "        SiegfriedFricker2018, \n",
    "        tuple(lines)], \n",
    "        labels=['Smith and others, 2009 static outline',\n",
    "            'Siegfried & Fricker, 2018 outline', \n",
    "            'evolving outline ({} m threshold)'.format(threshold)], handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)})\n",
    "\n",
    "    # Plot inset map to show location \n",
    "    axIns = ax.inset_axes([0.01, 0.01, 0.25, 0.25]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "    axIns.set_aspect('equal')\n",
    "    moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    rect = Rectangle((x_min, y_min), (x_max-x_min), (y_max-y_min), fill=False, linewidth=2, color='k', zorder=3)\n",
    "    axIns.add_artist(rect) \n",
    "    axIns.axis('off')\n",
    "\n",
    "    plt.savefig(OUTPUT_DIR + '/figX_subglacial_lake_areas/plot_evolving_outlines/evolving_outlines_{}_{}km-buffer_{}m-threshold.png'\n",
    "        .format(ROI, buffer/1e3, threshold), dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_overlapping_polygons(gdf):\n",
    "    '''\n",
    "    Func to separate overlapping and non-overlapping polygons\n",
    "    stored in one geopandas geodataframe into two geopandas \n",
    "    geodataframes\n",
    "    \n",
    "    Inputs\n",
    "    * geopandas geodataframe\n",
    "    \n",
    "    Outputs\n",
    "    * two geopandas geodataframes, one with overlapping polygons,\n",
    "    one with non-overlapping polygons\n",
    "    '''\n",
    "    overlapping = []\n",
    "    non_overlapping = []\n",
    "    for n, p in enumerate(list(gdf.geometry), 0):\n",
    "        if any(p.overlaps(g) for g in list(gdf.geometry)):\n",
    "            # Store the index from the original dataframe\n",
    "            overlapping.append(n)\n",
    "        if not any(p.overlaps(g) for g in list(gdf.geometry)):\n",
    "            non_overlapping.append(n)\n",
    "\n",
    "    # Create a new dataframes and reset their indexes\n",
    "    gdf_overlapping = gdf.iloc[overlapping]  \n",
    "    gdf_overlapping.reset_index(drop=True, inplace=True)\n",
    "    gdf_non_overlapping = gdf.iloc[non_overlapping]\n",
    "    gdf_non_overlapping.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return gdf_overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_outlier_polygon(gdf, axis='x', max_or_min='max'):\n",
    "    '''\n",
    "    Func to remove outline outliers by removing a polygon\n",
    "    with the most extreme x or y coordinates\n",
    "    \n",
    "    Inputs\n",
    "    * geopandas geodataframe\n",
    "    * axis indicates whether the x or y axis will \n",
    "    have its extreme polygon removed\n",
    "    * max_or_min indicates whether the max or min centroid axis\n",
    "    value will be removed\n",
    "    \n",
    "    Ouputs\n",
    "    * geopandas geodataframe with one outline removed\n",
    "    '''\n",
    "    \n",
    "    # Check if the GeoDataFrame is empty\n",
    "    if gdf.empty:\n",
    "        print(\"GeoDataFrame is empty. Nothing to remove.\")\n",
    "        return gdf\n",
    "    \n",
    "    # Choose the axis and max or min for extreme value\n",
    "    if axis not in ['x', 'y']:\n",
    "        raise ValueError(\"Invalid axis. Use 'x' or 'y'.\")\n",
    "    if max_or_min not in ['max', 'min']:\n",
    "        raise ValueError(\"Invalid max_or_min. Use 'max' or 'min'.\")\n",
    "\n",
    "    # Calculate the centroid and extreme value\n",
    "    if axis == 'x':\n",
    "        if max_or_min == max:\n",
    "            extreme_value = gdf.centroid.x.abs().idxmax()\n",
    "        else:\n",
    "            extreme_value = gdf.centroid.x.abs().idxmin()\n",
    "    else:\n",
    "        if max_or_min == max:\n",
    "            extreme_value = gdf.centroid.y.abs().idxmax()\n",
    "        else:\n",
    "            extreme_value = gdf.centroid.y.abs().idxmin()\n",
    "\n",
    "    # Remove the row with the extreme value\n",
    "    gdf_filtered = gdf[gdf.index != extreme_value].copy()\n",
    "\n",
    "    print(f\"Removed polygon with extreme {max_or_min}{axis}-value at index {extreme_value}.\")\n",
    "\n",
    "    return gdf_filtered\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'gdf' is your GeoDataFrame\n",
    "# new_gdf = remove_extreme_polygon(gdf, axis='x', positive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def csv_export(ROI, gdf):\n",
    "    '''\n",
    "    Func to take modified gdf and calculated delta area per time slice and export as csv\n",
    "    '''\n",
    "    if Sauthoff2023_outlines['name'].str.contains(ROI).any():\n",
    "        # Isolate individual lake using gpd buffer\n",
    "        lake_gpd = Sauthoff2023_outlines.loc[Sauthoff2023_outlines['name'] == lake]  \n",
    "        gdf['static area (km^2)'] = np.divide(sum(lake_gpd.area), 1e6)\n",
    "    gdf.to_csv('area_time_series_{}.csv'.format(ROI),\n",
    "        #lake_gpd['Name'].values[0]), \n",
    "        columns=['date', 'static area (km^2)', 'evolving area (km^2)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_evolving_outlines_dArea(ROI, gdf):\n",
    "    '''\n",
    "    Func to plot evolving outlines in aggregate and the dArea time series\n",
    "    '''\n",
    "    # lake_S09 = Smith2009_outlines[(Smith2009_outlines['Name'] == 'Whillans_7')]\n",
    "    # S09_color = 'lightseagreen'\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, layout=\"constrained\", figsize=(15,5))\n",
    "\n",
    "    # Plot 0 - plot evolving outlines\n",
    "    # Set up dataset\n",
    "    dataset1 = CS2_dh\n",
    "    dataset2 = ATL15_dh\n",
    "\n",
    "    # Find mid-cycle dates of combined CS2 and IS2 data\n",
    "    dates = []\n",
    "    for idx in range(len(dataset1.delta_h[:-12])):\n",
    "        if dataset1.identifier_product_DOI == 'doi:10.5194/tc-11-451-2017':\n",
    "            newdate = datetime.datetime(int(dataset1.time.values[idx]), 1, 1) + datetime.timedelta(days = (dataset1.time.values[idx] % 1) * 365.25)\n",
    "            newdate1 = datetime.datetime(int(dataset1.time.values[idx+1]), 1, 1) + datetime.timedelta(days = (dataset1.time.values[idx+1] % 1) * 365.25)\n",
    "            midcycdays = newdate1 - newdate\n",
    "            midcycdate = newdate + midcycdays/2\n",
    "            dates += [midcycdate]\n",
    "        elif dataset1.Title == 'Land Ice Elevation Thematic Gridded Product':\n",
    "            date_time_str = '70-01-01'\n",
    "            date_time_obj = datetime.datetime.strptime(date_time_str, '%y-%m-%d')\n",
    "            newdate = date_time_obj + datetime.timedelta(seconds = ds_sub.time.values[idx].astype(float))\n",
    "            newdate1 = date_time_obj + datetime.timedelta(seconds = ds_sub.time.values[idx+1].astype(float))\n",
    "            midcycdays = newdate1 - newdate\n",
    "            midcycdate = newdate + midcycdays/2\n",
    "    for idx in range(len(dataset2.delta_h)-1): \n",
    "        if dataset2.identifier_product_DOI == 'doi:10.5067/ATLAS/ATL15.002':    \n",
    "            date_time_str = '18-01-01'\n",
    "            date_time_obj = datetime.datetime.strptime(date_time_str, '%y-%m-%d')\n",
    "            newdate = date_time_obj + datetime.timedelta(days=dataset2.time.values[idx])\n",
    "            newdate1 = date_time_obj + datetime.timedelta(days=dataset2.time.values[idx+1])\n",
    "            midcycdays = newdate1 - newdate\n",
    "            midcycdate = newdate + midcycdays/2\n",
    "            dates += [midcycdate]\n",
    "\n",
    "    # Set colormap and normalize to date values\n",
    "    cmap = plt.get_cmap('plasma', len(dates)-1)\n",
    "    norm = plt.Normalize(datetime2fracyear(dates[0]), datetime2fracyear(dates[-1]))\n",
    "    m = plt.cm.ScalarMappable(cmap=cmap)\n",
    "    m.set_array(np.array([datetime2fracyear(date) for date in dates[0:-1]]))\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"bottom\", size=\"2.5%\", pad=0.9)\n",
    "    fig.colorbar(m, cax=cax, orientation='horizontal').set_label('evolving outline year', size=25)\n",
    "\n",
    "    # Add black background to see outlines better\n",
    "    rect = Rectangle((x_min, y_min), (x_max-x_min), (y_max-y_min), fill=True, linewidth=2, color='k', zorder=1)\n",
    "    ax.add_artist(rect) \n",
    "\n",
    "    # Overlay published active lake outlines for visual comparison and grounding line\n",
    "    S09_color = 'cyan'\n",
    "    SF18_color  = 'darkcyan'\n",
    "    S23_color = 'deepskyblue'\n",
    "    Smith2009_outlines.boundary.plot(ax=ax, facecolor=S09_color, linestyle=(0, (1, 10)), linewidth=2, alpha=0.25)\n",
    "    Smith2009_outlines.boundary.plot(ax=ax, edgecolor=S09_color, linestyle=(0, (1, 10)), linewidth=2)\n",
    "    SiegfriedFricker2018_SF18outlines.boundary.plot(ax=ax, facecolor=SF18_color, linestyle=(0, (1, 5)), linewidth=2, alpha=0.25)\n",
    "    SiegfriedFricker2018_SF18outlines.boundary.plot(ax=ax, edgecolor=SF18_color, linestyle=(0, (1, 5)), linewidth=2)\n",
    "    Sauthoff2023_S23outlines.boundary.plot(ax=ax, facecolor=S23_color, linestyle=(0, (1, 1)), linewidth=2, alpha=0.25)\n",
    "    Sauthoff2023_S23outlines.boundary.plot(ax=ax, edgecolor=S23_color, linestyle=(0, (1, 1)), linewidth=2)\n",
    "    moa_2014_groundingline.boundary.plot(ax=ax, edgecolor='w', linewidth=1)\n",
    "\n",
    "    # Use for loop to plot each outline in the geopandas dataframe and color by date\n",
    "    lines = []  # list of lines to be used for the legend\n",
    "    for idx, dt in enumerate(dates[0:-1], 0):\n",
    "        x = 1; y = 1\n",
    "        line, = ax.plot(x, y, color=cmap(norm(datetime2fracyear(dates[idx]))), linewidth=3)\n",
    "        lines.append(line)\n",
    "        gdf[gdf['datetime'] == dt].boundary.plot(ax=ax, \n",
    "            color=cmap(norm(datetime2fracyear(dates[idx])))\n",
    "            )\n",
    "\n",
    "    # Change polar stereographic m to km for cleaner-looking axes labels\n",
    "    km_scale = 1e3\n",
    "    ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax.xaxis.set_major_formatter(ticks_x)\n",
    "    ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax.yaxis.set_major_formatter(ticks_y)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Label axes and set limits\n",
    "    ax.set_xlabel('x [km]', size=25)\n",
    "    ax.set_ylabel('y [km]', size=25)\n",
    "    ax.set(xlim=(x_min, x_max), ylim=(y_min, y_max))\n",
    "\n",
    "    # Create lines for legend\n",
    "    Smith2009 = plt.Line2D((0, 1), (0, 0), color=S09_color, linestyle=(0, (1, 10)), linewidth=3)\n",
    "    SiegfriedFricker2018 = plt.Line2D((0, 1), (0, 0), color=SF18_color, linestyle=(0, (1, 5)), linewidth=3)\n",
    "    \n",
    "    # Create legend\n",
    "    ax.legend(handles=[Smith2009, \n",
    "        SiegfriedFricker2018, \n",
    "        tuple(lines)], \n",
    "        labels=['Smith and others, 2009 static outline',\n",
    "            'Siegfried & Fricker, 2018 outline', \n",
    "            'evolving outline ({} m threshold)'.format(threshold)], handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)})\n",
    "\n",
    "    # Plot inset map to show location \n",
    "    axIns = ax.inset_axes([0.01, 0.01, 0.25, 0.25]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "    axIns.set_aspect('equal')\n",
    "    moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    rect = Rectangle((x_min, y_min), (x_max-x_min), (y_max-y_min), fill=False, linewidth=2, color='k', zorder=3)\n",
    "    axIns.add_artist(rect) \n",
    "    axIns.axis('off')\n",
    "\n",
    "    # Plot 1 - plot area change time series\n",
    "    axs[1].axhline(np.divide(sum(lake_S09.area), 1e6), color=S09_color, linestyle=(0, (1, 4)))\n",
    "    axs[1].plot(dates_unique, areas_evolving, color='k', linestyle=(0, (1, 1)))\n",
    "    locator = mdates.AutoDateLocator(minticks=1, maxticks=7)\n",
    "    formatter = mdates.ConciseDateFormatter(locator)\n",
    "    axs[1].xaxis.set_major_locator(locator)\n",
    "    axs[1].xaxis.set_major_formatter(formatter)\n",
    "    min_area = min(np.divide(sum(lake_S09.area), 1e6), \n",
    "                    # np.divide(sum(lake_SF18.area), 1e6), \n",
    "                    min(areas_evolving))\n",
    "    max_area = max(np.divide(sum(lake_S09.area), 1e6), \n",
    "                    # np.divide(sum(lake_SF18.area), 1e6), \n",
    "                    max(areas_evolving))\n",
    "    axs[1].set(ylim=((min_area - (max_area - min_area)*0.05), (max_area + (max_area - min_area)*0.05)),\n",
    "        xlim=(dates[0], dates[-1]))\n",
    "    axs[1].set_ylabel('lake area [km$^2$]', size=17.5, labelpad=4)\n",
    "    axs[1].yaxis.tick_right()\n",
    "    axs[1].yaxis.set_label_position(\"right\")\n",
    "    Smith2009 = plt.Line2D((0, 1), (0, 0), color=S09_color, linestyle=(0, (1, 4)), linewidth=3)\n",
    "    evolving_outlines = plt.Line2D((0, 1), (0, 0), color='k', linestyle=(0, (1, 1)), linewidth=3)\n",
    "    legend2 = axs[1].legend([Smith2009,\n",
    "        #SiegfriedFricker2018,\n",
    "        evolving_outlines],\n",
    "                    ['static outline',\n",
    "        #'Siegfried & Fricker, 2018 static outline',\n",
    "        # '±{} m variable outlines'.format(thres)], \n",
    "        'evolving outlines'],\n",
    "        loc='upper right')\n",
    "    legend2.get_frame().set_linewidth(0.0)\n",
    "    axs[1].patch.set_alpha(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROI = 'Whillans_7'\n",
    "buffer = 5000\n",
    "\n",
    "x_min, x_max, y_min, y_max =  find_bbox_coords(ROI, buffer)\n",
    "\n",
    "# Manually set bbox coords because of new lobe on Whillans7\n",
    "x_min = -560e3; x_max = -525e3\n",
    "y_min = -512e3; y_max = -494e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(png_path, avi_path) = make_dirs_file_paths('find_evolving_outlines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "thresholds = [0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "for threshold in thresholds: \n",
    "    find_evolving_outlines(x_min, x_max, y_min, y_max, threshold, CS2_dh, ATL15_dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polys = find_evolving_contours(x_min, x_max, y_min, y_max, threshold, CS2_dh, ATL15_dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = store_evolving_contours(polys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evolving_outlines_time_series(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_overlapping = find_overlapping_polygons(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_filtered = remove_outlier_polygon(gdf_overlapping, x, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evolving_outlines_time_series(gdf_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_export(ROI, gdf_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evolving_outlines_dArea(ROI, gdf_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(Sauthoff2023_outlines)):\n",
    "    ROI = Sauthoff2023_outlines['name'][idx]\n",
    "    buffer = 5000\n",
    "    threshold = 0.5\n",
    "    gdf = S09SF18S23_outline_timeseries_gdf(lakename, buffer, threshold, CS2_dh, ATL15_dh)\n",
    "    plot_variable_outlines(gdf, lakename, buffer, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Scrap code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Editing to stitch CS2 and IS2 data together\n",
    "\n",
    "thres=0.5\n",
    "dataset1=CS2_dh\n",
    "dataset2=ATL15_dh\n",
    "\n",
    "lake_S09 = Smith2009_outlines[(Smith2009_outlines['Name'] == 'Whillans_7')]\n",
    "lake_SF18 = SiegfriedFricker2018_outlines[SiegfriedFricker2018_outlines['name'] == 'Whillans_7']\n",
    "S09_color = 'lightseagreen'\n",
    "SF18_color = 'teal'\n",
    "\n",
    "# Manually set because of new lake lobe\n",
    "x_min = -560e3; x_max = -525e3\n",
    "y_min = -512e3; y_max = -494e3\n",
    "\n",
    "# Subset dataset and MOA imagery to region of interest using data masks\n",
    "mask_x = (dataset1.x >= x_min) & (dataset1.x <= x_max)\n",
    "mask_y = (dataset1.y >= y_min) & (dataset1.y <= y_max)\n",
    "ds_sub1 = dataset1.where(mask_x & mask_y, drop=True)\n",
    "mask_x = (dataset2.x >= x_min) & (dataset2.x <= x_max)\n",
    "mask_y = (dataset2.y >= y_min) & (dataset2.y <= y_max)\n",
    "ds_sub2 = dataset2.where(mask_x & mask_y, drop=True)\n",
    "# mask_x = (moa_highres_da.x >= x_min) & (moa_highres_da.x <= x_max)\n",
    "# mask_y = (moa_highres_da.y >= y_min) & (moa_highres_da.y <= y_max)\n",
    "# moa_highres_da_sub = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "\n",
    "# get dates\n",
    "dates = []\n",
    "for idx in range(len(dataset1.delta_h[:-12])):\n",
    "    dhdt = dataset1.delta_h[idx+1,:,:]-dataset1.delta_h[idx,:,:]\n",
    "    # calculate mid-cycle dates for plotting\n",
    "    newdate = datetime.datetime(int(dataset1.time.values[idx]), 1, 1) + datetime.timedelta(days = (dataset1.time.values[idx] % 1) * 365.25)\n",
    "    newdate1 = datetime.datetime(int(dataset1.time.values[idx+1]), 1, 1) + datetime.timedelta(days = (dataset1.time.values[idx+1] % 1) * 365.25)\n",
    "    midcycdays = newdate1 - newdate\n",
    "    midcycdate = newdate + midcycdays/2\n",
    "    dates += [midcycdate]\n",
    "for idx in range(len(dataset2.delta_h)-1): \n",
    "    dhdt = dataset2.delta_h[idx+1,:,:]-dataset2.delta_h[idx,:,:]\n",
    "    date_time_str = '18-01-01'\n",
    "    date_time_obj = datetime.datetime.strptime(date_time_str, '%y-%m-%d')\n",
    "    newdate = date_time_obj + datetime.timedelta(days=dataset2.time.values[idx])\n",
    "    newdate1 = date_time_obj + datetime.timedelta(days=dataset2.time.values[idx+1])\n",
    "    midcycdays = newdate1 - newdate\n",
    "    midcycdate = newdate + midcycdays/2\n",
    "    dates += [midcycdate]\n",
    "        \n",
    "# Pick colormap and make continuous cmap discrete\n",
    "cmap = cm.get_cmap('plasma', len(dates)-1)\n",
    "# Norm to time variable\n",
    "norm = plt.Normalize(ds_sub1.time.values[0], ds_sub2.time.values[-1])\n",
    "\n",
    "# Plot figure\n",
    "fig, axs = plt.subplots(1, 2, layout=\"constrained\", figsize=(10,5))#, gridspec_kw={'width_ratios': [2, 1, 1], 'height_ratios': [1]})\n",
    "\n",
    "# Overlay published active lake outlines for visual comparison\n",
    "Smith2009_outlines.boundary.plot(ax=axs[0], facecolor='cyan', linestyle=(0, (1, 5)), linewidth=1, alpha=0.25)\n",
    "Smith2009_outlines.boundary.plot(ax=axs[0], edgecolor='cyan', linestyle=(0, (1, 5)), linewidth=1)\n",
    "SiegfriedFricker2018_SF18outlines.boundary.plot(ax=axs[0], facecolor='darkcyan', linestyle=(0, (1, 1)), linewidth=1, alpha=0.25)\n",
    "SiegfriedFricker2018_SF18outlines.boundary.plot(ax=axs[0], edgecolor='darkcyan', linestyle=(0, (1, 1)), linewidth=1)\n",
    "Sauthoff2023_S23outlines.boundary.plot(ax=axs[0], facecolor='deepskyblue', linestyle=(0, (1, 10)), linewidth=1, alpha=0.25)\n",
    "Sauthoff2023_S23outlines.boundary.plot(ax=axs[0], edgecolor='deepskyblue', linestyle=(0, (1, 10)), linewidth=1)\n",
    "\n",
    "# Set axes limits\n",
    "axs[0].set(#xticks=[], yticks=[], \n",
    "           xlim=(x_min, x_max), ylim=(y_min, y_max))\n",
    "\n",
    "# Change polar stereographic m to km for cleaner-looking axes\n",
    "km_scale = 1e3\n",
    "ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "axs[0].xaxis.set_major_formatter(ticks_x)\n",
    "ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "axs[0].yaxis.set_major_formatter(ticks_y)  \n",
    "\n",
    "# Label axes\n",
    "axs[0].set_xlabel('x [km]', size=20)\n",
    "axs[0].set_ylabel('y [km]', size=20)\n",
    "\n",
    "# Create lines for legend\n",
    "Smith2009 = plt.Line2D((0, 1), (0, 0), color=S09_color, linestyle=(0, (1, 5)), linewidth=4)\n",
    "SiegfriedFricker2018 = plt.Line2D((0, 1), (0, 0), color=SF18_color, linestyle=(0, (1, 1)), linewidth=3)\n",
    "uplift = plt.Line2D((0, 1), (0, 0), color='k', linestyle=(0, (3, 1, 1, 1)), linewidth=3)\n",
    "subsidence = plt.Line2D((0, 1), (0, 0), color='k', linestyle=(0, (5, 1)), linewidth=3)\n",
    "\n",
    "# Plot inset map to show location \n",
    "axIns = axs[0].inset_axes([0.72, 0.68, 0.5, 0.5]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "axIns.set_aspect('equal')\n",
    "moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=1)\n",
    "moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=1) \n",
    "axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', linewidth=1, edgecolor='k', facecolor='r', s=150, zorder=3)\n",
    "axIns.axis('off')\n",
    "\n",
    "# Create colorbar \n",
    "m = plt.cm.ScalarMappable(cmap=cmap)\n",
    "m.set_array(np.array([datetime2fracyear(date) for date in dates[0:]]))\n",
    "cax = inset_axes(axs[0],\n",
    "                    width=\"100%\",  \n",
    "                    height=\"10%\",\n",
    "                    loc='lower center',\n",
    "                    borderpad=-7\n",
    "                   )\n",
    "fig.colorbar(m, cax=cax, orientation='horizontal').set_label('Variable lake outline year', size=15)\n",
    "\n",
    "# Create empty lists to store calculated data\n",
    "lkavgdhdt_S09 = []\n",
    "lkavgdhdt_SF18 = []\n",
    "lkavgdhdt_var = []\n",
    "areas_var = []\n",
    "vols_S09 = []\n",
    "vols_SF18 = []\n",
    "vols_var = []\n",
    "\n",
    "for idx in range(len(dates)-1):\n",
    "    # Stitch together CryoSat-2 and ICESat-2 time series\n",
    "    if idx < 35:\n",
    "        dhdt = ds_sub1.delta_h[idx+1,:,:]-ds_sub1.delta_h[idx,:,:]\n",
    "    elif idx > 34:\n",
    "        dhdt = ds_sub2.delta_h[(idx-40)+1,:,:]-ds_sub2.delta_h[(idx-40),:,:]\n",
    "\n",
    "    # Clip dh/dt to published static outlines\n",
    "    dhdt.rio.write_crs(3031, inplace=True)\n",
    "    dhdt_clip_S09 = dhdt.rio.clip(lake_S09.geometry.values, lake_S09.crs, drop=False, invert=False)\n",
    "    dhdt_clip_SF18 = dhdt.rio.clip(lake_SF18.geometry.values, lake_SF18.crs, drop=False, invert=False)\n",
    "\n",
    "    # Calculate lake avg. dh/dt and dv/dt time series using published stationary outlines\n",
    "    avg_lk_dhdt_S09 = np.nanmean(dhdt_clip_S09)\n",
    "    avg_lk_dhdt_SF18 = np.nanmean(dhdt_clip_SF18)\n",
    "    lkavgdhdt_S09 += [avg_lk_dhdt_S09]\n",
    "    lkavgdhdt_SF18 += [avg_lk_dhdt_SF18]\n",
    "    vol_S09 = avg_lk_dhdt_S09*sum(lake_S09.area) # FIXME: change to geodesic area\n",
    "    vol_SF18 = avg_lk_dhdt_SF18*sum(lake_SF18.area)\n",
    "    vols_S09 += [vol_S09]\n",
    "    vols_SF18 += [vol_SF18]\n",
    "\n",
    "    # Create contours of ice surface elevation height changes to delineate lake outlines\n",
    "    contours_fill = []\n",
    "    contours_drain = []\n",
    "    polys = []\n",
    "    contour = measure.find_contours(dhdt.values, thres)\n",
    "    if len(contour) > 0: \n",
    "        contours_fill += [contour]\n",
    "    contour = measure.find_contours(dhdt.values, -thres)\n",
    "    if len(contour) > 0: \n",
    "        contours_drain += [contour]\n",
    "\n",
    "    # Create mapping conversion factor to map array location to polar stereographic x,y\n",
    "    x_conv = (x_max-x_min)/dhdt.shape[1]\n",
    "    y_conv = (y_max-y_min)/dhdt.shape[0]\n",
    "\n",
    "    # Plot and make polygons from variable outlines\n",
    "    for i in range(len(contours_fill)): \n",
    "        for j in range(len(contours_fill[i])):\n",
    "            if len(contours_fill[i][j][:, 1]) > 2: \n",
    "                axs[0].plot(x_min+contours_fill[i][j][:, 1]*x_conv, y_max-contours_fill[i][j][:, 0]*y_conv, \n",
    "                    # color=cmap(norm(dates[idx])), \n",
    "                    linestyle=(0, (3, 1, 1, 1)), linewidth=1, zorder=3)\n",
    "                poly = Polygon(list(zip(x_min+contours_fill[i][j][:, 1]*x_conv, y_max-contours_fill[i][j][:, 0]*y_conv)))\n",
    "                polys += [poly]\n",
    "    for i in range(len(contours_drain)): \n",
    "        for j in range(len(contours_drain[i])):\n",
    "            if len(contours_drain[i][j][:, 1]) > 2: \n",
    "                axs[0].plot(x_min+contours_drain[i][j][:, 1]*x_conv, y_max-contours_drain[i][j][:, 0]*y_conv, \n",
    "                    # color=cmap(norm(dates[idx])), \n",
    "                    linestyle=(0, (5, 1)), linewidth=1, zorder=3)\n",
    "                poly = Polygon(list(zip(x_min+contours_drain[i][j][:, 1]*x_conv, y_max-contours_drain[i][j][:, 0]*y_conv))) \n",
    "                polys += [poly]\n",
    "\n",
    "    # Start with baseline variable outline area of zero\n",
    "    area_var = 0\n",
    "    # If polygons are present at time step, \n",
    "    if len(polys) > 0: \n",
    "        # Clip data to polygons \n",
    "        dhdt_clip = dhdt.rio.clip(polys, drop=False, invert=False)\n",
    "        # Then calculate on-lake averages of dh/dt\n",
    "        avg_lk_dhdt = np.nanmean(dhdt_clip)\n",
    "        # Occasionally small polygons will completely clip data resulting in nan's for lake avg. dh/dt\n",
    "        # eplace with zeros\n",
    "        if math.isnan(avg_lk_dhdt): \n",
    "            avg_lk_dhdt = 0\n",
    "            lkavgdhdt_var += [avg_lk_dhdt]\n",
    "        else:\n",
    "            lkavgdhdt_var += [avg_lk_dhdt]                \n",
    "        # and dv/dt\n",
    "        for i in range(len(polys)):\n",
    "            area_var = area_var + polys[i].area # TODO: CHANGE TO GEODESIC AREA\n",
    "        # Store areas in list \n",
    "        areas_var += [area_var]\n",
    "        # Calculate and store volumes in list\n",
    "        vol_var = avg_lk_dhdt*area_var\n",
    "        vols_var += [vol_var]\n",
    "    else: \n",
    "        # Store areas in list \n",
    "        areas_var += [area_var]\n",
    "        # Set lake dhdt to zero because no variable outline polygons\n",
    "        avg_lk_dhdt = 0\n",
    "        # Store dhdt's in list \n",
    "        lkavgdhdt_var += [avg_lk_dhdt]\n",
    "        # Calculate variable volumes\n",
    "        vol_var = avg_lk_dhdt*area_var\n",
    "        # Store variable volumes in list\n",
    "        vols_var += [vol_var]\n",
    "\n",
    "# Calculate bias\n",
    "# S09_var_dh_bias = [a_i - b_i for a_i, b_i in zip(lkavgdhdt_var, lkavgdhdt_S09)]\n",
    "# S09_var_vol_bias = [a_i - b_i for a_i, b_i in zip(np.cumsum(vols_var), np.cumsum(vols_S09))]\n",
    "\n",
    "# Plot 1 - plot area change time series\n",
    "axs[1].axhline(np.divide(sum(lake_S09.area), 1e6), color=S09_color, linestyle=(0, (1, 5)))\n",
    "axs[1].plot(dates[:idx+1], np.divide(areas_var[:idx+1], 1e6), color='k', linestyle='solid')\n",
    "locator = mdates.AutoDateLocator(minticks=1, maxticks=7)\n",
    "formatter = mdates.ConciseDateFormatter(locator)\n",
    "axs[1].xaxis.set_major_locator(locator)\n",
    "axs[1].xaxis.set_major_formatter(formatter)\n",
    "min_area = min(np.divide(sum(lake_S09.area), 1e6), \n",
    "                np.divide(sum(lake_SF18.area), 1e6), \n",
    "                min(np.divide(np.cumsum(areas_var),1e6)))\n",
    "max_area = max(np.divide(sum(lake_S09.area), 1e6), \n",
    "                np.divide(sum(lake_SF18.area), 1e6), \n",
    "                max(np.divide(np.cumsum(areas_var),1e6)))\n",
    "axs[1].set(ylim=((min_area - (max_area - min_area)*0.05), (max_area + (max_area - min_area)*0.05)))\n",
    "axs[1].set_ylabel('area [km$^2$]', size=17.5, labelpad=4)\n",
    "axs[1].yaxis.tick_right()\n",
    "axs[1].yaxis.set_label_position(\"right\")\n",
    "variable_outlines = plt.Line2D((0, 1), (0, 0), color='k', linestyle='solid', linewidth=3)\n",
    "legend2 = axs[1].legend([Smith2009,\n",
    "    #SiegfriedFricker2018,\n",
    "    variable_outlines],\n",
    "                ['static outline',\n",
    "    #'Siegfried & Fricker, 2018 static outline',\n",
    "    # '±{} m variable outlines'.format(thres)], \n",
    "    'variable outlines'],\n",
    "    loc='upper right')\n",
    "legend2.get_frame().set_linewidth(0.0)\n",
    "axs[1].patch.set_alpha(1)\n",
    "\n",
    "# # Plot 2 - height change time series\n",
    "# axs[2].plot(dates[:-1], lkavgdhdt_S09, color=S09_color, linestyle=(0, (1, 5)))\n",
    "# axs[2].plot(dates[:-1], lkavgdhdt_var, color='k', linestyle='solid')\n",
    "# axs[2].plot(dates[:-1], S09_var_dh_bias, color='r', linestyle='solid')\n",
    "# locator = mdates.AutoDateLocator(minticks=1, maxticks=7)\n",
    "# formatter = mdates.ConciseDateFormatter(locator)\n",
    "# axs[2].xaxis.set_major_locator(locator)\n",
    "# axs[2].xaxis.set_major_formatter(formatter)\n",
    "# axs[2].set_xlabel('year', size=17.5)\n",
    "# axs[2].set_ylabel('height change [m]', size=17.5, labelpad=5)\n",
    "# axs[2].yaxis.tick_right()\n",
    "# axs[2].yaxis.set_label_position(\"right\")\n",
    "# bias = plt.Line2D((0, 1), (0, 0), color='r', linestyle='solid', linewidth=3)\n",
    "# legend = axs[2].legend([#Smith2009,\n",
    "#     #SiegfriedFricker2018,\n",
    "#     bias],\n",
    "#                 [#'static outline',\n",
    "#     #'Siegfried & Fricker, 2018 static outline',\n",
    "#     # '±{} m variable outlines'.format(thres)], \n",
    "#     'bias'],\n",
    "#     loc='lower right')\n",
    "# legend.get_frame().set_linewidth(0.0)\n",
    "# axs[2].patch.set_alpha(1)\n",
    "\n",
    "# # Plot 3 - volume change time series\n",
    "# axs[3].plot(dates[:-1], np.divide(np.cumsum(vols_S09), 1e+9), color=S09_color, linestyle=(0, (1, 5)))\n",
    "# axs[3].plot(dates[:-1], np.divide(np.cumsum(vols_var), 1e+9), color='k', linestyle='solid')\n",
    "# axs[3].plot(dates[:-1], np.divide(S09_var_vol_bias, 1e+9), color='r', linestyle='solid')\n",
    "# locator = mdates.AutoDateLocator(minticks=1, maxticks=7)\n",
    "# formatter = mdates.ConciseDateFormatter(locator)\n",
    "# axs[3].xaxis.set_major_locator(locator)\n",
    "# axs[3].xaxis.set_major_formatter(formatter)\n",
    "# min_vol = min(np.divide(min(np.cumsum(vols_S09)), 1e+9), \n",
    "#             #   np.divide(min(np.cumsum(vols_SF18)), 1e+9), \n",
    "#                 np.divide(min(np.cumsum(vols_var)), 1e+9))\n",
    "# max_vol = max(np.divide(max(np.cumsum(vols_S09)), 1e+9), \n",
    "#             #   np.divide(max(np.cumsum(vols_SF18)), 1e+9), \n",
    "#                 np.divide(max(np.cumsum(vols_var)), 1e+9))\n",
    "\n",
    "# axs[3].set(xlim=(dates[0], dates[-1]), \n",
    "#             ylim=((min_vol - (max_vol - min_vol)*0.05),\n",
    "#                     (max_vol + (max_vol - min_vol)*0.05)))\n",
    "# axs[3].set_xlabel('year', size=17.5)\n",
    "# axs[3].set_ylabel('vol. change [km$^3$]', size=17.5, labelpad=0)\n",
    "# axs[3].yaxis.tick_right()\n",
    "# axs[3].yaxis.set_label_position(\"right\")\n",
    "# axs[3].patch.set_alpha(1)\n",
    "\n",
    "# Save and close figure\n",
    "# plt.savefig('/Users/Wilson/Documents/0-code/output/output_S09SF18varoutlines-visual-imagery/S09SF18varoutlines_agg_moa_plot/regions/S09SF18varoutlines_agg_moa_plot-{}-{}.png'.format(region_name,dataset.Title), dpi=300, bbox_inches = \"tight\") # change to name after region name given as func input?\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# (left) colormap of outlines not matching colorbar\n",
    "# (right) fix y axis bounds on da/dt plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def S09SF18S23_outline_timeseries_gdf(ROI, buffer, threshold, dataset1, dataset2):\n",
    "    '''\n",
    "    Create time series of variable outlines compared to known lakes or across a larger region \n",
    "    to additionally find lake candidates. Uses geopandas buffer created bounding box around \n",
    "    previously  identified lakes in Siegfried and Fricker, 2018 (SF18) inventory if lake is input.\n",
    "    Creates contours of surface-height change using skimage contour to create evolving outlines. \n",
    "    \n",
    "    Inputs: \n",
    "        ROI: lake of interest from SiegfriedFricker2018_outlines inventory\n",
    "        buffer: horizontal distance in meters away from lake outline used to create bounding box that is displayed around lake of interest\n",
    "        threshold: vertical distance in meters to delineate ice surface deformation contour\n",
    "        dataset1: dataset to be analyzed (CS2_dh or CS2SARIn_elev)\n",
    "        dataset2: dataset to be analyzed (ATL15_dh)\n",
    "    \n",
    "    Outputs: \n",
    "        gdf: geodataframe of variable outlines\n",
    "    '''\n",
    "    \n",
    "    x_min, x_max, y_min, y_max = find_bbox_coords(ROI, buffer)    \n",
    "    \n",
    "    # Subset CryoSat-2 SARIn data set to region of interest\n",
    "    mask_x = (dataset1.x >= x_min) & (dataset1.x <= x_max)\n",
    "    mask_y = (dataset1.y >= y_min) & (dataset1.y <= y_max)\n",
    "    ds1_clipped = dataset1.where(mask_x & mask_y, drop=True)\n",
    "    \n",
    "    # Subset ATL15 data set to region of interest\n",
    "    mask_x = (dataset2.x >= x_min) & (dataset2.x <= x_max)\n",
    "    mask_y = (dataset2.y >= y_min) & (dataset2.y <= y_max)\n",
    "    ds2_clipped = dataset2.where(mask_x & mask_y, drop=True)\n",
    "\n",
    "    # Create empty list to store polygons, areas, perimeters and dates\n",
    "    polys = []\n",
    "    areas = []\n",
    "    perims = []\n",
    "    datetimes = []\n",
    "\n",
    "    # Find mid-cycle dates of combined CS2 and IS2 data\n",
    "    dates = []\n",
    "    for idx in range(len(dataset1.delta_h[:-12])):\n",
    "        if dataset1.identifier_product_DOI == 'doi:10.5194/tc-11-451-2017':\n",
    "            newdate = datetime.datetime(int(dataset1.time.values[idx]), 1, 1) + datetime.timedelta(days = (dataset1.time.values[idx] % 1) * 365.25)\n",
    "            newdate1 = datetime.datetime(int(dataset1.time.values[idx+1]), 1, 1) + datetime.timedelta(days = (dataset1.time.values[idx+1] % 1) * 365.25)\n",
    "            midcycdays = newdate1 - newdate\n",
    "            midcycdate = newdate + midcycdays/2\n",
    "            dates += [midcycdate]\n",
    "        elif dataset1.Title == 'Land Ice Elevation Thematic Gridded Product':\n",
    "            date_time_str = '70-01-01'\n",
    "            date_time_obj = datetime.datetime.strptime(date_time_str, '%y-%m-%d')\n",
    "            newdate = date_time_obj + datetime.timedelta(seconds = ds_sub.time.values[idx].astype(float))\n",
    "            newdate1 = date_time_obj + datetime.timedelta(seconds = ds_sub.time.values[idx+1].astype(float))\n",
    "            midcycdays = newdate1 - newdate\n",
    "            midcycdate = newdate + midcycdays/2\n",
    "    for idx in range(len(dataset2.delta_h)-1): \n",
    "        if dataset2.identifier_product_DOI == 'NOT SET':    \n",
    "            print('working')\n",
    "            date_time_str = '18-01-01'\n",
    "            date_time_obj = datetime.datetime.strptime(date_time_str, '%y-%m-%d')\n",
    "            cyc_start_date = date_time_obj + datetime.timedelta(days=dataset2.time.values[idx])\n",
    "            cyc_end_date = date_time_obj + datetime.timedelta(days=dataset2.time.values[idx+1])\n",
    "            midcyc_days = cyc_end_date - cyc_start_date\n",
    "            midcyc_date = cyc_start_date + midcyc_days/2\n",
    "            cyc_start_dates += [cyc_start_date]\n",
    "            cyc_end_dates += [cyc_end_date]\n",
    "            midcyc_dates += [midcyc_date]\n",
    "        if dataset2.identifier_product_DOI == 'doi:10.5067/ATLAS/ATL15.002':    \n",
    "            date_time_str = '18-01-01'\n",
    "            date_time_obj = datetime.datetime.strptime(date_time_str, '%y-%m-%d')\n",
    "            newdate = date_time_obj + datetime.timedelta(days=dataset2.time.values[idx])\n",
    "            newdate1 = date_time_obj + datetime.timedelta(days=dataset2.time.values[idx+1])\n",
    "            midcycdays = newdate1 - newdate\n",
    "            midcycdate = newdate + midcycdays/2\n",
    "            dates += [midcycdate]\n",
    "\n",
    "    # Calculate cycle-to-cycle dh/dt at each cycle of the spliced data sets\n",
    "    for idx in range(len(dates)-1): \n",
    "        if idx < 35:\n",
    "            dhdt = ds1_clipped.delta_h[idx+1,:,:]-ds1_clipped.delta_h[idx,:,:]\n",
    "            midcycdate = dates[idx]\n",
    "        elif idx > 34:\n",
    "            dhdt = ds2_clipped.delta_h[(idx-40)+1,:,:]-ds2_clipped.delta_h[idx-40,:,:]\n",
    "            midcycdate = dates[idx]\n",
    "       \n",
    "        # Create empty lists to store contours \n",
    "        contours_pos = []\n",
    "        contours_neg = []\n",
    "\n",
    "        # Create contours of ice surface elevation height changes to delineate variable lake outlines\n",
    "        contour = measure.find_contours(dhdt.values, threshold)\n",
    "        if len(contour) > 0: \n",
    "            contours_pos += [contour]\n",
    "        contour = measure.find_contours(dhdt.values, -threshold)\n",
    "        if len(contour) > 0: \n",
    "            contours_neg += [contour]\n",
    "\n",
    "        # Create mapping conversion factor to map array location to polar stereographic x,y\n",
    "        x_conv = (x_max-x_min)/dhdt.shape[1]\n",
    "        y_conv = (y_max-y_min)/dhdt.shape[0]\n",
    "\n",
    "        # Make contours and make into polygons\n",
    "        for i in range(len(contours_pos)): \n",
    "            for j in range(len(contours_pos[i])):\n",
    "                x = x_min+contours_pos[i][j][:,1]*x_conv\n",
    "                y = y_max-contours_pos[i][j][:,0]*y_conv\n",
    "\n",
    "                # Make polygons from variable outlines and store to list\n",
    "                if len(contours_pos[i][j][:, 1]) > 2: \n",
    "                    poly = Polygon(list(zip(x, y))) \n",
    "                    polys += [poly]\n",
    "\n",
    "                    # Calc polygon area and perimeter and store to lists\n",
    "                    lon, lat = XY_TO_LL.transform(x,y)\n",
    "                    poly_area = abs(GEOD.polygon_area_perimeter(lon, lat)[0])/1e6  # km^2\n",
    "                    poly_perim = abs(GEOD.polygon_area_perimeter(lon, lat)[1])/1e3  # km\n",
    "                    areas += [poly_area]\n",
    "                    perims += [poly_perim]\n",
    "                    datetimes += [midcycdate]\n",
    "\n",
    "        for i in range(len(contours_neg)): \n",
    "            for j in range(len(contours_neg[i])):\n",
    "                x = x_min+contours_neg[i][j][:,1]*x_conv\n",
    "                y = y_max-contours_neg[i][j][:,0]*y_conv\n",
    "\n",
    "                # Make polygons from variable outlines\n",
    "                if len(contours_neg[i][j][:, 1]) > 2: \n",
    "                    poly = Polygon(list(zip(x, y)))\n",
    "                    polys += [poly]\n",
    "\n",
    "                    # Calc polygon area and perimeter and store to lists\n",
    "                    lon, lat = XY_TO_LL.transform(x,y)\n",
    "                    poly_area = abs(GEOD.polygon_area_perimeter(lon, lat)[0])/1e6\n",
    "                    poly_perim = abs(GEOD.polygon_area_perimeter(lon, lat)[1])/1e3\n",
    "                    areas += [poly_area]\n",
    "                    perims += [poly_perim]\n",
    "                    datetimes += [midcycdate]\n",
    "    \n",
    "    # Store polygons in geopandas geodataframe for analysis\n",
    "    d = {'geometry': polys, 'area (km^2)': areas, 'perim (km)': perims, 'datetime': datetimes}\n",
    "    gdf = gpd.GeoDataFrame(d, crs=\"EPSG:3031\")\n",
    "    gdf['date'] = pd.to_datetime(gdf['datetime']).apply(lambda x: x.date())\n",
    "    gdf['centroid'] = gdf['geometry'].centroid\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdf = S09SF18S23_outline_timeseries_gdf('Whillans_7', 20000, 0.1, CS2_dh, ATL15_dh)\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove polygons with area above the 95th percentile and below the 5th percentile\n",
    "gdf2 = gdf[(gdf['area (km^2)'] < gdf['area (km^2)'].quantile(0.99)) &\n",
    "           (gdf['area (km^2)'] > gdf['area (km^2)'].quantile(0.01))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gdf2_overlapping_remove_outliers['date'], gdf2_overlapping_remove_outliers['area (km^2)'], 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
