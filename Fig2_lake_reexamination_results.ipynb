{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ece7efe-8f32-445e-be30-e37df0f40e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to Fig. S1 of Sauthoff and others, 2024\n",
    "# This code run continental-scale operations on multiple datasets and\n",
    "# requires a 64 GB server or local memory\n",
    "#\n",
    "# Written 2023-11-11 by W. Sauthoff (wsauthoff.github.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ed2a12-f0d3-4b07-9a36-e26a20fe5d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb0490b8-0bb2-4bf8-8ea1-d93c28a1610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import datetime\n",
    "# import earthaccess\n",
    "# import geopandas as gpd\n",
    "# from IPython.display import clear_output\n",
    "import matplotlib\n",
    "# import matplotlib.cm as cm\n",
    "# from matplotlib.collections import LineCollection\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.dates as mdates\n",
    "# from matplotlib.patches import Rectangle\n",
    "# from matplotlib.legend_handler import HandlerTuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "# from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "# from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "# import numpy as np\n",
    "import os\n",
    "# from os import path\n",
    "# from pyproj import CRS, Transformer\n",
    "import rioxarray\n",
    "# from rioxarray.exceptions import NoDataInBounds\n",
    "# from shapely.geometry import box, Polygon\n",
    "# from shapely.ops import unary_union\n",
    "# from skimage import measure\n",
    "# import xarray as xr\n",
    "\n",
    "# from IPython.display import Audio, display\n",
    "# def play_sound():\n",
    "#     display(Audio(url=\"http://codeskulptor-demos.commondatastorage.googleapis.com/pang/pop.mp3\", autoplay=True))\n",
    "\n",
    "# Define data directories dependent on home environment\n",
    "# Replace with your directory file paths\n",
    "if os.getenv('HOME') == '/home/jovyan':\n",
    "    DATA_DIR = '/home/jovyan/data'\n",
    "    SCRIPT_DIR = '/home/jovyan/repos_my/script_dir'\n",
    "    OUTPUT_DIR = '/home/jovyan/1_outlines_candidates/output/Fig2_lake_reexamination.ipynb'\n",
    "\n",
    "# # Define constants and coordinate transforms for the geodesic area calculation\n",
    "# CRS_LL = \"EPSG:4326\" # wgs84 in lon,lat\n",
    "# GEOD = CRS(CRS_LL).get_geod() # geod object for calculating geodesic area on defined ellipsoid\n",
    "# CRS_XY = \"EPSG:3031\" # Antarctic Polar Stereographic in x, y\n",
    "# XY_TO_LL = Transformer.from_crs(CRS_XY, CRS_LL, always_xy = True) # make coord transformer\n",
    "\n",
    "# # Change default font to increase font size\n",
    "# plt.rcParams.update({'font.size': 8})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6a7d91-6d40-45b7-8ff8-0e29f2e55c5f",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96782175-23f8-4e91-82aa-630404b21f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad5d1083-ac7e-4bc0-be61-e069e7848354",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae841541-da2b-4fbf-b92c-6ec5c7e9221b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import subglacial lake outlines \n",
    "exec(open(SCRIPT_DIR + '/Sauthoff2024_outlines.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe18173c-0ef2-4897-b7a7-11852526e9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning 1: TIFFReadDirectory:Invalid data type for tag StripByteCounts\n",
      "Warning 1: TIFFFetchNormalTag:Incorrect value for \"GeoKeyDirectory\"; tag ignored\n",
      "Warning 1: TIFFFetchNormalTag:ASCII value for tag \"GeoASCIIParams\" contains null byte in value; value incorrectly truncated during reading due to implementation limitations\n"
     ]
    }
   ],
   "source": [
    "# Import MODIS Mosaic of Antarctica (MOA) surface imagery\n",
    "# https://nsidc.org/data/nsidc-0730/versions/1\n",
    "# Relocate to data_dir\n",
    "# Open into an xarray.DataArray\n",
    "# moa_lowres = DATA_DIR + '/surface_imagery/MODIS_MOA/2014/moa750_2014_hp1_v01.tif' \n",
    "# moa_lowres_da = rioxarray.open_rasterio(moa_lowres)\n",
    "\n",
    "moa_highres = DATA_DIR + '/surface_imagery/MODIS_MOA/2014/moa125_2014_hp1_v01.tif' \n",
    "moa_highres_da = rioxarray.open_rasterio(moa_highres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1cf07dd-b864-4517-bbc9-60048ea9878a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cyc_dates = pd.read_csv('output/cycle_dates.csv', parse_dates=['cyc_start_dates', 'midcyc_dates', 'cyc_end_dates'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fb3684-d6dd-41de-9dd5-6da5a97958f9",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Fig. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ace865-2a03-40ea-ba34-870bddd33b7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,4, figsize=(15,12))\n",
    "\n",
    "# Define colors and linestyles that will be reused and create lines for legend\n",
    "S09_color = 'cyan'\n",
    "SF18_color  = 'darkcyan'\n",
    "S09_linestyle=(0, (1, 2))\n",
    "SF18_linestyle=(0, (1, 1))\n",
    "Smith2009 = plt.Line2D((0, 1), (0, 0), color=S09_color, linestyle=S09_linestyle, linewidth=2)\n",
    "SiegfriedFricker2018 = plt.Line2D((0, 1), (0, 0), color=SF18_color, linestyle=SF18_linestyle, linewidth=2)\n",
    "\n",
    "# Panel - evolving outlines\n",
    "\n",
    "# Plot static and evolving outlines onto MOA surface imagery\n",
    "# Subset dataset to region of interest for plotting\n",
    "S09_lake_gdf = Smith2009_outlines[Smith2009_outlines['Name'] == 'Whillans_4']\n",
    "SF18_lake_gdf = SiegfriedFricker2018_outlines[SiegfriedFricker2018_outlines['name'] == 'ConwaySubglacialLake']\n",
    "outlines_gdf = gpd.read_file('outlines/evolving_outlines/{}.geojson'.format(SF18_lake_gdf['name'].values[0]))\n",
    "x_min, y_min, x_max, y_max = outlines_gdf.total_bounds\n",
    "buffer_frac = 0.2\n",
    "x_buffer = abs(x_max-x_min)*buffer_frac\n",
    "y_buffer = abs(y_max-y_min)*buffer_frac\n",
    "mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "ax[0,0].imshow(moa_highres_da_subset[0,:,:], cmap=\"gray\", clim=[14000, 17000], extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "\n",
    "# Pick colormap and make continuous cmap discrete for evolving outlines\n",
    "colormap = 'plasma'\n",
    "continuous_cmap = matplotlib.colormaps[colormap]\n",
    "discrete_cmap = colors.ListedColormap(continuous_cmap(np.linspace(0, 1, len(cyc_dates['midcyc_dates'])-1)))\n",
    "\n",
    "# Norm to time variable\n",
    "norm = plt.Normalize(mdates.date2num(cyc_dates['midcyc_dates'].iloc[0]), \n",
    "                     mdates.date2num(cyc_dates['midcyc_dates'].iloc[-1]))\n",
    "\n",
    "# Use for loop to store each time slice as line segment to use in legend\n",
    "# And plot each outline in the geopandas geodataframe and color by date\n",
    "lines = []  # list of lines to be used for the legend\n",
    "for idx, dt in enumerate(cyc_dates['midcyc_dates'], 0):\n",
    "    x = 1; y = 1\n",
    "    line, = ax[0,0].plot(x, y, color=discrete_cmap(norm(mdates.date2num(cyc_dates['midcyc_dates'][idx]))), linewidth=3)\n",
    "    lines.append(line)\n",
    "    \n",
    "    # Filter rows that match the current time slice\n",
    "    outlines_gdf_dt_sub = outlines_gdf[outlines_gdf['midcyc_datetime'] == dt]\n",
    "\n",
    "    # Plotting the subset\n",
    "    outlines_gdf_dt_sub.boundary.plot(ax=ax[0,0], color=discrete_cmap(norm(mdates.date2num(cyc_dates['midcyc_dates'][idx]))), linewidth=0.75)\n",
    "\n",
    "# Set axes limit\n",
    "ax[0,0].set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "# Panel - da/dt\n",
    "ax[0,1].axhline(np.divide(S09_lake_gdf['Area (m^2)'], 1e6).values, color=S09_color, linestyle=(0, (1, 2)), linewidth=3)\n",
    "ax[0,1].axhline(np.divide(SF18_lake_gdf['area (m^2)'], 1e6).values, color=SF18_color, linestyle=(0, (1, 2)), linewidth=3)\n",
    "\n",
    "# Group by the 'date' column and sum the 'area'\n",
    "grouped_data = list(gdf_subset.groupby('datetime')['area (m^2)'].sum())\n",
    "grouped_data_dates = list(pd.unique(gdf_subset['datetime']))\n",
    "\n",
    "# Plot multi-colored line\n",
    "x=mdates.date2num(grouped_data_dates)\n",
    "y=np.divide(grouped_data, 1e6)\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "# Create a continuous norm'ed to map from data points to colors\n",
    "norm = plt.Normalize(x.min(), x.max())\n",
    "lc = LineCollection(segments, cmap=colormap, norm=norm, linestyle=(0,(1,1)))\n",
    "# Set the values used for colormapping\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(2)\n",
    "line = ax[0,1].add_collection(lc)\n",
    "\n",
    "locator = mdates.AutoDateLocator(minticks=1, maxticks=7)\n",
    "formatter = mdates.ConciseDateFormatter(locator)\n",
    "ax[0,1].xaxis.set_major_locator(locator)\n",
    "ax[0,1].xaxis.set_major_formatter(formatter)\n",
    "# min_area = min(np.divide(sum(lake_S09.area), 1e6), \n",
    "#     # np.divide(sum(lake_SF18.area), 1e6), \n",
    "#     min(np.divide(areas_var, 1e6)))\n",
    "# max_area = max(np.divide(sum(lake_S09.area), 1e6), \n",
    "#     # np.divide(sum(lake_SF18.area), 1e6), \n",
    "#     max(np.divide(areas_var,1e6)))\n",
    "\n",
    "ax[0,1].scatter(x, y, c=x, cmap=colormap)\n",
    "\n",
    "ax[0,1].set(xlim=(midcyc_dates[0], midcyc_dates[-1]),\n",
    "           # ylim=((min_area - (max_area - min_area)*0.1), \n",
    "           #       (max_area + (max_area - min_area)*0.1))\n",
    "          )\n",
    "ax[0,1].set_ylabel('wetted area [km$^2$]', size=17.5, labelpad=8)\n",
    "ax[0,1].set_yticks(np.arange(0, 150, 25))\n",
    "\n",
    "\n",
    "\n",
    "# Panel C - dh/dt\n",
    "ax[0,2].axhline(0, color='k', linestyle='solid', linewidth=1)\n",
    "# ax[2].axhline(np.divide(ROI['area (m^2)'], 1e6).values, color=S09_color, linestyle=(0, (1, 2)), linewidth=3)\n",
    "\n",
    "# Plot static outline time series\n",
    "ax[0,2].plot(midcyc_dates, np.cumsum(lkavgdhdt_S09), color=S09_color, linestyle=(0, (1, 2)), linewidth=3)\n",
    "\n",
    "# Group by the 'date' column and sum the 'area'\n",
    "grouped_data = list(gdf_subset.groupby('datetime')['dh (m)'].sum())\n",
    "grouped_data_dates = list(pd.unique(gdf_subset['datetime']))\n",
    "\n",
    "# Plot multi-colored line\n",
    "cmap_str='plasma'\n",
    "x=mdates.date2num(grouped_data_dates)\n",
    "y=np.cumsum(grouped_data)\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "# Create a continuous norm to map from data points to colors\n",
    "norm = plt.Normalize(x.min(), x.max())\n",
    "lc = LineCollection(segments, cmap=cmap_str, norm=norm, linestyle=(0,(1,1)))\n",
    "# Set the values used for colormapping\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(2)\n",
    "line = ax[0,2].add_collection(lc)\n",
    "\n",
    "locator = mdates.AutoDateLocator(minticks=1, maxticks=7)\n",
    "formatter = mdates.ConciseDateFormatter(locator)\n",
    "ax[0,2].xaxis.set_major_locator(locator)\n",
    "ax[0,2].xaxis.set_major_formatter(formatter)\n",
    "# min_area = min(np.divide(sum(lake_S09.area), 1e6), \n",
    "#     # np.divide(sum(lake_SF18.area), 1e6), \n",
    "#     min(np.divide(areas_var, 1e6)))\n",
    "# max_area = max(np.divide(sum(lake_S09.area), 1e6), \n",
    "#     # np.divide(sum(lake_SF18.area), 1e6), \n",
    "#     max(np.divide(areas_var,1e6)))\n",
    "\n",
    "ax[0,2].scatter(x, y, c=x, cmap=cmap_str)\n",
    "\n",
    "ax[0,2].set(xlim=(midcyc_dates[0], midcyc_dates[-1]),\n",
    "           # ylim=((min_area - (max_area - min_area)*0.1), \n",
    "           #       (max_area + (max_area - min_area)*0.1))\n",
    "          )\n",
    "ax[0,2].set_ylabel('cumulative\\nheight change [m]', size=17.5, labelpad=8)\n",
    "# ax[2].set_yticks(np.arange(0, 150, 25))\n",
    "\n",
    "\n",
    "\n",
    "# Panel D - dv/dt\n",
    "ax[0,3].axhline(0, color='k', linestyle='solid', linewidth=1)\n",
    "# ax[2].axhline(np.divide(ROI['area (m^2)'], 1e6).values, color=S09_color, linestyle=(0, (1, 2)), linewidth=3)\n",
    "\n",
    "# Plot static outline time series\n",
    "ax[0,3].plot(midcyc_dates, np.divide(np.cumsum(vols_S09), 1e+9), color=S09_color, linestyle=(0, (1, 2)), linewidth=3)\n",
    "\n",
    "# Group by the 'date' column and sum the 'area'\n",
    "grouped_data = list(gdf_subset.groupby('datetime')['vol (m^3)'].sum())\n",
    "grouped_data_dates = list(pd.unique(gdf_subset['datetime']))\n",
    "\n",
    "# Calc bias and plot\n",
    "S09_S24_bias = [a_i - b_i for a_i, b_i in zip(np.cumsum(grouped_data), np.cumsum(vols_S09))]\n",
    "ax[0,3].plot(grouped_data_dates, np.divide(S09_S24_bias, 1e+9), color='red', linestyle='solid', linewidth=2)\n",
    "\n",
    "# Plot multi-colored line\n",
    "x=mdates.date2num(grouped_data_dates)\n",
    "y=np.divide(np.cumsum(grouped_data), 1e9)\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "# Create a continuous norm to map from data points to colors\n",
    "norm = plt.Normalize(x.min(), x.max())\n",
    "lc = LineCollection(segments, cmap=cmap_str, norm=norm, linestyle=(0,(1,1)))\n",
    "# Set the values used for colormapping\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(2)\n",
    "line = ax[0,3].add_collection(lc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Plot static and evolving outlines onto MOA surface imagery\n",
    "# # Subset dataset to region of interest for plotting\n",
    "# # lake_gdf = SiegfriedFricker2018_outlines[SiegfriedFricker2018_outlines['name'] == 'Whillans_7']\n",
    "# lake_gdf = SiegfriedFricker2018_outlines[SiegfriedFricker2018_outlines['name'] == 'Foundation_5']\n",
    "# outlines_gdf = gpd.read_file('outlines/evolving_outlines/{}.geojson'.format(lake_gdf['name'].values[0]))\n",
    "# x_min, y_min, x_max, y_max = outlines_gdf.total_bounds\n",
    "# buffer_frac = 0.2\n",
    "# x_buffer = abs(x_max-x_min)*buffer_frac\n",
    "# y_buffer = abs(y_max-y_min)*buffer_frac\n",
    "# mask_x = (moa_highres_da.x >= x_min-buffer) & (moa_highres_da.x <= x_max+buffer)\n",
    "# mask_y = (moa_highres_da.y >= y_min-buffer) & (moa_highres_da.y <= y_max+buffer)\n",
    "# moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "# ax[1,0].imshow(moa_highres_da_subset[0,:,:], cmap=\"gray\", clim=[14000, 17000], extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "\n",
    "# # Use for loop to plot each outline in the geopandas geodataframe and color by date\n",
    "# for idx, dt in enumerate(cyc_dates['midcyc_dates'], 0):\n",
    "#     # Filter rows that match the current time slice\n",
    "#     outlines_gdf_dt_sub = outlines_gdf[outlines_gdf['midcyc_datetime'] == dt]\n",
    "\n",
    "#     # Plotting the subset\n",
    "#     outlines_gdf_dt_sub.boundary.plot(ax=ax[1,0], color=discrete_cmap(norm(mdates.date2num(cyc_dates['midcyc_dates'][idx]))), linewidth=0.75)\n",
    "\n",
    "# # Set axes limit\n",
    "# ax[1,0].set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "# # Plot static and evolving outlines onto MOA surface imagery\n",
    "# # Subset dataset to region of interest for plotting\n",
    "# lake_gdf = SiegfriedFricker2018_outlines[SiegfriedFricker2018_outlines['name'] == 'Slessor_23']\n",
    "# outlines_gdf = gpd.read_file('outlines/evolving_outlines/{}.geojson'.format(lake_gdf['name'].values[0]))\n",
    "# x_min, y_min, x_max, y_max = outlines_gdf.total_bounds\n",
    "# buffer_frac = 0.4\n",
    "# x_buffer = abs(x_max-x_min)*buffer_frac\n",
    "# y_buffer = abs(y_max-y_min)*buffer_frac\n",
    "# mask_x = (moa_highres_da.x >= x_min-buffer) & (moa_highres_da.x <= x_max+buffer)\n",
    "# mask_y = (moa_highres_da.y >= y_min-buffer) & (moa_highres_da.y <= y_max+buffer)\n",
    "# moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "# ax[2,0].imshow(moa_highres_da_subset[0,:,:], cmap=\"gray\", clim=[14000, 17000], extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "\n",
    "# # Use for loop to plot each outline in the geopandas geodataframe and color by date\n",
    "# for idx, dt in enumerate(cyc_dates['midcyc_dates']):\n",
    "#     # Filter rows that match the current time slice\n",
    "#     outlines_gdf_dt_sub = outlines_gdf[outlines_gdf['midcyc_datetime'] == dt]\n",
    "\n",
    "#     # Plotting the subset\n",
    "#     # outlines_gdf_dt_sub.boundary.plot(ax=ax[2,0], color=discrete_cmap(norm(mdates.date2num(cyc_dates['midcyc_dates'][idx]))), linewidth=0.75)\n",
    "#     outlines_gdf_dt_sub.geometry.centroid.plot(ax=ax[2,0], color=discrete_cmap(norm(mdates.date2num(cyc_dates['midcyc_dates'][idx]))), linewidth=1, marker='+')\n",
    "\n",
    "# # Set axes limit\n",
    "# ax[2,0].set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "# for i in ax: \n",
    "#     Smith2009_outlines.boundary.plot(ax=i, edgecolor=S09_color, facecolor='none', linestyle=(0, (1, 2)), linewidth=3, alpha=1, zorder=0)\n",
    "#     SiegfriedFricker2018_SF18outlines.boundary.plot(ax=i, edgecolor=SF18_color, facecolor='none', linestyle=(0, (1, 1)), linewidth=3, alpha=1, zorder=0)\n",
    "\n",
    "# # Plot inset map\n",
    "# axIns = ax.inset_axes([0.01, 0.01, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "# axIns.set_aspect('equal')\n",
    "# moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "# moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "# axIns.axis('off')\n",
    "\n",
    "# # Plot black rectangle to indicate location\n",
    "# rect = Rectangle((x_min, y_min), (x_max-x_min), (y_max-y_min), fill=False, linewidth=2, color='k', zorder=3)\n",
    "# axIns.add_artist(rect)\n",
    "\n",
    "# Label axes\n",
    "ax[2,0].set_xlabel('x [km]', size=16)\n",
    "ax[1,0].set_ylabel('y [km]', size=16)\n",
    "\n",
    "for subplot in [ax[0,0], ax[1,0], ax[2,0]]:\n",
    "    # Plot previous static outline inventories\n",
    "    Smith2009_outlines.boundary.plot(ax=subplot, edgecolor=S09_color, facecolor='none', linestyle=S09_linestyle, linewidth=3, alpha=1, zorder=0)\n",
    "    SiegfriedFricker2018_SF18outlines.boundary.plot(ax=subplot, edgecolor=SF18_color, facecolor='none', linestyle=S18_linestyle, linewidth=3, alpha=1, zorder=0)\n",
    "    \n",
    "    # Change polar stereographic m to km\n",
    "    km_scale = 1e3\n",
    "    ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    subplot.xaxis.set_major_formatter(ticks_x)\n",
    "    ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    subplot.yaxis.set_major_formatter(ticks_y)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0c38a-5447-405a-adea-cddda8cefae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Smith2009_outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73887d67-d020-4e60-a343-5ead8af21229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Smith2009_outlines_lonlat = gpd.read_file(DATA_DIR + '/boundaries/Smith2009_subglacial_lakes/Antarctic_lakes.kml', driver='KML')\n",
    "Smith2009_outlines_test = Smith2009_outlines_lonlat.to_crs(3031)\n",
    "Smith2009_outlines_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe56f6b-a8b4-48c7-9801-ba0313f70c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67899650-c529-4c5f-82d4-f68c07809239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d9f7a-bbbc-4283-8ce1-209d9a7dd5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fd69824-8e60-4c53-9a99-dea857eb331d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2010-08-17 06:45:00\n",
      "1 2010-11-16 11:15:00\n",
      "2 2011-02-15 15:45:00\n",
      "3 2011-05-17 23:15:00\n",
      "4 2011-08-17 06:45:00\n",
      "5 2011-11-16 11:15:00\n",
      "6 2012-02-15 15:45:00\n",
      "7 2012-05-16 23:15:00\n",
      "8 2012-08-16 06:45:00\n",
      "9 2012-11-15 23:15:00\n",
      "10 2013-02-15 15:45:00\n",
      "11 2013-05-17 23:15:00\n",
      "12 2013-08-17 06:45:00\n",
      "13 2013-11-16 11:15:00\n",
      "14 2014-02-15 15:45:00\n",
      "15 2014-05-17 23:15:00\n",
      "16 2014-08-17 06:45:00\n",
      "17 2014-11-16 11:15:00\n",
      "18 2015-02-15 15:45:00\n",
      "19 2015-05-17 23:15:00\n",
      "20 2015-08-17 06:45:00\n",
      "21 2015-11-16 11:15:00\n",
      "22 2016-02-15 15:45:00\n",
      "23 2016-05-16 23:15:00\n",
      "24 2016-08-16 06:45:00\n",
      "25 2016-11-15 23:15:00\n",
      "26 2017-02-15 15:45:00\n",
      "27 2017-05-17 23:15:00\n",
      "28 2017-08-17 06:45:00\n",
      "29 2017-11-16 11:15:00\n",
      "30 2018-02-15 15:45:00\n",
      "31 2018-05-17 23:15:00\n",
      "32 2018-08-17 06:45:00\n",
      "33 2018-11-16 14:15:00\n",
      "34 2019-02-15 21:45:00\n",
      "35 2019-05-18 05:15:00\n",
      "36 2019-08-17 12:45:00\n",
      "37 2019-11-16 20:15:00\n",
      "38 2020-02-16 03:45:00\n",
      "39 2020-05-17 11:15:00\n",
      "40 2020-08-16 18:45:00\n",
      "41 2020-11-16 02:15:00\n",
      "42 2021-02-15 09:45:00\n",
      "43 2021-05-17 17:15:00\n",
      "44 2021-08-17 00:45:00\n",
      "45 2021-11-16 08:15:00\n",
      "46 2022-02-15 15:45:00\n",
      "47 2022-05-17 23:15:00\n",
      "48 2022-08-17 06:45:00\n",
      "49 2022-11-16 14:15:00\n",
      "50 2023-02-15 21:45:00\n"
     ]
    }
   ],
   "source": [
    "for idx, dt in enumerate(cyc_dates['midcyc_dates'], 0):\n",
    "    print(idx, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93ea1417-e6c6-44cc-9720-a3715f94d76f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2010-08-17 06:45:00\n",
      "1 2010-11-16 11:15:00\n",
      "2 2011-02-15 15:45:00\n",
      "3 2011-05-17 23:15:00\n",
      "4 2011-08-17 06:45:00\n",
      "5 2011-11-16 11:15:00\n",
      "6 2012-02-15 15:45:00\n",
      "7 2012-05-16 23:15:00\n",
      "8 2012-08-16 06:45:00\n",
      "9 2012-11-15 23:15:00\n",
      "10 2013-02-15 15:45:00\n",
      "11 2013-05-17 23:15:00\n",
      "12 2013-08-17 06:45:00\n",
      "13 2013-11-16 11:15:00\n",
      "14 2014-02-15 15:45:00\n",
      "15 2014-05-17 23:15:00\n",
      "16 2014-08-17 06:45:00\n",
      "17 2014-11-16 11:15:00\n",
      "18 2015-02-15 15:45:00\n",
      "19 2015-05-17 23:15:00\n",
      "20 2015-08-17 06:45:00\n",
      "21 2015-11-16 11:15:00\n",
      "22 2016-02-15 15:45:00\n",
      "23 2016-05-16 23:15:00\n",
      "24 2016-08-16 06:45:00\n",
      "25 2016-11-15 23:15:00\n",
      "26 2017-02-15 15:45:00\n",
      "27 2017-05-17 23:15:00\n",
      "28 2017-08-17 06:45:00\n",
      "29 2017-11-16 11:15:00\n",
      "30 2018-02-15 15:45:00\n",
      "31 2018-05-17 23:15:00\n",
      "32 2018-08-17 06:45:00\n",
      "33 2018-11-16 14:15:00\n",
      "34 2019-02-15 21:45:00\n",
      "35 2019-05-18 05:15:00\n",
      "36 2019-08-17 12:45:00\n",
      "37 2019-11-16 20:15:00\n",
      "38 2020-02-16 03:45:00\n",
      "39 2020-05-17 11:15:00\n",
      "40 2020-08-16 18:45:00\n",
      "41 2020-11-16 02:15:00\n",
      "42 2021-02-15 09:45:00\n",
      "43 2021-05-17 17:15:00\n",
      "44 2021-08-17 00:45:00\n",
      "45 2021-11-16 08:15:00\n",
      "46 2022-02-15 15:45:00\n",
      "47 2022-05-17 23:15:00\n",
      "48 2022-08-17 06:45:00\n",
      "49 2022-11-16 14:15:00\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(cyc_dates['midcyc_dates'])-1):  # Less one because cycle-to-cycle differencing reduces length by one\n",
    "    print(idx, cyc_dates['midcyc_dates'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b73f4e4-a02b-495e-941b-bf7746029745",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cyc_start_dates</th>\n",
       "      <th>midcyc_dates</th>\n",
       "      <th>cyc_end_dates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-07-02 15:00:00</td>\n",
       "      <td>2010-08-17 06:45:00</td>\n",
       "      <td>2010-10-01 22:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-10-01 22:30:00</td>\n",
       "      <td>2010-11-16 11:15:00</td>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>2011-02-15 15:45:00</td>\n",
       "      <td>2011-04-02 07:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-04-02 07:30:00</td>\n",
       "      <td>2011-05-17 23:15:00</td>\n",
       "      <td>2011-07-02 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-07-02 15:00:00</td>\n",
       "      <td>2011-08-17 06:45:00</td>\n",
       "      <td>2011-10-01 22:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2011-10-01 22:30:00</td>\n",
       "      <td>2011-11-16 11:15:00</td>\n",
       "      <td>2012-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2012-01-01 00:00:00</td>\n",
       "      <td>2012-02-15 15:45:00</td>\n",
       "      <td>2012-04-01 07:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2012-04-01 07:30:00</td>\n",
       "      <td>2012-05-16 23:15:00</td>\n",
       "      <td>2012-07-01 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2012-07-01 15:00:00</td>\n",
       "      <td>2012-08-16 06:45:00</td>\n",
       "      <td>2012-09-30 22:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2012-09-30 22:30:00</td>\n",
       "      <td>2012-11-15 23:15:00</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2013-02-15 15:45:00</td>\n",
       "      <td>2013-04-02 07:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2013-04-02 07:30:00</td>\n",
       "      <td>2013-05-17 23:15:00</td>\n",
       "      <td>2013-07-02 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2013-07-02 15:00:00</td>\n",
       "      <td>2013-08-17 06:45:00</td>\n",
       "      <td>2013-10-01 22:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2013-10-01 22:30:00</td>\n",
       "      <td>2013-11-16 11:15:00</td>\n",
       "      <td>2014-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2014-01-01 00:00:00</td>\n",
       "      <td>2014-02-15 15:45:00</td>\n",
       "      <td>2014-04-02 07:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2014-04-02 07:30:00</td>\n",
       "      <td>2014-05-17 23:15:00</td>\n",
       "      <td>2014-07-02 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2014-07-02 15:00:00</td>\n",
       "      <td>2014-08-17 06:45:00</td>\n",
       "      <td>2014-10-01 22:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2014-10-01 22:30:00</td>\n",
       "      <td>2014-11-16 11:15:00</td>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>2015-02-15 15:45:00</td>\n",
       "      <td>2015-04-02 07:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2015-04-02 07:30:00</td>\n",
       "      <td>2015-05-17 23:15:00</td>\n",
       "      <td>2015-07-02 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2015-07-02 15:00:00</td>\n",
       "      <td>2015-08-17 06:45:00</td>\n",
       "      <td>2015-10-01 22:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2015-10-01 22:30:00</td>\n",
       "      <td>2015-11-16 11:15:00</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>2016-02-15 15:45:00</td>\n",
       "      <td>2016-04-01 07:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2016-04-01 07:30:00</td>\n",
       "      <td>2016-05-16 23:15:00</td>\n",
       "      <td>2016-07-01 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2016-07-01 15:00:00</td>\n",
       "      <td>2016-08-16 06:45:00</td>\n",
       "      <td>2016-09-30 22:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2016-09-30 22:30:00</td>\n",
       "      <td>2016-11-15 23:15:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-02-15 15:45:00</td>\n",
       "      <td>2017-04-02 07:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2017-04-02 07:30:00</td>\n",
       "      <td>2017-05-17 23:15:00</td>\n",
       "      <td>2017-07-02 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2017-07-02 15:00:00</td>\n",
       "      <td>2017-08-17 06:45:00</td>\n",
       "      <td>2017-10-01 22:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2017-10-01 22:30:00</td>\n",
       "      <td>2017-11-16 11:15:00</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>2018-02-15 15:45:00</td>\n",
       "      <td>2018-04-02 07:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2018-04-02 07:30:00</td>\n",
       "      <td>2018-05-17 23:15:00</td>\n",
       "      <td>2018-07-02 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2018-07-02 15:00:00</td>\n",
       "      <td>2018-08-17 06:45:00</td>\n",
       "      <td>2018-10-01 22:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2018-10-01 22:30:00</td>\n",
       "      <td>2018-11-16 14:15:00</td>\n",
       "      <td>2019-01-01 06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2019-01-01 06:00:00</td>\n",
       "      <td>2019-02-15 21:45:00</td>\n",
       "      <td>2019-04-02 13:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2019-04-02 13:30:00</td>\n",
       "      <td>2019-05-18 05:15:00</td>\n",
       "      <td>2019-07-02 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2019-07-02 21:00:00</td>\n",
       "      <td>2019-08-17 12:45:00</td>\n",
       "      <td>2019-10-02 04:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2019-10-02 04:30:00</td>\n",
       "      <td>2019-11-16 20:15:00</td>\n",
       "      <td>2020-01-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2020-01-01 12:00:00</td>\n",
       "      <td>2020-02-16 03:45:00</td>\n",
       "      <td>2020-04-01 19:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2020-04-01 19:30:00</td>\n",
       "      <td>2020-05-17 11:15:00</td>\n",
       "      <td>2020-07-02 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2020-07-02 03:00:00</td>\n",
       "      <td>2020-08-16 18:45:00</td>\n",
       "      <td>2020-10-01 10:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2020-10-01 10:30:00</td>\n",
       "      <td>2020-11-16 02:15:00</td>\n",
       "      <td>2020-12-31 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2020-12-31 18:00:00</td>\n",
       "      <td>2021-02-15 09:45:00</td>\n",
       "      <td>2021-04-02 01:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2021-04-02 01:30:00</td>\n",
       "      <td>2021-05-17 17:15:00</td>\n",
       "      <td>2021-07-02 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2021-07-02 09:00:00</td>\n",
       "      <td>2021-08-17 00:45:00</td>\n",
       "      <td>2021-10-01 16:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2021-10-01 16:30:00</td>\n",
       "      <td>2021-11-16 08:15:00</td>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>2022-02-15 15:45:00</td>\n",
       "      <td>2022-04-02 07:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2022-04-02 07:30:00</td>\n",
       "      <td>2022-05-17 23:15:00</td>\n",
       "      <td>2022-07-02 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2022-07-02 15:00:00</td>\n",
       "      <td>2022-08-17 06:45:00</td>\n",
       "      <td>2022-10-01 22:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2022-10-01 22:30:00</td>\n",
       "      <td>2022-11-16 14:15:00</td>\n",
       "      <td>2023-01-01 06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2023-01-01 06:00:00</td>\n",
       "      <td>2023-02-15 21:45:00</td>\n",
       "      <td>2023-04-02 13:30:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cyc_start_dates        midcyc_dates       cyc_end_dates\n",
       "0  2010-07-02 15:00:00 2010-08-17 06:45:00 2010-10-01 22:30:00\n",
       "1  2010-10-01 22:30:00 2010-11-16 11:15:00 2011-01-01 00:00:00\n",
       "2  2011-01-01 00:00:00 2011-02-15 15:45:00 2011-04-02 07:30:00\n",
       "3  2011-04-02 07:30:00 2011-05-17 23:15:00 2011-07-02 15:00:00\n",
       "4  2011-07-02 15:00:00 2011-08-17 06:45:00 2011-10-01 22:30:00\n",
       "5  2011-10-01 22:30:00 2011-11-16 11:15:00 2012-01-01 00:00:00\n",
       "6  2012-01-01 00:00:00 2012-02-15 15:45:00 2012-04-01 07:30:00\n",
       "7  2012-04-01 07:30:00 2012-05-16 23:15:00 2012-07-01 15:00:00\n",
       "8  2012-07-01 15:00:00 2012-08-16 06:45:00 2012-09-30 22:30:00\n",
       "9  2012-09-30 22:30:00 2012-11-15 23:15:00 2013-01-01 00:00:00\n",
       "10 2013-01-01 00:00:00 2013-02-15 15:45:00 2013-04-02 07:30:00\n",
       "11 2013-04-02 07:30:00 2013-05-17 23:15:00 2013-07-02 15:00:00\n",
       "12 2013-07-02 15:00:00 2013-08-17 06:45:00 2013-10-01 22:30:00\n",
       "13 2013-10-01 22:30:00 2013-11-16 11:15:00 2014-01-01 00:00:00\n",
       "14 2014-01-01 00:00:00 2014-02-15 15:45:00 2014-04-02 07:30:00\n",
       "15 2014-04-02 07:30:00 2014-05-17 23:15:00 2014-07-02 15:00:00\n",
       "16 2014-07-02 15:00:00 2014-08-17 06:45:00 2014-10-01 22:30:00\n",
       "17 2014-10-01 22:30:00 2014-11-16 11:15:00 2015-01-01 00:00:00\n",
       "18 2015-01-01 00:00:00 2015-02-15 15:45:00 2015-04-02 07:30:00\n",
       "19 2015-04-02 07:30:00 2015-05-17 23:15:00 2015-07-02 15:00:00\n",
       "20 2015-07-02 15:00:00 2015-08-17 06:45:00 2015-10-01 22:30:00\n",
       "21 2015-10-01 22:30:00 2015-11-16 11:15:00 2016-01-01 00:00:00\n",
       "22 2016-01-01 00:00:00 2016-02-15 15:45:00 2016-04-01 07:30:00\n",
       "23 2016-04-01 07:30:00 2016-05-16 23:15:00 2016-07-01 15:00:00\n",
       "24 2016-07-01 15:00:00 2016-08-16 06:45:00 2016-09-30 22:30:00\n",
       "25 2016-09-30 22:30:00 2016-11-15 23:15:00 2017-01-01 00:00:00\n",
       "26 2017-01-01 00:00:00 2017-02-15 15:45:00 2017-04-02 07:30:00\n",
       "27 2017-04-02 07:30:00 2017-05-17 23:15:00 2017-07-02 15:00:00\n",
       "28 2017-07-02 15:00:00 2017-08-17 06:45:00 2017-10-01 22:30:00\n",
       "29 2017-10-01 22:30:00 2017-11-16 11:15:00 2018-01-01 00:00:00\n",
       "30 2018-01-01 00:00:00 2018-02-15 15:45:00 2018-04-02 07:30:00\n",
       "31 2018-04-02 07:30:00 2018-05-17 23:15:00 2018-07-02 15:00:00\n",
       "32 2018-07-02 15:00:00 2018-08-17 06:45:00 2018-10-01 22:30:00\n",
       "33 2018-10-01 22:30:00 2018-11-16 14:15:00 2019-01-01 06:00:00\n",
       "34 2019-01-01 06:00:00 2019-02-15 21:45:00 2019-04-02 13:30:00\n",
       "35 2019-04-02 13:30:00 2019-05-18 05:15:00 2019-07-02 21:00:00\n",
       "36 2019-07-02 21:00:00 2019-08-17 12:45:00 2019-10-02 04:30:00\n",
       "37 2019-10-02 04:30:00 2019-11-16 20:15:00 2020-01-01 12:00:00\n",
       "38 2020-01-01 12:00:00 2020-02-16 03:45:00 2020-04-01 19:30:00\n",
       "39 2020-04-01 19:30:00 2020-05-17 11:15:00 2020-07-02 03:00:00\n",
       "40 2020-07-02 03:00:00 2020-08-16 18:45:00 2020-10-01 10:30:00\n",
       "41 2020-10-01 10:30:00 2020-11-16 02:15:00 2020-12-31 18:00:00\n",
       "42 2020-12-31 18:00:00 2021-02-15 09:45:00 2021-04-02 01:30:00\n",
       "43 2021-04-02 01:30:00 2021-05-17 17:15:00 2021-07-02 09:00:00\n",
       "44 2021-07-02 09:00:00 2021-08-17 00:45:00 2021-10-01 16:30:00\n",
       "45 2021-10-01 16:30:00 2021-11-16 08:15:00 2022-01-01 00:00:00\n",
       "46 2022-01-01 00:00:00 2022-02-15 15:45:00 2022-04-02 07:30:00\n",
       "47 2022-04-02 07:30:00 2022-05-17 23:15:00 2022-07-02 15:00:00\n",
       "48 2022-07-02 15:00:00 2022-08-17 06:45:00 2022-10-01 22:30:00\n",
       "49 2022-10-01 22:30:00 2022-11-16 14:15:00 2023-01-01 06:00:00\n",
       "50 2023-01-01 06:00:00 2023-02-15 21:45:00 2023-04-02 13:30:00"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cyc_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d3fad6-5232-4f80-9b66-c648e1bee882",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
