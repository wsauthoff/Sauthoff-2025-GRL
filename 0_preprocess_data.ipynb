{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec9c76e3-d9bd-4fc7-b5f2-6a4154a59709",
   "metadata": {},
   "source": [
    "Notebook pre-processes altimetry data sets to construct a multi-mission CryoSat-2 to ICESat-2 (2010 to present) time series.\n",
    "\n",
    "Written 2023-05-10 by W. Sauthoff (sauthoff@mines.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff75638-a801-40a3-bd59-7d6536dbc054",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set up computing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1703807-de8f-44de-b7d1-8e357e81bd05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install dependency not pre-installed\n",
    "%pip install openpyxl --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d7db60-16e9-4c9f-b517-bf0def622fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install dependency not pre-installed\n",
    "%pip install ipympl --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65574818-ef0e-4bfc-85f6-bc845cc22edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import datetime\n",
    "import earthaccess\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyproj import CRS, Transformer\n",
    "from scipy.ndimage import zoom\n",
    "import xarray as xr\n",
    "from shapely.geometry import MultiPolygon, Point, Polygon\n",
    "\n",
    "# Magic functions\n",
    "%matplotlib widget\n",
    "\n",
    "# Define data and output directories dependent on home environment\n",
    "# Replace with your directory file paths\n",
    "if os.getenv('HOME') == '/home/jovyan':\n",
    "    DATA_DIR = '/home/jovyan/data'\n",
    "    # Notebook creates many output plots for QC'ing data that were stored outside the GitHub repo\n",
    "    OUTPUT_DIR = '/home/jovyan/1_evolving_lakes/output/0_preprocess_data'\n",
    "\n",
    "# Define utility functions\n",
    "def combine_quadrants_by_coords(quadrant_dir, ds_name='dataset', x_dim='x', y_dim='y'):\n",
    "    \"\"\"\n",
    "    Combine quadrant NetCDF files into a full dataset using xarray.combine_by_coords.\n",
    "    Overlapping x=0 and/or y=0 rows/columns are dropped depending on quadrant index.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    quadrant_dir : str\n",
    "        Directory containing the quadrant NetCDF files.\n",
    "    ds_name : str\n",
    "        Base name prefix of the quadrant files (e.g., 'CryoSat2_SARIn_delta_h').\n",
    "    x_dim : str\n",
    "        Name of the x dimension.\n",
    "    y_dim : str\n",
    "        Name of the y dimension.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        Combined dataset with overlaps removed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Find all quadrant files matching the pattern\n",
    "    files = sorted(glob.glob(os.path.join(quadrant_dir, f'{ds_name}_A*.nc')))\n",
    "    \n",
    "    datasets = []\n",
    "    for i, file in enumerate(files):\n",
    "        ds = xr.open_dataset(file)\n",
    "\n",
    "        # Drop overlap according to quadrant order\n",
    "        if i == 0:\n",
    "            datasets.append(ds)  # A1: keep all\n",
    "        elif i == 1:\n",
    "            datasets.append(ds.drop_sel({x_dim: 0}, errors='ignore'))  # A2: drop x=0\n",
    "        elif i == 2:\n",
    "            datasets.append(ds.drop_sel({y_dim: 0}, errors='ignore'))  # A3: drop y=0\n",
    "        elif i == 3:\n",
    "            datasets.append(ds.drop_sel({x_dim: 0, y_dim: 0}, errors='ignore'))  # A4: drop x=0 and y=0\n",
    "\n",
    "    # Merge all together based on coordinates\n",
    "    combined = xr.combine_by_coords(datasets)\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "def ll2ps(lon, lat):\n",
    "    \"\"\"\n",
    "    Transform coordinates from geodetic coordinates (lon, lat)\n",
    "    to Antarctic Polar Stereograph coordinates (x, y)\n",
    "    x, y = ll2ps(lon, lat)\n",
    "    \"\"\"\n",
    "    if not (-180 <= lon <= 180 and -90 <= lat <= 90):\n",
    "        raise ValueError(\"Invalid coordinates: longitude must be between -180 and 180, latitude between -90 and 90\")\n",
    "\n",
    "    try:\n",
    "        crs_ll = CRS(\"EPSG:4326\")\n",
    "        crs_xy = CRS(\"EPSG:3031\")\n",
    "        ll_to_xy = Transformer.from_crs(crs_ll, crs_xy, always_xy = True)\n",
    "        x, y = ll_to_xy.transform(lon, lat)\n",
    "        return x, y\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error transforming coordinates: {e}\")\n",
    "\n",
    "def transform_geometry(geometry):\n",
    "    \"\"\"\n",
    "    Transform the geometry of a GeoDataFrame row from lon/lat to polar stereographic.\n",
    "    Handles Points, Polygons, and MultiPolygons.\n",
    "    \"\"\"\n",
    "    if geometry.geom_type == 'Polygon':\n",
    "        exterior = [(x, y) for x, y in zip(*geometry.exterior.coords.xy)]\n",
    "        transformed_exterior = [ll2ps(lon, lat) for lon, lat in exterior]\n",
    "        new_polygon = Polygon(transformed_exterior)\n",
    "        return new_polygon\n",
    "    elif geometry.geom_type == 'Point':\n",
    "        x, y = ll2ps(*geometry.coords[0])\n",
    "        return Point(x, y)\n",
    "    elif geometry.geom_type == 'MultiPolygon':\n",
    "        new_polygons = []\n",
    "        for polygon in geometry.geoms:\n",
    "            exterior = [(x, y) for x, y in zip(*polygon.exterior.coords.xy)]\n",
    "            transformed_exterior = [ll2ps(lon, lat) for lon, lat in exterior]\n",
    "            new_polygons.append(Polygon(transformed_exterior))\n",
    "        return MultiPolygon(new_polygons)\n",
    "    else:\n",
    "        # Add support for other geometry types as needed\n",
    "        raise ValueError(f\"Unsupported geometry type: {geometry.geom_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5e0e1c-b26e-4ee3-87eb-ceea82bc5d73",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505b5778-cc61-4d44-b9a8-8e94a59149d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import previously identified subglacial lake stationary outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88af4f5c-4bae-4f9f-92da-9346f3e6f156",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import stationary subglacial lake outlines\n",
    "stationary_lakes_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/stationary_outlines_gdf.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ab92f0-4a8d-4ec5-8575-04da102e56ab",
   "metadata": {},
   "source": [
    "## Import CryoSat-2 SARIn gridded dh data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cc2dd2-224b-4368-ad22-cc2196e16132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Smith and Sauthoff, 2025 (CryoSat-2 SARIn Height Change and Reference DEM for Antarctica)\n",
    "# https://doi.org/10.5281/zenodo.14963550\n",
    "CryoSat2_SARIn = combine_quadrants_by_coords(DATA_DIR, ds_name='CryoSat2_SARIn_delta_h')\n",
    "CryoSat2_SARIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2521e12c-3b08-47ea-9d68-3b3e4200c234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View time data variable\n",
    "CryoSat2_SARIn['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afee6d1-9df6-4376-a6ac-d15f30bc183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove time slices that occur during the ICESat-2 era that will not be used \n",
    "# to conserve memory when loaded for data analysis\n",
    "\n",
    "# end_date includes one quarter of overlapping data with ICESat-2 time series\n",
    "# to allow for cyc-to-cyc differencing to remove datum from delta_h to create cycle-to-cycle dh\n",
    "end_date = '2019-01-01T00:00:00.000000000'\n",
    "\n",
    "CryoSat2_SARIn = CryoSat2_SARIn.sel(time=slice(None, end_date))\n",
    "\n",
    "# Preview temporally subset data set's time variable\n",
    "CryoSat2_SARIn['time']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be346c4d-0db5-41f6-bfa7-b4b65794d7ac",
   "metadata": {},
   "source": [
    "## Import the ATL15 gridded dh data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de63cd55-ca51-4120-8ef8-581ce63993c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find ICESat-2 ATL15 v004 data granules\n",
    "results = earthaccess.search_data(\n",
    "    doi='10.5067/ATLAS/ATL15.004',\n",
    "    bounding_box=(180, -90, -180, -60),  # (lower_left_lon, lower_left_lat , upper_right_lon, upper_right_lat))\n",
    "    cloud_hosted=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea29da5-07cb-477c-bfb9-45309a88521a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open data granules as s3 files to stream\n",
    "files = earthaccess.open(results)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa1c83-01e9-4682-8720-9c838a333cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# After viewing files, index the files you wish to open\n",
    "print(files[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc157e1-cf00-440d-97fb-d7c9186c41a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open each file, which are quadrants in polar stereographic coordinations \n",
    "# around the Geographic South Pole\n",
    "ATL15_A1 = xr.open_dataset(files[11], group='delta_h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75adcc6c-d680-4411-8497-34480da4a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add datasets attributes\n",
    "ATL15_A1.attrs['identifier_product_DOI'] = 'doi:10.5067/ATLAS/ATL15.004'\n",
    "ATL15_A1.attrs['shortName'] = 'ATL15'\n",
    "\n",
    "# View data set\n",
    "ATL15_A1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7102e46d-5f17-4ed1-9c2f-3cce41e24646",
   "metadata": {},
   "source": [
    "# Satellite mission cycle datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65045ab1-7f79-412c-b5d5-405be0d28d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CryoSat2_SARIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031d9735-7024-4012-a6be-19705b46d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATL15_A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9594a0-0fe2-4973-bcbf-25824eead19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store satellite cycle dates for later use\n",
    "\n",
    "# Define data sets being used\n",
    "dataset1 = CryoSat2_SARIn\n",
    "dataset2 = ATL15_A1\n",
    "\n",
    "# Create empty lists to store data\n",
    "cyc_start_datetimes = []\n",
    "cyc_end_datetimes = []\n",
    "dataset = []\n",
    "\n",
    "# CryoSat-2 data\n",
    "if dataset1.attrs['identifier_product_DOI'] == 'doi:10.5281/zenodo.14963550':\n",
    "    for idx in range(len(dataset1.delta_h) - 1):  # Less one time step that is used for differencing to get dh values for the last CS2 time step\n",
    "        cyc_start_datetime = dataset1.time.values[idx]\n",
    "        if idx < len(dataset1.delta_h)-1:\n",
    "            cyc_end_datetime = dataset1.time.values[idx+1]\n",
    "        else:\n",
    "            # For last cycle, use same length as previous cycle\n",
    "            last_cycle_length = dataset1.time.values[idx] - dataset1.time.values[idx-1]\n",
    "            cyc_end_datetime = cyc_start_datetime + last_cycle_length\n",
    "\n",
    "        cyc_start_datetimes += [cyc_start_datetime]\n",
    "        cyc_end_datetimes += [cyc_end_datetime]\n",
    "        dataset += ['CryoSat2_SARIn']\n",
    "\n",
    "# ICESat-2 data        \n",
    "if dataset2.attrs['identifier_product_DOI'] == 'doi:10.5067/ATLAS/ATL15.004':\n",
    "    for idx in range(len(dataset2.delta_h)):\n",
    "        cyc_start_datetime = dataset2.time.values[idx]\n",
    "        if idx < len(dataset2.delta_h)-1:\n",
    "            cyc_end_datetime = dataset2.time.values[idx+1]\n",
    "        else:\n",
    "            # For last cycle, use same length as previous cycle\n",
    "            last_cycle_length = dataset2.time.values[idx] - dataset2.time.values[idx-1]\n",
    "            cyc_end_datetime = cyc_start_datetime + last_cycle_length\n",
    "\n",
    "        cyc_start_datetimes += [cyc_start_datetime]\n",
    "        cyc_end_datetimes += [cyc_end_datetime]\n",
    "        dataset += ['ICESat2_ATL15']\n",
    "\n",
    "# Generate list of height change (dh) mid-point datetimes\n",
    "mid_pt_datetimes = []\n",
    "for i in range(len(cyc_start_datetimes) - 1):\n",
    "    mid_pt_date = cyc_start_datetimes[i] + 0.5 * (cyc_end_datetimes[i+1] - cyc_start_datetimes[i])\n",
    "    mid_pt_datetimes.append(mid_pt_date)\n",
    "\n",
    "# Add a NaT for the last entry to keep lengths aligned\n",
    "mid_pt_datetimes.append(np.datetime64('NaT'))\n",
    "mid_pt_datetimes = np.array(mid_pt_datetimes)\n",
    "\n",
    "# Concatenate list into pandas dataframe\n",
    "cyc_dates = pd.DataFrame({'cyc_start_datetimes': cyc_start_datetimes, \n",
    "                          'cyc_end_datetimes': cyc_end_datetimes, \n",
    "                          'mid_pt_datetimes': mid_pt_datetimes, \n",
    "                          'dataset': dataset})\n",
    "cyc_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b55ba4-316d-4b8f-9723-84af92f3bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store cycle dates list as csv for future use\n",
    "cyc_dates.to_csv('output/cycle_dates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c6b9bf-6f04-492c-9ab5-45df6f17823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cyc_dates to ensure writing worked properly \n",
    "cyc_dates = pd.read_csv('output/cycle_dates.csv', parse_dates=['cyc_start_datetimes', 'cyc_end_datetimes', 'mid_pt_datetimes'])\n",
    "\n",
    "# View dates\n",
    "cyc_dates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f0dcb9-d027-4e36-a9e7-1f70418ab10f",
   "metadata": {},
   "source": [
    "# CryoSat-2 SARIn mode mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a83715-849f-497a-a2b4-9a191d29569b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CryoSat-2 Geographical Mode Mask\n",
    "# https://earth.esa.int/eogateway/instruments/siral/geographical-mode-mask\n",
    "# Downloaded kml files of each version during pre-ICEsat-2 era of CryoSat-2 missin (version 3.1 to 3.9)\n",
    "# and coverted to geojson files using terminal commands, e.g.:\n",
    "# !ogr2ogr -f GeoJSON Cryosat2_mode_mask_v3.1.geojson Cryosat2_mode_mask_v3.1.kml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb6ea0-70c4-4c16-9e8b-01a4575f58a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the GeoDataFrames\n",
    "gdfs = {}\n",
    "\n",
    "# Loop over the specified range\n",
    "for idx in range(1, 10):  # Only doing up to version 9 because that temporally covers the pre-ICESat-2 era\n",
    "    # Construct the file path\n",
    "    file_path = f'input/CryoSat2_mode_masks/Cryosat2_mode_mask_v3.{idx}.geojson'\n",
    "    # Read the GeoJSON file and store it in the dictionary with a key corresponding to the current index\n",
    "    gdfs[f'gdf_3.{idx}'] = gpd.read_file(file_path)\n",
    "    \n",
    "# View head of first gdf to ensure readin worked properly\n",
    "gdfs['gdf_3.1'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b2046-fd8e-41a7-b04d-50a80c35fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names to filter on\n",
    "descrip_1 = 'LRM over Antarctica ice sheet'  # Interior Antarctica LRM limit\n",
    "descrip_2 = 'SARIn in Antarctica'  # Coastal Antarctic SARIn mode limit\n",
    "\n",
    "# Ensure geodataframes are not already transformed to EPSG:3031\n",
    "if gdfs['gdf_3.1'].crs != 'EPSG:3031':\n",
    "    for key, gdf in gdfs.items():\n",
    "        # Create a boolean mask for filtering\n",
    "        mask = gdf['description'].isin([descrip_1, descrip_2])\n",
    "\n",
    "        # Apply the mask using .loc to filter rows\n",
    "        gdf_filtered = gdf.loc[mask].copy()  # Using .copy() to explicitly make a copy\n",
    "\n",
    "        # Apply the transformation to the geometry of the filtered GeoDataFrame\n",
    "        gdf_filtered['geometry'] = gdf_filtered['geometry'].apply(transform_geometry)\n",
    "\n",
    "        # Set the new CRS for the filtered GeoDataFrame to EPSG:3031\n",
    "        gdf_filtered.set_crs(\"EPSG:3031\", inplace=True, allow_override=True)\n",
    "\n",
    "        # Update the dictionary with the filtered and transformed GeoDataFrame\n",
    "        gdfs[key] = gdf_filtered\n",
    "    \n",
    "# View head of first gdf to ensure transformation worked properly\n",
    "gdfs['gdf_3.1'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d9328a-d2d4-46e2-a270-17c0b7f7d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all versions of CryoSat-2 mode mask to see how it's changed\n",
    "\n",
    "# Magic function\n",
    "%matplotlib inline\n",
    "\n",
    "# Prepare a list of colors, enough to cover the number of GeoDataFrames\n",
    "colors = ['red', 'green', 'blue', 'orange', 'purple', 'brown', 'pink', 'gray', 'cyan']\n",
    "\n",
    "# Create a new figure and axis for plotting\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Initialize a list to store custom legend entries\n",
    "legend_patches = []\n",
    "\n",
    "# Iterate over each GeoDataFrame in the dictionary\n",
    "for (key, gdf), color in zip(gdfs.items(), colors):\n",
    "    # Plot each GeoDataFrame with a unique color\n",
    "    gdf.plot(ax=ax, edgecolor=color, facecolor='none')\n",
    "\n",
    "    # Create a patch for the legend\n",
    "    legend_patches.append(mpatches.Patch(color=color, label=key))\n",
    "\n",
    "# Overlay previously identified lake locations \n",
    "stationary_lakes_gdf.boundary.plot(ax=ax, color='blue')\n",
    "\n",
    "# Add a custom legend for the layers and stationary lakes\n",
    "legend_patches.append(mpatches.Patch(color='blue', label='stationary lakes'))\n",
    "\n",
    "# Add the custom legend to the plot\n",
    "ax.legend(handles=legend_patches)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2cc1c6-7994-40dc-a6a1-87d3138cf660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LRM mode limit has migrated during the CryoSat-2 era before the ICESat-2 era (2010-2018)\n",
    "# We can use area of the LRM mode to find when the mode mask shifted\n",
    "# Iterate over each GeoDataFrame in the dictionary\n",
    "for key, gdf in gdfs.items():\n",
    "    \n",
    "    # Calculate the area of each geometry and assign it to a new column 'area'\n",
    "    # Planar area, but we are only using to look at relative areas, so geodesic area isn't necessary here\n",
    "    gdf['area'] = gdf.geometry.area\n",
    "    \n",
    "    # Update the dictionary with the modified GeoDataFrame\n",
    "    gdfs[key] = gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8480fa96-8e6b-45d5-a83e-fea30257d60a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if running in an interactive environment like Jupyter Notebook\n",
    "# and try to import the display function from IPython.display\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    interactive_environment = True\n",
    "except ImportError:\n",
    "    interactive_environment = False\n",
    "    \n",
    "# The description to filter by\n",
    "filter_description = 'LRM over Antarctica ice sheet'\n",
    "\n",
    "# Iterate over each GeoDataFrame in the dictionary\n",
    "for key, gdf in gdfs.items():\n",
    "    print(f\"GeoDataFrame Key: {key} - Rows with Description: '{filter_description}'\")\n",
    "    \n",
    "    # Filter the GeoDataFrame based on the 'description' column\n",
    "    filtered_gdf = gdf[gdf['description'] == filter_description]\n",
    "    \n",
    "    # Check if the filtered GeoDataFrame is not empty\n",
    "    if not filtered_gdf.empty:\n",
    "        # If in an interactive environment, use display for a nicer format\n",
    "        if interactive_environment:\n",
    "            display(filtered_gdf['area'])\n",
    "        else:\n",
    "            print(filtered_gdf['area'])\n",
    "    else:\n",
    "        print(\"No rows match the specified description.\")\n",
    "    print(\"\\n\")  # Add a newline for better separation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178326db-fb13-43e1-a0d5-e8bee7eb7c07",
   "metadata": {},
   "source": [
    "According to the geodataframes of SARIn mode masks, version 3.6 (October 2014 according to https://earth.esa.int/eogateway/instruments/siral/geographical-mode-mask) is when the LRM mask shrank to smaller area, adding a bit more SARIn coverage in the continental interior of Anartica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3aa81-7781-4a3c-80c5-4a0ede37e2d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Requires larger 64 GB server instance\n",
    "\n",
    "# Plot CS2 counts with the SARIn mode mask\n",
    "\n",
    "os.makedirs(OUTPUT_DIR + '/CS2_counts', exist_ok=True)\n",
    "\n",
    "# Set x, y min, max bounds for plotting\n",
    "x_min=CryoSat2_SARIn['x'].min()\n",
    "x_max=CryoSat2_SARIn['x'].max()\n",
    "y_min=CryoSat2_SARIn['y'].min()\n",
    "y_max=CryoSat2_SARIn['y'].max()\n",
    "\n",
    "# Get total number of time steps for status tracking\n",
    "time_values = CryoSat2_SARIn['time'].values\n",
    "total_time_steps = len(time_values)\n",
    "\n",
    "for i, time_value in enumerate(time_values, 1):\n",
    "    # Print status update\n",
    "    print(f\"Plotting time step {i} of {total_time_steps}: {time_value}\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "    # Check if the time value is less than 2014.75\n",
    "    if time_value < np.datetime64('2014-10-01T18:00:00.000000000'):\n",
    "        # Access the part of the dataset corresponding to this time\n",
    "        # This might involve selecting data based on the 'time' coordinate\n",
    "        selected_data = CryoSat2_SARIn.sel(time=time_value)\n",
    "        m = ax.imshow(selected_data['data_count'][:,:], \n",
    "        extent=[x_min, x_max, y_min, y_max],\n",
    "        cmap='viridis', \n",
    "        vmax=1,  # Set colormapping max to 1 to highlight data-poor regions\n",
    "        origin='lower')\n",
    "    \n",
    "        # Plotting CS2 mode mask polygons from version 3.1 for time periods during version 3.1 to 3.5 since they don't change\n",
    "        gdfs['gdf_3.1'].plot(ax=ax, edgecolor='red', facecolor='none', zorder=2, label='SARIn mode mask 3.1')\n",
    "        # Plotting inland buffer to show how far data_counts extends inland of CS2 SARIn mode mask boundary\n",
    "        # to include edge case lakes Byrd_s1, Foundation_2, and Mac3 when including CS2 data into evolving outline search\n",
    "        gdfs['gdf_3.1'].buffer(-15e3).plot(ax=ax, edgecolor='purple', facecolor='none', zorder=2, label='SARIn mode mask 3.1 - 15 km inland buffer')\n",
    "        gdfs['gdf_3.6'].plot(ax=ax, edgecolor='blue', facecolor='none', zorder=2, label='SARIn mode mask 3.6')\n",
    "        \n",
    "        # Plot subglacial lake outlines\n",
    "        stationary_lakes_gdf.plot(ax=ax, edgecolor='blue', facecolor='none', zorder=3, label='active subglacial lake')\n",
    "\n",
    "    elif time_value >= np.datetime64('2014-10-01T18:00:00.000000000'):\n",
    "        # Access the part of the dataset corresponding to this time\n",
    "        # This might involve selecting data based on the 'time' coordinate\n",
    "        selected_data = CryoSat2_SARIn.sel(time=time_value)\n",
    "        m = ax.imshow(selected_data['data_count'][:,:], \n",
    "        extent=[x_min, x_max, y_min, y_max],\n",
    "        cmap='viridis', \n",
    "        vmax=1,  # Set colormapping max to 1 to highlight data-poor regions\n",
    "        origin='lower')\n",
    "    \n",
    "        # Plotting CS2 mode mask polygons from version 3.6 for time periods during version 3.6 to 3.9 since they don't change\n",
    "        gdfs['gdf_3.1'].plot(ax=ax, edgecolor='red', facecolor='none', zorder=2, label='SARIn mode mask 3.1')\n",
    "        gdfs['gdf_3.6'].plot(ax=ax, edgecolor='blue', facecolor='none', zorder=2, label='SARIn mode mask 3.6')\n",
    "        gdfs['gdf_3.6'].buffer(-15e3).plot(ax=ax, edgecolor='purple', facecolor='none', zorder=2, label='SARIn mode mask 3.1 - 15 km inland buffer')\n",
    "\n",
    "        # Plot subglacial lake outlines\n",
    "        stationary_lakes_gdf.plot(ax=ax, edgecolor='blue', facecolor='none', zorder=3, label='active subglacial lake')\n",
    "        \n",
    "    else:\n",
    "        print('time_value doesn\\'t match')\n",
    "\n",
    "    # Add colorbar\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "    cbar = fig.colorbar(m, cax=cax, extend='max')\n",
    "    cbar.set_label('count')\n",
    "    \n",
    "    # Creating custom legend entries as lines\n",
    "    line_3_1 = mlines.Line2D([], [], color='red', linewidth=2, label='v3.1')\n",
    "    line_3_6 = mlines.Line2D([], [], color='blue', linewidth=2, label='v3.6')\n",
    "\n",
    "    # Adding the custom legend to the plot with the created lines\n",
    "    ax.legend(handles=[line_3_1, line_3_6], title='CryoSat-2 SARIn mode mask')\n",
    "\n",
    "    # Add title\n",
    "    ax.set_title('CryoSat-2 SARIn counts quarter starting {}'.format(time_value))\n",
    "    \n",
    "    # Save and close fig\n",
    "    plt.savefig(OUTPUT_DIR + '/CS2_counts/CS2_counts_{}.png'.format(time_value))\n",
    "    plt.close()\n",
    "\n",
    "    # Clear output\n",
    "    clear_output(wait=True)\n",
    "\n",
    "print(f\"Completed plotting all {total_time_steps} time steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c0f2e8-d1f4-4ebd-ae90-d8e0f61ef443",
   "metadata": {},
   "source": [
    "Examining data_count plots at each time slice with SARIn mode boundaries saved in output directory, we observe that CryoSat-2's SARIn mode expanded inland starting 2013.75 and not the release date on mode mask version 3.6, October 2014 (2014.75)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4687a8-0818-4c56-9f40-f0b733f05262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV with date parsing\n",
    "cyc_dates = pd.read_csv('output/cycle_dates.csv', parse_dates=['cyc_start_datetimes', 'cyc_end_datetimes'])\n",
    "\n",
    "# Add a new \"note\" column with default empty strings\n",
    "cyc_dates['note'] = ''\n",
    "\n",
    "# Add the specific note to the matching row\n",
    "target_date = pd.Timestamp('2013-10-01T18:00:00.000000000')\n",
    "cyc_dates.loc[cyc_dates['cyc_start_datetimes'] == target_date, 'note'] = 'CryoSat-2 SARIn mode mask expands'\n",
    "\n",
    "# Save it back to CSV\n",
    "cyc_dates.to_csv('output/cycle_dates.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b700fd10-abb7-42bb-92f7-b7f4dcdffdc1",
   "metadata": {},
   "source": [
    "# Pre-process CS2 SARIn mode mask for plotting in Fig. 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe0284-2499-4a8a-8fb9-738e9b988af5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CS2 graphical mode mask in versions 3.6 and later of the original kml have line to pole\n",
    "gdfs['gdf_3.6'].boundary.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c6a7fd-3ea5-4f87-bf0c-6ad9f76b08b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Investigate aberrant line to near pole\n",
    "# Extract vertices of polygons\n",
    "points_list = []\n",
    "\n",
    "for polygon in gdfs['gdf_3.6'][gdfs['gdf_3.6']['description'] == 'LRM over Antarctica ice sheet'].geometry:\n",
    "    if polygon.geom_type == 'Polygon':\n",
    "        exterior_coords = polygon.exterior.coords\n",
    "        points_list.extend([Point(c) for c in exterior_coords])\n",
    "    elif polygon.geom_type == 'MultiPolygon':\n",
    "        for part in polygon:\n",
    "            exterior_coords = part.exterior.coords\n",
    "            points_list.extend([Point(c) for c in exterior_coords])\n",
    "\n",
    "# Create a new GeoDataFrame with these points\n",
    "points_gdf = gpd.GeoDataFrame(geometry=points_list)\n",
    "\n",
    "# Plot points\n",
    "fig, ax = plt.subplots()\n",
    "points_gdf.plot(ax=ax, marker='o', color='red', markersize=5)\n",
    "plt.show()\n",
    "\n",
    "# Delete temp gdf\n",
    "del points_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896e6336-76c3-4117-b545-01c7ca04e5e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create functions to remove point closest to the a specified of a polygon\n",
    "def remove_closest_point_to_centroid(polygon):\n",
    "    # Calculate the centroid of the polygon\n",
    "    point_of_interest = Point(0,0)\n",
    "    # Extract the points (coordinates) of the polygon's exterior\n",
    "    coords = list(polygon.exterior.coords)\n",
    "    \n",
    "    # Find the index of the point closest to the centroid\n",
    "    closest_point_index = np.argmin([Point(c).distance(point_of_interest) for c in coords[:-1]]) # Exclude the last point because it's the same as the first\n",
    "    \n",
    "    # Remove the closest point\n",
    "    new_coords = [coords[i] for i in range(len(coords)-1) if i != closest_point_index] # Exclude the last repeating point for accurate removal\n",
    "    \n",
    "    # Create a new polygon from the remaining points\n",
    "    new_polygon = Polygon(new_coords)\n",
    "    \n",
    "    return new_polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c89ca-0edb-4a5c-8b12-4f3c35230991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the function to each polygon in the GeoDataFrame\n",
    "# gdfs['gdf_3.6']['geometry'] = gdfs['gdf_3.6']['geometry'].apply(remove_closest_point_to_centroid)\n",
    "gdfs['gdf_3.6']['modified_geometry'] = gdfs['gdf_3.6']['geometry'].apply(remove_closest_point_to_centroid)\n",
    "\n",
    "# If you want to replace the original geometry with the modified one\n",
    "gdfs['gdf_3.6']['geometry'] = gdfs['gdf_3.6']['modified_geometry']\n",
    "del gdfs['gdf_3.6']['modified_geometry']\n",
    "\n",
    "# # Seems you need to run twice to work on all polygons\n",
    "# Apply the function to each polygon in the GeoDataFrame\n",
    "gdfs['gdf_3.6']['modified_geometry'] = gdfs['gdf_3.6']['geometry'].apply(remove_closest_point_to_centroid)\n",
    "\n",
    "# If you want to replace the original geometry with the modified one\n",
    "gdfs['gdf_3.6']['geometry'] = gdfs['gdf_3.6']['modified_geometry']\n",
    "del gdfs['gdf_3.6']['modified_geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caff351a-9060-4065-99db-b9c19f81d093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdfs['gdf_3.6'].boundary.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213794d9-2708-4be5-89aa-1313e8f65cee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now to find the area difference between the two boundaries for plotting purposes\n",
    "gdf_SARIn_3_1 = gdfs['gdf_3.1'][gdfs['gdf_3.1']['description'] == 'SARIn in Antarctica']\n",
    "gdf_LRM_3_1 = gdfs['gdf_3.1'][gdfs['gdf_3.1']['description'] == 'LRM over Antarctica ice sheet']\n",
    "gdf_SARIn_3_6 = gdfs['gdf_3.6'][gdfs['gdf_3.6']['description'] == 'SARIn in Antarctica']\n",
    "gdf_LRM_3_6 = gdfs['gdf_3.6'][gdfs['gdf_3.6']['description'] == 'LRM over Antarctica ice sheet']\n",
    "\n",
    "# Calculate the difference (area between the circles)\n",
    "difference_area_3_1 = gdf_SARIn_3_1['geometry'].iloc[0].difference(gdf_LRM_3_1['geometry'].iloc[0])\n",
    "difference_area_3_6 = gdf_SARIn_3_6['geometry'].iloc[0].difference(gdf_LRM_3_6['geometry'].iloc[0])\n",
    "\n",
    "# Compute the symmetric difference of the two SARIn masks to show area that SARIn mode increased\n",
    "symmetric_diff = difference_area_3_1.symmetric_difference(difference_area_3_6)\n",
    "\n",
    "# Delete variables no longer needed\n",
    "del gdf_LRM_3_1 \n",
    "del gdf_LRM_3_6\n",
    "\n",
    "# Create a new GeoDataFrame with the difference area\n",
    "gdf_SARIn_3_1 = gpd.GeoDataFrame([{'geometry': difference_area_3_1}], crs='EPSG:3031')\n",
    "gdf_SARIn_3_6 = gpd.GeoDataFrame([{'geometry': difference_area_3_6}], crs='EPSG:3031')\n",
    "\n",
    "# Create a new GeoDataFrame with the symmetric difference\n",
    "gdf_SARIn_3_1_3_6_diff = gpd.GeoDataFrame([{'geometry': symmetric_diff}], crs='EPSG:3031')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f220ebbe-e794-4548-8110-44b888552958",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot to ensure worked\n",
    "fix, ax = plt.subplots()\n",
    "gdf_SARIn_3_1.plot(ax=ax, alpha=0.5, color='blue')\n",
    "gdf_SARIn_3_6.plot(ax=ax, alpha=0.25, facecolor='none', edgecolor='k', hatch=r'\\\\')\n",
    "gdf_SARIn_3_1_3_6_diff.plot(ax=ax, alpha=0.5, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b602d-43ce-40ee-917d-221e44997ba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Difference of two mode masks has many small polygons \n",
    "gdf_SARIn_3_1_3_6_diff.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c155bd7-3d8d-4ebd-959f-573d66ad17c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove those to get a cleaner looking plot\n",
    "# Define function to do the job\n",
    "def keep_largest_polygon(geometry):\n",
    "    if isinstance(geometry, MultiPolygon):\n",
    "        # Use the .geoms attribute to explicitly iterate over polygons in a MultiPolygon\n",
    "        largest_polygon = max(geometry.geoms, key=lambda p: p.area)\n",
    "        return largest_polygon\n",
    "    else:\n",
    "        # If the geometry is not a MultiPolygon, just return it as is\n",
    "        return geometry\n",
    "\n",
    "# Apply the function to each geometry in the GeoDataFrame\n",
    "gdf_SARIn_3_1_3_6_diff['geometry'] = gdf_SARIn_3_1_3_6_diff['geometry'].apply(keep_largest_polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbed955-ac1c-42b0-a24f-421da496f35e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdf_SARIn_3_1_3_6_diff.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba539db4-dbe8-444d-bb94-32eef5a00e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot to ensure worked\n",
    "fix, ax = plt.subplots()\n",
    "gdf_SARIn_3_1.plot(ax=ax, alpha=0.5, color='blue')\n",
    "gdf_SARIn_3_6.plot(ax=ax, alpha=0.25, facecolor='none', edgecolor='k', hatch=r'\\\\')\n",
    "gdf_SARIn_3_1_3_6_diff.plot(ax=ax, alpha=0.5, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5146b967-736e-40c4-8400-f402d2315100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Export the geodataframes to use in Fig. 1\n",
    "gdf_SARIn_3_1.to_file('output/CryoSat2_SARIn_mode_masks/gdf_SARIn_3_1.geojson', driver='GeoJSON')\n",
    "gdf_SARIn_3_1_3_6_diff.to_file('output/CryoSat2_SARIn_mode_masks/gdf_SARIn_3_1_3_6_diff.geojson', driver='GeoJSON')\n",
    "gdf_SARIn_3_6.to_file('output/CryoSat2_SARIn_mode_masks/gdf_SARIn_3_6.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c10988-d3a3-4c65-8d26-e5ac102a1a68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
