{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a180750-d435-44ea-94f0-c96b9dbea099",
   "metadata": {
    "user_expressions": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Clip CS2 to SARIn mode mask? (at least for each epochs before it moved inland?\n",
    "# SARIn mode masks - should you set 4326 then convert to 3031?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff75638-a801-40a3-bd59-7d6536dbc054",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Set up computing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1703807-de8f-44de-b7d1-8e357e81bd05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install dependencies not pre-installed\n",
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47cdb0-00aa-4268-8691-9e44b3bfda2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import datetime\n",
    "import earthaccess\n",
    "import geopandas as gpd\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "import os\n",
    "from pyproj import CRS, Transformer\n",
    "from scipy.ndimage import zoom\n",
    "import xarray as xr\n",
    "from shapely.geometry import MultiPolygon, Point, Polygon\n",
    "\n",
    "# Magic functions\n",
    "%matplotlib widget\n",
    "\n",
    "# Define data directories dependent on home environment\n",
    "# Replace with your directory file paths\n",
    "if os.getenv('HOME') == '/home/jovyan':\n",
    "    DATA_DIR = '/home/jovyan/data'\n",
    "    SCRIPT_DIR = '/home/jovyan/repos_my/script_dir'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4adef9-57d4-4ab4-b066-ba4a4ebb38a5",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e66d080-b95c-4b12-81a7-3a4203464906",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ll2ps(lon, lat):\n",
    "    \"\"\"\n",
    "    Transform coordinates from geodetic coordinates (lon, lat)\n",
    "    to Antarctic Polar Stereograph coordinates (x, y)\n",
    "    x, y = ll2ps(lon, lat)\n",
    "    \"\"\"\n",
    "    crs_ll = CRS(\"EPSG:4326\")\n",
    "    crs_xy = CRS(\"EPSG:3031\")\n",
    "    ll_to_xy = Transformer.from_crs(crs_ll, crs_xy, always_xy = True)\n",
    "    x, y = ll_to_xy.transform(lon, lat)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb1610c-e580-4c1c-ac07-0e6b6d7fe5d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_geometry(geometry):\n",
    "    \"\"\"\n",
    "    Transform the geometry of a GeoDataFrame row from lon/lat to polar stereographic.\n",
    "    Handles Points, Polygons, and MultiPolygons.\n",
    "    \"\"\"\n",
    "    if geometry.geom_type == 'Polygon':\n",
    "        exterior = [(x, y) for x, y in zip(*geometry.exterior.coords.xy)]\n",
    "        transformed_exterior = [ll2ps(lon, lat) for lon, lat in exterior]\n",
    "        new_polygon = Polygon(transformed_exterior)\n",
    "        return new_polygon\n",
    "    elif geometry.geom_type == 'Point':\n",
    "        x, y = ll2ps(*geometry.coords[0])\n",
    "        return Point(x, y)\n",
    "    elif geometry.geom_type == 'MultiPolygon':\n",
    "        new_polygons = []\n",
    "        for polygon in geometry.geoms:\n",
    "            exterior = [(x, y) for x, y in zip(*polygon.exterior.coords.xy)]\n",
    "            transformed_exterior = [ll2ps(lon, lat) for lon, lat in exterior]\n",
    "            new_polygons.append(Polygon(transformed_exterior))\n",
    "        return MultiPolygon(new_polygons)\n",
    "    else:\n",
    "        # Add support for other geometry types as needed\n",
    "        raise ValueError(f\"Unsupported geometry type: {geometry.geom_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5e0e1c-b26e-4ee3-87eb-ceea82bc5d73",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505b5778-cc61-4d44-b9a8-8e94a59149d9",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Import previously identified subglacial lake locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e568cb9b-182f-4cdd-b4f5-eb42884fc60e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import subglacial lake outlines \n",
    "exec(open('0_lake_locations.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d3d484-68c7-446d-aea2-399d32f53b01",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Import the ATL14 DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b17078-d831-4b2d-a16d-214e5c79b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log into NASA Earthdata to search for datasets\n",
    "earthaccess.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3cc99-ac22-4a30-a2c2-70234d17995a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find ICESat-2 ATL14 data granules\n",
    "results = earthaccess.search_data(\n",
    "    short_name='ATL14',\n",
    "    version='003',\n",
    "    cloud_hosted=True,\n",
    "    bounding_box=(1, -89, -1, -89)  # (lower_left_lon, lower_left_lat , upper_right_lon, upper_right_lat))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500bc22c-4b6e-4040-8c01-584ff56b680a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open data granules as s3 files to stream\n",
    "files = earthaccess.open(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a874de29-7183-4165-b47e-6558f4811d30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd2806-3d07-435e-9927-a9e588a23a37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open each file, which are quadrants in polar stereographic coordinations around the Geographic South Pole\n",
    "ATL14_A1 = xr.open_dataset(files[3])\n",
    "ATL14_A2 = xr.open_dataset(files[1])\n",
    "ATL14_A3 = xr.open_dataset(files[0])\n",
    "ATL14_A4 = xr.open_dataset(files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd01bb1-87ff-4094-a16f-ffc2643af900",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the variables to keep\n",
    "variables_to_keep = ['x', 'y', 'h']\n",
    "\n",
    "# List of xarray datasets\n",
    "datasets = [ATL14_A1, ATL14_A2, ATL14_A3, ATL14_A4]\n",
    "\n",
    "# Function to drop variables not in variables_to_keep from a dataset\n",
    "def drop_unwanted_variables(dataset):\n",
    "    variables_to_drop = [var for var in dataset.variables if var not in variables_to_keep]\n",
    "    return dataset.drop_vars(variables_to_drop)\n",
    "\n",
    "# Apply the function to each dataset\n",
    "ATL14_A1, ATL14_A2, ATL14_A3, ATL14_A4 = [drop_unwanted_variables(ds) for ds in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170f6277-a5e0-4258-8a05-e87638414362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use xarray concatenation to stitch two quadrants togethers\n",
    "# Use xarray index selecting to occlude the duplicated x=0 vector of data\n",
    "ATL14_A12 = xr.concat([ATL14_A2.isel(x=slice(0,-1)), ATL14_A1], dim=\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce2748-eaca-4164-aa26-a3706cd5f34d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete quadrants no longer needed\n",
    "del ATL14_A1, ATL14_A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddce6f1-2487-4e8d-9f1d-f6144d16372d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use xarray concatenation to stitch two quadrants togethers\n",
    "# Use xarray index selecting to occlude the duplicated x=0 vector of data\n",
    "ATL14_A34 = xr.concat([ATL14_A3.isel(x=slice(0,-1)), ATL14_A4], dim='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb046819-0b08-493a-96f4-24d907f49b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete quadrants no longer needed\n",
    "del ATL14_A3, ATL14_A4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c167ed0-3bcf-400a-bc6f-9a6c37b3a121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use xarray concatenation to stitch two quadrants togethers\n",
    "# Use xarray index selecting to occlude the duplicated x=0 vector of data\n",
    "ATL14 = xr.concat([ATL14_A34.isel(y=slice(0,-1)), ATL14_A12], dim='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68162a96-0bec-4c33-bedc-00a0e5bb6792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete intermediary files no longer needed\n",
    "del ATL14_A12, ATL14_A34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc101ce-f777-4156-a47c-b94904b4b94e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ATL14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ab92f0-4a8d-4ec5-8575-04da102e56ab",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Import the Smith and others, 2017, TC method CryoSat-2 SARIn gridded DEM and dh data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f660db-d8ef-41aa-90ce-4892cb911a6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Smith and others, 2017, TC method CryoSat-2 (CS2) SARIn DEM data \n",
    "# https://doi.org/10.5194/tc-11-451-2017\n",
    "# (closed source aquired from Ben Smith)\n",
    "CS2_Smith2017_DEM = xr.open_dataset(\n",
    "    DATA_DIR + '/altimetry/CryoSat2/CS2_SARIn_Smith2017method/mos_z0_2016.0.nc')\n",
    "\n",
    "# View dataset metadata\n",
    "CS2_Smith2017_DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c5d64-8f83-4d5c-9478-979935235a59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Contstruct DEM using z0, SMB, and FAC\n",
    "# The DEM (z0) is corrected for SMB_a and FAC: \n",
    "# To get the actual surface elevation, add SMB_a+FAC.  \n",
    "# To get the FAC-free elevation, just add SMB_a\n",
    "CS2_Smith2017_DEM = CS2_Smith2017_DEM.assign(DEM = CS2_Smith2017_DEM['z0'] + CS2_Smith2017_DEM['SMB_a'] + CS2_Smith2017_DEM['FAC'])\n",
    "CS2_Smith2017_DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98065212-d54a-4586-a285-1644ec3194db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CS2_Smith2017_DEM['DEM'][:,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581bb594-a477-4df3-892d-b3890c4bc34e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Smith and others, 2017, TC method CryoSat-2 SARIn delta height data (closed source aquired from Ben Smith)\n",
    "CS2_Smith2017_dh = xr.open_dataset(DATA_DIR + '/altimetry/CryoSat2/CS2_SARIn_Smith2017method/mos_2010.5_2021.5.nc')\n",
    "CS2_Smith2017_dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df11853a-d29a-47ed-bb9d-0533d3dfbf82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Interpolate CS2_DEM_agg onto the coordinate grid of CS2_dh['delta_h']\n",
    "CS2_Smith2017_DEM_agg = CS2_Smith2017_DEM['DEM'].interp(x=CS2_Smith2017_dh['delta_h'].x, y=CS2_Smith2017_dh['delta_h'].y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795d50a5-e70e-4046-b8ff-6e144e5a606c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print shape of the datasets to be combined\n",
    "print(CS2_Smith2017_DEM.DEM.shape)\n",
    "print(CS2_Smith2017_DEM_agg.shape)\n",
    "print(CS2_Smith2017_dh.delta_h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daac0444-9ca1-4e86-abcd-c9eec74afec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign actual heights by combing dh's with the 2016 DEM\n",
    "CS2_Smith2017_dh['h'] = CS2_Smith2017_dh['delta_h'] + CS2_Smith2017_DEM_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91634fb9-135e-4f43-9f27-56a92d57c0a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reassign (rename) geodataframe to reflect more than dh\n",
    "CS2_Smith2017 = CS2_Smith2017_dh\n",
    "\n",
    "# Delete original reference (name)\n",
    "del CS2_Smith2017_dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d70389-cdc8-4dbe-b003-7be3fa25909d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete delta_h data variable so; we can replace with delta_h relative to ATL14 DEM\n",
    "CS2_Smith2017 = CS2_Smith2017.drop_vars('delta_h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ae2d6e-b213-44a2-a2c5-ca5ce6d6b9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename data variable to match ICESat-2 ATL15 data product\n",
    "CS2_Smith2017 = CS2_Smith2017.rename({'count': 'data_count'})\n",
    "CS2_Smith2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109bbab7-1815-4f4e-bd48-692b019a793c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assign delta heights differencing absolute heights with the ATL14 DEM\n",
    "CS2_Smith2017['delta_h'] = CS2_Smith2017['h'] - ATL14['h']\n",
    "# CS2_dh = CS2_dh.assign(delta_h = CS2_dh['h'] - ATL14['h'])\n",
    "\n",
    "# Add a 'description' attribute to the 'h' data variable\n",
    "CS2_Smith2017['delta_h'].attrs['description'] = 'Height change relative to the ATL14 datum (Jan 1, 2020) surface'\n",
    "\n",
    "CS2_Smith2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154874f2-79f6-4fb7-a89a-611b72113a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create lists to store data\n",
    "cyc_dates = []\n",
    "\n",
    "# Change time to match ICESat-2 time format\n",
    "for idx in range(len(CS2_Smith2017['time'])):\n",
    "    # Smith and others, 2017 method CryoSat-2 SARIn data\n",
    "    cyc_date = datetime.datetime(int(CS2_Smith2017['time'].values[idx]), 1, 1) + datetime.timedelta(days = (CS2_Smith2017['time'].values[idx] % 1) * 365.25)\n",
    "    cyc_date_np_dt = np.datetime64(cyc_date)  # Convert to numpy.datetime64 format\n",
    "    cyc_dates += [cyc_date_np_dt]\n",
    "CS2_Smith2017_newtime = CS2_Smith2017.assign_coords(time=cyc_dates)\n",
    "\n",
    "# Add a 'description' attribute to the 'time' data variable\n",
    "CS2_Smith2017_newtime.coords['time'].attrs['description'] = 'Time for each node'\n",
    "\n",
    "# Delete unneeded list\n",
    "del cyc_dates\n",
    "del CS2_Smith2017\n",
    "\n",
    "CS2_Smith2017_newtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afee6d1-9df6-4376-a6ac-d15f30bc183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove time slices that occur during the ICESat-2 era that will not be used to conserve memory when loaded in the future\n",
    "# Subset the dataset for a specific time range\n",
    "start_date = np.datetime64('2010-07-02T15:00:00.000000000')\n",
    "# end_date = np.datetime64('2018-07-02T15:00:00.000000000')  # No temporal overlap\n",
    "end_date = np.datetime64('2018-10-01T22:30:00.000000000')  # Includes one quarter of overlapping data allow for cyc start/end accounting \n",
    "\n",
    "CS2_Smith2017_subset = CS2_Smith2017_newtime.sel(time=slice(start_date, end_date))\n",
    "\n",
    "CS2_Smith2017_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f0dcb9-d027-4e36-a9e7-1f70418ab10f",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# CryoSat-2 SARIn mode mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a656a5f-c01d-4d8c-be8b-6c99b11aab6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CryoSat-2 Geographical Mode Mask\n",
    "# https://earth.esa.int/eogateway/instruments/siral/geographical-mode-mask\n",
    "# Downloaded kml files and coverted to geojson files in terminal using command:\n",
    "# ogr2ogr -f GeoJSON Cryosat2_mode_mask_v3.1.geojson Cryosat2_mode_mask_v3.1.kml\n",
    "\n",
    "# Initialize an empty dictionary to store the GeoDataFrames\n",
    "gdfs = {}\n",
    "\n",
    "# Loop over the specified range\n",
    "for idx in range(1, 10):  # Only doing up to version 9 because that temporally covers the pre-ICESat-2 era\n",
    "    # Construct the file path\n",
    "    file_path = f'input/CryoSat2_mode_masks/Cryosat2_mode_mask_v3.{idx}.geojson'\n",
    "    # Read the GeoJSON file and store it in the dictionary with a key corresponding to the current index\n",
    "    gdfs[f'gdf_3.{idx}'] = gpd.read_file(file_path)\n",
    "    \n",
    "# View head of first gdf to ensure readin worked properly\n",
    "gdfs['gdf_3.1'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b2046-fd8e-41a7-b04d-50a80c35fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names to filter by\n",
    "descrip_1 = 'LRM over Antarctica ice sheet'  # Interior Antarctica LRM limit\n",
    "descrip_2 = 'SARIn in Antarctica'  # Coastal Antarctic SARIn mode limit\n",
    "\n",
    "# Ensure geodataframes are not already transformed to epsg3031\n",
    "if gdfs['gdf_3.1'].crs != 'EPSG:3031':\n",
    "    for key, gdf in gdfs.items():\n",
    "        # Create a boolean mask for filtering\n",
    "        mask = gdf['description'].isin([descrip_1, descrip_2])\n",
    "\n",
    "        # Apply the mask using .loc to filter rows\n",
    "        gdf_filtered = gdf.loc[mask].copy()  # Using .copy() to explicitly make a copy\n",
    "\n",
    "        # Apply the transformation to the geometry of the filtered GeoDataFrame\n",
    "        gdf_filtered['geometry'] = gdf_filtered['geometry'].apply(transform_geometry)\n",
    "\n",
    "        # Set the new CRS for the filtered GeoDataFrame to EPSG:3031\n",
    "        gdf_filtered.set_crs(\"EPSG:3031\", inplace=True, allow_override=True)\n",
    "\n",
    "        # Update the dictionary with the filtered and transformed GeoDataFrame\n",
    "        gdfs[key] = gdf_filtered\n",
    "    \n",
    "# View head of first gdf to ensure transformation worked properly\n",
    "gdfs['gdf_3.1'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b47c3-993f-4b57-bd09-4c5f1619c9cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot all version of CS2 mode mask to see how it's changed\n",
    "\n",
    "# Prepare a list of colors, enough to cover the number of GeoDataFrames\n",
    "colors = ['red', 'green', 'blue', 'orange', 'purple', 'brown', 'pink', 'gray', 'cyan']\n",
    "\n",
    "# Create a new figure and axis for plotting\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Iterate over each GeoDataFrame in the dictionary\n",
    "for (key, gdf), color in zip(gdfs.items(), colors):\n",
    "    # Plot each GeoDataFrame with a unique color\n",
    "    gdf.plot(ax=ax, edgecolor=color, facecolor='none', label=key)\n",
    "\n",
    "# Overlay previously identified lake locations \n",
    "lakes_gdf.boundary.plot(ax=ax, color='blue', label=key)\n",
    "    \n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2cc1c6-7994-40dc-a6a1-87d3138cf660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LRM mode limit has migrated during the CS2 era not including IS2 era (2010-2018); we can use area of the LRM mode to find when the mode mask shifted\n",
    "# Iterate over each GeoDataFrame in the dictionary\n",
    "for key, gdf in gdfs.items():\n",
    "    # Calculate the area of each geometry and assign it to a new column 'area'\n",
    "    # Note: This assumes the geometries are in a suitable CRS for area calculation\n",
    "    gdf['area'] = gdf.geometry.area\n",
    "    \n",
    "    # Update the dictionary with the modified GeoDataFrame\n",
    "    gdfs[key] = gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8480fa96-8e6b-45d5-a83e-fea30257d60a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if running in an interactive environment like Jupyter Notebook\n",
    "# and try to import the display function from IPython.display\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    interactive_environment = True\n",
    "except ImportError:\n",
    "    interactive_environment = False\n",
    "    \n",
    "# The description to filter by\n",
    "filter_description = 'LRM over Antarctica ice sheet'\n",
    "\n",
    "# Iterate over each GeoDataFrame in the dictionary\n",
    "for key, gdf in gdfs.items():\n",
    "    print(f\"GeoDataFrame Key: {key} - Rows with Description: '{filter_description}'\")\n",
    "    \n",
    "    # Filter the GeoDataFrame based on the 'description' column\n",
    "    filtered_gdf = gdf[gdf['description'] == filter_description]\n",
    "    \n",
    "    # Check if the filtered GeoDataFrame is not empty\n",
    "    if not filtered_gdf.empty:\n",
    "        # If in an interactive environment, use display for a nicer format\n",
    "        if interactive_environment:\n",
    "            display(filtered_gdf)\n",
    "        else:\n",
    "            print(filtered_gdf)\n",
    "    else:\n",
    "        print(\"No rows match the specified description.\")\n",
    "    print(\"\\n\")  # Add a newline for better separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34894ff2-461f-4756-9f0a-c84fbb6ee953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 3.6 (October 2014) is when the LRM mask shrank to smaller area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3aa81-7781-4a3c-80c5-4a0ede37e2d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot CS2 counts with the SARIn mode mask\n",
    "import matplotlib.lines as mlines\n",
    "# Set x, y min, max bounds for plotting\n",
    "x_min=CS2_Smith2017_subset['x'].min()\n",
    "x_max=CS2_Smith2017_subset['x'].max()\n",
    "y_min=CS2_Smith2017_subset['y'].min()\n",
    "y_max=CS2_Smith2017_subset['y'].max()\n",
    "\n",
    "for time_value in CS2_Smith2017_subset['time'].values:\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "    # Check if the time value is less than 2014.75\n",
    "    if time_value < np.datetime64('2013-10-01T22:20:00.000000000'):\n",
    "        # Access the part of the dataset corresponding to this time\n",
    "        # This might involve selecting data based on the 'time' coordinate\n",
    "        selected_data = CS2_Smith2017_subset.sel(time=time_value)\n",
    "        m = ax.imshow(selected_data['data_count'][:,:], \n",
    "        extent=[x_min, x_max, y_min, y_max],\n",
    "        cmap='viridis', \n",
    "        vmax=1,  # Set colormapping max to 1 to highlight data-poor regions\n",
    "        origin='lower')\n",
    "    \n",
    "        # Plotting CS2 mode mask polygons from version 3.1 for time periods during version 3.1 to 3.5 since they don't change\n",
    "        gdfs['gdf_3.1'].plot(ax=ax, edgecolor='red', facecolor='none', zorder=2, label='SARIn mode mask 3.1')\n",
    "        # Plotting inland buffer to show how far data_counts extends inland of CS2 SARIn mode mask boundary\n",
    "        # to include edge case lakes Byrd_s1, Foundation_2, and Mac3 when including CS2 data into evolving outline search\n",
    "        gdfs['gdf_3.1'].buffer(-15e3).plot(ax=ax, edgecolor='purple', facecolor='none', zorder=2, label='SARIn mode mask 3.1 - 15 km inland buffer')\n",
    "        gdfs['gdf_3.6'].plot(ax=ax, edgecolor='blue', facecolor='none', zorder=2, label='SARIn mode mask 3.6')\n",
    "        \n",
    "        # Plot subglacial lake outlines\n",
    "        lakes_gdf.plot(ax=ax, edgecolor='blue', facecolor='none', zorder=3, label='active subglacial lake')\n",
    "\n",
    "    elif time_value >= np.datetime64('2013-10-01T22:20:00.000000000'):\n",
    "        # Access the part of the dataset corresponding to this time\n",
    "        # This might involve selecting data based on the 'time' coordinate\n",
    "        selected_data = CS2_Smith2017_subset.sel(time=time_value)\n",
    "        m = ax.imshow(selected_data['data_count'][:,:], \n",
    "        extent=[x_min, x_max, y_min, y_max],\n",
    "        cmap='viridis', \n",
    "        vmax=1,  # Set colormapping max to 1 to highlight data-poor regions\n",
    "        origin='lower')\n",
    "    \n",
    "        # Plotting CS2 mode mask polygons from version 3.6 for time periods during version 3.6 to 3.9 since they don't change\n",
    "        gdfs['gdf_3.1'].plot(ax=ax, edgecolor='red', facecolor='none', zorder=2, label='SARIn mode mask 3.1')\n",
    "        gdfs['gdf_3.6'].plot(ax=ax, edgecolor='blue', facecolor='none', zorder=2, label='SARIn mode mask 3.6')\n",
    "        gdfs['gdf_3.6'].buffer(-15e3).plot(ax=ax, edgecolor='purple', facecolor='none', zorder=2, label='SARIn mode mask 3.1 - 15 km inland buffer')\n",
    "\n",
    "        # Plot subglacial lake outlines\n",
    "        lakes_gdf.plot(ax=ax, edgecolor='blue', facecolor='none', zorder=3, label='active subglacial lake')\n",
    "        \n",
    "    else:\n",
    "        print('time_value doesn\\'t match')\n",
    "\n",
    "    # Add colorbar\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "    cbar = fig.colorbar(m, cax=cax, extend='max')\n",
    "    cbar.set_label('count')\n",
    "    \n",
    "    # Creating custom legend entries as lines\n",
    "    line_3_1 = mlines.Line2D([], [], color='red', linewidth=2, label='v3.1')\n",
    "    line_3_6 = mlines.Line2D([], [], color='blue', linewidth=2, label='v3.6')\n",
    "\n",
    "    # Adding the custom legend to the plot with the created lines\n",
    "    ax.legend(handles=[line_3_1, line_3_6], title='CryoSat-2 SARIn mode mask')\n",
    "\n",
    "    # Add title\n",
    "    ax.set_title('CryoSat-2 SARIn counts quarter starting {}'.format(time_value))\n",
    "    \n",
    "    # Plot and save fig\n",
    "    # plt.savefig('/home/jovyan/1_outlines_candidates/output/0_preprocess_data.ipynb/CS2_counts/CS2_counts_{}.png'.format(time_value))\n",
    "\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989fad94-4564-4ab7-a7e3-e0d5590a1f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining counts plot with SARIn mode boundaries CS2 moves LRM boundary inland starting 2013.75\n",
    "# and not the release date on mode mask version 3.6 (October 2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b700fd10-abb7-42bb-92f7-b7f4dcdffdc1",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Pre-process CS2 SARIn mode mask for plotting in Fig. 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe0284-2499-4a8a-8fb9-738e9b988af5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CS2 graphical mode mask in versions 3.6 and later of the original kml have line to pole\n",
    "gdfs['gdf_3.6'].boundary.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c6a7fd-3ea5-4f87-bf0c-6ad9f76b08b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Investigate aberrant line to near pole\n",
    "# Extract vertices of polygons\n",
    "points_list = []\n",
    "\n",
    "for polygon in gdfs['gdf_3.6'][gdfs['gdf_3.6']['description'] == 'LRM over Antarctica ice sheet'].geometry:\n",
    "    if polygon.geom_type == 'Polygon':\n",
    "        exterior_coords = polygon.exterior.coords\n",
    "        points_list.extend([Point(c) for c in exterior_coords])\n",
    "    elif polygon.geom_type == 'MultiPolygon':\n",
    "        for part in polygon:\n",
    "            exterior_coords = part.exterior.coords\n",
    "            points_list.extend([Point(c) for c in exterior_coords])\n",
    "\n",
    "# Create a new GeoDataFrame with these points\n",
    "points_gdf = gpd.GeoDataFrame(geometry=points_list)\n",
    "\n",
    "# Plot points\n",
    "fig, ax = plt.subplots()\n",
    "points_gdf.plot(ax=ax, marker='o', color='red', markersize=5)\n",
    "plt.show()\n",
    "\n",
    "# Delete temp gdf\n",
    "del points_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896e6336-76c3-4117-b545-01c7ca04e5e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create functions to remove point closest to the a specified of a polygon\n",
    "def remove_closest_point_to_centroid(polygon):\n",
    "    # Calculate the centroid of the polygon\n",
    "    point_of_interest = Point(0,0)\n",
    "    # Extract the points (coordinates) of the polygon's exterior\n",
    "    coords = list(polygon.exterior.coords)\n",
    "    \n",
    "    # Find the index of the point closest to the centroid\n",
    "    closest_point_index = np.argmin([Point(c).distance(point_of_interest) for c in coords[:-1]]) # Exclude the last point because it's the same as the first\n",
    "    \n",
    "    # Remove the closest point\n",
    "    new_coords = [coords[i] for i in range(len(coords)-1) if i != closest_point_index] # Exclude the last repeating point for accurate removal\n",
    "    \n",
    "    # Create a new polygon from the remaining points\n",
    "    new_polygon = Polygon(new_coords)\n",
    "    \n",
    "    return new_polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c89ca-0edb-4a5c-8b12-4f3c35230991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the function to each polygon in the GeoDataFrame\n",
    "# gdfs['gdf_3.6']['geometry'] = gdfs['gdf_3.6']['geometry'].apply(remove_closest_point_to_centroid)\n",
    "gdfs['gdf_3.6']['modified_geometry'] = gdfs['gdf_3.6']['geometry'].apply(remove_closest_point_to_centroid)\n",
    "\n",
    "# If you want to replace the original geometry with the modified one\n",
    "gdfs['gdf_3.6']['geometry'] = gdfs['gdf_3.6']['modified_geometry']\n",
    "del gdfs['gdf_3.6']['modified_geometry']\n",
    "\n",
    "# # Seems you need to run twice to work on all polygons\n",
    "# Apply the function to each polygon in the GeoDataFrame\n",
    "gdfs['gdf_3.6']['modified_geometry'] = gdfs['gdf_3.6']['geometry'].apply(remove_closest_point_to_centroid)\n",
    "\n",
    "# If you want to replace the original geometry with the modified one\n",
    "gdfs['gdf_3.6']['geometry'] = gdfs['gdf_3.6']['modified_geometry']\n",
    "del gdfs['gdf_3.6']['modified_geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caff351a-9060-4065-99db-b9c19f81d093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdfs['gdf_3.6'].boundary.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213794d9-2708-4be5-89aa-1313e8f65cee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now to find the area difference between the two boundaries for plotting purposes\n",
    "gdf_SARIn_3_1 = gdfs['gdf_3.1'][gdfs['gdf_3.1']['description'] == 'SARIn in Antarctica']\n",
    "gdf_LRM_3_1 = gdfs['gdf_3.1'][gdfs['gdf_3.1']['description'] == 'LRM over Antarctica ice sheet']\n",
    "gdf_SARIn_3_6 = gdfs['gdf_3.6'][gdfs['gdf_3.6']['description'] == 'SARIn in Antarctica']\n",
    "gdf_LRM_3_6 = gdfs['gdf_3.6'][gdfs['gdf_3.6']['description'] == 'LRM over Antarctica ice sheet']\n",
    "\n",
    "# Calculate the difference (area between the circles)\n",
    "difference_area_3_1 = gdf_SARIn_3_1['geometry'].iloc[0].difference(gdf_LRM_3_1['geometry'].iloc[0])\n",
    "difference_area_3_6 = gdf_SARIn_3_6['geometry'].iloc[0].difference(gdf_LRM_3_6['geometry'].iloc[0])\n",
    "\n",
    "# Compute the symmetric difference of the two SARIn masks to show area that SARIn mode increased\n",
    "symmetric_diff = difference_area_3_1.symmetric_difference(difference_area_3_6)\n",
    "\n",
    "# Delete variables no longer needed\n",
    "del gdf_LRM_3_1 \n",
    "del gdf_LRM_3_6\n",
    "\n",
    "# Create a new GeoDataFrame with the difference area\n",
    "gdf_SARIn_3_1 = gpd.GeoDataFrame([{'geometry': difference_area_3_1}], crs='EPSG:3031')\n",
    "gdf_SARIn_3_6 = gpd.GeoDataFrame([{'geometry': difference_area_3_6}], crs='EPSG:3031')\n",
    "\n",
    "# Create a new GeoDataFrame with the symmetric difference\n",
    "gdf_SARIn_3_1_3_6_diff = gpd.GeoDataFrame([{'geometry': symmetric_diff}], crs='EPSG:3031')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f220ebbe-e794-4548-8110-44b888552958",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot to ensure worked\n",
    "fix, ax = plt.subplots()\n",
    "gdf_SARIn_3_1.plot(ax=ax, alpha=0.5, color='blue')\n",
    "gdf_SARIn_3_6.plot(ax=ax, alpha=0.25, facecolor='none', edgecolor='k', hatch=r'\\\\')\n",
    "gdf_SARIn_3_1_3_6_diff.plot(ax=ax, alpha=0.5, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b602d-43ce-40ee-917d-221e44997ba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Difference of two mode masks has many small polygons \n",
    "gdf_SARIn_3_1_3_6_diff.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c155bd7-3d8d-4ebd-959f-573d66ad17c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove those to get a cleaner looking plot\n",
    "# Define function to do the job\n",
    "def keep_largest_polygon(geometry):\n",
    "    if isinstance(geometry, MultiPolygon):\n",
    "        # Use the .geoms attribute to explicitly iterate over polygons in a MultiPolygon\n",
    "        largest_polygon = max(geometry.geoms, key=lambda p: p.area)\n",
    "        return largest_polygon\n",
    "    else:\n",
    "        # If the geometry is not a MultiPolygon, just return it as is\n",
    "        return geometry\n",
    "\n",
    "# Apply the function to each geometry in the GeoDataFrame\n",
    "gdf_SARIn_3_1_3_6_diff['geometry'] = gdf_SARIn_3_1_3_6_diff['geometry'].apply(keep_largest_polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbed955-ac1c-42b0-a24f-421da496f35e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdf_SARIn_symmetric_diff.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba539db4-dbe8-444d-bb94-32eef5a00e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot to ensure worked\n",
    "fix, ax = plt.subplots()\n",
    "gdf_SARIn_3_1.plot(ax=ax, alpha=0.5, color='blue')\n",
    "gdf_SARIn_3_6.plot(ax=ax, alpha=0.25, facecolor='none', edgecolor='k', hatch=r'\\\\')\n",
    "gdf_SARIn_3_1_3_6_diff.plot(ax=ax, alpha=0.5, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5146b967-736e-40c4-8400-f402d2315100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's export the geodataframe to use in Fig. 1\n",
    "gdf_SARIn_3_1.to_file('output/CS2_SARIn_mode_masks/gdf_SARIn_3_1.geojson', driver='GeoJSON')\n",
    "gdf_SARIn_3_1_3_6_diff.to_file('output/CS2_SARIn_mode_masks/gdf_SARIn_3_1_3_6_diff.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d44ae82-cae9-40c2-9f34-33e0b34c21e6",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# CS2 SARIn mode mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871df1b0-914a-402d-84cd-3c75d3674d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CS2 counts with active subglacial lake outlines and SARIn mode mask at two time slices \n",
    "# to see if clipping to SARIn mode mask would remove useful data at previously identified lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9328614-36a7-455a-b68b-4299bd33b20c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CS2_Smith2017_subset.time.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8958337c-e622-4b06-8cea-45aa809193cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CS2_Smith2017_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f402628-1c0a-4f27-bfae-7d9cc58588c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "x_min=CS2_Smith2017_subset['x'].min()\n",
    "x_max=CS2_Smith2017_subset['x'].max()\n",
    "y_min=CS2_Smith2017_subset['y'].min()\n",
    "y_max=CS2_Smith2017_subset['y'].max()\n",
    "\n",
    "m = ax.imshow(CS2_Smith2017_subset['count'].sel(time=np.datetime64('2010-07-02T15:00:00.000000000')), \n",
    "    extent=[x_min, x_max, y_min, y_max],\n",
    "    cmap='viridis', vmax=1,\n",
    "    origin='lower'\n",
    "             )\n",
    "lakes_gdf.boundary.plot(ax=ax, color='blue', zorder=2)\n",
    "gdf_SARIn_3_1.boundary.plot(ax=ax, color='red', zorder=2)\n",
    "gdf_SARIn_symmetric_diff.boundary.plot(ax=ax, color='red', zorder=2)\n",
    "\n",
    "# Add colorbar\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "cbar = fig.colorbar(m, cax=cax, extend='max')\n",
    "cbar.set_label('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becc3d0f-4678-47b9-9a06-fdb68f19445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There appears to be some useful data and the boundary and within the LRM mode mask, \n",
    "# so I will not clip out the LRM mode from the data\n",
    "# Instead I will plot CS2 counts at each time slice at previously identified lakes\n",
    "# to detemine when time slices have inadequate data to warrant generating an evolving outline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee0833e-8406-4315-ad71-d36177c470c2",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Export pre-processed CS2 data to netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88deafe9-3c72-4861-9c97-b02ee1da362b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CS2_Smith2017_subset.to_netcdf(DATA_DIR + '/altimetry/CryoSat2/CS2_SARIn_Smith2017method/mos_2010.5_2019.0_relative_to_ATL14.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6712ba9e-9ba5-41d9-b371-ad558daeff71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure data import of exported netcdf works properly\n",
    "# Import Smith and others, 2017, TC method CryoSat-2 SARIn height and dheight data (closed source aquired from Ben Smith)\n",
    "CS2_Smith2017 = xr.open_dataset(DATA_DIR + '/altimetry/CryoSat2/CS2_SARIn_Smith2017method/mos_2010.5_2021.5_relative_to_ATL14.nc')\n",
    "CS2_Smith2017"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
