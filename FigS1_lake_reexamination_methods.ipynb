{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22381a81-8f1c-4d0d-9058-48c340a47d1a",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* add uncorr dh to Fig S1 and other output plots\n",
    "* add union of cumulative dh and dV panels of plot_evolving_and_stationary_comparison and plot_evolving_and_stationary_comparison_sequential functions\n",
    "* add kwarg that allows you to plot cumulative dh (vs cyc to cyc dh) for the output visualizations\n",
    "* \"/home/jovyan/1_evolving_lakes/Sauthoff-202X-evolving-lakes/output/lake_outlines/evolving_outlines_union_gdf.geojson\" appears to be the same as \"/home/jovyan/1_evolving_lakes/Sauthoff-202X-evolving-lakes/output/lake_outlines/stationary_outlines/stationary_outlines_gdf.geojson\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c199d3a-7236-40c4-b1ed-33f946685e07",
   "metadata": {},
   "source": [
    "Code to do data analysis of re-examined active subglacial lakes and create Fig. S1 in Sauthoff and others, 202X, _Journal_.\n",
    "\n",
    "Written 2023-07-11 by W. Sauthoff (wsauthoff.github.io)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff75638-a801-40a3-bd59-7d6536dbc054",
   "metadata": {},
   "source": [
    "# Set up computing environment\n",
    "\n",
    "This code runs continental-scale operations on multiple datasets and requires a ~64 GB server or local memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44cf437b-64b9-4534-b7a0-4604b1ede61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install earthaccess --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67b2a28f-a575-4f1e-b1c9-f601327149aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openpyxl --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ffcf4c9-ea9f-4c3c-b28a-e5b7c358bd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78f802c6-6310-465e-9710-7d60a356cc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyogrio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad47cdb0-00aa-4268-8691-9e44b3bfda2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "  var py_version = '3.2.2'.replace('rc', '-rc.').replace('.dev', '-dev.');\n",
       "  var is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1;\n",
       "  var reloading = false;\n",
       "  var Bokeh = root.Bokeh;\n",
       "  var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n",
       "\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks;\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "    if (js_modules == null) js_modules = [];\n",
       "    if (js_exports == null) js_exports = {};\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    if (!reloading) {\n",
       "      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    }\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "    window._bokeh_on_load = on_load\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    var skip = [];\n",
       "    if (window.requirejs) {\n",
       "      window.requirejs.config({'packages': {}, 'paths': {'jspanel': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/jspanel', 'jspanel-modal': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal', 'jspanel-tooltip': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip', 'jspanel-hint': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint', 'jspanel-layout': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout', 'jspanel-contextmenu': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu', 'jspanel-dock': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@7.2.3/dist/gridstack-all', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'jspanel': {'exports': 'jsPanel'}, 'gridstack': {'exports': 'GridStack'}}});\n",
       "      require([\"jspanel\"], function(jsPanel) {\n",
       "\twindow.jsPanel = jsPanel\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-modal\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-tooltip\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-hint\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-layout\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-contextmenu\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-dock\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"gridstack\"], function(GridStack) {\n",
       "\twindow.GridStack = GridStack\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"notyf\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      root._bokeh_is_loading = css_urls.length + 9;\n",
       "    } else {\n",
       "      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n",
       "    }\n",
       "\n",
       "    var existing_stylesheets = []\n",
       "    var links = document.getElementsByTagName('link')\n",
       "    for (var i = 0; i < links.length; i++) {\n",
       "      var link = links[i]\n",
       "      if (link.href != null) {\n",
       "\texisting_stylesheets.push(link.href)\n",
       "      }\n",
       "    }\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      if (existing_stylesheets.indexOf(url) !== -1) {\n",
       "\ton_load()\n",
       "\tcontinue;\n",
       "      }\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }    if (((window['jsPanel'] !== undefined) && (!(window['jsPanel'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/jspanel.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/gridstack/gridstack@7.2.3/dist/gridstack-all.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    var existing_scripts = []\n",
       "    var scripts = document.getElementsByTagName('script')\n",
       "    for (var i = 0; i < scripts.length; i++) {\n",
       "      var script = scripts[i]\n",
       "      if (script.src != null) {\n",
       "\texisting_scripts.push(script.src)\n",
       "      }\n",
       "    }\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n",
       "\tif (!window.requirejs) {\n",
       "\t  on_load();\n",
       "\t}\n",
       "\tcontinue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    for (var i = 0; i < js_modules.length; i++) {\n",
       "      var url = js_modules[i];\n",
       "      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n",
       "\tif (!window.requirejs) {\n",
       "\t  on_load();\n",
       "\t}\n",
       "\tcontinue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      element.type = \"module\";\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    for (const name in js_exports) {\n",
       "      var url = js_exports[name];\n",
       "      if (skip.indexOf(url) >= 0 || root[name] != null) {\n",
       "\tif (!window.requirejs) {\n",
       "\t  on_load();\n",
       "\t}\n",
       "\tcontinue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.type = \"module\";\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      element.textContent = `\n",
       "      import ${name} from \"${url}\"\n",
       "      window.${name} = ${name}\n",
       "      window._bokeh_on_load()\n",
       "      `\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    if (!js_urls.length && !js_modules.length) {\n",
       "      on_load()\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.2.2.min.js\", \"https://cdn.holoviz.org/panel/1.2.3/dist/panel.min.js\"];\n",
       "  var js_modules = [];\n",
       "  var js_exports = {};\n",
       "  var css_urls = [];\n",
       "  var inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }\n",
       "      // Cache old bokeh versions\n",
       "      if (Bokeh != undefined && !reloading) {\n",
       "\tvar NewBokeh = root.Bokeh;\n",
       "\tif (Bokeh.versions === undefined) {\n",
       "\t  Bokeh.versions = new Map();\n",
       "\t}\n",
       "\tif (NewBokeh.version !== Bokeh.version) {\n",
       "\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n",
       "\t}\n",
       "\troot.Bokeh = Bokeh;\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    }\n",
       "    root._bokeh_is_initializing = false\n",
       "  }\n",
       "\n",
       "  function load_or_wait() {\n",
       "    // Implement a backoff loop that tries to ensure we do not load multiple\n",
       "    // versions of Bokeh and its dependencies at the same time.\n",
       "    // In recent versions we use the root._bokeh_is_initializing flag\n",
       "    // to determine whether there is an ongoing attempt to initialize\n",
       "    // bokeh, however for backward compatibility we also try to ensure\n",
       "    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n",
       "    // before older versions are fully initialized.\n",
       "    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n",
       "      root._bokeh_is_initializing = false;\n",
       "      root._bokeh_onload_callbacks = undefined;\n",
       "      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n",
       "      load_or_wait();\n",
       "    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n",
       "      setTimeout(load_or_wait, 100);\n",
       "    } else {\n",
       "      Bokeh = root.Bokeh;\n",
       "      bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n",
       "      root._bokeh_is_initializing = true\n",
       "      root._bokeh_onload_callbacks = []\n",
       "      if (!reloading && (!bokeh_loaded || is_dev)) {\n",
       "\troot.Bokeh = undefined;\n",
       "      }\n",
       "      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n",
       "\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "\trun_inline_js();\n",
       "      });\n",
       "    }\n",
       "  }\n",
       "  // Give older versions of the autoload script a head-start to ensure\n",
       "  // they initialize before we start loading newer version.\n",
       "  setTimeout(load_or_wait, 100)\n",
       "}(window));"
      ],
      "application/vnd.holoviews_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.2.2'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1;\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n  var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'jspanel': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/jspanel', 'jspanel-modal': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal', 'jspanel-tooltip': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip', 'jspanel-hint': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint', 'jspanel-layout': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout', 'jspanel-contextmenu': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu', 'jspanel-dock': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@7.2.3/dist/gridstack-all', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'jspanel': {'exports': 'jsPanel'}, 'gridstack': {'exports': 'GridStack'}}});\n      require([\"jspanel\"], function(jsPanel) {\n\twindow.jsPanel = jsPanel\n\ton_load()\n      })\n      require([\"jspanel-modal\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-tooltip\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-hint\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-layout\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-contextmenu\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-dock\"], function() {\n\ton_load()\n      })\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 9;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window['jsPanel'] !== undefined) && (!(window['jsPanel'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/jspanel.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/gridstack/gridstack@7.2.3/dist/gridstack-all.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.2.2.min.js\", \"https://cdn.holoviz.org/panel/1.2.3/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      Bokeh = root.Bokeh;\n      bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      if (!reloading && (!bokeh_loaded || is_dev)) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "if ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n",
       "  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n",
       "}\n",
       "\n",
       "\n",
       "    function JupyterCommManager() {\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n",
       "      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        comm_manager.register_target(comm_id, function(comm) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        });\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        });\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n",
       "          var messages = comm.messages[Symbol.asyncIterator]();\n",
       "          function processIteratorResult(result) {\n",
       "            var message = result.value;\n",
       "            console.log(message)\n",
       "            var content = {data: message.data, comm_id};\n",
       "            var buffers = []\n",
       "            for (var buffer of message.buffers || []) {\n",
       "              buffers.push(new DataView(buffer))\n",
       "            }\n",
       "            var metadata = message.metadata || {};\n",
       "            var msg = {content, buffers, metadata}\n",
       "            msg_handler(msg);\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "          return messages.next().then(processIteratorResult);\n",
       "        })\n",
       "      }\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n",
       "      if (comm_id in window.PyViz.comms) {\n",
       "        return window.PyViz.comms[comm_id];\n",
       "      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n",
       "        if (msg_handler) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        }\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n",
       "        comm.open();\n",
       "        if (msg_handler) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        }\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        var comm_promise = google.colab.kernel.comms.open(comm_id)\n",
       "        comm_promise.then((comm) => {\n",
       "          window.PyViz.comms[comm_id] = comm;\n",
       "          if (msg_handler) {\n",
       "            var messages = comm.messages[Symbol.asyncIterator]();\n",
       "            function processIteratorResult(result) {\n",
       "              var message = result.value;\n",
       "              var content = {data: message.data};\n",
       "              var metadata = message.metadata || {comm_id};\n",
       "              var msg = {content, metadata}\n",
       "              msg_handler(msg);\n",
       "              return messages.next().then(processIteratorResult);\n",
       "            }\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "        }) \n",
       "        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n",
       "          return comm_promise.then((comm) => {\n",
       "            comm.send(data, metadata, buffers, disposeOnDone);\n",
       "          });\n",
       "        };\n",
       "        var comm = {\n",
       "          send: sendClosure\n",
       "        };\n",
       "      }\n",
       "      window.PyViz.comms[comm_id] = comm;\n",
       "      return comm;\n",
       "    }\n",
       "    window.PyViz.comm_manager = new JupyterCommManager();\n",
       "    \n",
       "\n",
       "\n",
       "var JS_MIME_TYPE = 'application/javascript';\n",
       "var HTML_MIME_TYPE = 'text/html';\n",
       "var EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\n",
       "var CLASS_NAME = 'output';\n",
       "\n",
       "/**\n",
       " * Render data to the DOM node\n",
       " */\n",
       "function render(props, node) {\n",
       "  var div = document.createElement(\"div\");\n",
       "  var script = document.createElement(\"script\");\n",
       "  node.appendChild(div);\n",
       "  node.appendChild(script);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when a new output is added\n",
       " */\n",
       "function handle_add_output(event, handle) {\n",
       "  var output_area = handle.output_area;\n",
       "  var output = handle.output;\n",
       "  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "    return\n",
       "  }\n",
       "  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "  if (id !== undefined) {\n",
       "    var nchildren = toinsert.length;\n",
       "    var html_node = toinsert[nchildren-1].children[0];\n",
       "    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var scripts = [];\n",
       "    var nodelist = html_node.querySelectorAll(\"script\");\n",
       "    for (var i in nodelist) {\n",
       "      if (nodelist.hasOwnProperty(i)) {\n",
       "        scripts.push(nodelist[i])\n",
       "      }\n",
       "    }\n",
       "\n",
       "    scripts.forEach( function (oldScript) {\n",
       "      var newScript = document.createElement(\"script\");\n",
       "      var attrs = [];\n",
       "      var nodemap = oldScript.attributes;\n",
       "      for (var j in nodemap) {\n",
       "        if (nodemap.hasOwnProperty(j)) {\n",
       "          attrs.push(nodemap[j])\n",
       "        }\n",
       "      }\n",
       "      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n",
       "      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n",
       "      oldScript.parentNode.replaceChild(newScript, oldScript);\n",
       "    });\n",
       "    if (JS_MIME_TYPE in output.data) {\n",
       "      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n",
       "    }\n",
       "    output_area._hv_plot_id = id;\n",
       "    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n",
       "      window.PyViz.plot_index[id] = Bokeh.index[id];\n",
       "    } else {\n",
       "      window.PyViz.plot_index[id] = null;\n",
       "    }\n",
       "  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "    var bk_div = document.createElement(\"div\");\n",
       "    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var script_attrs = bk_div.children[0].attributes;\n",
       "    for (var i = 0; i < script_attrs.length; i++) {\n",
       "      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "    }\n",
       "    // store reference to server id on output_area\n",
       "    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when an output is cleared or removed\n",
       " */\n",
       "function handle_clear_output(event, handle) {\n",
       "  var id = handle.cell.output_area._hv_plot_id;\n",
       "  var server_id = handle.cell.output_area._bokeh_server_id;\n",
       "  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n",
       "  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n",
       "  if (server_id !== null) {\n",
       "    comm.send({event_type: 'server_delete', 'id': server_id});\n",
       "    return;\n",
       "  } else if (comm !== null) {\n",
       "    comm.send({event_type: 'delete', 'id': id});\n",
       "  }\n",
       "  delete PyViz.plot_index[id];\n",
       "  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n",
       "    var doc = window.Bokeh.index[id].model.document\n",
       "    doc.clear();\n",
       "    const i = window.Bokeh.documents.indexOf(doc);\n",
       "    if (i > -1) {\n",
       "      window.Bokeh.documents.splice(i, 1);\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle kernel restart event\n",
       " */\n",
       "function handle_kernel_cleanup(event, handle) {\n",
       "  delete PyViz.comms[\"hv-extension-comm\"];\n",
       "  window.PyViz.plot_index = {}\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle update_display_data messages\n",
       " */\n",
       "function handle_update_output(event, handle) {\n",
       "  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n",
       "  handle_add_output(event, handle)\n",
       "}\n",
       "\n",
       "function register_renderer(events, OutputArea) {\n",
       "  function append_mime(data, metadata, element) {\n",
       "    // create a DOM node to render to\n",
       "    var toinsert = this.create_output_subarea(\n",
       "    metadata,\n",
       "    CLASS_NAME,\n",
       "    EXEC_MIME_TYPE\n",
       "    );\n",
       "    this.keyboard_manager.register_events(toinsert);\n",
       "    // Render to node\n",
       "    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "    render(props, toinsert[0]);\n",
       "    element.append(toinsert);\n",
       "    return toinsert\n",
       "  }\n",
       "\n",
       "  events.on('output_added.OutputArea', handle_add_output);\n",
       "  events.on('output_updated.OutputArea', handle_update_output);\n",
       "  events.on('clear_output.CodeCell', handle_clear_output);\n",
       "  events.on('delete.Cell', handle_clear_output);\n",
       "  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n",
       "\n",
       "  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "    safe: true,\n",
       "    index: 0\n",
       "  });\n",
       "}\n",
       "\n",
       "if (window.Jupyter !== undefined) {\n",
       "  try {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  } catch(err) {\n",
       "  }\n",
       "}\n"
      ],
      "application/vnd.holoviews_load.v0+json": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"logo-block\">\n",
       "<img src='data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAYAAACqaXHeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAAB+wAAAfsBxc2miwAAABl0RVh0U29mdHdhcmUAd3d3Lmlua3NjYXBlLm9yZ5vuPBoAAA6zSURB\n",
       "VHic7ZtpeFRVmsf/5966taWqUlUJ2UioBBJiIBAwCZtog9IOgjqACsogKtqirT2ttt069nQ/zDzt\n",
       "tI4+CrJIREFaFgWhBXpUNhHZQoKBkIUASchWla1S+3ar7r1nPkDaCAnZKoQP/D7mnPOe9/xy76n3\n",
       "nFSAW9ziFoPFNED2LLK5wcyBDObkb8ZkxuaoSYlI6ZcOKq1eWFdedqNzGHQBk9RMEwFAASkk0Xw3\n",
       "ETacDNi2vtvc7L0ROdw0AjoSotQVkKSvHQz/wRO1lScGModBFbDMaNRN1A4tUBCS3lk7BWhQkgpD\n",
       "lG4852/+7DWr1R3uHAZVQDsbh6ZPN7CyxUrCzJMRouusj0ipRwD2uKm0Zn5d2dFwzX1TCGhnmdGo\n",
       "G62Nna+isiUqhkzuKrkQaJlPEv5mFl2fvGg2t/VnzkEV8F5ioioOEWkLG86fvbpthynjdhXYZziQ\n",
       "x1hC9J2NFyi8vCTt91Fh04KGip0AaG9zuCk2wQCVyoNU3Hjezee9bq92duzzTmxsRJoy+jEZZZYo\n",
       "GTKJ6SJngdJqAfRzpze0+jHreUtPc7gpBLQnIYK6BYp/uGhw9YK688eu7v95ysgshcg9qSLMo3JC\n",
       "4jqLKQFBgdKDPoQ+Pltb8dUyQLpeDjeVgI6EgLIQFT5tEl3rn2losHVsexbZ3EyT9wE1uGdkIPcy\n",
       "BGxn8QUq1QrA5nqW5i2tLqvrrM9NK6AdkVIvL9E9bZL/oyfMVd/jqvc8LylzRBKDJSzIExwhQzuL\n",
       "QYGQj4rHfFTc8mUdu3E7yoLtbTe9gI4EqVgVkug2i5+uXGo919ixbRog+3fTbQ8qJe4ZOYNfMoTI\n",
       "OoshUNosgO60AisX15aeI2PSIp5KiFLI9ubb1vV3Qb2ltwLakUCDAkWX7/nHKRmmGIl9VgYsUhJm\n",
       "2NXjKYADtM1ygne9QQDIXlk49FBstMKx66D1v4+XuQr7vqTe0VcBHQlRWiOCbmmSYe2SqtL6q5rJ\n",
       "zsTb7lKx3FKOYC4DoqyS/B5bvLPxvD9Qtf6saxYLQGJErmDOdOMr/zo96km1nElr8bmPOBwI9COv\n",
       "HnFPRIwmkSOv9kcAS4heRsidOkpeWBgZM+UBrTFAXNYL5Vf2ii9c1trNzpYdaoVil3WIc+wdk+gQ\n",
       "noie3ecCcxt9ITcLAPWt/laGEO/9U6PmzZkenTtsSMQ8uYywJVW+grCstAvCIaAdArAsIWkRDDs/\n",
       "KzLm2YcjY1Lv0UdW73HabE9n6V66cxSzfEmuJssTpKGVp+0vHq73FwL46eOjpMpbRAnNmJFrGJNu\n",
       "Ukf9Yrz+3rghiumCKNXXWPhLYcjxGsIpoCMsIRoFITkW8AuyM8jC1+/QLx4bozCEJIq38+1rtpR6\n",
       "V/yzb8eBlRb3fo5l783N0CWolAzJHaVNzkrTzlEp2bQ2q3TC5gn6wpnoQAmwSiGh2GitnTmVMc5O\n",
       "UyfKWUKCIsU7+fZDKwqdT6DDpvkzAX4/+AMFjk0tDp5GRXLpQ2MUmhgDp5gxQT8+Y7hyPsMi8uxF\n",
       "71H0oebujHALECjFKaW9Lm68n18wXp2kVzIcABytD5iXFzg+WVXkegpAsOOYziqo0OkK76GyquC3\n",
       "ltZAzMhhqlSNmmWTE5T6e3IN05ITFLM4GdN0vtZ3ob8Jh1NAKXFbm5PtLU/eqTSlGjkNAJjdgn/N\n",
       "aedXa0tdi7+t9G0FIF49rtMSEgAs1kDLkTPO7ebm4IUWeyh1bKomXqlgMG6kJmHcSM0clYLJ8XtR\n",
       "1GTnbV3F6I5wCGikAb402npp1h1s7LQUZZSMIfALFOuL3UUrfnS8+rez7v9qcold5tilgHbO1fjK\n",
       "9ubb17u9oshxzMiUBKXWqJNxd+fqb0tLVs4lILFnK71H0Ind7uiPgACVcFJlrb0tV6DzxqqTIhUM\n",
       "CwDf1/rrVhTa33/3pGPxJYdQ2l2cbgVcQSosdx8uqnDtbGjh9SlDVSMNWhlnilfqZk42Th2ZpLpf\n",
       "xrHec5e815zrr0dfBZSwzkZfqsv+1FS1KUknUwPARVvItfKUY+cn57yP7qv07UE3p8B2uhUwLk09\n",
       "e0SCOrK+hbdYHYLjRIl71wWzv9jpEoeOHhGRrJAzyEyNiJuUqX0g2sBN5kGK6y2Blp5M3lsB9Qh4\n",
       "y2Ja6x6+i0ucmKgwMATwhSjdUu49tKrQ/pvN5d53ml2CGwCmJipmKjgmyuaXzNeL2a0AkQ01Th5j\n",
       "2DktO3Jyk8f9vcOBQHV94OK+fPumJmvQHxJoWkaKWq9Vs+yUsbq0zGT1I4RgeH2b5wef7+c7bl8F\n",
       "eKgoHVVZa8ZPEORzR6sT1BzDUAD/d9F78e2Tzv99v8D+fLVTqAKAsbGamKey1Mt9Ann4eH3gTXTz\n",
       "idWtAJ8PQWOk7NzSeQn/OTHDuEikVF1R4z8BQCy+6D1aWRfY0tTGG2OM8rRoPaeIj5ZHzJxszElN\n",
       "VM8K8JS5WOfv8mzRnQAKoEhmt8gyPM4lU9SmBK1MCQBnW4KONT86v1hZ1PbwSXPw4JWussVjtH9Y\n",
       "NCoiL9UoH/6PSu8jFrfY2t36erQHXLIEakMi1SydmzB31h3GGXFDFNPaK8Rme9B79Ixrd0WN+1ij\n",
       "NRQ/doRmuFLBkHSTOm5GruG+pFjFdAmorG4IXH1Qua6ASniclfFtDYt+oUjKipPrCQB7QBQ2lrgP\n",
       "fFzm+9XWUtcqJ3/5vDLDpJ79XHZk3u8nGZ42qlj1+ydtbxysCezrydp6ugmipNJ7WBPB5tydY0jP\n",
       "HaVNzs3QzeE4ZpTbI+ZbnSFPbVOw9vsfnVvqWnirPyCNGD08IlqtYkh2hjZ5dErEQzoNm+6ykyOt\n",
       "Lt5/PQEuSRRKo22VkydK+vvS1XEKlhCJAnsqvcVvH7f/ZU2R67eXbMEGAMiIV5oWZWiWvz5Fv2xG\n",
       "sjqNJQRvn3Rs2lji/lNP19VjAQDgD7FHhujZB9OGqYxRkZxixgRDVlqS6uEOFaJUVu0rPFzctrnF\n",
       "JqijImVp8dEKVWyUXDk92zAuMZ6bFwpBU1HrOw6AdhQgUooChb0+ItMbWJitSo5Ws3IAOGEOtL53\n",
       "0vHZih9sC4vtofZ7Qu6523V/fmGcds1TY3V36pUsBwAbSlxnVh2xLfAD/IAIMDf7XYIkNmXfpp2l\n",
       "18rkAJAy9HKFaIr/qULkeQQKy9zf1JgDB2uaeFNGijo5QsUyacNUUTOnGO42xSnv4oOwpDi1zYkc\n",
       "efUc3I5Gk6PhyTuVKaOGyLUAYPGIoY9Pu/atL/L92+4q9wbflRJ2Trpm/jPjdBtfnqB/dIThcl8A\n",
       "KG7hbRuKnb8qsQsVvVlTrwQAQMUlf3kwJI24Z4JhPMtcfng5GcH49GsrxJpGvvHIaeem2ma+KSjQ\n",
       "lIwUdYyCY8j4dE1KzijNnIP2llF2wcXNnsoapw9XxsgYAl6k+KzUXbi2yP3KR2ecf6z3BFsBICdW\n",
       "nvnIaG3eHybqX7vbpEqUMT+9OL4Qpe8VON7dXuFd39v19FoAABRVePbGGuXTszO0P7tu6lghUonE\n",
       "llRdrhArLvmKdh9u29jcFiRRkfLUxBiFNiqSU9icoZQHo5mYBI1MBgBH6wMNb+U7Pnw337H4gi1Y\n",
       "ciWs+uks3Z9fztUvfzxTm9Ne8XXkvQLHNytOOZeiD4e0PgkAIAYCYknKUNUDSXEKzdWNpnil7r4p\n",
       "xqkjTarZMtk/K8TQ6Qve78qqvXurGwIJqcOUKfUWHsm8KGvxSP68YudXq4pcj39X49uOK2X142O0\n",
       "Tz5/u/7TVybqH0rSya6ZBwD21/gubbrgWdDgEOx9WUhfBaC2ibcEBYm7a7x+ukrBMNcEZggyR0TE\n",
       "T8zUPjikQ4VosQZbTpS4vqizBKvqmvjsqnpfzaZyx9JPiz1/bfGKdgD45XB1zoIMzYbfTdS/NClB\n",
       "Gct0USiY3YL/g0LHy/uq/Ef6uo5+n0R/vyhp17Klpge763f8rMu6YU/zrn2nml+2WtH+Z+5IAAFc\n",
       "2bUTdTDOSNa9+cQY7YLsOIXhevEkCvzph7a8laecz/Un/z4/Ae04XeL3UQb57IwU9ZDr9UuKVajv\n",
       "nxp1+1UVIo/LjztZkKH59fO3G/JemqCfmaCRqbqbd90ZZ8FfjtkfAyD0J/9+C2h1hDwsSxvGjNDc\n",
       "b4zk5NfrSwiQblLHzZhg+Jf4aPlUwpDqkQqa9nimbt1/TDH8OitGMaQnj+RJS6B1fbF7SY1TqO5v\n",
       "/v0WAADl1f7zokgS7s7VT2DZ7pegUjBM7mjtiDZbcN4j0YrHH0rXpCtY0qPX0cVL0rv5jv/ZXend\n",
       "0u/EESYBAFBU4T4Qa5TflZOhTe7pmKpaP8kCVUVw1+yhXfJWvn1P3hnXi33JsTN6PnP3hHZ8Z3/h\n",
       "aLHzmkNPuPj7Bc/F/Q38CwjTpSwQXgE4Vmwry9tpfq/ZFgqFMy4AVDtCvi8rvMvOmv0N4YwbVgEA\n",
       "sPM72/KVnzfspmH7HQGCRLG2yL1+z8XwvPcdCbsAANh+xPzstgMtxeGKt+6MK3/tacfvwhWvIwMi\n",
       "oKEBtm0H7W+UVfkc/Y1V0BhoPlDr/w1w/eu1vjIgAgDg22OtX6/eYfnEz/focrZTHAFR+PSs56/7\n",
       "q32nwpjazxgwAQCwcU/T62t3WL7r6/jVRa6/byp1rei+Z98ZUAEAhEPHPc8fKnTU9nbgtnOe8h0l\n",
       "9hcGIqmODLQAHCy2Xti6v/XNRivf43f4fFvIteu854+VHnR7q9tfBlwAAGz+pnndB9vM26UebAe8\n",
       "SLHujPOTPVW+rwY+sxskAAC2HrA8t2Vvc7ffP1r9o+vwR2dcr92InIAbKKC1FZ5tB1tf+/G8p8sv\n",
       "N/9Q5zd/XR34LYCwV5JdccMEAMDBk45DH243r/X4xGvqxFa/GNpS7n6rwOwNWwHVE26oAADYurf1\n",
       "zx/utOzt+DMKYM0p17YtZZ5VNzqfsB2HewG1WXE8PoZ7gOclbTIvynZf9JV+fqZtfgs/8F/Nu5rB\n",
       "EIBmJ+8QRMmpU7EzGRsf2FzuePqYRbzh/zE26EwdrT10f6r6o8HOYzCJB9Dpff8tbnGLG8L/A/WE\n",
       "roTBs2RqAAAAAElFTkSuQmCC'\n",
       "     style='height:25px; border-radius:12px; display: inline-block; float: left; vertical-align: middle'></img>\n",
       "\n",
       "\n",
       "  <img src='data:image/png;base64,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'\n",
       "       style='height:15px; border-radius:12px; display: inline-block; float: left'></img>\n",
       "  \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries\n",
    "from aiohttp import ClientResponseError\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "import datetime\n",
    "import earthaccess\n",
    "import fiona\n",
    "import gc\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "import hvplot.pandas\n",
    "from IPython.display import clear_output\n",
    "import logging\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Set backend to Agg to avoid NavigationToolbar issue\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pyogrio\n",
    "from pyproj import CRS, Geod, Transformer\n",
    "import re\n",
    "import rioxarray\n",
    "from rioxarray.exceptions import NoDataInBounds\n",
    "from shapely.geometry import box, MultiPolygon, Point, Polygon\n",
    "from shapely.ops import unary_union\n",
    "from shapely.validation import make_valid\n",
    "import shutil\n",
    "from skimage import measure\n",
    "import time\n",
    "import traceback\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "import xarray as xr\n",
    "\n",
    "# Magic functions\n",
    "%matplotlib widget\n",
    "# %matplotlib inline\n",
    "\n",
    "# Define data directories dependent on home environment\n",
    "# Replace with your directory file paths\n",
    "if os.getenv('HOME') == '/home/jovyan':\n",
    "    DATA_DIR = '/home/jovyan/data'\n",
    "    OUTPUT_DIR = '/home/jovyan/1_evolving_lakes/output/FigS1_lake_reexamination_methods'\n",
    "    OUTPUT_DIR_GIT = '/home/jovyan/1_evolving_lakes/Sauthoff-202X-evolving-lakes/output'\n",
    "\n",
    "# Define constants and coordinate transforms for the geodesic area calculation\n",
    "CRS_LL = \"EPSG:4326\" # wgs84 in lon,lat\n",
    "GEOD = CRS(CRS_LL).get_geod() # geod object for calculating geodesic area on defined ellipsoid\n",
    "CRS_XY = \"EPSG:3031\" # Antarctic Polar Stereographic in x, y\n",
    "XY_TO_LL = Transformer.from_crs(CRS_XY, CRS_LL, always_xy = True) # make coord transformer\n",
    "geod = Geod(ellps=\"WGS84\") # Create a Geod object for calculating area on the WGS84 ellipsoid\n",
    "\n",
    "# Change default font to increase font size\n",
    "plt.rcParams.update({'font.size': 8})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4adef9-57d4-4ab4-b066-ba4a4ebb38a5",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac48698c-9656-4c01-b62d-37c460a384e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_quarter_year(date):\n",
    "    \"\"\"Convert datetime64 to year.quarter.\"\"\"\n",
    "    if isinstance(date, np.datetime64):\n",
    "        date = pd.Timestamp(date)\n",
    "    \n",
    "    return date.year + (date.quarter - 1) * 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "495b08ae-6a2c-4f57-8a9c-1dd26c80b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_multiple_buffer(ref_polygon, area_multiple, precision=100, exclude_inner=False):\n",
    "    '''\n",
    "    This function takes a polygon and returns a polygon with a buffer such that the area of the buffered polygon\n",
    "    is approximately the specified multiple of the original polygon area. Optionally excludes the inner polygon\n",
    "    to create a ring-like shape.\n",
    "    \n",
    "    Inputs:\n",
    "    * param polygon: Shapely Polygon object\n",
    "    * param area_multiple: the multiple of the original polygon area you wish the buffered polygon to be\n",
    "    * param precision: Precision for the iterative process to find the buffer distance\n",
    "    * param exclude_inner: If True, returns the difference between the buffered area and original polygon\n",
    "    * return: Buffered Polygon or Ring-like Polygon (if exclude_inner=True)\n",
    "    '''\n",
    "    # Ensure we're working with a single geometry, not a Series\n",
    "    if hasattr(ref_polygon, 'iloc'):\n",
    "        ref_polygon = ref_polygon.iloc[0]\n",
    "        \n",
    "    original_area = ref_polygon.area\n",
    "    target_area = area_multiple * original_area\n",
    "    buffer_distance = 0\n",
    "    area_multiple_polygon = ref_polygon\n",
    "    \n",
    "    while area_multiple_polygon.area < target_area:\n",
    "        buffer_distance += precision\n",
    "        area_multiple_polygon = ref_polygon.buffer(buffer_distance)\n",
    "    \n",
    "    if exclude_inner:\n",
    "        # Return the difference between the buffered polygon and the original polygon\n",
    "        return area_multiple_polygon.difference(ref_polygon)\n",
    "    else:\n",
    "        return area_multiple_polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c858ca39-7f32-483a-a037-81ebf05b20d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_gdf_by_folder_contents(gdf, folder_path, exclude=True, prefix=None, suffix=None, suffix_pattern=None, file_extension=None):\n",
    "    '''\n",
    "    Filter the GeoDataFrame based on processed lake names from the folder contents.\n",
    "    \n",
    "    Args:\n",
    "    gdf: GeoDataFrame to be filtered.\n",
    "    folder_path: Path to the directory containing files and/or subdirectories.\n",
    "    exclude: If True, excludes gdf rows where the 'name' is in the folder_path directories or files.\n",
    "             If False, includes only gdf rows where the 'name' is in the folder_path directories or files.\n",
    "    prefix: Optional string to remove from the beginning of filenames.\n",
    "    suffix: Optional string to remove from the end of filenames.\n",
    "    suffix_pattern: Optional regex pattern to remove from the end of filenames.\n",
    "    file_extension: Optional string specifying the file extension to filter (e.g., 'png', 'txt').\n",
    "    \n",
    "    Returns:\n",
    "    GeoDataFrame filtered based on the presence of 'name' in folder_path.\n",
    "\n",
    "    # Example usage:\n",
    "    remaining_lakes = filter_gdf_by_folder_contents(\n",
    "        stationary_outlines_gdf, \n",
    "        folder_path,\n",
    "        # prefix='plot_evolving_outlines_time_series_', \n",
    "        suffix_pattern=r'\\d+\\.\\d+m-level_\\d+x-with',\n",
    "        file_extension='txt'\n",
    "    )\n",
    "    '''\n",
    "    # Return empty GeoDataFrame if input is empty\n",
    "    if gdf is None or gdf.empty:\n",
    "        return gdf\n",
    "\n",
    "    def process_name(name):\n",
    "        \"\"\"Helper function to remove prefix and suffix from a name\"\"\"\n",
    "        processed_name = name\n",
    "        \n",
    "        # First strip the file extension if it exists\n",
    "        processed_name = os.path.splitext(processed_name)[0]\n",
    "        \n",
    "        # if prefix and processed_name.startswith(prefix):\n",
    "        #     processed_name = processed_name[len(prefix):]\n",
    "            \n",
    "        if suffix_pattern:\n",
    "            processed_name = re.sub(suffix_pattern + '$', '', processed_name)\n",
    "        elif suffix and processed_name.endswith(suffix):\n",
    "            processed_name = processed_name[:-len(suffix)]\n",
    "            \n",
    "        return processed_name.lower().strip()\n",
    "    \n",
    "    # Get all files and filter by extension if specified\n",
    "    all_files = os.listdir(folder_path)\n",
    "    if file_extension:\n",
    "        clean_extension = file_extension.lstrip('.')\n",
    "        all_files = [f for f in all_files if f.lower().endswith(f'.{clean_extension.lower()}')]\n",
    "    \n",
    "    # Process filenames to get lake names\n",
    "    names_in_folder = {\n",
    "        process_name(name)\n",
    "        for name in all_files\n",
    "    }\n",
    "    \n",
    "    # Filter without adding and then dropping a new column\n",
    "    gdf_filtered = gdf[gdf['name'].str.lower().str.strip().apply(\n",
    "        lambda x: (x not in names_in_folder) if exclude else (x in names_in_folder)\n",
    "    )]\n",
    "    \n",
    "    return gdf_filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b00b45-250f-4252-a8a7-3a6e77b05786",
   "metadata": {},
   "source": [
    "## find_and_save_optimal_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c09df69b-9a8d-4199-9b64-5298e6a70804",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_and_save_optimal_parameters(lake_gdf, within_area_multiples=range(2, 16)):\n",
    "    '''\n",
    "    Find and save optimal levels for each lake at various within evaluation boundaries.\n",
    "    '''\n",
    "    results = find_optimal_parameters(lake_gdf=lake_gdf, within_area_multiples=within_area_multiples)\n",
    "    save_search_results(lake_gdf, results)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "def find_optimal_parameters(lake_gdf, \n",
    "                          within_area_multiples=range(2, 16),\n",
    "                          initial_level=0.01,\n",
    "                          level_increment=0.01,\n",
    "                          within_fraction_target=0.95):\n",
    "    '''\n",
    "    Find optimal search extent and level parameters for a lake.     \n",
    "    '''\n",
    "    # Initialize results DataFrame\n",
    "    results_df = pd.DataFrame(\n",
    "        columns=['within_area_multiple', 'level', 'within_percent', 'dataset_dois'])\n",
    "\n",
    "    # Prepare datasets\n",
    "    dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(lake_gdf, 30)\n",
    "    \n",
    "    try:\n",
    "        for area_multiple in within_area_multiples:\n",
    "            print('Finding optimal levels at within evaluation boundaries for', lake_gdf['name'].iloc[0])\n",
    "            level = initial_level\n",
    "            # found_for_current_multiple = False  # Track if we found result for this multiple\n",
    "            # stop_incrementing = False\n",
    "            \n",
    "            # while level <= 2.0 and not found_for_current_multiple and not stop_incrementing:\n",
    "            while level <= 2.0:\n",
    "                outlines_gdf = find_evolving_outlines(\n",
    "                    lake_gdf=lake_gdf,\n",
    "                    within_area_multiple=area_multiple,\n",
    "                    level=level,\n",
    "                    dataset1_masked=dataset1_masked,\n",
    "                    dataset2_masked=dataset2_masked, \n",
    "                    search_extent_poly=search_extent_poly,\n",
    "                    plot=False\n",
    "                )\n",
    "\n",
    "                if outlines_gdf is not None and not outlines_gdf.empty:\n",
    "                    within_fraction = calculate_within_fraction(\n",
    "                        outlines_gdf, lake_gdf['geometry'], area_multiple)\n",
    "                    print(f\"within_area_multiple: {area_multiple}, level: {level}, within: {round(within_fraction*100)}%\")\n",
    "\n",
    "                    if within_fraction == 0.0:\n",
    "                        print(f\"Within fraction is 0.0, stopping search for area_multiple: {area_multiple}\")\n",
    "                        # stop_incrementing = True\n",
    "                        break\n",
    "\n",
    "                    if within_fraction >= within_fraction_target:\n",
    "                        onlake_outlines, offlake_outlines = extract_intersecting_polygons_recursive(\n",
    "                            outlines_gdf, lake_gdf['geometry'].iloc[0]\n",
    "                        )\n",
    "                        \n",
    "                        if not onlake_outlines.empty:\n",
    "                            # Add result\n",
    "                            dois = [doi for doi in [dataset1_doi, dataset2_doi] if doi is not None]\n",
    "                            results_df.loc[len(results_df)] = {\n",
    "                                'within_area_multiple': area_multiple,\n",
    "                                'level': level,\n",
    "                                'within_percent': within_fraction * 100,\n",
    "                                'dataset_dois': ', '.join(dois)\n",
    "                            }\n",
    "                            found_for_current_multiple = True  # Mark that we found result for this area_multiple\n",
    "                            break  # Found a good result for this area_multiple, move to next\n",
    "                        else:\n",
    "                            print(\"onlake_outlines is empty, continuing search\")\n",
    "                else:\n",
    "                    print(f\"within_area_multiple: {area_multiple}, level: {level}, within: no outlines found\")\n",
    "                    print(f\"No outlines found, stopping search for area_multiple: {area_multiple}\")\n",
    "                    # stop_incrementing = True  # Stop if no outlines found\n",
    "                    break\n",
    "\n",
    "                # Increment level\n",
    "                level = round(level + level_increment, 2)\n",
    "\n",
    "            # Clear output for each within_area_multiple\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing lake {lake_gdf['name']}: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "    finally:\n",
    "        gc.collect()\n",
    "\n",
    "    # Sort the DataFrame\n",
    "    results_df = results_df.sort_values(\n",
    "        by=['level', 'within_area_multiple'], \n",
    "        ascending=[True, True])\n",
    "\n",
    "    results_df = remove_higher_duplicates(results_df)\n",
    "        \n",
    "    return results_df\n",
    "\n",
    "def calculate_within_fraction(evolving_outlines_gdf, stationary_outline, area_multiple):\n",
    "    \"\"\"\n",
    "    Calculate fraction of evolving outlines that fall within the search extent.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    evolving_outlines_gdf : GeoDataFrame\n",
    "        The evolving outlines to analyze\n",
    "    stationary_outline : Geometry\n",
    "        The original lake outline\n",
    "    area_multiple : int\n",
    "        Area multiple for within evaluation boundary\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Fraction of outlines that are within the search extent (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    # Create search extent boundary\n",
    "    within_evaluation_poly = area_multiple_buffer(\n",
    "        ref_polygon=stationary_outline, \n",
    "        area_multiple=area_multiple)\n",
    "    within_evaluation_gdf = gpd.GeoDataFrame(\n",
    "        geometry=gpd.GeoSeries([within_evaluation_poly]), \n",
    "        crs=3031)\n",
    "\n",
    "    # Validate geometries\n",
    "    valid_outlines = evolving_outlines_gdf.loc[\n",
    "        evolving_outlines_gdf.is_valid & ~evolving_outlines_gdf.is_empty].copy()\n",
    "    valid_within_evaluation_poly = within_evaluation_gdf.loc[\n",
    "        within_evaluation_gdf.is_valid & ~within_evaluation_gdf.is_empty].copy()\n",
    "    \n",
    "    if valid_outlines.empty or valid_within_evaluation_poly.empty:\n",
    "        return 0.0\n",
    "\n",
    "    # Convert geometries to ensure they're polygons\n",
    "    valid_outlines.loc[:, 'geometry'] = valid_outlines['geometry'].apply(\n",
    "        lambda geom: geom if isinstance(geom, Polygon) else Polygon())\n",
    "    valid_within_evaluation_poly.loc[:, 'geometry'] = valid_within_evaluation_poly['geometry'].apply(\n",
    "        lambda geom: geom if isinstance(geom, Polygon) else Polygon())\n",
    "\n",
    "    # Perform spatial analysis\n",
    "    within = gpd.sjoin(valid_outlines, within_evaluation_gdf, predicate='within')\n",
    "    overlaps = gpd.sjoin(valid_outlines, within_evaluation_gdf, predicate='overlaps')\n",
    "\n",
    "    # Calculate fraction that are within\n",
    "    total = len(within) + len(overlaps)\n",
    "    if total > 0:\n",
    "        return round(len(within) / total, 2)\n",
    "    return 0.0\n",
    "\n",
    "def remove_higher_duplicates(df):\n",
    "    \"\"\"\n",
    "    Remove rows that have duplicate 'level' values at higher 'within_area_multiple' values.\n",
    "    Assumes the dataframe is already sorted by 'level' and 'within_area_multiple'.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame with 'level' and 'within_area_multiple' columns\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with duplicates removed\n",
    "    \"\"\"\n",
    "    # Keep track of levels we've seen\n",
    "    seen_levels = set()\n",
    "    # Create a boolean mask for rows to keep\n",
    "    mask = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # If we haven't seen this level before, keep the row\n",
    "        if row['level'] not in seen_levels:\n",
    "            mask.append(True)\n",
    "            seen_levels.add(row['level'])\n",
    "        else:\n",
    "            # If we have seen this level, don't keep the row\n",
    "            mask.append(False)\n",
    "    \n",
    "    # Return filtered dataframe\n",
    "    return df[mask]\n",
    "\n",
    "def save_search_results(lake_gdf, results_df):\n",
    "    '''\n",
    "    Save search results to files. Creates appropriate files for lakes with no results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lake_gdf : GeoDataFrame row\n",
    "        Single lake record containing name\n",
    "    results_df : DataFrame  \n",
    "        Results from find_optimal_parameters, may be empty\n",
    "    '''\n",
    "    os.makedirs(OUTPUT_DIR + '/levels', exist_ok=True)\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        results_df.to_csv(\n",
    "            OUTPUT_DIR + f'/levels/{lake_name}.csv',\n",
    "            index=False\n",
    "        )\n",
    "        print(f\"Saved optimal parameters for {lake_name}\")\n",
    "    else:\n",
    "        print(f\"No outlines found for {lake_name}\")\n",
    "        write_no_outlines(OUTPUT_DIR + f'/levels/{lake_name}.txt')\n",
    "        write_no_outlines(OUTPUT_DIR_GIT + f'/lake_outlines/evolving_outlines/{lake_name}.txt')\n",
    "\n",
    "def write_no_outlines(filepath):\n",
    "    \"\"\"Write file indicating no outlines found\"\"\"\n",
    "    try:\n",
    "        # Ensure directory exists\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        # Write file\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(\"There are no evolving outlines for this lake.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing no outlines file to {filepath}: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "def prepare_datasets(lake_gdf, area_multiple):\n",
    "    \"\"\"\n",
    "    Prepare masked datasets based on lake parameters\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lake_gdf : GeoDataFrame row\n",
    "        Single lake record containing CS2_SARIn_start and geometry\n",
    "    area_multiple : int\n",
    "        Area multiple for buffering\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked))\n",
    "    \"\"\"\n",
    "    # Extract CS2_SARIn_start as a single value, not a Series\n",
    "    if isinstance(lake_gdf, pd.Series):\n",
    "        CS2_SARIn_start = lake_gdf['CS2_SARIn_start']\n",
    "    else:\n",
    "        CS2_SARIn_start = lake_gdf.iloc[0]['CS2_SARIn_start']\n",
    "    \n",
    "    # Initialize dataset1\n",
    "    dataset1 = None\n",
    "    dataset1_doi = None\n",
    "    \n",
    "    # Check time period using proper null checking\n",
    "    if pd.isna(CS2_SARIn_start) or str(CS2_SARIn_start) == '<NA>':\n",
    "        dataset1 = None\n",
    "        dataset1_doi = None\n",
    "    elif CS2_SARIn_start == '2013.75':\n",
    "        dataset1 = CS2_Smith2017.sel(time=slice(np.datetime64('2013-10-01T22:30:00.000000000'), None))\n",
    "        dataset1_doi = dataset1.attrs.get('identifier_product_DOI', 'Unknown')\n",
    "    elif CS2_SARIn_start == '2010.5':\n",
    "        dataset1 = CS2_Smith2017\n",
    "        dataset1_doi = dataset1.attrs.get('identifier_product_DOI', 'Unknown')\n",
    "    \n",
    "    dataset2 = ATL15_dh\n",
    "    dataset2_doi = dataset2.attrs.get('identifier_product_DOI', 'Unknown')\n",
    "    \n",
    "    # Get geometry properly\n",
    "    geometry = lake_gdf['geometry'] if isinstance(lake_gdf, pd.Series) else lake_gdf.iloc[0]['geometry']\n",
    "    \n",
    "    # Mask datasets\n",
    "    search_extent_poly = area_multiple_buffer(geometry, area_multiple)\n",
    "    x_min, y_min, x_max, y_max = search_extent_poly.bounds\n",
    "    \n",
    "    dataset1_masked, dataset2_masked = mask_datasets(dataset1, dataset2, search_extent_poly, x_min, x_max, y_min, y_max)\n",
    "    \n",
    "    return dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked)\n",
    "    \n",
    "def mask_datasets(dataset1, dataset2, search_extent_poly, x_min, x_max, y_min, y_max):\n",
    "    \"\"\"Apply masks to both datasets\"\"\"\n",
    "    dataset1_masked = None\n",
    "    if dataset1 is not None:\n",
    "        dataset1_masked = apply_mask_to_dataset(dataset1, search_extent_poly, x_min, x_max, y_min, y_max)\n",
    "    \n",
    "    dataset2_masked = apply_mask_to_dataset(dataset2, search_extent_poly, x_min, x_max, y_min, y_max)\n",
    "    return dataset1_masked, dataset2_masked\n",
    "\n",
    "def apply_mask_to_dataset(dataset, search_extent_poly, x_min, x_max, y_min, y_max):\n",
    "    \"\"\"Apply mask to a single dataset\"\"\"\n",
    "    dataset_sub = dataset.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "    mask = np.array([[search_extent_poly.contains(Point(x, y)) \n",
    "                     for x in dataset_sub['x'].values] \n",
    "                     for y in dataset_sub['y'].values])\n",
    "    mask_da = xr.DataArray(mask, coords=[dataset_sub.y, dataset_sub.x], dims=[\"y\", \"x\"])\n",
    "    return dataset.where(mask_da, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a2b739-2388-4441-9815-34df44591a67",
   "metadata": {},
   "source": [
    "## find_evolving_outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77ea53f8-1207-4dbf-980c-c063334bba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_evolving_outlines(lake_gdf, \n",
    "                           within_area_multiple, \n",
    "                           level, \n",
    "                           dataset1_masked, \n",
    "                           dataset2_masked, \n",
    "                           search_extent_poly, \n",
    "                           plot=False): \n",
    "    '''\n",
    "    Create time-variable outlines using skimage contour to generate evolving outlines from surface height anomalies and optionally plot.\n",
    "    If plot=True, create planview dh/dt plots of ice surface height changes with evolving outlines found,\n",
    "    along with a subplot showing data counts used in the gridded dh data.\n",
    "\n",
    "    Inputs:\n",
    "    * lake_gdf: GeoDataFrame containing lake information\n",
    "    * within_area_multiple: Factor used elsewhere to multiply lake area to create a polygon used to calculate the within_fraction\n",
    "    * level: vertical dh in meters to delineate ice surface dh anomaly contour\n",
    "    * dataset1_masked: masked dataset1 to be analyzed\n",
    "    * dataset2_masked: masked dataset2 to be analyzed\n",
    "    * search_extent_poly: buffered polygon that is the extent of masked data available for making outlines\n",
    "    * plot: boolean, if True, create and save plots; default is False for faster production when searching for optimal levels at search extents\n",
    "    using find_and_save_optimal_parameters func\n",
    "    \n",
    "    Outputs: \n",
    "    * geopandas geodataframe of polygons created at each step\n",
    "    * If plot=True, sequence of planview dh visuals with variable ice surface dh contours \n",
    "    plotted to delineate evolving lake boundaries, along with data count subplot.\n",
    "\n",
    "    # Example usage\n",
    "    >>> outlines_gdf = find_evolving_outlines(lake_gdf=lake_gdf, within_area_multiple=2, level=0.1, \n",
    "        dataset1_masked=dataset1_masked, dataset2_masked=dataset2_masked, plot=False)\n",
    "    '''    \n",
    "    # Define lake name and polygon\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "\n",
    "    # Get the time period for this lake\n",
    "    time_period = lake_gdf['CS2_SARIn_start'].iloc[0]\n",
    "\n",
    "    # Establish x_min, x_max, y_min, y_max\n",
    "    x_min, y_min, x_max, y_max = search_extent_poly.bounds\n",
    "    x_buffer, y_buffer = abs(x_max-x_min)*0.05, abs(y_max-y_min)*0.05\n",
    "    \n",
    "    # Create empty lists to store polygons, areas, dh's, dV's and dates\n",
    "    polys = []\n",
    "    areas = []\n",
    "    dhs = []\n",
    "    dVs = []\n",
    "    midcyc_datetimes = []\n",
    "\n",
    "    # Calculate time steps and diffs for each dataset\n",
    "    if dataset1_masked is not None:\n",
    "        # Get dh values of cycle-to-cycle change instead of relative to datum for dataset1\n",
    "        dataset1_dh = dataset1_masked['delta_h'].diff('time')\n",
    "\n",
    "        # Calculate mid-cycle datetimes\n",
    "        dataset1_midcyc_times = []\n",
    "        dataset1_datetimes = dataset1_masked['time'].values\n",
    "        for i in range(1, len(dataset1_datetimes)):\n",
    "            midcyc_days = dataset1_datetimes[i] - dataset1_datetimes[i-1]\n",
    "            midcyc_date = dataset1_datetimes[i-1] + midcyc_days/2\n",
    "            dataset1_midcyc_times.append(midcyc_date)\n",
    "        dataset1_midcyc_times = np.array(dataset1_midcyc_times)\n",
    "\n",
    "        # Write CRS after diff operation\n",
    "        dataset1_dh.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "    else:\n",
    "        dataset1_dh = None\n",
    "        dataset1_midcyc_times = np.array([])\n",
    "\n",
    "    # Get dh values of cycle-to-cycle change instead of relative to datum for dataset2\n",
    "    dataset2_dh = dataset2_masked['delta_h'].diff('time')\n",
    "\n",
    "    # Calculate mid-cycle datetimes\n",
    "    dataset2_midcyc_times = []\n",
    "    dataset2_datetimes = dataset2_masked['time'].values    \n",
    "    for i in range(1, len(dataset2_datetimes)):\n",
    "        midcyc_days = dataset2_datetimes[i] - dataset2_datetimes[i-1]\n",
    "        midcyc_date = dataset2_datetimes[i-1] + midcyc_days/2\n",
    "        dataset2_midcyc_times.append(midcyc_date)\n",
    "    dataset2_midcyc_times = np.array(dataset2_midcyc_times)\n",
    "\n",
    "    # Write CRS after diff operation\n",
    "    dataset2_dh.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "    # Only proceed with plotting if plot=True\n",
    "    if plot:\n",
    "        # Find magnitude of dh for colorbar mapping across all time slices\n",
    "        height_anom_pos = []\n",
    "        height_anom_neg = []\n",
    "        max_counts = []\n",
    "\n",
    "        # Process dataset1 if available\n",
    "        if dataset1_masked is not None:\n",
    "            for dh_slice in dataset1_dh:\n",
    "                if np.any(~np.isnan(dh_slice)):\n",
    "                    pos = np.nanmax(dh_slice)\n",
    "                    neg = np.nanmin(dh_slice)\n",
    "                    height_anom_pos.append(pos)\n",
    "                    height_anom_neg.append(neg)\n",
    "                    max_counts.append(np.nanmax(dataset1_masked['data_count']))\n",
    "\n",
    "        # Process dataset2\n",
    "        for dh_slice in dataset2_dh:\n",
    "            if np.any(~np.isnan(dh_slice)):\n",
    "                pos = np.nanmax(dh_slice)\n",
    "                neg = np.nanmin(dh_slice)\n",
    "                height_anom_pos.append(pos)\n",
    "                height_anom_neg.append(neg)\n",
    "                max_counts.append(np.nanmax(dataset2_masked['data_count']))\n",
    "\n",
    "        if height_anom_pos:  # Check if we found any valid height anomalies\n",
    "            divnorm = colors.TwoSlopeNorm(vmin=min(height_anom_neg), \n",
    "                                        vcenter=0., \n",
    "                                        vmax=max(height_anom_pos))\n",
    "            countnorm = colors.Normalize(vmin=0, vmax=max(max_counts))\n",
    "        else:\n",
    "            print(\"No valid height anomalies found for plotting\")\n",
    "            return None\n",
    "\n",
    "        # Create plotting elements\n",
    "        stationary_lakes_color = 'darkturquoise'\n",
    "        stationary_line = plt.Line2D([], [], color=stationary_lakes_color, linestyle='solid', linewidth=2)\n",
    "        uplift = plt.Line2D([], [], color='blue', linestyle=(0, (3, 5, 1, 5)), linewidth=2)\n",
    "        subsidence = plt.Line2D([], [], color='red', linestyle=(0, (3, 5, 1, 5, 1, 5)), linewidth=2)\n",
    "        within_area_multiple_line = plt.Line2D([],[], color='dimgray', linestyle='solid', linewidth=2)\n",
    "\n",
    "    def process_timestep(dh, count, mid_cyc_time, dataset_name):\n",
    "        if np.any(~np.isnan(dh)):\n",
    "            x_conv = (x_max-x_min)/dh.shape[1]\n",
    "            y_conv = (y_max-y_min)/dh.shape[0]\n",
    "    \n",
    "            # Plot if requested\n",
    "            if plot:\n",
    "                create_and_save_plots(dh, count, mid_cyc_time, x_conv, y_conv)\n",
    "    \n",
    "            # Process contours\n",
    "            if np.any(~np.isnan(count)):\n",
    "                # Find positive contours\n",
    "                contours_pos = measure.find_contours(dh.values, level)\n",
    "                process_contours(contours_pos, x_conv, y_conv, dh, mid_cyc_time, is_positive=True)\n",
    "    \n",
    "                # Find negative contours\n",
    "                contours_neg = measure.find_contours(dh.values, -level)\n",
    "                process_contours(contours_neg, x_conv, y_conv, dh, mid_cyc_time, is_positive=False)\n",
    "    \n",
    "    \n",
    "    def create_and_save_plots(dh, count, mid_cyc_time, x_conv, y_conv):\n",
    "        \"\"\"Create and save plots for the current timestep\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    \n",
    "        # Plot data counts\n",
    "        img1 = ax1.imshow(count, extent=[x_min, x_max, y_min, y_max], \n",
    "            origin='lower', cmap='Greys', norm=countnorm)\n",
    "    \n",
    "        # Plot height change\n",
    "        img2 = ax2.imshow(dh, extent=[x_min, x_max, y_min, y_max], \n",
    "            origin='lower', cmap='coolwarm_r', norm=divnorm)\n",
    "    \n",
    "        # Find and plot contours\n",
    "        contour_pos = measure.find_contours(dh.values, level)\n",
    "        contour_neg = measure.find_contours(dh.values, -level)\n",
    "    \n",
    "        # Plot positive contours\n",
    "        for contour in contour_pos:\n",
    "            if not is_closed_contour(contour):\n",
    "                x = x_min + contour[:,1]*x_conv + (0.5*x_conv)\n",
    "                y = y_min + contour[:,0]*y_conv + (0.5*y_conv)\n",
    "                # Plot unclosed contours\n",
    "                ax1.plot(x, y, color='darkblue', linestyle='dashdot', linewidth=1)\n",
    "                ax2.plot(x, y, color='darkblue', linestyle='dashdot', linewidth=1)\n",
    "            else:\n",
    "                x = x_min + contour[:,1]*x_conv + (0.5*x_conv)\n",
    "                y = y_min + contour[:,0]*y_conv + (0.5*y_conv)\n",
    "                # Plot closed contours\n",
    "                ax1.plot(x, y, color='blue', linestyle=(0, (3, 5, 1, 5)), linewidth=1)\n",
    "                ax2.plot(x, y, color='blue', linestyle=(0, (3, 5, 1, 5)), linewidth=1)\n",
    "    \n",
    "        # Plot negative contours\n",
    "        for contour in contour_neg:\n",
    "            if not is_closed_contour(contour):\n",
    "                x = x_min + contour[:,1]*x_conv + (0.5*x_conv)\n",
    "                y = y_min + contour[:,0]*y_conv + (0.5*y_conv)\n",
    "                # Plot unclosed contours\n",
    "                ax1.plot(x, y, color='darkred', linestyle=(0, (3, 1, 1, 1)), linewidth=1)\n",
    "                ax2.plot(x, y, color='darkred', linestyle=(0, (3, 1, 1, 1)), linewidth=1)\n",
    "            else:\n",
    "                x = x_min + contour[:,1]*x_conv + (0.5*x_conv)\n",
    "                y = y_min + contour[:,0]*y_conv + (0.5*y_conv)\n",
    "                # Plot closed contours\n",
    "                ax1.plot(x, y, color='red', linestyle=(0, (3, 1, 1, 1)), linewidth=1)\n",
    "                ax2.plot(x, y, color='red', linestyle=(0, (3, 1, 1, 1)), linewidth=1)\n",
    "    \n",
    "        # Plot boundaries on both subplots\n",
    "        within_evaluation_poly = area_multiple_buffer(lake_poly, within_area_multiple)\n",
    "        gpd.GeoSeries(within_evaluation_poly).boundary.plot(ax=ax1, color='dimgray')\n",
    "        gpd.GeoSeries(within_evaluation_poly).boundary.plot(ax=ax2, color='dimgray')\n",
    "    \n",
    "        # Common plotting elements\n",
    "        for ax in [ax1, ax2]:\n",
    "            stationary_outlines_gdf.boundary.plot(ax=ax, edgecolor=stationary_lakes_color, linestyle='solid', linewidth=2)          \n",
    "            km_scale = 1e3\n",
    "            ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "            ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "            ax.set_xlabel('x [km]', size=15)\n",
    "            ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "    \n",
    "        # Additional ax1 elements\n",
    "        ax1.set_ylabel('y [km]', size=15)\n",
    "        axIns = ax1.inset_axes([0.01, -0.01, 0.2, 0.2])\n",
    "        axIns.set_aspect('equal')\n",
    "        moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        axIns.axis('off')\n",
    "        axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "            linewidth=1, edgecolor='k', facecolor='r', s=100, zorder=3)\n",
    "    \n",
    "        # Colorbars\n",
    "        divider1 = make_axes_locatable(ax1)\n",
    "        cax1 = divider1.append_axes('right', size='5%', pad=0.2)\n",
    "        fig.colorbar(img1, cax=cax1).set_label('data counts', size=15)\n",
    "    \n",
    "        divider2 = make_axes_locatable(ax2)\n",
    "        cax2 = divider2.append_axes('right', size='5%', pad=0.2)\n",
    "        fig.colorbar(img2, cax=cax2).set_label('height change (dh) [m]', size=15)\n",
    "    \n",
    "        # Legends\n",
    "        for ax, title in zip([ax1, ax2], ['Data counts', 'Height change']):\n",
    "            ax.legend([stationary_line, uplift, subsidence, within_area_multiple_line],\n",
    "                     ['stationary outline',\n",
    "                      f'evolving outline (+ {level} m)',\n",
    "                      f'evolving outline (− {level} m)',\n",
    "                      f'within evaluation line ({int(within_area_multiple)}x)'],\n",
    "                     loc='upper left')\n",
    "            ax.set_title(title)\n",
    "    \n",
    "        # Save plot\n",
    "        fig.suptitle(f'Mid-cycle date: {pd.Timestamp(mid_cyc_time)}', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{OUTPUT_DIR}/find_evolving_outlines/{lake_name}/find_evolving_outlines_{lake_name}_{level}m-level_{within_area_multiple}x-within_{pd.Timestamp(mid_cyc_time).strftime('%Y-%m-%d')}.png\", \n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "            \n",
    "    def process_contours(contours, x_conv, y_conv, dh, time, is_positive=True):\n",
    "        for contour in contours:\n",
    "            if is_closed_contour(contour):\n",
    "                x = x_min + contour[:,1]*x_conv + (0.5*x_conv)\n",
    "                y = y_min + contour[:,0]*y_conv + (0.5*y_conv)\n",
    "                \n",
    "                # Create and process polygon\n",
    "                poly = Polygon(list(zip(x, y)))\n",
    "                try:\n",
    "                    dhdt_poly = dh.rio.clip([poly])\n",
    "                    lon, lat = XY_TO_LL.transform(x,y)\n",
    "                    poly_area = abs(GEOD.polygon_area_perimeter(lon, lat)[0])\n",
    "                    \n",
    "                    if np.any(~np.isnan(dhdt_poly)):\n",
    "                        poly_dh = np.nanmean(dhdt_poly)\n",
    "                        poly_dV = poly_dh*poly_area\n",
    "                        polys.append(poly)\n",
    "                        areas.append(poly_area)\n",
    "                        dhs.append(poly_dh)\n",
    "                        dVs.append(poly_dV)\n",
    "                        midcyc_datetimes.append(time)\n",
    "                except NoDataInBounds:\n",
    "                    pass\n",
    "                except Exception as e:\n",
    "                    raise\n",
    "\n",
    "    # Process dataset1 if available\n",
    "    if dataset1_masked is not None:\n",
    "        for i, (dh_slice, mid_cyc_time) in enumerate(zip(dataset1_dh, dataset1_midcyc_times)):\n",
    "            process_timestep(dh_slice, dataset1_masked['data_count'][i], mid_cyc_time, 'dataset1')\n",
    "\n",
    "    # Process dataset2\n",
    "    for i, (dh_slice, mid_cyc_time) in enumerate(zip(dataset2_dh, dataset2_midcyc_times)):\n",
    "        process_timestep(dh_slice, dataset2_masked['data_count'][i], mid_cyc_time, 'dataset2')\n",
    "\n",
    "    # Return None if no polygons were found\n",
    "    if not polys:\n",
    "        return None\n",
    "\n",
    "    # Create GeoDataFrame if we found any polygons\n",
    "    gdf = gpd.GeoDataFrame({\n",
    "        'within_area_multiple': [within_area_multiple] * len(polys),\n",
    "        'level': [level] * len(polys),\n",
    "        'geometry': polys, \n",
    "        'area (m^2)': areas, \n",
    "        'dh (m)': dhs, \n",
    "        'vol (m^3)': dVs,\n",
    "        'midcyc_datetime': midcyc_datetimes\n",
    "    }, crs=\"EPSG:3031\")\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae1f1491-6ba7-4c8f-8ec0-0174fe1a8c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_closed_contour(contour, tolerance=1.0):\n",
    "    \"\"\"\n",
    "    Check if a contour is closed by comparing its start and end points.\n",
    "    \n",
    "    Args:\n",
    "        contour: numpy array of shape (N, 2) containing the contour points\n",
    "        tolerance: maximum distance between start and end points to consider contour closed;\n",
    "        default of 1.0 means start and end points must be within one pixel\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if contour is closed, False otherwise\n",
    "    \"\"\"\n",
    "    if len(contour) < 3:\n",
    "        return False\n",
    "        \n",
    "    # Get first and last points\n",
    "    start_point = contour[0]\n",
    "    end_point = contour[-1]\n",
    "    \n",
    "    # Calculate Euclidean distance between start and end points\n",
    "    distance = np.sqrt(np.sum((start_point - end_point) ** 2))\n",
    "    \n",
    "    # Consider contour closed if start and end points are within tolerance\n",
    "    return distance < tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a56f96c6-7d4a-44d3-a9b5-3578bb4ee54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_duplicate_files(directory_path, keep_extension, delete_extension):\n",
    "    \"\"\"\n",
    "    Search a directory for files with matching names but different extensions,\n",
    "    keeping files with one extension and deleting files with another.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to the directory to search\n",
    "        keep_extension (str): File extension to keep (e.g., 'csv')\n",
    "        delete_extension (str): File extension to delete (e.g., 'txt')\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (list of deleted files, list of errors encountered)\n",
    "    \n",
    "    Example usage:\n",
    "        directory = \"path/to/your/directory\"\n",
    "        deleted, errors = cleanup_duplicate_files(directory, \"csv\", \"txt\")\n",
    "        \n",
    "        if deleted:\n",
    "            print(\"\\nDeleted files:\")\n",
    "            for file in deleted:\n",
    "                print(f\"- {file}\")\n",
    "        \n",
    "        if errors:\n",
    "            print(\"\\nErrors encountered:\")\n",
    "            for error in errors:\n",
    "                print(f\"- {error}\")\n",
    "    \"\"\"\n",
    "    # Setup logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    deleted_files = []\n",
    "    errors = []\n",
    "    \n",
    "    try:\n",
    "        # Convert directory path to Path object\n",
    "        dir_path = Path(directory_path)\n",
    "        \n",
    "        # Validate inputs\n",
    "        if not dir_path.exists():\n",
    "            raise FileNotFoundError(f\"Directory {directory_path} does not exist\")\n",
    "            \n",
    "        if not isinstance(keep_extension, str) or not isinstance(delete_extension, str):\n",
    "            raise ValueError(\"File extensions must be strings\")\n",
    "            \n",
    "        # Normalize extensions (remove dots if present)\n",
    "        keep_extension = keep_extension.lstrip('.')\n",
    "        delete_extension = delete_extension.lstrip('.')\n",
    "        \n",
    "        if keep_extension == delete_extension:\n",
    "            raise ValueError(\"Keep and delete extensions must be different\")\n",
    "        \n",
    "        # Get all files of both types\n",
    "        files_to_keep = {f.stem: f for f in dir_path.glob(f\"*.{keep_extension}\")}\n",
    "        files_to_delete = {f.stem: f for f in dir_path.glob(f\"*.{delete_extension}\")}\n",
    "        \n",
    "        # Find matching files\n",
    "        for filename_stem in files_to_delete.keys():\n",
    "            if filename_stem in files_to_keep:\n",
    "                delete_path = files_to_delete[filename_stem]\n",
    "                try:\n",
    "                    delete_path.unlink()  # Delete the file\n",
    "                    deleted_files.append(str(delete_path))\n",
    "                    logger.info(f\"Deleted: {delete_path}\")\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Error deleting {delete_path}: {str(e)}\"\n",
    "                    errors.append(error_msg)\n",
    "                    logger.error(error_msg)\n",
    "        \n",
    "        # Summary\n",
    "        logger.info(f\"Processing complete. Deleted {len(deleted_files)} files. Encountered {len(errors)} errors.\")\n",
    "        \n",
    "        return deleted_files, errors\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Fatal error: {str(e)}\"\n",
    "        errors.append(error_msg)\n",
    "        logger.error(error_msg)\n",
    "        return deleted_files, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d9081-e42b-4ee5-90c2-fa0f5bc63ca2",
   "metadata": {},
   "source": [
    "## visualize_and_save_evolving_outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57775bed-cddb-4cdc-bd48-d2df79fead0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_save_evolving_outlines(lake_gdf, row_index=0):\n",
    "    '''\n",
    "    Visualize the evolving outlines for each stationary lake using optimal level and within_area_multiple combination.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    stationary_outlines_gdf : GeoDataFrame\n",
    "        The dataset of stationary lakes\n",
    "    row_index : int, optional (default=0)\n",
    "        Index of the row to use from the sorted levels_df dataframe.\n",
    "        0 gives the smallest level and corresponding within_area_multiple,\n",
    "        -1 gives the largest level and corresponding within_area_multiple found using the find_and_save_optimal_parameters function.\n",
    "    '''\n",
    "    # Store lake name and polygon\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "    print('Visualizing outlines for', lake_name)\n",
    "\n",
    "    try:\n",
    "        # Load levels dataframe\n",
    "        csv_path = OUTPUT_DIR + '/levels/{}.csv'.format(lake_name)\n",
    "        txt_path = OUTPUT_DIR + '/levels/{}.txt'.format(lake_name)\n",
    "        if os.path.exists(csv_path):\n",
    "            levels_df = pd.read_csv(csv_path)\n",
    "        elif os.path.exists(txt_path):\n",
    "            output_path = os.path.join(OUTPUT_DIR_GIT, f\"lake_outlines/evolving_outlines/{lake_name}.txt\")\n",
    "            print(f'Found no levels CSV file but found \"no outlines\" TXT file for {lake_name}. So writing \"no outlines\" TXT file to {output_path}.')\n",
    "            write_no_outlines(output_path)\n",
    "            return\n",
    "        else:\n",
    "            print(f'No levels file found for {lake_name}. Skipping lake.')\n",
    "            return\n",
    "        \n",
    "        # Select row based on provided index\n",
    "        if abs(row_index) >= len(levels_df):\n",
    "            print(f\"Warning: row_index {row_index} out of bounds for {lake_name}. Skipping.\")\n",
    "            return\n",
    "        else:\n",
    "            selected_row = levels_df.iloc[row_index]\n",
    "\n",
    "        # Print parameters\n",
    "        print(f\"Parameters: row_index={row_index}, within_area_multiple={selected_row['within_area_multiple']}, level={selected_row['level']}, doi(s)={selected_row['dataset_dois']}\")\n",
    "\n",
    "        # Prepare data sets\n",
    "        dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(lake_gdf, 25)\n",
    "\n",
    "        # Create output folders\n",
    "        os.makedirs(OUTPUT_DIR + f'/find_evolving_outlines/{lake_name}', exist_ok=True)\n",
    "        os.makedirs('output/lake_outlines/evolving_outlines', exist_ok=True)\n",
    "        os.makedirs(OUTPUT_DIR + '/plot_evolving_outlines_time_series', exist_ok=True)\n",
    "        os.makedirs(OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines', exist_ok=True)\n",
    "\n",
    "        # Find evolving outlines\n",
    "        evolving_outlines_gdf = find_evolving_outlines(\n",
    "            lake_gdf=lake_gdf, \n",
    "            within_area_multiple=selected_row['within_area_multiple'], \n",
    "            level=selected_row['level'], \n",
    "            dataset1_masked=dataset1_masked,\n",
    "            dataset2_masked=dataset2_masked,\n",
    "            search_extent_poly=search_extent_poly,\n",
    "            plot=True\n",
    "        )\n",
    "       \n",
    "        if evolving_outlines_gdf is None or evolving_outlines_gdf.empty:\n",
    "            print('No evolving outlines found.')\n",
    "            \n",
    "        try:\n",
    "            onlake_outlines, offlake_outlines = extract_intersecting_polygons_recursive(\n",
    "                evolving_outlines_gdf, \n",
    "                lake_gdf['geometry'].iloc[0]\n",
    "            )\n",
    "            \n",
    "            if onlake_outlines is None or onlake_outlines.empty:\n",
    "                print('No valid filtered outlines found this within_area_multiple and level. Deleting levels CSV and writing \"no outlines\" TXT file.')\n",
    "\n",
    "                # Delete levels CSV file and write \"no outlines\" TXT file\n",
    "                os.remove(OUTPUT_DIR + f'/levels/{lake_name}.csv')\n",
    "                write_no_outlines(OUTPUT_DIR + f'/levels/{lake_name}.txt')\n",
    "                write_no_outlines(OUTPUT_DIR_GIT + f'/lake_outlines/evolving_outlines/{lake_name}.txt')\n",
    "                \n",
    "                # Clean up generated images\n",
    "                img_extension = 'png'\n",
    "                images_folder = os.path.join(OUTPUT_DIR, f\"find_evolving_outlines/{lake_name}\")\n",
    "                image_files = sorted(glob.glob(os.path.join(images_folder, f\"*.{img_extension}\")))\n",
    "                for image_file in image_files:\n",
    "                    os.remove(image_file)\n",
    "                try:\n",
    "                    shutil.rmtree(images_folder)\n",
    "                    print(f\"Cleaned up folder: {images_folder}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error cleaning up folder for {lake_name}: {str(e)}\")\n",
    "                    \n",
    "                return\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting polygons for {lake_name}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            \n",
    "        # Save results and plot\n",
    "        if not onlake_outlines.empty:\n",
    "            try:\n",
    "                # Add metadata to onlake_outlines\n",
    "                onlake_outlines = onlake_outlines.copy()\n",
    "                onlake_outlines.loc[:, 'within_area_multiple'] = selected_row['within_area_multiple']\n",
    "                onlake_outlines.loc[:, 'level'] = selected_row['level']\n",
    "                onlake_outlines.loc[:, 'row_index'] = row_index\n",
    "                # onlake_outlines.loc[:, 'within_percent'] = selected_row['within_percent']\n",
    "\n",
    "                if not offlake_outlines.empty:\n",
    "                    offlake_outlines = offlake_outlines.copy()\n",
    "                    offlake_outlines.loc[:, 'within_area_multiple'] = selected_row['within_area_multiple'] \n",
    "                    offlake_outlines.loc[:, 'level'] = selected_row['level']\n",
    "                    offlake_outlines.loc[:, 'row_index'] = row_index\n",
    "                \n",
    "                # Export evolving outlines GeoDataFrame to GeoJSON\n",
    "                filepath = os.path.join(OUTPUT_DIR_GIT, f\"lake_outlines/evolving_outlines/{lake_name}.geojson\")\n",
    "                onlake_outlines.to_file(filename=filepath, driver='GeoJSON')\n",
    "                filepath = OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines/{}.geojson'.format(lake_name)\n",
    "                offlake_outlines.to_file(filename=filepath, driver='GeoJSON')\n",
    "                print(f\"Saved outlines to: {filepath}\")\n",
    "                \n",
    "                # Convert images to video\n",
    "                try:\n",
    "                    video_from_images(lake_gdf, output_dir=OUTPUT_DIR, \n",
    "                                      row_index=row_index, fps=0.25, img_extension='png')\n",
    "        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error creating video for {lake_name}: {str(e)}\")\n",
    "                    traceback.print_exc()\n",
    "                    \n",
    "                # Plot the outlines\n",
    "                try:\n",
    "                    plot_evolving_outlines_time_series(\n",
    "                        lake_gdf=lake_gdf,\n",
    "                        evolving_outlines_gdf=onlake_outlines,\n",
    "                        offlake_outlines_gdf=offlake_outlines\n",
    "                    )\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error creating evolving outlines time series plot for {lake_name}: {str(e)}\")\n",
    "                    traceback.print_exc()\n",
    "\n",
    "                # Clear output\n",
    "                clear_output(wait=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving results for {lake_name}: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "        else:\n",
    "            print(f\"No filtered outlines to save for {lake_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {lake_name}: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        cleanup_vars = [\n",
    "            'dataset1_masked', 'dataset2_masked', 'evolving_outlines_gdf', \n",
    "            'onlake_outlines', 'offlake_outlines', 'levels_df', 'selected_row',\n",
    "            'search_extent_poly'\n",
    "        ]\n",
    "        for var in cleanup_vars:\n",
    "            if var in locals():\n",
    "                del locals()[var]\n",
    "        gc.collect()\n",
    "\n",
    "def video_from_images(lake_gdf, output_dir=OUTPUT_DIR, row_index=0, fps=1, img_extension='png', max_retries=3):\n",
    "    \"\"\"\n",
    "    Creates a video from still images with additional validation and retry mechanism.\n",
    "    \n",
    "    Parameters:\n",
    "    - lake_gdf: GeoDataFrame containing lake information (expects a 'name' column)\n",
    "    - output_dir: Base directory where the images and video are stored/created\n",
    "    - row_index: row of optimal parameters used to generate evolving outlines\n",
    "    - fps: Frames per second for the output video\n",
    "    - img_extension: Extension of the images to look for in the folder\n",
    "    - max_retries: Maximum number of attempts to create the video\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if video creation was successful, False otherwise\n",
    "    \"\"\"\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    print('Making video for', lake_name)\n",
    "    \n",
    "    def validate_images(image_files):\n",
    "        \"\"\"Validate that all images are readable and have consistent dimensions\"\"\"\n",
    "        if not image_files:\n",
    "            return False, None\n",
    "            \n",
    "        reference_frame = cv2.imread(image_files[0])\n",
    "        if reference_frame is None:\n",
    "            return False, None\n",
    "            \n",
    "        height, width = reference_frame.shape[:2]\n",
    "        \n",
    "        for img_file in image_files:\n",
    "            frame = cv2.imread(img_file)\n",
    "            if frame is None:\n",
    "                print(f\"Could not read image: {img_file}\")\n",
    "                return False, None\n",
    "            if frame.shape[:2] != (height, width):\n",
    "                print(f\"Inconsistent dimensions in {img_file}\")\n",
    "                return False, None\n",
    "                \n",
    "        return True, (width, height)\n",
    "\n",
    "    def attempt_video_creation(attempt=1):\n",
    "        try:\n",
    "            # Load levels dataframe\n",
    "            levels_df = pd.read_csv(os.path.join(OUTPUT_DIR, 'levels', f'{lake_name}.csv'))\n",
    "            if levels_df.empty:\n",
    "                print('levels_df empty.')\n",
    "                return False\n",
    "\n",
    "            # Select row based on provided index\n",
    "            if abs(row_index) >= len(levels_df):\n",
    "                print(f\"Warning: row_index {row_index} out of bounds for {lake_name}. Using first row.\")\n",
    "                selected_row = levels_df.iloc[0]\n",
    "            else:\n",
    "                selected_row = levels_df.iloc[row_index]\n",
    "\n",
    "            # Derive paths\n",
    "            images_folder = os.path.join(OUTPUT_DIR, f\"find_evolving_outlines/{lake_name}\")\n",
    "            output_video_file = os.path.join(\n",
    "                output_dir,\n",
    "                f\"find_evolving_outlines/{lake_name}_{row_index}-idx_{selected_row['level']}m-level_{selected_row['within_area_multiple']}x-within.mp4\"\n",
    "            )\n",
    "\n",
    "            # Get and sort images\n",
    "            image_files = sorted(glob.glob(os.path.join(images_folder, f\"*.{img_extension}\")))\n",
    "            \n",
    "            # Validate images\n",
    "            print(f\"Validating {len(image_files)} images...\")\n",
    "            images_valid, dimensions = validate_images(image_files)\n",
    "            if not images_valid:\n",
    "                print(f\"Image validation failed on attempt {attempt}\")\n",
    "                return False\n",
    "\n",
    "            # Try different codecs if needed\n",
    "            codecs = ['mp4v', 'avc1', 'H264']\n",
    "            success = False\n",
    "            \n",
    "            for codec in codecs:\n",
    "                try:\n",
    "                    fourcc = cv2.VideoWriter_fourcc(*codec)\n",
    "                    video = cv2.VideoWriter(output_video_file, fourcc, fps, dimensions)\n",
    "                    \n",
    "                    if not video.isOpened():\n",
    "                        print(f\"Failed to open video writer with codec {codec}\")\n",
    "                        continue\n",
    "                        \n",
    "                    # Write frames\n",
    "                    for image_file in image_files:\n",
    "                        frame = cv2.imread(image_file)\n",
    "                        if frame is not None:\n",
    "                            video.write(frame)\n",
    "                    \n",
    "                    video.release()\n",
    "                    \n",
    "                    # Verify the video was created and is not empty\n",
    "                    if os.path.exists(output_video_file) and os.path.getsize(output_video_file) > 0:\n",
    "                        success = True\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"Video file empty or not created with codec {codec}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error with codec {codec}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            if success:\n",
    "                print(f\"Video created successfully on attempt {attempt}\")\n",
    "                # Clean up images only after successful video creation\n",
    "                for image_file in image_files:\n",
    "                    os.remove(image_file)\n",
    "                try:\n",
    "                    shutil.rmtree(images_folder)\n",
    "                    print(f\"Cleaned up folder: {images_folder}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not delete folder {images_folder}: {e}\")\n",
    "                return True\n",
    "            \n",
    "            return False\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error on attempt {attempt}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    # Main retry loop\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        print(f\"\\nAttempt {attempt} of {max_retries}\")\n",
    "        if attempt > 1:\n",
    "            print(\"Waiting 2 seconds before retry...\")\n",
    "            time.sleep(2)  # Add delay between attempts\n",
    "            \n",
    "        if attempt_video_creation(attempt):\n",
    "            return True\n",
    "            \n",
    "    print(f\"Failed to create video after {max_retries} attempts\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "008328b0-3f7c-4fe0-baa5-a6e787214f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_intersecting_polygons_recursive(gdf, reference_geometry):\n",
    "    \"\"\"\n",
    "    Extract and separate intersecting and non-intersecting polygons with topology validation and cleaning.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        Input GeoDataFrame containing polygons to analyze\n",
    "    reference_geometry : Geometry\n",
    "        Reference geometry to check for intersections\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple (GeoDataFrame, GeoDataFrame)\n",
    "        First GeoDataFrame contains intersecting polygons\n",
    "        Second GeoDataFrame contains non-intersecting polygons\n",
    "    \"\"\"\n",
    "    import shapely.geometry\n",
    "    from shapely.validation import make_valid\n",
    "    import geopandas as gpd\n",
    "    import pandas as pd\n",
    "    \n",
    "    empty_gdf = gpd.GeoDataFrame(geometry=[], crs=gdf.crs if gdf is not None else None)\n",
    "    \n",
    "    if gdf is None or gdf.empty:\n",
    "        return empty_gdf, empty_gdf\n",
    "        \n",
    "    try:\n",
    "        # Create a copy and clean geometries\n",
    "        gdf_copy = gdf.copy()\n",
    "        \n",
    "        # Clean reference geometry\n",
    "        if not reference_geometry.is_valid:\n",
    "            print(\"Cleaning reference geometry...\")\n",
    "            reference_geometry = make_valid(reference_geometry)\n",
    "            \n",
    "        # Clean all geometries in the GeoDataFrame\n",
    "        gdf_copy['geometry'] = gdf_copy['geometry'].apply(lambda geom: make_valid(geom) if not geom.is_valid else geom)\n",
    "        \n",
    "        # Find directly intersecting polygons\n",
    "        try:\n",
    "            directly_intersecting = gdf_copy.loc[gdf_copy.geometry.intersects(reference_geometry)].copy()\n",
    "        except Exception as e:\n",
    "            print(f\"Error in direct intersection: {str(e)}\")\n",
    "            return empty_gdf, empty_gdf\n",
    "            \n",
    "        if directly_intersecting.empty:\n",
    "            # If no direct intersections, return empty intersecting and full non-intersecting\n",
    "            return empty_gdf, gdf_copy\n",
    "            \n",
    "        # Initialize already_found with directly intersecting polygons\n",
    "        already_found = directly_intersecting.copy()\n",
    "        already_found_indices = set(already_found.index)\n",
    "        \n",
    "        # Recursive intersection\n",
    "        try:\n",
    "            while True:\n",
    "                # Create union with buffer to handle small topology issues\n",
    "                union_geom = already_found.geometry.union_all()\n",
    "                if not union_geom.is_valid:\n",
    "                    print(\"Cleaning union geometry...\")\n",
    "                    union_geom = make_valid(union_geom)\n",
    "                \n",
    "                # Find new intersecting polygons\n",
    "                new_intersecting = gdf_copy.loc[\n",
    "                    ~gdf_copy.index.isin(already_found_indices) & \n",
    "                    gdf_copy.geometry.intersects(union_geom)\n",
    "                ].copy()\n",
    "                \n",
    "                if new_intersecting.empty:\n",
    "                    break\n",
    "                    \n",
    "                # Add new indices to our set\n",
    "                already_found_indices.update(new_intersecting.index)\n",
    "                \n",
    "                # Combine results\n",
    "                already_found = gpd.GeoDataFrame(\n",
    "                    pd.concat([already_found, new_intersecting], ignore_index=False)\n",
    "                ).copy()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error in recursive intersection: {str(e)}\")\n",
    "            print(\"Returning directly intersecting polygons only\")\n",
    "            non_intersecting = gdf_copy[~gdf_copy.index.isin(directly_intersecting.index)].copy()\n",
    "            return directly_intersecting, non_intersecting\n",
    "        \n",
    "        # Get non-intersecting polygons using the set of indices we've collected\n",
    "        non_intersecting = gdf_copy[~gdf_copy.index.isin(already_found_indices)].copy()\n",
    "        return already_found, non_intersecting\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_intersecting_polygons_recursive: {str(e)}\")\n",
    "        return empty_gdf, empty_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add471d5-79dc-4caf-921d-3151164afa97",
   "metadata": {},
   "source": [
    "## plot_evolving_outlines_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b9e96a18-e31b-4901-9617-09c2f60b4ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evolving_outlines_time_series(lake_gdf, evolving_outlines_gdf, offlake_outlines_gdf):\n",
    "    '''\n",
    "    Plot evolving outlines time series (on- and off-lake outlines) overlain on ice-surface imagery background.\n",
    "    '''\n",
    "    try:\n",
    "        # Define lake name and polygon\n",
    "        lake_name = lake_gdf['name'].iloc[0]\n",
    "        print(f\"Creating evolving outlines time series plot for lake: {lake_name}\")\n",
    "        \n",
    "        # Validate inputs\n",
    "        if evolving_outlines_gdf is None or evolving_outlines_gdf.empty:\n",
    "            print(f\"No evolving outlines provided for {lake_name}\")\n",
    "            return\n",
    "        \n",
    "        # Get parameters using iloc\n",
    "        within_area_multiple = evolving_outlines_gdf['within_area_multiple'].iloc[0]\n",
    "        level = evolving_outlines_gdf['level'].iloc[0]\n",
    "        row_index = evolving_outlines_gdf['row_index'].iloc[0]\n",
    "\n",
    "        print(f\"Parameters: row_index={row_index}, within_area_multiple={within_area_multiple}, level={level}\")\n",
    "        \n",
    "        stationary_outline = lake_gdf['geometry']\n",
    "        if stationary_outline is None:\n",
    "            print(f\"Error: No geometry found for {lake_name}\")\n",
    "            return\n",
    "\n",
    "        # Create search extent and within evaluation polygons\n",
    "        search_extent_poly = area_multiple_buffer(\n",
    "            stationary_outline, 25)\n",
    "        within_evaluation_poly = area_multiple_buffer(\n",
    "            stationary_outline, within_area_multiple)\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(5,5))\n",
    "        \n",
    "        # Plot search extent and within evaluation polygons\n",
    "        gpd.GeoDataFrame(geometry=[search_extent_poly]).boundary.plot(\n",
    "            ax=ax, edgecolor='k', facecolor='none', linewidth=1)\n",
    "        gpd.GeoDataFrame(geometry=[within_evaluation_poly]).boundary.plot(\n",
    "            ax=ax, edgecolor='dimgray', facecolor='none', linewidth=1)\n",
    "\n",
    "        # # Set up colormap\n",
    "        # min_date = pd.to_datetime(cyc_start_dates[1])\n",
    "        # max_date = pd.to_datetime(cyc_end_dates[-1])\n",
    "        # date_range = pd.date_range(min_date, max_date, periods=len(mid_cyc_dates[1:]))\n",
    "        # years = pd.date_range(min_date, max_date, freq='YE')\n",
    "\n",
    "        # # Get number of dates\n",
    "        # n_dates = len(mid_cyc_dates[1:])\n",
    "        # cmap = plt.get_cmap('plasma', n_dates)\n",
    "        # norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "        # m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        # m.set_array(np.linspace(mdates.date2num(min_date), mdates.date2num(max_date), n_dates))\n",
    "        \n",
    "        # # Add colorbar\n",
    "        # divider = make_axes_locatable(ax)\n",
    "        # cax = divider.append_axes('bottom', size='2.5%', pad=0.5)\n",
    "        # cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "\n",
    "        # # Set ticks for all years but labels only for odd years\n",
    "        # tick_locations = [mdates.date2num(date) for date in years]\n",
    "        # tick_labels = [date.strftime('%Y') if date.year % 2 == 1 else '' for date in years]\n",
    "        # cbar.set_ticks(tick_locations)\n",
    "        # cbar.set_ticklabels(tick_labels)\n",
    "\n",
    "        # # Add minor ticks for quarters\n",
    "        # cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Quarter intervals only\n",
    "        # cbar.set_label('year', size=10, labelpad=10)\n",
    "\n",
    "        # Set up colormap\n",
    "        min_date = pd.to_datetime(cyc_start_dates[1])\n",
    "        max_date = pd.to_datetime(cyc_end_dates[-1])\n",
    "        date_range = pd.date_range(min_date, max_date, periods=len(mid_cyc_dates[1:]))\n",
    "        years = date_range.year.unique()\n",
    "        years = pd.to_datetime(years, format='%Y')\n",
    "        n_dates = len(mid_cyc_dates[1:])\n",
    "        cmap = plt.get_cmap('plasma', n_dates)\n",
    "        norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "        m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        m.set_array(np.linspace(mdates.date2num(min_date), mdates.date2num(max_date), n_dates))\n",
    "        \n",
    "        # Add colorbar\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('bottom', size='2.5%', pad=0.5)\n",
    "        cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "    \n",
    "        # Set ticks for all years but labels only for odd years\n",
    "        tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "        tick_labels = [date.strftime('%Y') if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "        cbar.set_ticks(tick_locations)\n",
    "        cbar.set_ticklabels(tick_labels)\n",
    "        \n",
    "        # Add minor ticks for quarters\n",
    "        cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Quarter intervals only\n",
    "        cbar.set_label('year', size=10, labelpad=10)\n",
    "\n",
    "        # Set plot bounds\n",
    "        x_min, y_min, x_max, y_max = search_extent_poly.bounds\n",
    "        x_buffer = abs(x_max-x_min)*0.05\n",
    "        y_buffer = abs(y_max-y_min)*0.05\n",
    "        ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "        # Plot MOA surface imagery\n",
    "        mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "        mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "        moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "        ax.imshow(moa_highres_da_subset[0,:,:], cmap='gray', clim=[14000, 17000], \n",
    "                  extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "\n",
    "        # Plot stationary lakes\n",
    "        stationary_lake_color = 'darkturquoise'\n",
    "        stationary_outlines_gdf.boundary.plot(\n",
    "            ax=ax, facecolor=stationary_lake_color, linestyle='solid', linewidth=2, alpha=0.25)\n",
    "        stationary_outlines_gdf.boundary.plot(\n",
    "            ax=ax, edgecolor=stationary_lake_color, linestyle='solid', linewidth=2)\n",
    "\n",
    "        # Plot evolving outlines\n",
    "        onlake_lines, offlake_lines = [], []\n",
    "        for idx, dt in enumerate(mid_cyc_dates[1:]):\n",
    "            x, y = 1, 1\n",
    "            date_num = mdates.date2num(pd.to_datetime(dt))\n",
    "            onlake_line, = ax.plot(x, y, color=cmap(norm(date_num)), linewidth=2)\n",
    "            onlake_lines.append(onlake_line)\n",
    "            offlake_line, = ax.plot(x, y, color=cmap(norm(date_num)), linewidth=2, alpha=0.2)\n",
    "            offlake_lines.append(offlake_line)\n",
    "            \n",
    "            evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "            offlake_outlines_dt = offlake_outlines_gdf[offlake_outlines_gdf['midcyc_datetime'] == dt]\n",
    "            if not evolving_outlines_dt.empty:\n",
    "                evolving_outlines_dt.boundary.plot(ax=ax, \n",
    "                    color=cmap(norm(date_num)), \n",
    "                    linewidth=0.5)\n",
    "            if not offlake_outlines_dt.empty:\n",
    "                offlake_outlines_dt.boundary.plot(ax=ax, \n",
    "                    color=cmap(norm(date_num)), \n",
    "                    linewidth=0.5, alpha=0.25)\n",
    "\n",
    "        # Format axes\n",
    "        km_scale = 1e3\n",
    "        ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "        ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_xlabel('x [km]', size=10)\n",
    "        ax.set_ylabel('y [km]', size=10)\n",
    "        \n",
    "        # Add legend\n",
    "        stationary_line = plt.Line2D([],[], color=stationary_lake_color, linestyle='solid', linewidth=2)\n",
    "        within_area_multiple_line = plt.Line2D([],[], color='dimgray', linestyle='solid', linewidth=2)\n",
    "        search_extent_line = plt.Line2D([],[], color='black', linestyle='solid', linewidth=2)\n",
    "\n",
    "        ax.legend(\n",
    "            handles=[stationary_line, \n",
    "                     tuple(onlake_lines), \n",
    "                     tuple(offlake_lines),\n",
    "                     within_area_multiple_line, \n",
    "                     search_extent_line],\n",
    "            labels=['stationary outline', \n",
    "                    f'evolving outlines ({level} m)', \n",
    "                    'off-lake evolving outlines', \n",
    "                    f'within evaluation boundary ({int(within_area_multiple)}x)',\n",
    "                    'search extent'],\n",
    "            handlelength=3,\n",
    "            handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "            loc='upper left',\n",
    "            bbox_to_anchor=(0, 1.3))\n",
    "\n",
    "        # Add inset map\n",
    "        axIns = ax.inset_axes([0.02, 0.01, 0.25, 0.25])\n",
    "        axIns.set_aspect('equal')\n",
    "        moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        axIns.scatter(\n",
    "            ((x_max+x_min)/2), ((y_max+y_min)/2),\n",
    "            marker='*', linewidth=0.1, color='k', s=30, zorder=3\n",
    "        )\n",
    "        axIns.axis('off')\n",
    "\n",
    "        # Add title\n",
    "        ax.set_title(f'{lake_name}', size=12, y=1.3)\n",
    "\n",
    "        # Generate output filename and save\n",
    "        output_filename = os.path.join(OUTPUT_DIR, 'plot_evolving_outlines_time_series',\n",
    "            f'{lake_name}_{int(row_index)}-idx_{level}m-level_{int(within_area_multiple)}x-within.png'\n",
    "        )\n",
    "        \n",
    "        plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Successfully saved plot to: {output_filename}\")\n",
    "        \n",
    "        plt.close()\n",
    "\n",
    "        # Clear intermediate objects to conserve memory\n",
    "        try:\n",
    "            plt.close('all')\n",
    "            del moa_highres_da_subset\n",
    "            del onlake_lines, offlake_lines\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f\"Error cleaning up plot resources: {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in plot_evolving_outlines_time_series function for {lake_name}:\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        print(\"Error traceback:\")\n",
    "        traceback.print_exc()\n",
    "        plt.close('all')\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9f3dcb-40f4-450f-9f76-62c482c07b46",
   "metadata": {},
   "source": [
    "## Functions to analyze lake groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "182521e4-b89f-41b7-b1fe-92ec47350fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_group_gdf(stationary_outlines_gdf, lake_group):\n",
    "    '''\n",
    "    Prepare a GeoDataFrame row representing a group of lakes.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    stationary_outlines_gdf : GeoDataFrame\n",
    "        The complete dataset of stationary lakes\n",
    "    lake_group : list\n",
    "        List of lake names to be analyzed together\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    GeoDataFrame\n",
    "        Single-row GeoDataFrame with combined group properties\n",
    "    '''\n",
    "    try:\n",
    "        # Filter GeoDataFrame for lakes in the group\n",
    "        group_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'].isin(lake_group)].copy()\n",
    "        if group_gdf.empty:\n",
    "            print(f\"No lakes found for group: {lake_group}\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"Preparing group geodataframe for lake group: {lake_group}\")\n",
    "        \n",
    "        # Create a combined name for the group\n",
    "        group_name = \"_\".join(lake_group)\n",
    "        \n",
    "        # Create a union of all lake geometries for the group\n",
    "        group_stationary_outline = group_gdf.geometry.union_all()\n",
    "        if not group_stationary_outline.is_valid:\n",
    "            print(\"Cleaning group geometry...\")\n",
    "            group_stationary_outline = make_valid(group_stationary_outline)\n",
    "        \n",
    "        # Determine the group's time period\n",
    "        group_time_period = determine_group_time_period(group_gdf['CS2_SARIn_start'])\n",
    "        print(f\"Group CryoSat-2 SARIn time period determined as: {group_time_period}\")\n",
    "        \n",
    "        # Create a GeoDataFrame for the group\n",
    "        group_single_gdf = gpd.GeoDataFrame(\n",
    "            {\n",
    "                'name': [group_name],\n",
    "                'geometry': [group_stationary_outline],\n",
    "                'CS2_SARIn_start': [group_time_period]\n",
    "            },\n",
    "            crs=group_gdf.crs\n",
    "        )\n",
    "        \n",
    "        return group_single_gdf\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing group GeoDataFrame: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def determine_group_time_period(time_periods):\n",
    "    '''\n",
    "    Determine the most exclusive time period for a group of lakes.\n",
    "    \n",
    "    Rules:\n",
    "    - If any lake has <NA>, group gets <NA>\n",
    "    - If all lakes have '2010.5', group gets '2010.5'\n",
    "    - If all lakes have either '2013.75' or '2010.5', group gets '2013.75'\n",
    "    - Otherwise, group gets <NA>\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    time_periods : Series\n",
    "        Series of time periods from the group of lakes\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str or pd.NA\n",
    "        The determined time period for the group\n",
    "    '''\n",
    "    # If any lake has NA, group gets NA\n",
    "    if time_periods.isna().any():\n",
    "        return pd.NA\n",
    "        \n",
    "    # Convert to list and remove any NA values\n",
    "    periods = [p for p in time_periods if pd.notna(p)]\n",
    "    \n",
    "    # If all lakes have '2010.5'\n",
    "    if all(p == '2010.5' for p in periods):\n",
    "        return '2010.5'\n",
    "        \n",
    "    # If all lakes have either '2013.75' or '2010.5'\n",
    "    valid_periods = {'2013.75', '2010.5'}\n",
    "    if all(p in valid_periods for p in periods):\n",
    "        return '2013.75'\n",
    "        \n",
    "    # Default to NA for any other case\n",
    "    return pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a661a49-c0a1-41ed-a566-2cb87bef95c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard_lake_outlines(\n",
    "    lake_outlines_to_discard: List[str],\n",
    "    source_dir: str,\n",
    "    dest_dir: str\n",
    ") -> Dict[str, Tuple[bool, str]]:\n",
    "    \"\"\"\n",
    "    Move lake outlines from git repo to non-git repo and create indicator files.\n",
    "    Replaces existing files in destination directory.\n",
    "    \n",
    "    Args:\n",
    "        lake_outlines_to_discard: List of lake names to process\n",
    "        output_dir_git: Path to source git repository directory\n",
    "        output_dir: Path to destination non-git directory\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with lake names as keys and tuples of (success_bool, message) as values\n",
    "    \"\"\"\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    \n",
    "    results = {}\n",
    "    for lake in lake_outlines_to_discard:\n",
    "        source_filepath = os.path.join(source_dir, f\"{lake}.geojson\")\n",
    "        dest_filepath = os.path.join(dest_dir, f\"{lake}.geojson\")\n",
    "        txt_filepath = os.path.join(source_dir, f\"{lake}.txt\")\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(source_filepath):\n",
    "                results[lake] = (False, f\"Source file does not exist: {source_filepath}\")\n",
    "                continue\n",
    "                \n",
    "            # Remove check for existing destination file\n",
    "            # Use copy2 then remove original to ensure atomic operation\n",
    "            shutil.copy2(source_filepath, dest_filepath)\n",
    "            os.remove(source_filepath)\n",
    "            results[lake] = (True, \"Successfully moved and replaced existing file\")\n",
    "            \n",
    "            write_no_outlines(txt_filepath)\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[lake] = (False, f\"Error: {str(e)}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01edcb8e-2120-4629-8926-6b48ae8076ee",
   "metadata": {},
   "source": [
    "## evolving_outlines_geom_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c25ac08e-db7c-4273-bc64-a5bd89167348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evolving_outlines_geom_calc(stationary_outline_gdf, dataset1, dataset2): \n",
    "#     '''\n",
    "#     Create dataframe of active area, dh, dV calculations for evolving compared to stationary outlines\n",
    "#     Area measurements rounded to nearest whole number due to 1-km grid resolution\n",
    "#     Height measurements rounded to 2 decimal places to match 9 cm measurement precision\n",
    "#     Volume measurements rounded to whole numbers due to combined uncertainties\n",
    "#     '''\n",
    "#     # Define lake name and polygon and buffered polygon to use as off-lake region\n",
    "#     lake_name = stationary_outline_gdf['name'].iloc[0]\n",
    "#     print(f\"Processing lake: {lake_name}\")\n",
    "\n",
    "#     # Open evolving outlines geodataframe\n",
    "#     try:\n",
    "#         evolving_outlines_gdf = gpd.read_file(os.path.join(\n",
    "#             os.getcwd(), 'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)))\n",
    "#     except fiona.errors.DriverError:\n",
    "#         print(f\"File for {lake_name} not found. Skipping...\")\n",
    "#         return  \n",
    "\n",
    "#     # Ensure there are outlines in outines_gdf\n",
    "#     if evolving_outlines_gdf.empty:\n",
    "#         print('There are no evolving outlines in geodataframe.')\n",
    "#         return  \n",
    "\n",
    "#     # Define region of interest for slicing from evolving and stationary outlines\n",
    "#     evolving_union = evolving_outlines_gdf.union_all()\n",
    "#     evolving_union_region = area_multiple_buffer(evolving_union, 2, exclude_inner=True)\n",
    "#     x_min, y_min, x_max, y_max = evolving_union_region.bounds\n",
    "\n",
    "#     # Create empty lists to store metrics\n",
    "#     midcyc_datetimes = []\n",
    "#     evolving_outlines_areas = []\n",
    "#     evolving_outlines_dhs = []\n",
    "#     evolving_region_dhs = []\n",
    "#     evolving_outlines_dhs_corr = []\n",
    "#     evolving_outlines_dVs_corr = []\n",
    "\n",
    "#     with warnings.catch_warnings():\n",
    "#         warnings.simplefilter('ignore', category=RuntimeWarning)\n",
    "\n",
    "#         # Process dataset1 if available\n",
    "#         if dataset1 is not None:\n",
    "#             # Create masks and calculations for dataset1\n",
    "#             dataset1_ROI_subset = dataset1.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "#             dataset1_ROI_da = dataset1_ROI_subset['delta_h']\n",
    "            \n",
    "#             dataset1_evolving_region_mask = np.array([[evolving_union_region.contains(Point(x, y)) \n",
    "#                                                       for x in dataset1_ROI_da.x.values] \n",
    "#                                                       for y in dataset1_ROI_da.y.values])\n",
    "#             dataset1_evolving_region_masked = dataset1_ROI_da.where(xr.DataArray(dataset1_evolving_region_mask, \n",
    "#                                                             coords=[dataset1_ROI_da.y, dataset1_ROI_da.x], \n",
    "#                                                             dims=[\"y\", \"x\"]))\n",
    "            \n",
    "#             dataset1_ROI_dh = dataset1_ROI_da.diff('time')\n",
    "#             dataset1_evolving_region_dh = dataset1_evolving_region_masked.diff('time')\n",
    "\n",
    "#             # Calculate mid-cycle datetimes\n",
    "#             dataset1_midcyc_datetimes = []\n",
    "#             dataset1_datetimes = dataset1_ROI_da['time'].values\n",
    "#             for i in range(1, len(dataset1_datetimes)):\n",
    "#                 midcyc_days = dataset1_datetimes[i] - dataset1_datetimes[i-1]\n",
    "#                 midcyc_datetime = dataset1_datetimes[i-1] + midcyc_days/2\n",
    "#                 dataset1_midcyc_datetimes.append(midcyc_datetime)\n",
    "#             dataset1_midcyc_datetimes = np.array(dataset1_midcyc_datetimes)\n",
    "\n",
    "#             for i, midcyc_datetime in enumerate(dataset1_midcyc_datetimes):\n",
    "#                 print(midcyc_datetime)\n",
    "#                 evolving_outlines_geom_calc_process_timestep(\n",
    "#                     evolving_region_dh_slice=dataset1_evolving_region_dh.isel(time=i),\n",
    "#                     timestep_subset_evolving_outlines_gdf=evolving_outlines_gdf[\n",
    "#                         evolving_outlines_gdf['midcyc_datetime'] == midcyc_datetime],\n",
    "#                     ROI_dh_slice=dataset1_ROI_dh.isel(time=i),\n",
    "#                     timestep=midcyc_datetime,\n",
    "#                     midcyc_datetimes=midcyc_datetimes,\n",
    "#                     evolving_outlines_areas=evolving_outlines_areas,\n",
    "#                     evolving_outlines_dhs=evolving_outlines_dhs,\n",
    "#                     evolving_region_dhs=evolving_region_dhs,\n",
    "#                     evolving_outlines_dhs_corr=evolving_outlines_dhs_corr,\n",
    "#                     evolving_outlines_dVs_corr=evolving_outlines_dVs_corr\n",
    "#                 )\n",
    "\n",
    "#         # Process dataset2\n",
    "#         dataset2_ROI_subset = dataset2.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "#         dataset2_ROI_da = dataset2_ROI_subset['delta_h']\n",
    "        \n",
    "#         dataset2_evolving_region_mask = np.array([[evolving_union_region.contains(Point(x, y)) \n",
    "#                                                   for x in dataset2_ROI_da.x.values] \n",
    "#                                                   for y in dataset2_ROI_da.y.values])\n",
    "#         dataset2_evolving_region_masked = dataset2_ROI_da.where(xr.DataArray(dataset2_evolving_region_mask, \n",
    "#                                                         coords=[dataset2_ROI_da.y, dataset2_ROI_da.x], \n",
    "#                                                         dims=[\"y\", \"x\"]))\n",
    "\n",
    "#         dataset2_ROI_dh = dataset2_ROI_da.diff('time')\n",
    "#         dataset2_evolving_region_dh = dataset2_evolving_region_masked.diff('time')\n",
    "\n",
    "#         # Calculate mid-cycle datetimes\n",
    "#         dataset2_midcyc_datetimes = []\n",
    "#         dataset2_datetimes = dataset2_ROI_da['time'].values\n",
    "#         for i in range(1, len(dataset2_datetimes)):\n",
    "#             midcyc_days = dataset2_datetimes[i] - dataset2_datetimes[i-1]\n",
    "#             midcyc_datetime = dataset2_datetimes[i-1] + midcyc_days/2\n",
    "#             dataset2_midcyc_datetimes.append(midcyc_datetime)\n",
    "#         dataset2_midcyc_datetimes = np.array(dataset2_midcyc_datetimes)\n",
    "\n",
    "#         for i, midcyc_datetime in enumerate(dataset2_midcyc_datetimes):\n",
    "#             evolving_outlines_geom_calc_process_timestep(\n",
    "#                 evolving_region_dh_slice=dataset2_evolving_region_dh.isel(time=i),\n",
    "#                 timestep_subset_evolving_outlines_gdf=evolving_outlines_gdf[\n",
    "#                     evolving_outlines_gdf['midcyc_datetime'] == midcyc_datetime],\n",
    "#                 ROI_dh_slice=dataset2_ROI_dh.isel(time=i),\n",
    "#                 timestep=midcyc_datetime,\n",
    "#                 midcyc_datetimes=midcyc_datetimes,\n",
    "#                 evolving_outlines_areas=evolving_outlines_areas,\n",
    "#                 evolving_outlines_dhs=evolving_outlines_dhs,\n",
    "#                 evolving_region_dhs=evolving_region_dhs,\n",
    "#                 evolving_outlines_dhs_corr=evolving_outlines_dhs_corr,\n",
    "#                 evolving_outlines_dVs_corr=evolving_outlines_dVs_corr\n",
    "#             )\n",
    "\n",
    "#     # Create dataframe if we have data\n",
    "#     if len(midcyc_datetimes) > 0:\n",
    "#         d = {\n",
    "#             'midcyc_datetime': midcyc_datetimes,\n",
    "#             'evolving_outlines_area (m^2)': [round(x, 0) for x in evolving_outlines_areas],  # Round to whole numbers\n",
    "#             'evolving_outlines_dh (m)': [round(x, 2) for x in evolving_outlines_dhs],  # 2 decimals for height\n",
    "#             'evolving_outlines_region_dh (m)': [round(x, 2) for x in evolving_region_dhs],  # 2 decimals for height\n",
    "#             'evolving_outlines_dh_corr (m)': [round(x, 2) for x in evolving_outlines_dhs_corr],  # 2 decimals for height\n",
    "#             'evolving_outlines_dV_corr (m^3)': [round(x, 0) for x in evolving_outlines_dVs_corr],  # Round to whole numbers\n",
    "#         }\n",
    "        \n",
    "#         df = pd.DataFrame(d)\n",
    "#         df = df.fillna(0.0)\n",
    "        \n",
    "#         # Convert area and volume columns to integer type since we're using whole numbers\n",
    "#         df['evolving_outlines_area (m^2)'] = df['evolving_outlines_area (m^2)'].astype(int)\n",
    "#         df['evolving_outlines_dV_corr (m^3)'] = df['evolving_outlines_dV_corr (m^3)'].astype(int)\n",
    "        \n",
    "#         output_path = f'output/geometric_calcs/evolving_outlines_geom_calc/'\n",
    "#         output_file = os.path.join(output_path, f'{lake_name}.csv')\n",
    "#         df.to_csv(output_file, index=False)\n",
    "#         print(f\"Results saved to: {output_file}\")\n",
    "        \n",
    "#         return df\n",
    "#     else:\n",
    "#         print(f\"No data processed for {lake_name}\")\n",
    "#         return None\n",
    "\n",
    "# def evolving_outlines_geom_calc_process_timestep(evolving_region_dh_slice,\n",
    "#                      timestep_subset_evolving_outlines_gdf,\n",
    "#                      ROI_dh_slice,\n",
    "#                      timestep,\n",
    "#                      midcyc_datetimes,\n",
    "#                      evolving_outlines_areas,\n",
    "#                      evolving_outlines_dhs,\n",
    "#                      evolving_region_dhs,\n",
    "#                      evolving_outlines_dhs_corr,\n",
    "#                      evolving_outlines_dVs_corr):\n",
    "#     \"\"\"\n",
    "#     Process a single timestep of lake dh data and calculate various metrics.\n",
    "#     dh measurements rounded to 2 decimal places (due to 9 cm precision)\n",
    "#     Area measurements rounded to whole numbers (due to 1-km grid resolution)\n",
    "#     dV measurements rounded to whole numbers (due to combined uncertainty)\n",
    "#     \"\"\"    \n",
    "#     # Initialize evolving outlines variables with default values of 0\n",
    "#     evolving_outlines_area = 0\n",
    "#     evolving_outlines_dh = 0.0\n",
    "#     evolving_outlines_dh_corr = 0.0\n",
    "#     evolving_outlines_dV_corr = 0\n",
    "\n",
    "#     # Calculate metrics for evolving union region\n",
    "#     evolving_region_dh = round(float(np.nanmean(evolving_region_dh_slice)), 2)  # 2 decimals for height\n",
    "\n",
    "#     if not timestep_subset_evolving_outlines_gdf.empty:\n",
    "#         # Calculate metrics for evolving outlines\n",
    "#         evolving_outlines_area = round(float(timestep_subset_evolving_outlines_gdf['area (m^2)'].sum()), 0)\n",
    "#         union_timestep_subset_evolving_outlines = timestep_subset_evolving_outlines_gdf['geometry'].union_all()\n",
    "#         union_timestep_subset_evolving_outlines_mask = np.array([[union_timestep_subset_evolving_outlines.contains(Point(x, y)) \n",
    "#                                                                 for x in ROI_dh_slice['x'].values] \n",
    "#                                                                 for y in ROI_dh_slice['y'].values])\n",
    "#         union_timestep_subset_evolving_outlines_masked = ROI_dh_slice.where(xr.DataArray(union_timestep_subset_evolving_outlines_mask, \n",
    "#                                                                  coords=[ROI_dh_slice.y, ROI_dh_slice.x], \n",
    "#                                                                  dims=[\"y\", \"x\"]))\n",
    "\n",
    "#         evolving_outlines_dh = round(float(np.nanmean(union_timestep_subset_evolving_outlines_masked)), 2)  # 2 decimals\n",
    "#         evolving_outlines_dh_corr = round(evolving_outlines_dh - evolving_region_dh, 2)  # 2 decimals\n",
    "#         evolving_outlines_dV_corr = round(evolving_outlines_dh_corr * evolving_outlines_area, 0)  # Whole numbers\n",
    "    \n",
    "#     # Append all metrics to lists\n",
    "#     midcyc_datetimes.append(timestep)\n",
    "#     evolving_outlines_areas.append(evolving_outlines_area)\n",
    "#     evolving_outlines_dhs.append(evolving_outlines_dh)\n",
    "#     evolving_region_dhs.append(evolving_region_dh)\n",
    "#     evolving_outlines_dhs_corr.append(evolving_outlines_dh_corr)\n",
    "#     evolving_outlines_dVs_corr.append(evolving_outlines_dV_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e418c6d-e384-4788-8482-1a786f3c9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolving_outlines_geom_calc(stationary_outline_gdf, dataset1, dataset2, forward_fill=False): \n",
    "    '''\n",
    "    Create dataframe of active area, dh, dV calculations for evolving compared to stationary outlines\n",
    "    Area measurements rounded to nearest whole number due to 1-km grid resolution\n",
    "    Height measurements rounded to 2 decimal places to match 9 cm measurement precision\n",
    "    Volume measurements rounded to whole numbers due to combined uncertainties\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    stationary_outline_gdf : GeoDataFrame\n",
    "        Contains stationary lake outline information\n",
    "    dataset1 : xarray.Dataset\n",
    "        First dataset with delta_h variable\n",
    "    dataset2 : xarray.Dataset\n",
    "        Second dataset with delta_h variable\n",
    "    forward_fill : bool, default=False\n",
    "        If True, use Last Observation Carried Forward (LOCF) methodology by reading \n",
    "        from forward-filled geojson files\n",
    "    '''\n",
    "    # Define lake name and polygon and buffered polygon to use as off-lake region\n",
    "    lake_name = stationary_outline_gdf['name'].iloc[0]\n",
    "    print(f\"Processing lake: {lake_name}\")\n",
    "\n",
    "    # Open evolving outlines geodataframe\n",
    "    try:\n",
    "        # Choose the appropriate path based on forward_fill parameter\n",
    "        if forward_fill:\n",
    "            outlines_path = os.path.join(os.getcwd(), \n",
    "                'output/lake_outlines/evolving_outlines/forward_fill/{}.geojson'.format(lake_name))\n",
    "        else:\n",
    "            outlines_path = os.path.join(os.getcwd(), \n",
    "                'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name))\n",
    "            \n",
    "        evolving_outlines_gdf = gpd.read_file(outlines_path)\n",
    "    except fiona.errors.DriverError:\n",
    "        print(f\"File for {lake_name} not found. Skipping...\")\n",
    "        return  \n",
    "\n",
    "    # Ensure there are outlines in outines_gdf\n",
    "    if evolving_outlines_gdf.empty:\n",
    "        print('There are no evolving outlines in geodataframe.')\n",
    "        return  \n",
    "\n",
    "    # Define region of interest for slicing from evolving and stationary outlines\n",
    "    evolving_union = evolving_outlines_gdf.union_all()\n",
    "    evolving_union_region = area_multiple_buffer(evolving_union, 2, exclude_inner=True)\n",
    "    x_min, y_min, x_max, y_max = evolving_union_region.bounds\n",
    "\n",
    "    # Create empty lists to store metrics\n",
    "    midcyc_datetimes = []\n",
    "    evolving_outlines_areas = []\n",
    "    evolving_outlines_dhs = []\n",
    "    evolving_region_dhs = []\n",
    "    evolving_outlines_dhs_corr = []\n",
    "    evolving_outlines_dVs_corr = []\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore', category=RuntimeWarning)\n",
    "\n",
    "        # Process dataset1 if available\n",
    "        if dataset1 is not None:\n",
    "            # Create masks and calculations for dataset1\n",
    "            dataset1_ROI_subset = dataset1.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "            dataset1_ROI_da = dataset1_ROI_subset['delta_h']\n",
    "            \n",
    "            dataset1_evolving_region_mask = np.array([[evolving_union_region.contains(Point(x, y)) \n",
    "                                                      for x in dataset1_ROI_da.x.values] \n",
    "                                                      for y in dataset1_ROI_da.y.values])\n",
    "            dataset1_evolving_region_masked = dataset1_ROI_da.where(xr.DataArray(dataset1_evolving_region_mask, \n",
    "                                                            coords=[dataset1_ROI_da.y, dataset1_ROI_da.x], \n",
    "                                                            dims=[\"y\", \"x\"]))\n",
    "            \n",
    "            dataset1_ROI_dh = dataset1_ROI_da.diff('time')\n",
    "            dataset1_evolving_region_dh = dataset1_evolving_region_masked.diff('time')\n",
    "\n",
    "            # Calculate mid-cycle datetimes\n",
    "            dataset1_midcyc_datetimes = []\n",
    "            dataset1_datetimes = dataset1_ROI_da['time'].values\n",
    "            for i in range(1, len(dataset1_datetimes)):\n",
    "                midcyc_days = dataset1_datetimes[i] - dataset1_datetimes[i-1]\n",
    "                midcyc_datetime = dataset1_datetimes[i-1] + midcyc_days/2\n",
    "                dataset1_midcyc_datetimes.append(midcyc_datetime)\n",
    "            dataset1_midcyc_datetimes = np.array(dataset1_midcyc_datetimes)\n",
    "\n",
    "            for i, midcyc_datetime in enumerate(dataset1_midcyc_datetimes):\n",
    "                # Get the subset for this timestep\n",
    "                timestep_subset_evolving_outlines_gdf = evolving_outlines_gdf[\n",
    "                    evolving_outlines_gdf['midcyc_datetime'] == midcyc_datetime]\n",
    "                \n",
    "                evolving_outlines_geom_calc_process_timestep(\n",
    "                    evolving_region_dh_slice=dataset1_evolving_region_dh.isel(time=i),\n",
    "                    timestep_subset_evolving_outlines_gdf=timestep_subset_evolving_outlines_gdf,\n",
    "                    ROI_dh_slice=dataset1_ROI_dh.isel(time=i),\n",
    "                    timestep=midcyc_datetime,\n",
    "                    midcyc_datetimes=midcyc_datetimes,\n",
    "                    evolving_outlines_areas=evolving_outlines_areas,\n",
    "                    evolving_outlines_dhs=evolving_outlines_dhs,\n",
    "                    evolving_region_dhs=evolving_region_dhs,\n",
    "                    evolving_outlines_dhs_corr=evolving_outlines_dhs_corr,\n",
    "                    evolving_outlines_dVs_corr=evolving_outlines_dVs_corr\n",
    "                )\n",
    "\n",
    "        # Process dataset2\n",
    "        dataset2_ROI_subset = dataset2.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "        dataset2_ROI_da = dataset2_ROI_subset['delta_h']\n",
    "        \n",
    "        dataset2_evolving_region_mask = np.array([[evolving_union_region.contains(Point(x, y)) \n",
    "                                                  for x in dataset2_ROI_da.x.values] \n",
    "                                                  for y in dataset2_ROI_da.y.values])\n",
    "        dataset2_evolving_region_masked = dataset2_ROI_da.where(xr.DataArray(dataset2_evolving_region_mask, \n",
    "                                                        coords=[dataset2_ROI_da.y, dataset2_ROI_da.x], \n",
    "                                                        dims=[\"y\", \"x\"]))\n",
    "\n",
    "        dataset2_ROI_dh = dataset2_ROI_da.diff('time')\n",
    "        dataset2_evolving_region_dh = dataset2_evolving_region_masked.diff('time')\n",
    "\n",
    "        # Calculate mid-cycle datetimes\n",
    "        dataset2_midcyc_datetimes = []\n",
    "        dataset2_datetimes = dataset2_ROI_da['time'].values\n",
    "        for i in range(1, len(dataset2_datetimes)):\n",
    "            midcyc_days = dataset2_datetimes[i] - dataset2_datetimes[i-1]\n",
    "            midcyc_datetime = dataset2_datetimes[i-1] + midcyc_days/2\n",
    "            dataset2_midcyc_datetimes.append(midcyc_datetime)\n",
    "        dataset2_midcyc_datetimes = np.array(dataset2_midcyc_datetimes)\n",
    "\n",
    "        for i, midcyc_datetime in enumerate(dataset2_midcyc_datetimes):\n",
    "            # Get the subset for this timestep\n",
    "            timestep_subset_evolving_outlines_gdf = evolving_outlines_gdf[\n",
    "                evolving_outlines_gdf['midcyc_datetime'] == midcyc_datetime]\n",
    "            \n",
    "            evolving_outlines_geom_calc_process_timestep(\n",
    "                evolving_region_dh_slice=dataset2_evolving_region_dh.isel(time=i),\n",
    "                timestep_subset_evolving_outlines_gdf=timestep_subset_evolving_outlines_gdf,\n",
    "                ROI_dh_slice=dataset2_ROI_dh.isel(time=i),\n",
    "                timestep=midcyc_datetime,\n",
    "                midcyc_datetimes=midcyc_datetimes,\n",
    "                evolving_outlines_areas=evolving_outlines_areas,\n",
    "                evolving_outlines_dhs=evolving_outlines_dhs,\n",
    "                evolving_region_dhs=evolving_region_dhs,\n",
    "                evolving_outlines_dhs_corr=evolving_outlines_dhs_corr,\n",
    "                evolving_outlines_dVs_corr=evolving_outlines_dVs_corr\n",
    "            )\n",
    "\n",
    "    # Create dataframe if we have data\n",
    "    if len(midcyc_datetimes) > 0:\n",
    "        d = {\n",
    "            'midcyc_datetime': midcyc_datetimes,\n",
    "            'evolving_outlines_area (m^2)': [round(x, 0) for x in evolving_outlines_areas],  # Round to whole numbers\n",
    "            'evolving_outlines_dh (m)': [round(x, 2) for x in evolving_outlines_dhs],  # 2 decimals for height\n",
    "            'evolving_outlines_region_dh (m)': [round(x, 2) for x in evolving_region_dhs],  # 2 decimals for height\n",
    "            'evolving_outlines_dh_corr (m)': [round(x, 2) for x in evolving_outlines_dhs_corr],  # 2 decimals for height\n",
    "            'evolving_outlines_dV_corr (m^3)': [round(x, 0) for x in evolving_outlines_dVs_corr],  # Round to whole numbers\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(d)\n",
    "        df = df.fillna(0.0)\n",
    "        \n",
    "        # Convert area and volume columns to integer type since we're using whole numbers\n",
    "        df['evolving_outlines_area (m^2)'] = df['evolving_outlines_area (m^2)'].astype(int)\n",
    "        df['evolving_outlines_dV_corr (m^3)'] = df['evolving_outlines_dV_corr (m^3)'].astype(int)\n",
    "\n",
    "        # Define the appropriate output path based on forward_fill parameter\n",
    "        if forward_fill:\n",
    "            output_path = f'output/geometric_calcs/evolving_outlines_geom_calc/forward_fill'\n",
    "        else:\n",
    "            output_path = f'output/geometric_calcs/evolving_outlines_geom_calc/'\n",
    "\n",
    "        output_file = os.path.join(output_path, f'{lake_name}.csv')\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Results saved to: {output_file}\")\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(f\"No data processed for {lake_name}\")\n",
    "        return None\n",
    "\n",
    "def evolving_outlines_geom_calc_process_timestep(evolving_region_dh_slice,\n",
    "                     timestep_subset_evolving_outlines_gdf,\n",
    "                     ROI_dh_slice,\n",
    "                     timestep,\n",
    "                     midcyc_datetimes,\n",
    "                     evolving_outlines_areas,\n",
    "                     evolving_outlines_dhs,\n",
    "                     evolving_region_dhs,\n",
    "                     evolving_outlines_dhs_corr,\n",
    "                     evolving_outlines_dVs_corr):\n",
    "    \"\"\"\n",
    "    Process a single timestep of lake dh data and calculate various metrics.\n",
    "    dh measurements rounded to 2 decimal places (due to 9 cm precision)\n",
    "    Area measurements rounded to whole numbers (due to 1-km grid resolution)\n",
    "    dV measurements rounded to whole numbers (due to combined uncertainty)\n",
    "    \"\"\"    \n",
    "    # Initialize evolving outlines variables with default values of 0\n",
    "    evolving_outlines_area = 0\n",
    "    evolving_outlines_dh = 0.0\n",
    "    evolving_outlines_dh_corr = 0.0\n",
    "    evolving_outlines_dV_corr = 0\n",
    "\n",
    "    # Calculate metrics for evolving union region\n",
    "    evolving_region_dh = round(float(np.nanmean(evolving_region_dh_slice)), 2)  # 2 decimals for height\n",
    "\n",
    "    if not timestep_subset_evolving_outlines_gdf.empty:\n",
    "        # Calculate metrics for evolving outlines\n",
    "        evolving_outlines_area = round(float(timestep_subset_evolving_outlines_gdf['area (m^2)'].sum()), 0)\n",
    "        union_timestep_subset_evolving_outlines = timestep_subset_evolving_outlines_gdf['geometry'].union_all()\n",
    "        union_timestep_subset_evolving_outlines_mask = np.array([[union_timestep_subset_evolving_outlines.contains(Point(x, y)) \n",
    "                                                                for x in ROI_dh_slice['x'].values] \n",
    "                                                                for y in ROI_dh_slice['y'].values])\n",
    "        union_timestep_subset_evolving_outlines_masked = ROI_dh_slice.where(xr.DataArray(union_timestep_subset_evolving_outlines_mask, \n",
    "                                                                 coords=[ROI_dh_slice.y, ROI_dh_slice.x], \n",
    "                                                                 dims=[\"y\", \"x\"]))\n",
    "\n",
    "        evolving_outlines_dh = round(float(np.nanmean(union_timestep_subset_evolving_outlines_masked)), 2)  # 2 decimals\n",
    "        evolving_outlines_dh_corr = round(evolving_outlines_dh - evolving_region_dh, 2)  # 2 decimals\n",
    "        evolving_outlines_dV_corr = round(evolving_outlines_dh_corr * evolving_outlines_area, 0)  # Whole numbers\n",
    "    \n",
    "    # Append all metrics to lists\n",
    "    midcyc_datetimes.append(timestep)\n",
    "    evolving_outlines_areas.append(evolving_outlines_area)\n",
    "    evolving_outlines_dhs.append(evolving_outlines_dh)\n",
    "    evolving_region_dhs.append(evolving_region_dh)\n",
    "    evolving_outlines_dhs_corr.append(evolving_outlines_dh_corr)\n",
    "    evolving_outlines_dVs_corr.append(evolving_outlines_dV_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb0d7f3-3375-4f45-8349-2db26da2f4a6",
   "metadata": {},
   "source": [
    "## stationary_outline_geom_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "862cd63c-acff-4cf7-b1d6-91786a521ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationary_outline_geom_calc(stationary_outline_gdf, dataset1, dataset2, sub_dir): \n",
    "    '''\n",
    "    Create dataframe of active area, dh, dV calculations for evolving compared to stationary outlines\n",
    "    Area measurements rounded to nearest whole number due to 1-km grid resolution\n",
    "    Height measurements rounded to 2 decimal places to match 9cm measurement precision\n",
    "    Volume measurements rounded to whole numbers due to combined uncertainties\n",
    "    '''\n",
    "    # Define lake name and polygon and buffered polygon to use as off-lake region\n",
    "    lake_name = stationary_outline_gdf['name'].iloc[0]\n",
    "    print(f\"Processing lake: {lake_name}\")\n",
    "    stationary_outline = stationary_outline_gdf['geometry'].iloc[0]\n",
    "    stationary_region = area_multiple_buffer(stationary_outline, 2, exclude_inner=True)\n",
    "    \n",
    "    # Create empty lists to store metrics\n",
    "    midcyc_datetimes = []\n",
    "    stationary_outline_dhs = []\n",
    "    stationary_region_dhs = []\n",
    "    stationary_outline_dhs_corr = []\n",
    "    stationary_dVs_corr = []\n",
    "\n",
    "    # Get bounds for data slicing\n",
    "    x_min, y_min, x_max, y_max = stationary_region.bounds\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore', category=RuntimeWarning)\n",
    "        \n",
    "        # Process dataset1 if available\n",
    "        if dataset1 is not None:\n",
    "            # Prepare dataset1 masks and calculations\n",
    "            dataset1_ROI_subset = dataset1.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "            dataset1_ROI_da = dataset1_ROI_subset['delta_h']\n",
    "            \n",
    "            # Create masks\n",
    "            dataset1_stationary_mask = np.array([[stationary_outline.contains(Point(x, y)) \n",
    "                                                for x in dataset1_ROI_da.x.values] \n",
    "                                                for y in dataset1_ROI_da.y.values])\n",
    "            dataset1_stationary_masked = dataset1_ROI_da.where(xr.DataArray(dataset1_stationary_mask, \n",
    "                                                            coords=[dataset1_ROI_da.y, dataset1_ROI_da.x],\n",
    "                                                            dims=[\"y\", \"x\"]))\n",
    "            \n",
    "            dataset1_stationary_region_mask = np.array([[stationary_region.contains(Point(x, y)) \n",
    "                                                        for x in dataset1_ROI_da.x.values] \n",
    "                                                        for y in dataset1_ROI_da.y.values])\n",
    "            dataset1_stationary_region_masked = dataset1_ROI_da.where(xr.DataArray(dataset1_stationary_region_mask, \n",
    "                                                            coords=[dataset1_ROI_da.y, dataset1_ROI_da.x], \n",
    "                                                            dims=[\"y\", \"x\"]))\n",
    "\n",
    "            # Calculate dh differences\n",
    "            dataset1_ROI_dh = dataset1_ROI_da.diff('time')\n",
    "            dataset1_stationary_dh = dataset1_stationary_masked.diff('time')\n",
    "            dataset1_stationary_region_dh = dataset1_stationary_region_masked.diff('time')\n",
    "\n",
    "            # Calculate mid-cycle datetimes\n",
    "            dataset1_midcyc_datetimes = []\n",
    "            dataset1_datetimes = dataset1_ROI_da['time'].values\n",
    "            for i in range(1, len(dataset1_datetimes)):\n",
    "                midcyc_days = dataset1_datetimes[i] - dataset1_datetimes[i-1]\n",
    "                midcyc_datetime = dataset1_datetimes[i-1] + midcyc_days/2\n",
    "                dataset1_midcyc_datetimes.append(midcyc_datetime)\n",
    "            dataset1_midcyc_datetimes = np.array(dataset1_midcyc_datetimes)\n",
    "            \n",
    "            # Process timesteps for dataset1\n",
    "            for i, midcyc_datetime in enumerate(dataset1_midcyc_datetimes):\n",
    "                stationary_outline_geom_calc_process_timestep(\n",
    "                    ROI_dh_slice=dataset1_ROI_dh.isel(time=i),\n",
    "                    stationary_dh_slice=dataset1_stationary_dh.isel(time=i),\n",
    "                    stationary_region_dh_slice=dataset1_stationary_region_dh.isel(time=i),\n",
    "                    stationary_outline_gdf=stationary_outline_gdf,\n",
    "                    timestep=midcyc_datetime,\n",
    "                    midcyc_datetimes=midcyc_datetimes,\n",
    "                    stationary_outline_dhs=stationary_outline_dhs,\n",
    "                    stationary_region_dhs=stationary_region_dhs,\n",
    "                    stationary_outline_dhs_corr=stationary_outline_dhs_corr,\n",
    "                    stationary_dVs_corr=stationary_dVs_corr\n",
    "                )\n",
    "        \n",
    "        # Process dataset2\n",
    "        # Prepare dataset2 masks and calculations\n",
    "        dataset2_ROI_subset = dataset2.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "        dataset2_ROI_da = dataset2_ROI_subset['delta_h']\n",
    "        \n",
    "        # Create masks\n",
    "        dataset2_stationary_mask = np.array([[stationary_outline.contains(Point(x, y)) \n",
    "                                            for x in dataset2_ROI_da.x.values] \n",
    "                                            for y in dataset2_ROI_da.y.values])\n",
    "        dataset2_stationary_masked = dataset2_ROI_da.where(xr.DataArray(dataset2_stationary_mask,\n",
    "                                                        coords=[dataset2_ROI_da.y, dataset2_ROI_da.x],\n",
    "                                                        dims=[\"y\", \"x\"]))\n",
    "        \n",
    "        dataset2_stationary_region_mask = np.array([[stationary_region.contains(Point(x, y)) \n",
    "                                                    for x in dataset2_ROI_da.x.values] \n",
    "                                                    for y in dataset2_ROI_da.y.values])\n",
    "        dataset2_stationary_region_masked = dataset2_ROI_da.where(xr.DataArray(dataset2_stationary_region_mask, \n",
    "                                                        coords=[dataset2_ROI_da.y, dataset2_ROI_da.x], \n",
    "                                                        dims=[\"y\", \"x\"]))\n",
    "\n",
    "        # Calculate dh differences\n",
    "        dataset2_ROI_dh = dataset2_ROI_da.diff('time')\n",
    "        dataset2_stationary_dh = dataset2_stationary_masked.diff('time')\n",
    "        dataset2_stationary_region_dh = dataset2_stationary_region_masked.diff('time')\n",
    "\n",
    "        # Calculate mid-cycle datetimes\n",
    "        dataset2_midcyc_datetimes = []\n",
    "        dataset2_datetimes = dataset2_ROI_da['time'].values\n",
    "        for i in range(1, len(dataset2_datetimes)):\n",
    "            midcyc_days = dataset2_datetimes[i] - dataset2_datetimes[i-1]\n",
    "            midcyc_datetime = dataset2_datetimes[i-1] + midcyc_days/2\n",
    "            dataset2_midcyc_datetimes.append(midcyc_datetime)\n",
    "        dataset2_midcyc_datetimes = np.array(dataset2_midcyc_datetimes)\n",
    "\n",
    "        # Process timesteps for dataset2\n",
    "        for i, midcyc_datetime in enumerate(dataset2_midcyc_datetimes):\n",
    "            stationary_outline_geom_calc_process_timestep(\n",
    "                ROI_dh_slice=dataset2_ROI_dh.isel(time=i),\n",
    "                stationary_dh_slice=dataset2_stationary_dh.isel(time=i),\n",
    "                stationary_region_dh_slice=dataset2_stationary_region_dh.isel(time=i),\n",
    "                stationary_outline_gdf=stationary_outline_gdf,\n",
    "                timestep=midcyc_datetime,\n",
    "                midcyc_datetimes=midcyc_datetimes,\n",
    "                stationary_outline_dhs=stationary_outline_dhs,\n",
    "                stationary_region_dhs=stationary_region_dhs,\n",
    "                stationary_outline_dhs_corr=stationary_outline_dhs_corr,\n",
    "                stationary_dVs_corr=stationary_dVs_corr\n",
    "            )\n",
    "\n",
    "    # Create dataframe if we have data\n",
    "    if len(midcyc_datetimes) > 0:\n",
    "  \n",
    "        d = {\n",
    "            'midcyc_datetime': midcyc_datetimes,\n",
    "            'stationary_outline_area (m^2)': [stationary_outline_gdf['area (m^2)'].iloc[0]] * len(midcyc_datetimes),\n",
    "            'stationary_outline_dh (m)': [round(x, 2) for x in stationary_outline_dhs],\n",
    "            'stationary_outline_region_dh (m)': [round(x, 2) for x in stationary_region_dhs],\n",
    "            'stationary_outline_dh_corr (m)': [round(x, 2) for x in stationary_outline_dhs_corr],\n",
    "            'stationary_outline_dV_corr (m^3)': [round(x, 0) for x in stationary_dVs_corr]\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(d)\n",
    "        df = df.fillna(0.0)\n",
    "        \n",
    "        # Convert area and volume columns to integer type\n",
    "        df['stationary_outline_area (m^2)'] = df['stationary_outline_area (m^2)'].astype(int)\n",
    "        df['stationary_outline_dV_corr (m^3)'] = df['stationary_outline_dV_corr (m^3)'].astype(int)\n",
    "        \n",
    "        output_path = f'output/geometric_calcs/stationary_outline_geom_calc/'\n",
    "        output_file = os.path.join(output_path, sub_dir, f'{lake_name}.csv')\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Results saved to: {output_file}\")\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(f\"No data processed for {lake_name}\")\n",
    "        return None\n",
    "\n",
    "def stationary_outline_geom_calc_process_timestep(*,  # Force kwarg use\n",
    "                    ROI_dh_slice,\n",
    "                    stationary_dh_slice, \n",
    "                    stationary_region_dh_slice, \n",
    "                    stationary_outline_gdf,\n",
    "                    timestep,\n",
    "                    midcyc_datetimes,\n",
    "                    stationary_outline_dhs,\n",
    "                    stationary_region_dhs,\n",
    "                    stationary_outline_dhs_corr,\n",
    "                    stationary_dVs_corr):\n",
    "    \"\"\"\n",
    "    Process a single timestep of lake height data and calculate various metrics.\n",
    "    Height measurements rounded to 2 decimal places (9cm precision)\n",
    "    Area measurements rounded to nearest 1,000,000 m² (1 km grid resolution)\n",
    "    Volume measurements rounded to whole numbers (combined uncertainty)\n",
    "    \"\"\"    \n",
    "    # Calculate metrics for stationary outline\n",
    "    stationary_outline_dh = round(float(np.nanmean(stationary_dh_slice)), 2)  # 2 decimals for height\n",
    "    stationary_region_dh = round(float(np.nanmean(stationary_region_dh_slice)), 2)  # 2 decimals for height\n",
    "    stationary_outline_dh_corr = round(stationary_outline_dh - stationary_region_dh, 2)  # 2 decimals for height\n",
    "    \n",
    "    # Round area to nearest 1,000,000 m² before volume calculation\n",
    "    stationary_dV_corr = round(stationary_outline_dh_corr * stationary_outline_gdf['area (m^2)'].iloc[0], 0)  # Whole numbers for volume\n",
    "    \n",
    "    # Append all metrics to lists\n",
    "    midcyc_datetimes.append(timestep)\n",
    "    stationary_outline_dhs.append(stationary_outline_dh)\n",
    "    stationary_region_dhs.append(stationary_region_dh)\n",
    "    stationary_outline_dhs_corr.append(stationary_outline_dh_corr)\n",
    "    stationary_dVs_corr.append(stationary_dV_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e184bf11-21e2-4afe-9a6b-1a5db1df1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_geodesic_area(poly):\n",
    "    '''\n",
    "    Calculate geodesic area of polygon or multipolygon. Polygon or multipolygon must extracted from geodataframe\n",
    "    that has CRS EPSG:4326.\n",
    "    '''\n",
    "    # Ensure geom exists and geom is valid\n",
    "    if poly is None or not poly.is_valid:\n",
    "        return None\n",
    "\n",
    "    # Calculate geodesic area and return it\n",
    "    if isinstance(poly, Polygon):\n",
    "        return abs(geod.polygon_area_perimeter(poly.exterior.coords.xy[0], poly.exterior.coords.xy[1])[0])\n",
    "    elif isinstance(poly, MultiPolygon):\n",
    "        total_area = 0\n",
    "        for part in poly.geoms:\n",
    "            total_area += abs(geod.polygon_area_perimeter(part.exterior.coords.xy[0], part.exterior.coords.xy[1])[0])\n",
    "        return total_area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b4427d-c7a1-4d32-8d9d-c6217114d617",
   "metadata": {},
   "source": [
    "## plot_evolving_and_stationary_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd060e2f-4b17-41d3-be9c-92a1a8e106ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_evolving_and_stationary_comparison(lake_gdf):\n",
    "#     '''\n",
    "#     Plot and compare the evolving outlines and stationary outline of a lake along with active area, dh, and dV.\n",
    "\n",
    "#     This function generates plot for a given lake, showing the differences between the evolving \n",
    "#     and stationary outlines over time. It includes visualizations of the outlines on a map, as well as plots for \n",
    "#     active area, cumulative height change, and cumulative volume displacement. The results are saved as a PNG file.\n",
    "\n",
    "#     Parameters:\n",
    "#     lake_gdf (GeoDataFrame): A GeoDataFrame containing a single lake's data with attributes such as 'name' and 'geometry'.\n",
    "#                              The GeoDataFrame should have a single row corresponding to the lake.\n",
    "\n",
    "#     Returns:\n",
    "#     None: The results are saved as PNG files in the OUTPUT_DIR + '/plot_evolving_and_stationary_comparison/' directory \n",
    "#     with filenames corresponding to the lake names.\n",
    "\n",
    "#     Example:\n",
    "#     >>> lake_gdf = gpd.read_file('path_to_lake.geojson')\n",
    "#     >>> plot_evolving_and_stationary_comparison(lake_gdf)\n",
    "#     '''\n",
    "#     # First check if lake_gdf is valid and has data\n",
    "#     if lake_gdf is None or lake_gdf.empty:\n",
    "#         print(\"Empty lake_gdf provided. Skipping...\")\n",
    "#         return\n",
    "    \n",
    "#     # Define lake name and polygon and buffered polygon to use as off-lake region\n",
    "#     lake_name = lake_gdf['name'].values[0]\n",
    "#     lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "#     print(f'Processing lake: {lake_name}')\n",
    "    \n",
    "#     # Open evolving outline(s) and evolving outlines and geometric calculation comparison geodataframes for plotting\n",
    "#     # Attempt to open the evolving outlines GeoJSON file\n",
    "#     try:\n",
    "#         evolving_outlines_gdf = gpd.read_file(os.path.join(\n",
    "#             'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)))\n",
    "#         offlake_outlines_gdf = gpd.read_file(os.path.join(\n",
    "#             OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines/{}.geojson'.format(lake_name)))\n",
    "#         evolving_geom_calcs_df = pd.read_csv(f\"output/geometric_calcs/evolving_outlines_geom_calc/{lake_name}.csv\")\n",
    "#         evolving_union_geom_calcs_df = pd.read_csv(f\"output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/{lake_name}.csv\")\n",
    "#         stationary_geom_calcs_df = pd.read_csv(f\"output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes/{lake_name}.csv\")\n",
    "\n",
    "#     except (fiona.errors.DriverError, pyogrio.errors.DataSourceError, FileNotFoundError) as e:\n",
    "#         print(f\"Error loading files for {lake_name}: {str(e)}. Skipping...\")\n",
    "#         return\n",
    "    \n",
    "#     # Convert of strings to datetime\n",
    "#     evolving_geom_calcs_df['midcyc_datetime'] = pd.to_datetime(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "\n",
    "#     # Fig setup\n",
    "#     nows, ncols = 1, 4\n",
    "#     fig = plt.figure(figsize=(16, 5))\n",
    "    \n",
    "#     # Create GridSpec to control subplot sizes\n",
    "#     gs = fig.add_gridspec(nows, ncols, width_ratios=[1, 1, 1, 1])\n",
    "#     ax = [fig.add_subplot(gs[0, i]) for i in range(ncols)]\n",
    "    \n",
    "#     # Define colors and linestyles that will be reused and create lines for legend\n",
    "#     stationary_color  = 'darkturquoise'\n",
    "#     stationary_line = plt.Line2D([], [], color=stationary_color, linestyle='solid', linewidth=1)\n",
    "\n",
    "#     # Set up colormap\n",
    "#     cmap = plt.get_cmap('plasma', len(mid_cyc_dates)-1)\n",
    "    \n",
    "#     # Norm to time variable\n",
    "#     norm = plt.Normalize(mdates.date2num(cyc_dates['mid_cyc_dates'].iloc[0]), \n",
    "#                          mdates.date2num(cyc_dates['mid_cyc_dates'].iloc[-1]))\n",
    "\n",
    "    \n",
    "#     # Panel - evolving outlines ------------------------------------------------------\n",
    "\n",
    "#     # Create buffered polygon for the area multiple within evaluation boundary\n",
    "#     within_eval_poly = area_multiple_buffer(lake_gdf['geometry'].iloc[0], evolving_outlines_gdf['within_area_multiple'][0])    \n",
    "\n",
    "#     # Combine stationary outline(s) with evolving outlines in unary union to plot all within bounds of plot\n",
    "#     evolving_stationary_outlines_union = evolving_stationary_outlines_union_evolving_lakes_gdf[evolving_stationary_outlines_union_evolving_lakes_gdf['name'] == lake_name]['geometry'].iloc[0]\n",
    "#     all_poly_union = unary_union([within_eval_poly, evolving_stationary_outlines_union])\n",
    "#     x_min, y_min, x_max, y_max = all_poly_union.bounds\n",
    "#     del all_poly_union\n",
    "\n",
    "#     # # Make plots a uniform size\n",
    "#     # # Make x_min, y_min, x_max, and y_max define a square area centered at the original midpoints\n",
    "#     # # Calculate the midpoints of the current bounds\n",
    "#     # x_mid = (x_min + x_max) / 2\n",
    "#     # y_mid = (y_min + y_max) / 2\n",
    "    \n",
    "#     # # Calculate the current spans of the x and y dimensions\n",
    "#     # x_span = x_max - x_min\n",
    "#     # y_span = y_max - y_min\n",
    "    \n",
    "#     # # Determine the maximum span to ensure square dimensions\n",
    "#     # max_span = max(x_span, y_span)\n",
    "    \n",
    "#     # # Update the min and max values to match the new span, keeping the midpoint the same\n",
    "#     # x_min = x_mid - max_span / 2\n",
    "#     # x_max = x_mid + max_span / 2\n",
    "#     # y_min = y_mid - max_span / 2\n",
    "#     # y_max = y_mid + max_span / 2\n",
    "\n",
    "#     buffer_frac = 0.05\n",
    "#     x_buffer = abs(x_max-x_min)*buffer_frac\n",
    "#     y_buffer = abs(y_max-y_min)*buffer_frac\n",
    "#     mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "#     mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "    \n",
    "#     # Plot stationary and evolving outlines onto MOA surface imagery\n",
    "#     moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "#     ax[0].imshow(moa_highres_da_subset[0,:,:], cmap=\"gray\", clim=[14000, 17000], extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "    \n",
    "#     # Plot evolving outlines\n",
    "#     onlake_lines, offlake_lines = [], []\n",
    "#     for idx, dt in enumerate(mid_cyc_dates):\n",
    "#         x, y = 1, 1\n",
    "#         onlake_line, = ax[0].plot(x, y, color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))), linewidth=2)\n",
    "#         onlake_lines.append(onlake_line)\n",
    "#         offlake_line, = ax[0].plot(x, y, color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))), linewidth=2, alpha=0.2)\n",
    "#         offlake_lines.append(offlake_line)\n",
    "        \n",
    "#         evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "#         offlake_outlines_dt = offlake_outlines_gdf[offlake_outlines_gdf['midcyc_datetime'] == dt]\n",
    "#         if not evolving_outlines_dt.empty:\n",
    "#             evolving_outlines_dt.boundary.plot(ax=ax[0], \n",
    "#                 # color=cmap(norm(date_to_quarter_year(mid_cyc_dates[idx]))), \n",
    "#                 color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))),\n",
    "#                 linewidth=1)\n",
    "#         if not offlake_outlines_dt.empty:\n",
    "#             offlake_outlines_dt.boundary.plot(ax=ax[0], \n",
    "#                 # color=cmap(norm(date_to_quarter_year(mid_cyc_dates[idx]))), \n",
    "#                 color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))),\n",
    "#                 linewidth=1, alpha=0.25)\n",
    "\n",
    "#     # Plot within evaluation polygon\n",
    "#     gpd.GeoDataFrame(geometry=[within_eval_poly]).boundary.plot(ax=ax[0], edgecolor='darkgray', facecolor='none', linewidth=1)\n",
    "\n",
    "#     # Create evolving outlines unary union and plot\n",
    "#     evolving_union_gdf = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]\n",
    "#     evolving_union_gdf.boundary.plot(ax=ax[0], color='k', linestyle='dotted', linewidth=1)\n",
    "\n",
    "#     # Plot stationary outline\n",
    "#     stationary_outlines_gdf.boundary.plot(ax=ax[0], color=stationary_color, linewidth=1, zorder=0)\n",
    "        \n",
    "#     # Plot inset map\n",
    "#     axIns = ax[0].inset_axes([0.05, 0, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "#     axIns.set_aspect('equal')\n",
    "#     moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "#     moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "#     axIns.axis('off')\n",
    "#     # Plot star to indicate location\n",
    "#     axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "#         linewidth=0.1, color='k', s=30, zorder=3)\n",
    "\n",
    "#     # Create stationary region and evolving outlines region and plot\n",
    "#     stationary_region = area_multiple_buffer(lake_poly, 2)\n",
    "#     stationary_region = stationary_region.difference(lake_poly)\n",
    "#     evolving_union_region = area_multiple_buffer(evolving_union_gdf['geometry'], 2)\n",
    "#     evolving_union_region = evolving_union_region.difference(evolving_union_gdf['geometry'].iloc[0])\n",
    "#     gpd.GeoDataFrame(geometry=[stationary_region], crs='3031').plot(ax=ax[0], color=stationary_color, linewidth=2, alpha=0.2)\n",
    "#     gpd.GeoDataFrame(geometry=[evolving_union_region], crs='3031').plot(ax=ax[0], color='k', linewidth=2, alpha=0.2)\n",
    "\n",
    "#     # Set up colormap\n",
    "#     min_date = pd.to_datetime(cyc_start_dates[1])\n",
    "#     max_date = pd.to_datetime(cyc_end_dates[-1])\n",
    "#     date_range = pd.date_range(min_date, max_date, periods=len(mid_cyc_dates[1:]))\n",
    "#     years = date_range.year.unique()\n",
    "#     years = pd.to_datetime(years, format='%Y')\n",
    "#     cmap = plt.get_cmap('plasma', len(mid_cyc_dates[1:]))\n",
    "#     norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "#     m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "#     m.set_array(np.array([mdates.date2num(date) for date in mid_cyc_dates[1:]]))\n",
    "\n",
    "#     # Add colorbar\n",
    "#     divider = make_axes_locatable(ax[0])\n",
    "#     cax = divider.append_axes('bottom', size='2.5%', pad=0.5)\n",
    "#     cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "\n",
    "#     # Set ticks for all years but labels only for odd years\n",
    "#     tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "#     tick_labels = [date.strftime('%Y') if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "#     cbar.set_ticks(tick_locations)\n",
    "#     cbar.set_ticklabels(tick_labels)\n",
    "    \n",
    "#     # Add minor ticks for quarters\n",
    "#     cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Quarter intervals only\n",
    "#     cbar.set_label('year', size=10, labelpad=10)\n",
    "\n",
    "#     # Change polar stereographic m to km\n",
    "#     km_scale = 1e3\n",
    "#     ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "#     ax[0].xaxis.set_major_formatter(ticks_x)\n",
    "#     ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "#     ax[0].yaxis.set_major_formatter(ticks_y)\n",
    "    \n",
    "#     # Set axes limit, title, and axis label\n",
    "#     ax[0].set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "#     ax[0].set_xlabel('x [km]')\n",
    "#     ax[0].set_ylabel('y [km]')\n",
    "\n",
    "    \n",
    "#     # Panel - Active area ---------------------------------------------\n",
    "\n",
    "#     # Plot horizontal zero line for reference\n",
    "#     ax[1].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "    \n",
    "#     # Plot stationary outline and evolving outlines unary union areas\n",
    "#     ax[1].axhline(np.divide(lake_gdf['area (m^2)'], 1e6).values, \n",
    "#         color=stationary_color, linestyle='solid', linewidth=2)\n",
    "#     ax[1].axhline(np.divide(evolving_union_gdf['area (m^2)'], 1e6).values, \n",
    "#         color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "#     # Plot multi-colored line and scatter for data points\n",
    "#     x = mdates.date2num(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "#     y = np.divide(evolving_geom_calcs_df['evolving_outlines_area (m^2)'], 1e6)\n",
    "    \n",
    "#     # # Plot evolving outlines time series using LineCollection from points/segments to plot multi-colored line\n",
    "#     # points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "#     # segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "#     # lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "#     # lc.set_array(x)  # Set the values used for colormapping, using matplotlib dates for colors\n",
    "#     # lc.set_linewidth(2)\n",
    "#     # line = ax[1].add_collection(lc)\n",
    "#     # scatter = ax[1].scatter(x, y, c=x, cmap=cmap, s=9, norm=norm, zorder=2)\n",
    "\n",
    "#     # Filter out points where y is zero\n",
    "#     non_zero_mask = y != 0\n",
    "#     x_filtered = x[non_zero_mask]\n",
    "#     y_filtered = y[non_zero_mask]\n",
    "    \n",
    "#     # Create points and segments for LineCollection (only using non-zero points)\n",
    "#     if len(x_filtered) > 1:  # Need at least 2 points to create segments\n",
    "#         points = np.array([x_filtered, y_filtered]).T.reshape(-1, 1, 2)\n",
    "#         segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "        \n",
    "#         # Create a LineCollection, using the discrete colormap and norm\n",
    "#         lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "#         lc.set_array(x_filtered)\n",
    "#         lc.set_linewidth(2)\n",
    "#         # Plot multi-colored line and scatter for data points\n",
    "#         line = ax[1].add_collection(lc)\n",
    "    \n",
    "#     # Plot scatter points (only non-zero values)\n",
    "#     scatter = ax[1].scatter(x_filtered, y_filtered, c=x_filtered, cmap=cmap, s=9, norm=norm, zorder=2)\n",
    "\n",
    "#     # Panel C - Cumulative dh/dt -------------------------------------------------------\n",
    "#     # Plot horizontal zero line for reference\n",
    "#     ax[2].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "\n",
    "#     # Plot stationary outlines off-lake region dh\n",
    "#     ax[2].plot(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "#         np.cumsum(stationary_geom_calcs_df['stationary_outline_region_dh (m)']).astype(float), \n",
    "#         color='lightgray', linestyle='solid', linewidth=1)\n",
    "\n",
    "#     # Plot evolving outlines off-lake region dh\n",
    "#     ax[2].plot(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']), \n",
    "#         np.cumsum(evolving_geom_calcs_df['evolving_outlines_region_dh (m)']), color='dimgray', linestyle='solid', linewidth=1)\n",
    "\n",
    "#     # Plot stationary outline time series\n",
    "#     ax[2].plot(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "#         np.cumsum(stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), color=stationary_color, linestyle='solid', linewidth=1)\n",
    "\n",
    "#     # Plot multi-colored line and scatter for data points\n",
    "#     y = np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'])\n",
    "    \n",
    "#     # Plot evolving outlines time series using LineCollection from points/segments to plot multi-colored line\n",
    "#     points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "#     segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "#     lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "#     lc.set_array(x)\n",
    "#     lc.set_linewidth(2)\n",
    "#     line = ax[2].add_collection(lc)\n",
    "#     scatter = ax[2].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9, zorder=2)\n",
    "\n",
    "#     # Plot evolving outlines union outline time series\n",
    "#     ax[2].plot(mdates.date2num(evolving_union_geom_calcs_df['midcyc_datetime']), \n",
    "#         np.cumsum(evolving_union_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "#         color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "#     # Plot bias\n",
    "#     ax[2].plot(x, np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)']-\n",
    "#                             stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "#                             color='red', linestyle='solid', linewidth=1)\n",
    "    \n",
    "    \n",
    "#     # Panel D - Cumulative dV/dt --------------------------------------------------\n",
    "#     # Plot horizontal line at zero for reference\n",
    "#     ax[3].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "    \n",
    "#     # Plot stationary outline time series\n",
    "#     ax[3].plot(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "#         np.divide(np.cumsum(stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "#         color=stationary_color, linestyle='solid', linewidth=1)\n",
    "    \n",
    "#     # Plot multi-colored line and scatter for data points\n",
    "#     y = np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "    \n",
    "#     # Plot evolving outlines time series using LineCollection from points/segments to plot multi-colored line\n",
    "#     points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "#     segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "#     lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "#     lc.set_array(x)\n",
    "#     lc.set_linewidth(2)\n",
    "#     line = ax[3].add_collection(lc)\n",
    "#     scatter = ax[3].scatter(x, y, c=x, cmap=cmap, norm=norm, s=10, zorder=2)\n",
    "\n",
    "#     # Plot evolving outlines union outline time series\n",
    "#     ax[3].plot(mdates.date2num(evolving_union_geom_calcs_df['midcyc_datetime']), \n",
    "#         np.divide(np.cumsum(evolving_union_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "#         color='k', linestyle='dashed', linewidth=1)\n",
    "    \n",
    "#     # Plot bias\n",
    "#     ax[3].plot(x, np.divide(np.cumsum(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)']-\n",
    "#                             stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "#                             color='red', linestyle='solid', linewidth=1)\n",
    "\n",
    "#     for i in range(1, ncols):\n",
    "#         # Set x-axis limits\n",
    "#         ax[i].set_xlim(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "        \n",
    "#         # Set ticks for all years but labels only for even years (to match colorbar)\n",
    "#         tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "#         tick_labels = [date.strftime('%Y') if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "#         ax[i].set_xticks(tick_locations)\n",
    "#         ax[i].set_xticklabels(tick_labels)\n",
    "        \n",
    "#         # Add minor ticks for quarters\n",
    "#         ax[i].xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))\n",
    "        \n",
    "#         ax[i].set_xlabel('year')\n",
    "    \n",
    "#     # Add legends\n",
    "#     evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=1)\n",
    "#     within_eval_line = plt.Line2D([], [], color='darkgray', linestyle='solid', linewidth=1)\n",
    "#     stationary_region_patch = mpatches.Patch(color=stationary_color, alpha=0.2)\n",
    "#     evolving_union_region_patch = mpatches.Patch(color='k', alpha=0.2)\n",
    "    \n",
    "#     legend = ax[0].legend([tuple(onlake_lines),\n",
    "#                            tuple(offlake_lines),\n",
    "#                            evolving_union_line, \n",
    "#                            stationary_line, \n",
    "#                            within_eval_line,\n",
    "#                            stationary_region_patch,\n",
    "#                            evolving_union_region_patch], \n",
    "#         [f'evolving outlines ({evolving_outlines_gdf.level[0]} m)',\n",
    "#          'off-lake evolving outlines', \n",
    "#          'evolving outlines union',\n",
    "#          'stationary outline',\n",
    "#          f'within evaluation line ({int(evolving_outlines_gdf.within_area_multiple[0])}x)',\n",
    "#          'stationary region',\n",
    "#          'evolving union region'],\n",
    "#         handlelength=3,\n",
    "#         handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#         loc='upper left',\n",
    "#         bbox_to_anchor=(0, 1.47))\n",
    "\n",
    "#     legend = ax[1].legend([tuple(onlake_lines), \n",
    "#                            evolving_union_line, \n",
    "#                            stationary_line],\n",
    "#         ['evolving outlines', \n",
    "#          'evolving outlines union', \n",
    "#          'stationary outline'], \n",
    "#         handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#         fontsize='small', loc='upper left',\n",
    "#         bbox_to_anchor=(0, 1.15))\n",
    "\n",
    "#     evolving_region = plt.Line2D([], [], color='dimgray', linestyle='solid', linewidth=1)\n",
    "#     stationary_region = plt.Line2D([], [], color='lightgray', linestyle='solid', linewidth=1)\n",
    "#     bias = plt.Line2D([], [], color='red', linestyle='solid', linewidth=1)\n",
    "\n",
    "#     legend = ax[2].legend(\n",
    "#         [evolving_region,\n",
    "#          stationary_region,\n",
    "#          tuple(onlake_lines),\n",
    "#          evolving_union_line,\n",
    "#          stationary_line,  \n",
    "#          bias],\n",
    "#         ['evolving outlines region',\n",
    "#          'stationary outline region',\n",
    "#          'evolving outlines',\n",
    "#          'evolving outlines union',\n",
    "#          'stationary outline', \n",
    "#          'bias (evolving - stationary)'],\n",
    "#          handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#          fontsize='small', loc='upper left',\n",
    "#          bbox_to_anchor=(0, 1.25))\n",
    "\n",
    "#     legend = ax[3].legend([tuple(onlake_lines), \n",
    "#                            stationary_line,\n",
    "#                            evolving_union_line,\n",
    "#                            bias],\n",
    "#         ['evolving outlines', \n",
    "#          'stationary outline',\n",
    "#          'evolving outlines union',\n",
    "#          'bias (evolving - stationary)'], \n",
    "#         handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#         fontsize='small', loc='upper left',\n",
    "#         bbox_to_anchor=(0, 1.15))\n",
    "\n",
    "#     # Set titles\n",
    "#     ax[0].set_title(f'{lake_name}', size=12, y=1.47)\n",
    "#     ax[1].set_title('active area [km$^2$]', size=12, y=1.15)\n",
    "#     ax[2].set_title('cumulative dh [m]', size=12, y=1.25)\n",
    "#     ax[3].set_title('cumulative dV [km$^3$]', size=12, y=1.15)\n",
    "\n",
    "#     # Save and close plot\n",
    "#     plt.savefig(OUTPUT_DIR + '/plot_evolving_and_stationary_comparison/{}.png'.format(lake_name), dpi=300, bbox_inches='tight')\n",
    "    \n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59521698-9555-488b-9420-18a8816719e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evolving_and_stationary_comparison(lake_gdf, forward_fill=False):\n",
    "    '''\n",
    "    Plot and compare the evolving outlines and stationary outline of a lake along with active area, dh, and dV.\n",
    "\n",
    "    This function generates plot for a given lake, showing the differences between the evolving \n",
    "    and stationary outlines over time. It includes visualizations of the outlines on a map, as well as plots for \n",
    "    active area, cumulative height change, and cumulative volume displacement. The results are saved as a PNG file.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    lake_gdf : GeoDataFrame\n",
    "        A GeoDataFrame containing a single lake's data with attributes such as 'name' and 'geometry'.\n",
    "        The GeoDataFrame should have a single row corresponding to the lake.\n",
    "    forward_fill : bool, default=False\n",
    "        If True, use Last Observation Carried Forward (LOCF) methodology by reading \n",
    "        from forward-filled geojson files\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None: The results are saved as PNG files in the OUTPUT_DIR + '/plot_evolving_and_stationary_comparison/' directory \n",
    "    with filenames corresponding to the lake names.\n",
    "\n",
    "    Example:\n",
    "    >>> lake_gdf = gpd.read_file('path_to_lake.geojson')\n",
    "    >>> plot_evolving_and_stationary_comparison(lake_gdf, forward_fill=False)\n",
    "    '''\n",
    "    # First check if lake_gdf is valid and has data\n",
    "    if lake_gdf is None or lake_gdf.empty:\n",
    "        print(\"Empty lake_gdf provided. Skipping...\")\n",
    "        return\n",
    "    \n",
    "    # Define lake name and polygon and buffered polygon to use as off-lake region\n",
    "    lake_name = lake_gdf['name'].values[0]\n",
    "    lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "    print(f'Processing lake: {lake_name}')\n",
    "    \n",
    "    # Open evolving outline(s) and evolving outlines and geometric calculation comparison geodataframes for plotting\n",
    "    # Attempt to open the evolving outlines GeoJSON file\n",
    "    try:\n",
    "        # Modify file paths based on forward_fill parameter\n",
    "        if forward_fill:\n",
    "            evolving_outlines_path = os.path.join(\n",
    "                'output/lake_outlines/evolving_outlines/forward_fill/{}.geojson'.format(lake_name))\n",
    "            offlake_outlines_path = os.path.join(\n",
    "                OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines/{}.geojson'.format(lake_name))\n",
    "            evolving_geom_calcs_path = f\"output/geometric_calcs/evolving_outlines_geom_calc/forward_fill/{lake_name}.csv\"\n",
    "            evolving_union_geom_calcs_path = f\"output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/{lake_name}.csv\"\n",
    "            stationary_geom_calcs_path = f\"output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes/{lake_name}.csv\"\n",
    "        else:\n",
    "            evolving_outlines_path = os.path.join(\n",
    "                'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name))\n",
    "            offlake_outlines_path = os.path.join(\n",
    "                OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines/{}.geojson'.format(lake_name))\n",
    "            evolving_geom_calcs_path = f\"output/geometric_calcs/evolving_outlines_geom_calc/{lake_name}.csv\"\n",
    "            evolving_union_geom_calcs_path = f\"output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/{lake_name}.csv\"\n",
    "            stationary_geom_calcs_path = f\"output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes/{lake_name}.csv\"\n",
    "\n",
    "        # Read files using the modified paths\n",
    "        evolving_outlines_gdf = gpd.read_file(evolving_outlines_path)\n",
    "        offlake_outlines_gdf = gpd.read_file(offlake_outlines_path)\n",
    "        evolving_geom_calcs_df = pd.read_csv(evolving_geom_calcs_path)\n",
    "        evolving_union_geom_calcs_df = pd.read_csv(evolving_union_geom_calcs_path)\n",
    "        stationary_geom_calcs_df = pd.read_csv(stationary_geom_calcs_path)\n",
    "\n",
    "    except (fiona.errors.DriverError, pyogrio.errors.DataSourceError, FileNotFoundError) as e:\n",
    "        print(f\"Error loading files for {lake_name}: {str(e)}. Skipping...\")\n",
    "        return\n",
    "    \n",
    "    # Convert of strings to datetime\n",
    "    evolving_geom_calcs_df['midcyc_datetime'] = pd.to_datetime(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "\n",
    "    # Fig setup\n",
    "    nows, ncols = 1, 4\n",
    "    fig = plt.figure(figsize=(16, 5))\n",
    "    \n",
    "    # Create GridSpec to control subplot sizes\n",
    "    gs = fig.add_gridspec(nows, ncols, width_ratios=[1, 1, 1, 1])\n",
    "    ax = [fig.add_subplot(gs[0, i]) for i in range(ncols)]\n",
    "    \n",
    "    # Define colors and linestyles that will be reused and create lines for legend\n",
    "    stationary_color  = 'darkturquoise'\n",
    "    stationary_line = plt.Line2D([], [], color=stationary_color, linestyle='solid', linewidth=1)\n",
    "\n",
    "    # Set up colormap\n",
    "    cmap = plt.get_cmap('plasma', len(mid_cyc_dates)-1)\n",
    "    \n",
    "    # Norm to time variable\n",
    "    norm = plt.Normalize(mdates.date2num(cyc_dates['mid_cyc_dates'].iloc[0]), \n",
    "                         mdates.date2num(cyc_dates['mid_cyc_dates'].iloc[-1]))\n",
    "\n",
    "    \n",
    "    # Panel - evolving outlines ------------------------------------------------------\n",
    "\n",
    "    # Create buffered polygon for the area multiple within evaluation boundary\n",
    "    within_eval_poly = area_multiple_buffer(lake_gdf['geometry'].iloc[0], evolving_outlines_gdf['within_area_multiple'][0])    \n",
    "\n",
    "    # Combine stationary outline(s) with evolving outlines in unary union to plot all within bounds of plot\n",
    "    evolving_stationary_outlines_union = evolving_stationary_outlines_union_evolving_lakes_gdf[evolving_stationary_outlines_union_evolving_lakes_gdf['name'] == lake_name]['geometry'].iloc[0]\n",
    "    all_poly_union = unary_union([within_eval_poly, evolving_stationary_outlines_union])\n",
    "    x_min, y_min, x_max, y_max = all_poly_union.bounds\n",
    "    del all_poly_union\n",
    "\n",
    "    # # Make plots a uniform size\n",
    "    # # Make x_min, y_min, x_max, and y_max define a square area centered at the original midpoints\n",
    "    # # Calculate the midpoints of the current bounds\n",
    "    # x_mid = (x_min + x_max) / 2\n",
    "    # y_mid = (y_min + y_max) / 2\n",
    "    \n",
    "    # # Calculate the current spans of the x and y dimensions\n",
    "    # x_span = x_max - x_min\n",
    "    # y_span = y_max - y_min\n",
    "    \n",
    "    # # Determine the maximum span to ensure square dimensions\n",
    "    # max_span = max(x_span, y_span)\n",
    "    \n",
    "    # # Update the min and max values to match the new span, keeping the midpoint the same\n",
    "    # x_min = x_mid - max_span / 2\n",
    "    # x_max = x_mid + max_span / 2\n",
    "    # y_min = y_mid - max_span / 2\n",
    "    # y_max = y_mid + max_span / 2\n",
    "\n",
    "    buffer_frac = 0.05\n",
    "    x_buffer = abs(x_max-x_min)*buffer_frac\n",
    "    y_buffer = abs(y_max-y_min)*buffer_frac\n",
    "    mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "    mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "    \n",
    "    # Plot stationary and evolving outlines onto MOA surface imagery\n",
    "    moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "    ax[0].imshow(moa_highres_da_subset[0,:,:], cmap=\"gray\", clim=[14000, 17000], extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "    \n",
    "    # Plot evolving outlines\n",
    "    onlake_lines, offlake_lines = [], []\n",
    "    for idx, dt in enumerate(mid_cyc_dates):\n",
    "        x, y = 1, 1\n",
    "        onlake_line, = ax[0].plot(x, y, color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))), linewidth=2)\n",
    "        onlake_lines.append(onlake_line)\n",
    "        offlake_line, = ax[0].plot(x, y, color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))), linewidth=2, alpha=0.2)\n",
    "        offlake_lines.append(offlake_line)\n",
    "        \n",
    "        evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "        offlake_outlines_dt = offlake_outlines_gdf[offlake_outlines_gdf['midcyc_datetime'] == dt]\n",
    "        if not evolving_outlines_dt.empty:\n",
    "            evolving_outlines_dt.boundary.plot(ax=ax[0], \n",
    "                color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))),\n",
    "                linewidth=1)\n",
    "        if not offlake_outlines_dt.empty:\n",
    "            offlake_outlines_dt.boundary.plot(ax=ax[0], \n",
    "                color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))),\n",
    "                linewidth=1, alpha=0.25)\n",
    "\n",
    "    # Plot within evaluation polygon\n",
    "    gpd.GeoDataFrame(geometry=[within_eval_poly]).boundary.plot(ax=ax[0], edgecolor='darkgray', facecolor='none', linewidth=1)\n",
    "\n",
    "    # Create evolving outlines unary union and plot\n",
    "    evolving_union_gdf = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]\n",
    "    evolving_union_gdf.boundary.plot(ax=ax[0], color='k', linestyle='dotted', linewidth=1)\n",
    "\n",
    "    # Plot stationary outline\n",
    "    stationary_outlines_gdf.boundary.plot(ax=ax[0], color=stationary_color, linewidth=1, zorder=0)\n",
    "        \n",
    "    # Plot inset map\n",
    "    axIns = ax[0].inset_axes([0.05, 0, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "    axIns.set_aspect('equal')\n",
    "    moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    axIns.axis('off')\n",
    "    # Plot star to indicate location\n",
    "    axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "        linewidth=0.1, color='k', s=30, zorder=3)\n",
    "\n",
    "    # Create stationary region and evolving outlines region and plot\n",
    "    stationary_region = area_multiple_buffer(lake_poly, 2)\n",
    "    stationary_region = stationary_region.difference(lake_poly)\n",
    "    evolving_union_region = area_multiple_buffer(evolving_union_gdf['geometry'], 2)\n",
    "    evolving_union_region = evolving_union_region.difference(evolving_union_gdf['geometry'].iloc[0])\n",
    "    gpd.GeoDataFrame(geometry=[stationary_region], crs='3031').plot(ax=ax[0], color=stationary_color, linewidth=2, alpha=0.2)\n",
    "    gpd.GeoDataFrame(geometry=[evolving_union_region], crs='3031').plot(ax=ax[0], color='k', linewidth=2, alpha=0.2)\n",
    "\n",
    "    # Set up colormap\n",
    "    min_date = pd.to_datetime(cyc_start_dates[1])\n",
    "    max_date = pd.to_datetime(cyc_end_dates[-1])\n",
    "    date_range = pd.date_range(min_date, max_date, periods=len(mid_cyc_dates[1:]))\n",
    "    years = date_range.year.unique()\n",
    "    years = pd.to_datetime(years, format='%Y')\n",
    "    cmap = plt.get_cmap('plasma', len(mid_cyc_dates[1:]))\n",
    "    norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "    m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    m.set_array(np.array([mdates.date2num(date) for date in mid_cyc_dates[1:]]))\n",
    "\n",
    "    # Add colorbar\n",
    "    divider = make_axes_locatable(ax[0])\n",
    "    cax = divider.append_axes('bottom', size='2.5%', pad=0.5)\n",
    "    cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "\n",
    "    # Set ticks for all years but labels only for odd years\n",
    "    tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "    tick_labels = [date.strftime('%Y') if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "    cbar.set_ticks(tick_locations)\n",
    "    cbar.set_ticklabels(tick_labels)\n",
    "    \n",
    "    # Add minor ticks for quarters\n",
    "    cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Quarter intervals only\n",
    "    cbar.set_label('year', size=10, labelpad=10)\n",
    "\n",
    "    # Change polar stereographic m to km\n",
    "    km_scale = 1e3\n",
    "    ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax[0].xaxis.set_major_formatter(ticks_x)\n",
    "    ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax[0].yaxis.set_major_formatter(ticks_y)\n",
    "    \n",
    "    # Set axes limit, title, and axis label\n",
    "    ax[0].set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "    ax[0].set_xlabel('x [km]')\n",
    "    ax[0].set_ylabel('y [km]')\n",
    "\n",
    "    \n",
    "    # Panel - Active area ---------------------------------------------\n",
    "\n",
    "    # Plot horizontal zero line for reference\n",
    "    ax[1].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "    \n",
    "    # Plot stationary outline and evolving outlines unary union areas\n",
    "    ax[1].axhline(np.divide(lake_gdf['area (m^2)'], 1e6).values, \n",
    "        color=stationary_color, linestyle='solid', linewidth=2)\n",
    "    ax[1].axhline(np.divide(evolving_union_gdf['area (m^2)'], 1e6).values, \n",
    "        color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "    # Plot multi-colored line and scatter for data points\n",
    "    x = mdates.date2num(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "    y = np.divide(evolving_geom_calcs_df['evolving_outlines_area (m^2)'], 1e6)\n",
    "    \n",
    "    # # Plot evolving outlines time series using LineCollection from points/segments to plot multi-colored line\n",
    "    # points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    # segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    # lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "    # lc.set_array(x)  # Set the values used for colormapping, using matplotlib dates for colors\n",
    "    # lc.set_linewidth(2)\n",
    "    # line = ax[1].add_collection(lc)\n",
    "    # scatter = ax[1].scatter(x, y, c=x, cmap=cmap, s=9, norm=norm, zorder=2)\n",
    "\n",
    "    # Filter out points where y is zero\n",
    "    non_zero_mask = y != 0\n",
    "    x_filtered = x[non_zero_mask]\n",
    "    y_filtered = y[non_zero_mask]\n",
    "    \n",
    "    # Create points and segments for LineCollection (only using non-zero points)\n",
    "    if len(x_filtered) > 1:  # Need at least 2 points to create segments\n",
    "        points = np.array([x_filtered, y_filtered]).T.reshape(-1, 1, 2)\n",
    "        segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "        \n",
    "        # Create a LineCollection, using the discrete colormap and norm\n",
    "        lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "        lc.set_array(x_filtered)\n",
    "        lc.set_linewidth(2)\n",
    "        # Plot multi-colored line and scatter for data points\n",
    "        line = ax[1].add_collection(lc)\n",
    "    \n",
    "    # Plot scatter points (only non-zero values)\n",
    "    scatter = ax[1].scatter(x_filtered, y_filtered, c=x_filtered, cmap=cmap, s=9, norm=norm, zorder=2)\n",
    "\n",
    "    # Panel C - Cumulative dh/dt -------------------------------------------------------\n",
    "    # Plot horizontal zero line for reference\n",
    "    ax[2].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "\n",
    "    # Plot stationary outlines off-lake region dh\n",
    "    ax[2].plot(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "        np.cumsum(stationary_geom_calcs_df['stationary_outline_region_dh (m)']).astype(float), \n",
    "        color='lightgray', linestyle='solid', linewidth=1)\n",
    "\n",
    "    # Plot evolving outlines off-lake region dh\n",
    "    ax[2].plot(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']), \n",
    "        np.cumsum(evolving_geom_calcs_df['evolving_outlines_region_dh (m)']), color='dimgray', linestyle='solid', linewidth=1)\n",
    "\n",
    "    # Plot stationary outline time series\n",
    "    ax[2].plot(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "        np.cumsum(stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), color=stationary_color, linestyle='solid', linewidth=1)\n",
    "\n",
    "    # Plot multi-colored line and scatter for data points\n",
    "    y = np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'])\n",
    "    \n",
    "    # Plot evolving outlines time series using LineCollection from points/segments to plot multi-colored line\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "    lc.set_array(x)\n",
    "    lc.set_linewidth(2)\n",
    "    line = ax[2].add_collection(lc)\n",
    "    scatter = ax[2].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9, zorder=2)\n",
    "\n",
    "    # Plot evolving outlines union outline time series\n",
    "    ax[2].plot(mdates.date2num(evolving_union_geom_calcs_df['midcyc_datetime']), \n",
    "        np.cumsum(evolving_union_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "        color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "    # Plot bias\n",
    "    ax[2].plot(x, np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)']-\n",
    "                            stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "                            color='red', linestyle='solid', linewidth=1)\n",
    "    \n",
    "    \n",
    "    # Panel D - Cumulative dV/dt --------------------------------------------------\n",
    "    # Plot horizontal line at zero for reference\n",
    "    ax[3].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "    \n",
    "    # Plot stationary outline time series\n",
    "    ax[3].plot(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "        np.divide(np.cumsum(stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "        color=stationary_color, linestyle='solid', linewidth=1)\n",
    "    \n",
    "    # Plot multi-colored line and scatter for data points\n",
    "    y = np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "    \n",
    "    # Plot evolving outlines time series using LineCollection from points/segments to plot multi-colored line\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "    lc.set_array(x)\n",
    "    lc.set_linewidth(2)\n",
    "    line = ax[3].add_collection(lc)\n",
    "    scatter = ax[3].scatter(x, y, c=x, cmap=cmap, norm=norm, s=10, zorder=2)\n",
    "\n",
    "    # Plot evolving outlines union outline time series\n",
    "    ax[3].plot(mdates.date2num(evolving_union_geom_calcs_df['midcyc_datetime']), \n",
    "        np.divide(np.cumsum(evolving_union_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "        color='k', linestyle='dashed', linewidth=1)\n",
    "    \n",
    "    # Plot bias\n",
    "    ax[3].plot(x, np.divide(np.cumsum(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)']-\n",
    "                            stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "                            color='red', linestyle='solid', linewidth=1)\n",
    "\n",
    "    for i in range(1, ncols):\n",
    "        # Set x-axis limits\n",
    "        ax[i].set_xlim(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "        \n",
    "        # Set ticks for all years but labels only for even years (to match colorbar)\n",
    "        tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "        tick_labels = [date.strftime('%Y') if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "        ax[i].set_xticks(tick_locations)\n",
    "        ax[i].set_xticklabels(tick_labels)\n",
    "        \n",
    "        # Add minor ticks for quarters\n",
    "        ax[i].xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))\n",
    "        \n",
    "        ax[i].set_xlabel('year')\n",
    "    \n",
    "    # Add legends\n",
    "    evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=1)\n",
    "    within_eval_line = plt.Line2D([], [], color='darkgray', linestyle='solid', linewidth=1)\n",
    "    stationary_region_patch = mpatches.Patch(color=stationary_color, alpha=0.2)\n",
    "    evolving_union_region_patch = mpatches.Patch(color='k', alpha=0.2)\n",
    "    \n",
    "    legend = ax[0].legend([tuple(onlake_lines),\n",
    "                           tuple(offlake_lines),\n",
    "                           evolving_union_line, \n",
    "                           stationary_line, \n",
    "                           within_eval_line,\n",
    "                           stationary_region_patch,\n",
    "                           evolving_union_region_patch], \n",
    "        [f'evolving outlines ({evolving_outlines_gdf.level[0]} m)',\n",
    "         'off-lake evolving outlines', \n",
    "         'evolving outlines union',\n",
    "         'stationary outline',\n",
    "         f'within evaluation line ({int(evolving_outlines_gdf.within_area_multiple[0])}x)',\n",
    "         'stationary region',\n",
    "         'evolving union region'],\n",
    "        handlelength=3,\n",
    "        handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "        loc='upper left',\n",
    "        bbox_to_anchor=(0, 1.47))\n",
    "\n",
    "    legend = ax[1].legend([tuple(onlake_lines), \n",
    "                           evolving_union_line, \n",
    "                           stationary_line],\n",
    "        ['evolving outlines', \n",
    "         'evolving outlines union', \n",
    "         'stationary outline'], \n",
    "        handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "        fontsize='small', loc='upper left',\n",
    "        bbox_to_anchor=(0, 1.15))\n",
    "\n",
    "    evolving_region = plt.Line2D([], [], color='dimgray', linestyle='solid', linewidth=1)\n",
    "    stationary_region = plt.Line2D([], [], color='lightgray', linestyle='solid', linewidth=1)\n",
    "    bias = plt.Line2D([], [], color='red', linestyle='solid', linewidth=1)\n",
    "\n",
    "    legend = ax[2].legend(\n",
    "        [evolving_region,\n",
    "         stationary_region,\n",
    "         tuple(onlake_lines),\n",
    "         evolving_union_line,\n",
    "         stationary_line,  \n",
    "         bias],\n",
    "        ['evolving outlines region',\n",
    "         'stationary outline region',\n",
    "         'evolving outlines',\n",
    "         'evolving outlines union',\n",
    "         'stationary outline', \n",
    "         'bias (evolving - stationary)'],\n",
    "         handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "         fontsize='small', loc='upper left',\n",
    "         bbox_to_anchor=(0, 1.25))\n",
    "\n",
    "    legend = ax[3].legend([tuple(onlake_lines), \n",
    "                           stationary_line,\n",
    "                           evolving_union_line,\n",
    "                           bias],\n",
    "        ['evolving outlines', \n",
    "         'stationary outline',\n",
    "         'evolving outlines union',\n",
    "         'bias (evolving - stationary)'], \n",
    "        handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "        fontsize='small', loc='upper left',\n",
    "        bbox_to_anchor=(0, 1.15))\n",
    "\n",
    "    # Set titles\n",
    "    ax[0].set_title(f'{lake_name}', size=12, y=1.47)\n",
    "    ax[1].set_title('active area [km$^2$]', size=12, y=1.15)\n",
    "    ax[2].set_title('cumulative dh [m]', size=12, y=1.25)\n",
    "    ax[3].set_title('cumulative dV [km$^3$]', size=12, y=1.15)\n",
    "\n",
    "    # Modify the output path for saving the figure\n",
    "    if forward_fill:\n",
    "        output_path = os.path.join(OUTPUT_DIR, 'plot_evolving_and_stationary_comparison', 'forward_fill', f'{lake_name}.png')\n",
    "    else:\n",
    "        output_path = os.path.join(OUTPUT_DIR, 'plot_evolving_and_stationary_comparison', f'{lake_name}.png')\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Figure saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4654b17-a0d5-449f-862c-b1457a32466f",
   "metadata": {},
   "source": [
    "## plot_evolving_and_stationary_comparison_sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "648edc4c-ce9c-41c9-ab2d-57af95af924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_evolving_and_stationary_comparison_sequential(lake_gdf):\n",
    "#     '''\n",
    "#     Plot and compare the evolving outlines and stationary outline of a lake along with active area, dh, and dV,\n",
    "#     creating separate plots for each time step showing the progression of changes.\n",
    "\n",
    "#     Parameters:\n",
    "#     lake_gdf (GeoDataFrame): A GeoDataFrame containing a single lake's data with attributes.\n",
    "\n",
    "#     Returns:\n",
    "#     None: Results saved as PNG files in OUTPUT_DIR + '/plot_evolving_and_stationary_comparison/sequential/'\n",
    "#     '''\n",
    "#     # Define lake name and polygon\n",
    "#     lake_name = lake_gdf['name'].values[0]\n",
    "#     lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "#     print(f'Processing lake: {lake_name}')\n",
    "    \n",
    "#     # Make output directory if it doesn't yet exist\n",
    "#     sequential_dir = os.path.join(OUTPUT_DIR, f'plot_evolving_and_stationary_comparison_sequential/{lake_name}')\n",
    "#     os.makedirs(sequential_dir, exist_ok=True)\n",
    "\n",
    "#     # First check if lake_gdf is valid and has data\n",
    "#     if lake_gdf is None or lake_gdf.empty:\n",
    "#         print(\"Empty lake_gdf provided. Skipping...\")\n",
    "#         return\n",
    "    \n",
    "#     # Open required files\n",
    "#     try:\n",
    "#         evolving_outlines_gdf = gpd.read_file(os.path.join(\n",
    "#             'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)))\n",
    "#         offlake_outlines_gdf = gpd.read_file(os.path.join(\n",
    "#             OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines/{}.geojson'.format(lake_name)))\n",
    "#         evolving_geom_calcs_df = pd.read_csv(f\"output/geometric_calcs/evolving_outlines_geom_calc/{lake_name}.csv\")\n",
    "#         evolving_union_geom_calcs_df = pd.read_csv(f\"output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/{lake_name}.csv\")\n",
    "#         stationary_geom_calcs_df = pd.read_csv(f\"output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes/{lake_name}.csv\")\n",
    "#     except (fiona.errors.DriverError, pyogrio.errors.DataSourceError, FileNotFoundError) as e:\n",
    "#         print(f\"Error loading files for {lake_name}: {str(e)}. Skipping...\")\n",
    "#         return\n",
    "\n",
    "#     # Convert strings to datetime\n",
    "#     evolving_geom_calcs_df['midcyc_datetime'] = pd.to_datetime(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "\n",
    "#     # Get evolving outlines union\n",
    "#     evolving_union_gdf = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]\n",
    "\n",
    "#     # Define colors and setup\n",
    "#     stationary_color = 'darkturquoise'\n",
    "#     cmap = plt.get_cmap('plasma', len(mid_cyc_dates)-1)\n",
    "#     norm = plt.Normalize(mdates.date2num(mid_cyc_dates[0]), \n",
    "#                         mdates.date2num(mid_cyc_dates[-1]))\n",
    "\n",
    "#     # Prepare datasets\n",
    "#     dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(lake_gdf, 30)\n",
    "\n",
    "#     # Get plot bounds\n",
    "#     within_eval_poly = area_multiple_buffer(lake_gdf['geometry'].iloc[0], evolving_outlines_gdf['within_area_multiple'][0])\n",
    "#     evolving_stationary_outlines_union = evolving_stationary_outlines_union_evolving_lakes_gdf[evolving_stationary_outlines_union_evolving_lakes_gdf['name'] == lake_name]['geometry'].iloc[0]\n",
    "#     all_poly_union = unary_union([search_extent_poly, within_eval_poly, evolving_stationary_outlines_union])\n",
    "#     x_min, y_min, x_max, y_max = all_poly_union.bounds\n",
    "\n",
    "#     # Calculate time steps and diffs for each dataset\n",
    "#     dataset1_dh = None\n",
    "#     if dataset1_masked is not None:\n",
    "#         dataset1_dh = dataset1_masked['delta_h'].diff('time')\n",
    "#         dataset1_dh.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "#         dataset1_midcyc_datetimes = []\n",
    "#         dataset1_datetimes = dataset1_masked['time'].values\n",
    "#         for i in range(1, len(dataset1_datetimes)):\n",
    "#             midcyc_days = dataset1_datetimes[i] - dataset1_datetimes[i-1]\n",
    "#             midcyc_date = dataset1_datetimes[i-1] + midcyc_days/2\n",
    "#             dataset1_midcyc_datetimes.append(midcyc_date)\n",
    "#         dataset1_midcyc_times = np.array(dataset1_midcyc_datetimes)        \n",
    "\n",
    "#     # Get time differences for dataset2\n",
    "#     dataset2_dh = dataset2_masked['delta_h'].diff('time')\n",
    "#     dataset2_dh.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "#     # dataset2_datetimes = dataset2_masked['time'].values\n",
    "#     # dataset2_midcyc_times = dataset2_datetimes[1:] + np.diff(dataset2_datetimes) / 2\n",
    "#     dataset2_midcyc_datetimes = []\n",
    "#     dataset2_datetimes = dataset2_masked['time'].values\n",
    "#     for i in range(1, len(dataset2_datetimes)):\n",
    "#         midcyc_days = dataset2_datetimes[i] - dataset2_datetimes[i-1]\n",
    "#         midcyc_date = dataset2_datetimes[i-1] + midcyc_days/2\n",
    "#         dataset2_midcyc_datetimes.append(midcyc_date)\n",
    "#     dataset2_midcyc_times = np.array(dataset2_midcyc_datetimes)     \n",
    "\n",
    "#     # Find magnitude of dh for colorbar mapping\n",
    "#     height_anom_pos = []\n",
    "#     height_anom_neg = []\n",
    "\n",
    "#     # Process both datasets for height anomalies\n",
    "#     if dataset1_masked is not None:\n",
    "#         for dh_slice in dataset1_dh:\n",
    "#             if np.any(~np.isnan(dh_slice)):\n",
    "#                 height_anom_pos.append(np.nanmax(dh_slice))\n",
    "#                 height_anom_neg.append(np.nanmin(dh_slice))\n",
    "\n",
    "#     for dh_slice in dataset2_dh:\n",
    "#         if np.any(~np.isnan(dh_slice)):\n",
    "#             height_anom_pos.append(np.nanmax(dh_slice))\n",
    "#             height_anom_neg.append(np.nanmin(dh_slice))\n",
    "\n",
    "#     if not height_anom_pos:\n",
    "#         print(\"No valid height anomalies found for plotting\")\n",
    "#         return None\n",
    "\n",
    "#     # Create color normalization for height changes\n",
    "#     divnorm = colors.TwoSlopeNorm(vmin=min(height_anom_neg), \n",
    "#                                 vcenter=0., \n",
    "#                                 vmax=max(height_anom_pos))\n",
    "\n",
    "#     # Create on- and off-lake line segments and solid lines that will be used in legends \n",
    "#     fig, ax = plt.subplots()\n",
    "#     onlake_lines, offlake_lines = [], []\n",
    "#     for idx, dt in enumerate(mid_cyc_dates):\n",
    "#         x, y = 1, 1\n",
    "#         onlake_line, = ax.plot(x, y, color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))), linewidth=2)\n",
    "#         onlake_lines.append(onlake_line)\n",
    "#         offlake_line, = ax.plot(x, y, color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))), linewidth=2, alpha=0.2)\n",
    "#         offlake_lines.append(offlake_line)\n",
    "\n",
    "#     stationary_color  = 'darkturquoise'\n",
    "#     stationary_line = plt.Line2D([], [], color=stationary_color, linestyle='solid', linewidth=1)\n",
    "#     within_eval_line = plt.Line2D([], [], color='darkgray', linestyle='solid', linewidth=1)\n",
    "#     stationary_region_patch = mpatches.Patch(color=stationary_color, alpha=0.2)\n",
    "#     evolving_union_region_patch = mpatches.Patch(color='k', alpha=0.2)\n",
    "\n",
    "#     # Calculate y-axis limits for cumulative dh plot (Panel C)\n",
    "#     dh_data = pd.concat([\n",
    "#         pd.Series(np.cumsum(stationary_geom_calcs_df['stationary_outline_region_dh (m)'])),\n",
    "#         pd.Series(np.cumsum(evolving_geom_calcs_df['evolving_outlines_region_dh (m)'])),\n",
    "#         pd.Series(np.cumsum(stationary_geom_calcs_df['stationary_outline_dh_corr (m)'])),\n",
    "#         pd.Series(np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)']))\n",
    "#     ])\n",
    "#     dh_min, dh_max = dh_data.min(), dh_data.max()\n",
    "#     dh_range = dh_max - dh_min\n",
    "\n",
    "#     # Calculate y-axis limits for cumulative dV plot (Panel D)\n",
    "#     dv_data = pd.concat([\n",
    "#         pd.Series(np.cumsum(np.divide(stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)'], 1e9))),\n",
    "#         pd.Series(np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'], 1e9)))\n",
    "#     ])\n",
    "#     dv_min, dv_max = dv_data.min(), dv_data.max()\n",
    "#     dv_range = dv_max - dv_min\n",
    "\n",
    "#     # Iterate through each date to create sequential plots\n",
    "#     for date_idx, current_date in enumerate(evolving_geom_calcs_df['midcyc_datetime']):\n",
    "#         gc.collect()  # Garbage collection\n",
    "        \n",
    "#         current_date_pd = pd.Timestamp(current_date)\n",
    "#         print(f\"Creating plot for date: {current_date_pd}\")\n",
    "        \n",
    "#         # Create figure\n",
    "#         fig = plt.figure(figsize=(16, 5))\n",
    "#         nows, ncols = 1, 4\n",
    "#         gs = fig.add_gridspec(nows, ncols, \n",
    "#                               width_ratios=[1.2, 0.8, 0.8, 0.8],  # Make first panel wider\n",
    "#                               wspace=0.4)  # Add horizontal spacing between subplots\n",
    "#         ax = [fig.add_subplot(gs[0, i]) for i in range(ncols)]\n",
    "        \n",
    "#         # Filter data up to current date\n",
    "#         current_mask = evolving_geom_calcs_df['midcyc_datetime'] <= current_date\n",
    "#         current_evolving_geom_calcs = evolving_geom_calcs_df[current_mask]\n",
    "#         current_evolving_union_geom_calcs = evolving_union_geom_calcs_df[current_mask]\n",
    "#         current_stationary_geom_calcs = stationary_geom_calcs_df[current_mask]\n",
    "\n",
    "#         # Panel A - dh and evolving outlines\n",
    "#         # Find corresponding dh slice for the current date\n",
    "#         current_dh = None\n",
    "#         if dataset1_masked is not None and current_date in dataset1_midcyc_times:\n",
    "#             idx = np.where(dataset1_midcyc_times == current_date)[0][0]\n",
    "#             current_dh = dataset1_dh[idx]\n",
    "#         elif current_date in dataset2_midcyc_times:\n",
    "#             idx = np.where(dataset2_midcyc_times == current_date)[0][0]\n",
    "#             current_dh = dataset2_dh[idx]\n",
    "\n",
    "#         if current_dh is not None:\n",
    "#             # Plot height change\n",
    "#             img = ax[0].imshow(current_dh, extent=[x_min, x_max, y_min, y_max],\n",
    "#                 origin='lower', cmap='coolwarm_r', norm=divnorm)\n",
    "\n",
    "#         # Create stationary region and evolving outlines region and plot\n",
    "#         stationary_region = area_multiple_buffer(lake_poly, 2)\n",
    "#         stationary_region = stationary_region.difference(lake_poly)\n",
    "#         evolving_union_region = area_multiple_buffer(evolving_union_gdf['geometry'], 2)\n",
    "#         evolving_union_region = evolving_union_region.difference(evolving_union_gdf['geometry'].iloc[0])\n",
    "#         gpd.GeoDataFrame(geometry=[stationary_region], crs='3031').plot(ax=ax[0], color=stationary_color, linewidth=2, alpha=0.2)\n",
    "#         gpd.GeoDataFrame(geometry=[evolving_union_region], crs='3031').plot(ax=ax[0], color='k', linewidth=2, alpha=0.2)\n",
    "\n",
    "#         # Add colorbars\n",
    "#         # First create the divider\n",
    "#         divider = make_axes_locatable(ax[0])\n",
    "        \n",
    "#         # Create the vertical colorbar axes (for dh)\n",
    "#         cax_vertical = divider.append_axes('right', size='5%', pad=0.05)\n",
    "\n",
    "#         # Add dh colorbar\n",
    "#         plt.colorbar(img, cax=cax_vertical, label='height change (dh) [m]')\n",
    "        \n",
    "#         # Set up colormap\n",
    "#         min_date = pd.to_datetime(cyc_start_dates[1])\n",
    "#         max_date = pd.to_datetime(cyc_end_dates[-1])\n",
    "#         date_range = pd.date_range(min_date, max_date, periods=len(mid_cyc_dates[1:]))\n",
    "#         years = date_range.year.unique()\n",
    "#         years = pd.to_datetime(years, format='%Y')\n",
    "#         cmap = plt.get_cmap('plasma', len(mid_cyc_dates[1:]))\n",
    "#         norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "#         m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "#         m.set_array(np.array([mdates.date2num(date) for date in mid_cyc_dates[1:]]))\n",
    "    \n",
    "#         # Add colorbar\n",
    "#         divider = make_axes_locatable(ax[0])\n",
    "#         cax = divider.append_axes('bottom', size='2.5%', pad=0.5)\n",
    "#         cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "    \n",
    "#         # Set ticks for all years but labels only for odd years\n",
    "#         tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "#         tick_labels = [date.strftime('%Y') if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "#         cbar.set_ticks(tick_locations)\n",
    "#         cbar.set_ticklabels(tick_labels)\n",
    "        \n",
    "#         # Add minor ticks for quarters\n",
    "#         cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Quarter intervals only\n",
    "#         cbar.set_label('year', size=10, labelpad=10)\n",
    "        \n",
    "#         # Add inset map\n",
    "#         axIns = ax[0].inset_axes([0.05, 0, 0.3, 0.3])\n",
    "#         axIns.set_aspect('equal')\n",
    "#         moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "#         moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "#         axIns.axis('off')\n",
    "#         axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', linewidth=0.1, color='k', s=30, zorder=3)\n",
    "        \n",
    "#         # Change polar stereographic m to km\n",
    "#         km_scale = 1e3\n",
    "#         ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "#         ax[0].xaxis.set_major_formatter(ticks_x)\n",
    "#         ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "#         ax[0].yaxis.set_major_formatter(ticks_y)\n",
    "\n",
    "#         # Set axes limit, title, and axis label\n",
    "#         ax[0].set(xlim=(x_min, x_max), ylim=(y_min, y_max))\n",
    "#         ax[0].set_xlabel('x [km]')\n",
    "#         ax[0].set_ylabel('y [km]')\n",
    "\n",
    "#         # Store line segments for multi-colored line in legend\n",
    "#         onlake_lines, offlake_lines = [], []\n",
    "#         for idx, dt in enumerate(mid_cyc_dates):\n",
    "#             x, y = 1, 1\n",
    "#             onlake_line, = ax[0].plot(x, y, color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))), linewidth=2)\n",
    "#             onlake_lines.append(onlake_line)\n",
    "#             offlake_line, = ax[0].plot(x, y, color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))), linewidth=2, alpha=0.2)\n",
    "#             offlake_lines.append(offlake_line)\n",
    "\n",
    "#         # Plot evolving outlines up to current date\n",
    "#         for idx, dt in enumerate(evolving_geom_calcs_df['midcyc_datetime'][:date_idx + 1]):\n",
    "\n",
    "#             evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "#             offlake_outlines_dt = offlake_outlines_gdf[offlake_outlines_gdf['midcyc_datetime'] == dt]\n",
    "            \n",
    "#             if not evolving_outlines_dt.empty:\n",
    "#                 evolving_outlines_dt.boundary.plot(ax=ax[0], \n",
    "#                     color=cmap(norm(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime'].iloc[[idx]]))),\n",
    "#                     linewidth=1)\n",
    "#             if not offlake_outlines_dt.empty:\n",
    "#                 offlake_outlines_dt.boundary.plot(ax=ax[0], \n",
    "#                     color=cmap(norm(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime'].iloc[[idx]]))),\n",
    "#                     linewidth=1, alpha=0.25)\n",
    "\n",
    "#         # Add other map elements (evaluation boundary, stationary outline, etc.)\n",
    "#         gpd.GeoDataFrame(geometry=[within_eval_poly]).boundary.plot(ax=ax[0], edgecolor='darkgray', facecolor='none', linewidth=1)\n",
    "#         evolving_union_gdf.boundary.plot(ax=ax[0], color='k', linestyle='dotted', linewidth=1)\n",
    "#         stationary_outlines_gdf.boundary.plot(ax=ax[0], color=stationary_color, linewidth=1, zorder=0)\n",
    "\n",
    "\n",
    "#         # Panel B - Active area\n",
    "\n",
    "#         ax[1].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "#         ax[1].axhline(np.divide(lake_gdf['area (m^2)'], 1e6).values, \n",
    "#                       color=stationary_color, linestyle='solid', linewidth=2)\n",
    "#         ax[1].axhline(np.divide(evolving_union_gdf['area (m^2)'], 1e6).values, \n",
    "#                       color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "#         # Plot multi-colored evolving outlines time series\n",
    "#         x = mdates.date2num(current_evolving_geom_calcs['midcyc_datetime'])\n",
    "#         y = np.divide(current_evolving_geom_calcs['evolving_outlines_area (m^2)'], 1e6)\n",
    "        \n",
    "#         # # Plot multi-colored evolving outlines time series using LineCollection from points/segments\n",
    "#         # points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "#         # segments = np.concatenate([points[:-1], points[1:]], axis=1) if len(points) > 1 else []\n",
    "#         # if len(segments) > 0:\n",
    "#         #     lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "#         #     lc.set_array(x)\n",
    "#         #     lc.set_linewidth(2)\n",
    "#         #     ax[1].add_collection(lc)\n",
    "#         # ax[1].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9, zorder=2)\n",
    "\n",
    "#         # Filter out points where y is zero\n",
    "#         non_zero_mask = y != 0\n",
    "#         x_filtered = x[non_zero_mask]\n",
    "#         y_filtered = y[non_zero_mask]\n",
    "        \n",
    "#         # Create points and segments for LineCollection (only using non-zero points)\n",
    "#         if len(x_filtered) > 1:  # Need at least 2 points to create segments\n",
    "#             points = np.array([x_filtered, y_filtered]).T.reshape(-1, 1, 2)\n",
    "#             segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "            \n",
    "#             # Create a LineCollection, using the discrete colormap and norm\n",
    "#             lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "#             lc.set_array(x_filtered)\n",
    "#             lc.set_linewidth(2)\n",
    "#             # Plot multi-colored line and scatter for data points\n",
    "#             line = ax[1].add_collection(lc)\n",
    "        \n",
    "#         # Plot scatter points (only non-zero values)\n",
    "#         scatter = ax[1].scatter(x_filtered, y_filtered, c=x_filtered, cmap=cmap, s=9, norm=norm, zorder=2)\n",
    "\n",
    "\n",
    "#         # Panel C - Cumulative dh/dt\n",
    "#         ax[2].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "        \n",
    "#         # Plot cumulative values up to current date\n",
    "#         ax[2].plot(x, np.cumsum(current_stationary_geom_calcs['stationary_outline_region_dh (m)']).astype(float), \n",
    "#             color='lightgray', linestyle='solid', linewidth=1)\n",
    "#         ax[2].plot(x, np.cumsum(current_evolving_geom_calcs['evolving_outlines_region_dh (m)']), \n",
    "#             color='dimgray', linestyle='solid', linewidth=1)\n",
    "#         ax[2].plot(x, np.cumsum(current_stationary_geom_calcs['stationary_outline_dh_corr (m)']), \n",
    "#             color=stationary_color, linestyle='solid', linewidth=1)\n",
    "#         ax[2].plot(x, np.cumsum(current_evolving_union_geom_calcs['stationary_outline_dh_corr (m)']), \n",
    "#             color='k', linestyle='dashed', linewidth=1)\n",
    "#         ax[2].plot(x, np.cumsum(current_evolving_geom_calcs['evolving_outlines_dh_corr (m)']-\n",
    "#             current_stationary_geom_calcs['stationary_outline_dh_corr (m)']), \n",
    "#             color='red', linestyle='solid', linewidth=1)\n",
    "        \n",
    "#         # Plot multi-colored evolving outlines time series using LineCollection from points/segments\n",
    "#         y = np.cumsum(current_evolving_geom_calcs['evolving_outlines_dh_corr (m)'])\n",
    "#         points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "#         segments = np.concatenate([points[:-1], points[1:]], axis=1) if len(points) > 1 else []\n",
    "#         if len(segments) > 0:\n",
    "#             lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "#             lc.set_array(x)\n",
    "#             lc.set_linewidth(2)\n",
    "#             ax[2].add_collection(lc)\n",
    "#         ax[2].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9, zorder=2)\n",
    "\n",
    "#         # Set y-axis limits\n",
    "#         ax[2].set_ylim(dh_min - 0.1 * dh_range, dh_max + 0.1 * dh_range)\n",
    "\n",
    "\n",
    "#         # Panel D - Cumulative dV/dt\n",
    "#         ax[3].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "        \n",
    "#         ax[3].plot(x, np.divide(np.cumsum(current_stationary_geom_calcs['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "#             color=stationary_color, linestyle='solid', linewidth=1)\n",
    "#         ax[3].plot(x, np.divide(np.cumsum(current_evolving_union_geom_calcs['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "#             color='k', linestyle='dashed', linewidth=1)\n",
    "#         ax[3].plot(x, np.divide(np.cumsum(current_evolving_geom_calcs['evolving_outlines_dV_corr (m^3)']-\n",
    "#             current_stationary_geom_calcs['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "#             color='red', linestyle='solid', linewidth=1)\n",
    "\n",
    "#         # Plot multi-colored evolving outlines time series using LineCollection from points/segments\n",
    "#         y = np.cumsum(np.divide(current_evolving_geom_calcs['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "#         points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "#         segments = np.concatenate([points[:-1], points[1:]], axis=1) if len(points) > 1 else []\n",
    "#         if len(segments) > 0:\n",
    "#             lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "#             lc.set_array(x)\n",
    "#             lc.set_linewidth(2)\n",
    "#             ax[3].add_collection(lc)\n",
    "#         ax[3].scatter(x, y, c=x, cmap=cmap, norm=norm, s=10, zorder=2)\n",
    "\n",
    "#         # Add legends\n",
    "#         evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=1)\n",
    "#         within_eval_line = plt.Line2D([], [], color='darkgray', linestyle='solid', linewidth=1)\n",
    "#         stationary_region_patch = mpatches.Patch(color=stationary_color, alpha=0.2)\n",
    "#         evolving_union_region_patch = mpatches.Patch(color='k', alpha=0.2)\n",
    "      \n",
    "#         legend = ax[0].legend([tuple(onlake_lines),\n",
    "#                                tuple(offlake_lines),\n",
    "#                                evolving_union_line, \n",
    "#                                stationary_line, \n",
    "#                                within_eval_line,\n",
    "#                                stationary_region_patch,\n",
    "#                                evolving_union_region_patch], \n",
    "#             [f'evolving outlines ({evolving_outlines_gdf.level[0]} m)',\n",
    "#              'off-lake evolving outlines', \n",
    "#              'evolving outlines union',\n",
    "#              'stationary outline',\n",
    "#              f'within evaluation line ({int(evolving_outlines_gdf.within_area_multiple[0])}x)',\n",
    "#              'stationary region',\n",
    "#              'evolving union region'],\n",
    "#             handlelength=3,\n",
    "#             handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#             loc='upper left',\n",
    "#             bbox_to_anchor=(0, 1.5))\n",
    "        \n",
    "#         legend = ax[1].legend([tuple(onlake_lines), \n",
    "#                                evolving_union_line, \n",
    "#                                stationary_line],\n",
    "#             ['evolving outlines', \n",
    "#              'evolving outlines union', \n",
    "#              'stationary outline'], \n",
    "#             handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#             fontsize='small', loc='upper left',\n",
    "#             bbox_to_anchor=(0, 1.15))\n",
    "\n",
    "#         evolving_region = plt.Line2D([], [], color='dimgray', linestyle='solid', linewidth=1)\n",
    "#         stationary_region = plt.Line2D([], [], color='lightgray', linestyle='solid', linewidth=1)\n",
    "#         evolving_union_line = plt.Line2D([], [], color='k', linestyle='dashed', linewidth=1)\n",
    "#         bias = plt.Line2D([], [], color='red', linestyle='solid', linewidth=1)\n",
    "#         legend = ax[2].legend(\n",
    "#             [evolving_region,\n",
    "#              stationary_region,\n",
    "#              tuple(onlake_lines),\n",
    "#              evolving_union_line,\n",
    "#              stationary_line,  \n",
    "#              bias],\n",
    "#             ['evolving outlines region',\n",
    "#              'stationary outline region',\n",
    "#              'evolving outlines',\n",
    "#              'evolving outlines union',\n",
    "#              'stationary outline', \n",
    "#              'bias (evolving - stationary)'],\n",
    "#              handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#              fontsize='small', loc='upper left',\n",
    "#              bbox_to_anchor=(0, 1.22))\n",
    "\n",
    "#         legend = ax[3].legend([tuple(onlake_lines), \n",
    "#                                evolving_union_line, \n",
    "#                                stationary_line, \n",
    "#                                bias],\n",
    "#             ['evolving outlines',\n",
    "#              'evolving outlines union',\n",
    "#              'stationary outline',\n",
    "#              'bias (evolving - stationary)'], \n",
    "#             handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#             fontsize='small', loc='upper left',\n",
    "#             bbox_to_anchor=(0, 1.15))\n",
    "\n",
    "#         # Set y-axis limits\n",
    "#         ax[3].set_ylim(dv_min - 0.1 * dv_range, dv_max + 0.1 * dv_range)\n",
    "\n",
    "#         # # Set common attributes for time series panels\n",
    "#         # for i in range(1, 4):\n",
    "#         #     ax[i].set_xlim(mdates.date2num(np.min(cyc_start_dates)), mdates.date2num(np.max(cyc_end_dates)))\n",
    "#         #     ax[i].xaxis.set_major_locator(mdates.YearLocator(base=2))\n",
    "#         #     ax[i].xaxis.set_minor_locator(mdates.YearLocator(base=1))\n",
    "#         #     ax[i].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "#         #     ax[i].set_xlabel('year')\n",
    "#         for i in range(1, ncols):\n",
    "#             # Set x-axis limits\n",
    "#             ax[i].set_xlim(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "            \n",
    "#             # Set ticks for all years but labels only for even years (to match colorbar)\n",
    "#             tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "#             tick_labels = [date.strftime('%Y') if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "#             ax[i].set_xticks(tick_locations)\n",
    "#             ax[i].set_xticklabels(tick_labels)\n",
    "            \n",
    "#             # Add minor ticks for quarters\n",
    "#             ax[i].xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))\n",
    "            \n",
    "#             ax[i].set_xlabel('year')\n",
    "\n",
    "#         # Set titles\n",
    "#         ax[0].set_title(f'{lake_name}\\nmid-cyc date: {current_date_pd.strftime(\"%Y-%m-%d\")}', size=12, y=1.5)\n",
    "#         ax[1].set_title('active area [km$^2$]', size=12, y=1.15)\n",
    "#         ax[2].set_title('cumulative dh [m]', size=12, y=1.22)\n",
    "#         ax[3].set_title('cumulative dV [km$^3$]', size=12, y=1.15)\n",
    "\n",
    "#         # Save and close\n",
    "#         plt.savefig(os.path.join(sequential_dir, f'{lake_name}_{current_date_pd.strftime(\"%Y%m%d\")}.png'), \n",
    "#                     dpi=300, bbox_inches='tight')\n",
    "\n",
    "#         # Clean up to conserve memory\n",
    "#         current_evolving_geom_calcs = None\n",
    "#         current_stationary_geom_calcs = None\n",
    "#         evolving_outlines_dt = None\n",
    "#         offlake_outlines_dt = None\n",
    "#         del current_dh, fig\n",
    "#         if 'img' in locals():  # Only delete img if it exists\n",
    "#             del img\n",
    "\n",
    "#         # Explicitly clear the figure, close, and clear any reference to it\n",
    "#         plt.clf()\n",
    "#         plt.close('all')\n",
    "\n",
    "#         # Force garbage collection\n",
    "#         gc.collect() \n",
    "        \n",
    "#     # Convert images to video\n",
    "#     try:\n",
    "#         video_from_images_2(lake_gdf, output_dir=OUTPUT_DIR, \n",
    "#             fps=1, img_extension='png')\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error creating video for {lake_name}: {str(e)}\")\n",
    "#         traceback.print_exc()\n",
    "\n",
    "#     # Clear output\n",
    "#     clear_output(wait=True)\n",
    "\n",
    "# def video_from_images_2(lake_gdf, output_dir=OUTPUT_DIR, fps=1, img_extension='png'):\n",
    "#     \"\"\"\n",
    "#     Creates a video from still images stored in a folder based on the lake_gdf input, then deletes the images.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - lake_gdf: GeoDataFrame containing lake information (expects a 'name' column).\n",
    "#     - output_dir: Base directory where the images and video are stored/created.\n",
    "#     - fps: Frames per second for the output video.\n",
    "#     - img_extension: Extension of the images to look for in the folder.\n",
    "\n",
    "#     # Example usage\n",
    "#     video_from_images(lake_gdf, OUTPUT_DIR, fps=0.5, img_extension='png')\n",
    "#     \"\"\"\n",
    "#     lake_name = lake_gdf['name'].iloc[0]\n",
    "#     print('Making video for', lake_name)\n",
    "\n",
    "#     # Derive paths based on lake_gdf\n",
    "#     images_folder = os.path.join(OUTPUT_DIR, f\"plot_evolving_and_stationary_comparison_sequential/{lake_name}\")\n",
    "#     output_video_file = os.path.join(OUTPUT_DIR, \n",
    "#         f\"plot_evolving_and_stationary_comparison_sequential/{lake_name}.mp4\")\n",
    "    \n",
    "#     # Get all images in the folder with the specified extension\n",
    "#     image_files = glob.glob(os.path.join(images_folder, f\"*.{img_extension}\"))\n",
    "#     if not image_files:\n",
    "#         print(f\"No images found in {images_folder} with extension {img_extension}\")\n",
    "#         return\n",
    "    \n",
    "#     # Read the first image to determine the video size\n",
    "#     frame = cv2.imread(image_files[0])\n",
    "#     if frame is None:\n",
    "#         print(f\"Could not read the image {image_files[0]}\")\n",
    "#         return\n",
    "#     height, width, layers = frame.shape\n",
    "\n",
    "#     # Define the codec and create VideoWriter object\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "#     video = cv2.VideoWriter(output_video_file, fourcc, fps, (width, height))\n",
    "\n",
    "#     for image_file in sorted(image_files):\n",
    "#         frame = cv2.imread(image_file)\n",
    "#         if frame is not None:\n",
    "#             video.write(frame)\n",
    "#             # Clear the frame from memory\n",
    "#             frame = None\n",
    "#             gc.collect()\n",
    "\n",
    "#     # Release the VideoWriter object\n",
    "#     video.release()\n",
    "#     video = None\n",
    "#     gc.collect()\n",
    "#     print(f\"Video file {output_video_file} created successfully.\")\n",
    "\n",
    "#     # Delete the images in the directory\n",
    "#     for image_file in image_files:\n",
    "#         os.remove(image_file)\n",
    "#     print(f\"Deleted {len(image_files)} image(s) from {images_folder}\")\n",
    "\n",
    "#     # Force delete the folder and its contents\n",
    "#     try:\n",
    "#         shutil.rmtree(images_folder)\n",
    "#         print(f\"Deleted folder and all contents: {images_folder}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Could not delete folder {images_folder}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5d1573eb-1331-4d1e-9565-093b5107962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evolving_and_stationary_comparison_sequential(lake_gdf):\n",
    "    '''\n",
    "    Plot and compare the evolving outlines and stationary outline of a lake along with active area, dh, and dV,\n",
    "    creating separate plots for each time step showing the progression of changes.\n",
    "\n",
    "    Parameters:\n",
    "    lake_gdf (GeoDataFrame): A GeoDataFrame containing a single lake's data with attributes.\n",
    "\n",
    "    Returns:\n",
    "    None: Results saved as PNG files in OUTPUT_DIR + '/plot_evolving_and_stationary_comparison/sequential/'\n",
    "    '''\n",
    "    # Define lake name and polygon\n",
    "    lake_name = lake_gdf['name'].values[0]\n",
    "    lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "    print(f'Processing lake: {lake_name}')\n",
    "    \n",
    "    # Make output directory if it doesn't yet exist\n",
    "    sequential_dir = os.path.join(OUTPUT_DIR, f'plot_evolving_and_stationary_comparison_sequential/{lake_name}')\n",
    "    os.makedirs(sequential_dir, exist_ok=True)\n",
    "\n",
    "    # First check if lake_gdf is valid and has data\n",
    "    if lake_gdf is None or lake_gdf.empty:\n",
    "        print(\"Empty lake_gdf provided. Skipping...\")\n",
    "        return\n",
    "    \n",
    "    # Open required files\n",
    "    try:\n",
    "        evolving_outlines_gdf = gpd.read_file(os.path.join(\n",
    "            'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)))\n",
    "        offlake_outlines_gdf = gpd.read_file(os.path.join(\n",
    "            OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines/{}.geojson'.format(lake_name)))\n",
    "        evolving_geom_calcs_df = pd.read_csv(f\"output/geometric_calcs/evolving_outlines_geom_calc/{lake_name}.csv\")\n",
    "        evolving_union_geom_calcs_df = pd.read_csv(f\"output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/{lake_name}.csv\")\n",
    "        stationary_geom_calcs_df = pd.read_csv(f\"output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes/{lake_name}.csv\")\n",
    "    except (fiona.errors.DriverError, pyogrio.errors.DataSourceError, FileNotFoundError) as e:\n",
    "        print(f\"Error loading files for {lake_name}: {str(e)}. Skipping...\")\n",
    "        return\n",
    "\n",
    "    # Convert strings to datetime\n",
    "    evolving_geom_calcs_df['midcyc_datetime'] = pd.to_datetime(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "\n",
    "    # Get evolving outlines union\n",
    "    evolving_union_gdf = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]\n",
    "\n",
    "    # Define colors and setup\n",
    "    stationary_color = 'darkturquoise'\n",
    "    cmap = plt.get_cmap('plasma', len(mid_cyc_dates)-1)\n",
    "    norm = plt.Normalize(mdates.date2num(mid_cyc_dates[0]), \n",
    "                        mdates.date2num(mid_cyc_dates[-1]))\n",
    "\n",
    "    # Prepare datasets\n",
    "    dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(lake_gdf, 30)\n",
    "\n",
    "    # Get plot bounds\n",
    "    within_eval_poly = area_multiple_buffer(lake_gdf['geometry'].iloc[0], evolving_outlines_gdf['within_area_multiple'][0])\n",
    "    evolving_stationary_outlines_union = evolving_stationary_outlines_union_evolving_lakes_gdf[evolving_stationary_outlines_union_evolving_lakes_gdf['name'] == lake_name]['geometry'].iloc[0]\n",
    "    all_poly_union = unary_union([search_extent_poly, within_eval_poly, evolving_stationary_outlines_union])\n",
    "    x_min, y_min, x_max, y_max = all_poly_union.bounds\n",
    "\n",
    "    # Calculate time steps and diffs for each dataset\n",
    "    dataset1_dh = None\n",
    "    if dataset1_masked is not None:\n",
    "        dataset1_dh = dataset1_masked['delta_h'].diff('time')\n",
    "        dataset1_dh.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "        dataset1_midcyc_datetimes = []\n",
    "        dataset1_datetimes = dataset1_masked['time'].values\n",
    "        for i in range(1, len(dataset1_datetimes)):\n",
    "            midcyc_days = dataset1_datetimes[i] - dataset1_datetimes[i-1]\n",
    "            midcyc_date = dataset1_datetimes[i-1] + midcyc_days/2\n",
    "            dataset1_midcyc_datetimes.append(midcyc_date)\n",
    "        dataset1_midcyc_times = np.array(dataset1_midcyc_datetimes)        \n",
    "\n",
    "    # Get time differences for dataset2\n",
    "    dataset2_dh = dataset2_masked['delta_h'].diff('time')\n",
    "    dataset2_dh.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "    # dataset2_datetimes = dataset2_masked['time'].values\n",
    "    # dataset2_midcyc_times = dataset2_datetimes[1:] + np.diff(dataset2_datetimes) / 2\n",
    "    dataset2_midcyc_datetimes = []\n",
    "    dataset2_datetimes = dataset2_masked['time'].values\n",
    "    for i in range(1, len(dataset2_datetimes)):\n",
    "        midcyc_days = dataset2_datetimes[i] - dataset2_datetimes[i-1]\n",
    "        midcyc_date = dataset2_datetimes[i-1] + midcyc_days/2\n",
    "        dataset2_midcyc_datetimes.append(midcyc_date)\n",
    "    dataset2_midcyc_times = np.array(dataset2_midcyc_datetimes)     \n",
    "\n",
    "    # Find magnitude of dh for colorbar mapping\n",
    "    height_anom_pos = []\n",
    "    height_anom_neg = []\n",
    "\n",
    "    # Process both datasets for height anomalies\n",
    "    if dataset1_masked is not None:\n",
    "        for dh_slice in dataset1_dh:\n",
    "            if np.any(~np.isnan(dh_slice)):\n",
    "                height_anom_pos.append(np.nanmax(dh_slice))\n",
    "                height_anom_neg.append(np.nanmin(dh_slice))\n",
    "\n",
    "    for dh_slice in dataset2_dh:\n",
    "        if np.any(~np.isnan(dh_slice)):\n",
    "            height_anom_pos.append(np.nanmax(dh_slice))\n",
    "            height_anom_neg.append(np.nanmin(dh_slice))\n",
    "\n",
    "    if not height_anom_pos:\n",
    "        print(\"No valid height anomalies found for plotting\")\n",
    "        return None\n",
    "\n",
    "    # Create color normalization for height changes\n",
    "    divnorm = colors.TwoSlopeNorm(vmin=min(height_anom_neg), \n",
    "                                vcenter=0., \n",
    "                                vmax=max(height_anom_pos))\n",
    "\n",
    "    # Create on- and off-lake line segments and solid lines that will be used in legends \n",
    "    fig, ax = plt.subplots()\n",
    "    onlake_lines, offlake_lines = [], []\n",
    "    for idx, dt in enumerate(mid_cyc_dates):\n",
    "        x, y = 1, 1\n",
    "        onlake_line, = ax.plot(x, y, color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))), linewidth=2)\n",
    "        onlake_lines.append(onlake_line)\n",
    "        offlake_line, = ax.plot(x, y, color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))), linewidth=2, alpha=0.2)\n",
    "        offlake_lines.append(offlake_line)\n",
    "\n",
    "    stationary_color  = 'darkturquoise'\n",
    "    stationary_line = plt.Line2D([], [], color=stationary_color, linestyle='solid', linewidth=1)\n",
    "    within_eval_line = plt.Line2D([], [], color='darkgray', linestyle='solid', linewidth=1)\n",
    "    stationary_region_patch = mpatches.Patch(color=stationary_color, alpha=0.2)\n",
    "    evolving_union_region_patch = mpatches.Patch(color='k', alpha=0.2)\n",
    "\n",
    "    # Calculate y-axis limits for cumulative dh plot (Panel C)\n",
    "    dh_data = pd.concat([\n",
    "        pd.Series(np.cumsum(stationary_geom_calcs_df['stationary_outline_region_dh (m)'])),\n",
    "        pd.Series(np.cumsum(evolving_geom_calcs_df['evolving_outlines_region_dh (m)'])),\n",
    "        pd.Series(np.cumsum(stationary_geom_calcs_df['stationary_outline_dh_corr (m)'])),\n",
    "        pd.Series(np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)']))\n",
    "    ])\n",
    "    dh_min, dh_max = dh_data.min(), dh_data.max()\n",
    "    dh_range = dh_max - dh_min\n",
    "\n",
    "    # Calculate y-axis limits for cumulative dV plot (Panel D)\n",
    "    dv_data = pd.concat([\n",
    "        pd.Series(np.cumsum(np.divide(stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)'], 1e9))),\n",
    "        pd.Series(np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'], 1e9)))\n",
    "    ])\n",
    "    dv_min, dv_max = dv_data.min(), dv_data.max()\n",
    "    dv_range = dv_max - dv_min\n",
    "\n",
    "    # Iterate through each date to create sequential plots\n",
    "    for date_idx, current_date in enumerate(evolving_geom_calcs_df['midcyc_datetime']):\n",
    "        gc.collect()  # Garbage collection\n",
    "        \n",
    "        current_date_pd = pd.Timestamp(current_date)\n",
    "        print(f\"Creating plot for date: {current_date_pd}\")\n",
    "        \n",
    "        # Create figure\n",
    "        fig = plt.figure(figsize=(16, 5))\n",
    "        nows, ncols = 1, 4\n",
    "        gs = fig.add_gridspec(nows, ncols, \n",
    "                              width_ratios=[1.2, 0.8, 0.8, 0.8],  # Make first panel wider\n",
    "                              wspace=0.4)  # Add horizontal spacing between subplots\n",
    "        ax = [fig.add_subplot(gs[0, i]) for i in range(ncols)]\n",
    "        \n",
    "        # Filter data up to current date\n",
    "        current_mask = evolving_geom_calcs_df['midcyc_datetime'] <= current_date\n",
    "        current_evolving_geom_calcs = evolving_geom_calcs_df[current_mask]\n",
    "        current_evolving_union_geom_calcs = evolving_union_geom_calcs_df[current_mask]\n",
    "        current_stationary_geom_calcs = stationary_geom_calcs_df[current_mask]\n",
    "\n",
    "        # Panel A - dh and evolving outlines\n",
    "        # Find corresponding dh slice for the current date\n",
    "        current_dh = None\n",
    "        if dataset1_masked is not None and current_date in dataset1_midcyc_times:\n",
    "            idx = np.where(dataset1_midcyc_times == current_date)[0][0]\n",
    "            current_dh = dataset1_dh[idx]\n",
    "        elif current_date in dataset2_midcyc_times:\n",
    "            idx = np.where(dataset2_midcyc_times == current_date)[0][0]\n",
    "            current_dh = dataset2_dh[idx]\n",
    "\n",
    "        if current_dh is not None:\n",
    "            # Plot height change\n",
    "            img = ax[0].imshow(current_dh, extent=[x_min, x_max, y_min, y_max],\n",
    "                origin='lower', cmap='coolwarm_r', norm=divnorm)\n",
    "\n",
    "        # Create stationary region and evolving outlines region and plot\n",
    "        stationary_region = area_multiple_buffer(lake_poly, 2)\n",
    "        stationary_region = stationary_region.difference(lake_poly)\n",
    "        evolving_union_region = area_multiple_buffer(evolving_union_gdf['geometry'], 2)\n",
    "        evolving_union_region = evolving_union_region.difference(evolving_union_gdf['geometry'].iloc[0])\n",
    "        gpd.GeoDataFrame(geometry=[stationary_region], crs='3031').plot(ax=ax[0], color=stationary_color, linewidth=2, alpha=0.2)\n",
    "        gpd.GeoDataFrame(geometry=[evolving_union_region], crs='3031').plot(ax=ax[0], color='k', linewidth=2, alpha=0.2)\n",
    "\n",
    "        # Add colorbars\n",
    "        # First create the divider\n",
    "        divider = make_axes_locatable(ax[0])\n",
    "        \n",
    "        # Create the vertical colorbar axes (for dh)\n",
    "        cax_vertical = divider.append_axes('right', size='5%', pad=0.05)\n",
    "\n",
    "        # Add dh colorbar\n",
    "        plt.colorbar(img, cax=cax_vertical, label='height change (dh) [m]')\n",
    "        \n",
    "        # Set up colormap\n",
    "        min_date = pd.to_datetime(cyc_start_dates[1])\n",
    "        max_date = pd.to_datetime(cyc_end_dates[-1])\n",
    "        date_range = pd.date_range(min_date, max_date, periods=len(mid_cyc_dates[1:]))\n",
    "        years = date_range.year.unique()\n",
    "        years = pd.to_datetime(years, format='%Y')\n",
    "        cmap = plt.get_cmap('plasma', len(mid_cyc_dates[1:]))\n",
    "        norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "        m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        m.set_array(np.array([mdates.date2num(date) for date in mid_cyc_dates[1:]]))\n",
    "    \n",
    "        # Add colorbar\n",
    "        divider = make_axes_locatable(ax[0])\n",
    "        cax = divider.append_axes('bottom', size='2.5%', pad=0.5)\n",
    "        cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "    \n",
    "        # Set ticks for all years but labels only for odd years\n",
    "        tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "        tick_labels = [date.strftime('%Y') if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "        cbar.set_ticks(tick_locations)\n",
    "        cbar.set_ticklabels(tick_labels)\n",
    "        \n",
    "        # Add minor ticks for quarters\n",
    "        cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Quarter intervals only\n",
    "        cbar.set_label('year', size=10, labelpad=10)\n",
    "        \n",
    "        # Add inset map\n",
    "        axIns = ax[0].inset_axes([0.05, 0, 0.3, 0.3])\n",
    "        axIns.set_aspect('equal')\n",
    "        moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        axIns.axis('off')\n",
    "        axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', linewidth=0.1, color='k', s=30, zorder=3)\n",
    "        \n",
    "        # Change polar stereographic m to km\n",
    "        km_scale = 1e3\n",
    "        ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "        ax[0].xaxis.set_major_formatter(ticks_x)\n",
    "        ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "        ax[0].yaxis.set_major_formatter(ticks_y)\n",
    "\n",
    "        # Set axes limit, title, and axis label\n",
    "        ax[0].set(xlim=(x_min, x_max), ylim=(y_min, y_max))\n",
    "        ax[0].set_xlabel('x [km]')\n",
    "        ax[0].set_ylabel('y [km]')\n",
    "\n",
    "        # Store line segments for multi-colored line in legend\n",
    "        onlake_lines, offlake_lines = [], []\n",
    "        for idx, dt in enumerate(mid_cyc_dates):\n",
    "            x, y = 1, 1\n",
    "            onlake_line, = ax[0].plot(x, y, color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))), linewidth=2)\n",
    "            onlake_lines.append(onlake_line)\n",
    "            offlake_line, = ax[0].plot(x, y, color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))), linewidth=2, alpha=0.2)\n",
    "            offlake_lines.append(offlake_line)\n",
    "\n",
    "        # Plot evolving outlines up to current date\n",
    "        for idx, dt in enumerate(evolving_geom_calcs_df['midcyc_datetime'][:date_idx + 1]):\n",
    "\n",
    "            evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "            offlake_outlines_dt = offlake_outlines_gdf[offlake_outlines_gdf['midcyc_datetime'] == dt]\n",
    "            \n",
    "            if not evolving_outlines_dt.empty:\n",
    "                evolving_outlines_dt.boundary.plot(ax=ax[0], \n",
    "                    color=cmap(norm(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime'].iloc[[idx]]))),\n",
    "                    linewidth=1)\n",
    "            if not offlake_outlines_dt.empty:\n",
    "                offlake_outlines_dt.boundary.plot(ax=ax[0], \n",
    "                    color=cmap(norm(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime'].iloc[[idx]]))),\n",
    "                    linewidth=1, alpha=0.25)\n",
    "\n",
    "        # Add other map elements (evaluation boundary, stationary outline, etc.)\n",
    "        gpd.GeoDataFrame(geometry=[within_eval_poly]).boundary.plot(ax=ax[0], edgecolor='darkgray', facecolor='none', linewidth=1)\n",
    "        evolving_union_gdf.boundary.plot(ax=ax[0], color='k', linestyle='dotted', linewidth=1)\n",
    "        stationary_outlines_gdf.boundary.plot(ax=ax[0], color=stationary_color, linewidth=1, zorder=0)\n",
    "\n",
    "\n",
    "        # Panel B - Active area\n",
    "\n",
    "        ax[1].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "        ax[1].axhline(np.divide(lake_gdf['area (m^2)'], 1e6).values, \n",
    "                      color=stationary_color, linestyle='solid', linewidth=2)\n",
    "        ax[1].axhline(np.divide(evolving_union_gdf['area (m^2)'], 1e6).values, \n",
    "                      color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "        # Plot multi-colored evolving outlines time series\n",
    "        x = mdates.date2num(current_evolving_geom_calcs['midcyc_datetime'])\n",
    "        y = np.divide(current_evolving_geom_calcs['evolving_outlines_area (m^2)'], 1e6)\n",
    "        \n",
    "        # # Plot multi-colored evolving outlines time series using LineCollection from points/segments\n",
    "        # points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "        # segments = np.concatenate([points[:-1], points[1:]], axis=1) if len(points) > 1 else []\n",
    "        # if len(segments) > 0:\n",
    "        #     lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "        #     lc.set_array(x)\n",
    "        #     lc.set_linewidth(2)\n",
    "        #     ax[1].add_collection(lc)\n",
    "        # ax[1].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9, zorder=2)\n",
    "\n",
    "        # Filter out points where y is zero\n",
    "        non_zero_mask = y != 0\n",
    "        x_filtered = x[non_zero_mask]\n",
    "        y_filtered = y[non_zero_mask]\n",
    "        \n",
    "        # Create points and segments for LineCollection (only using non-zero points)\n",
    "        if len(x_filtered) > 1:  # Need at least 2 points to create segments\n",
    "            points = np.array([x_filtered, y_filtered]).T.reshape(-1, 1, 2)\n",
    "            segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "            \n",
    "            # Create a LineCollection, using the discrete colormap and norm\n",
    "            lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "            lc.set_array(x_filtered)\n",
    "            lc.set_linewidth(2)\n",
    "            # Plot multi-colored line and scatter for data points\n",
    "            line = ax[1].add_collection(lc)\n",
    "        \n",
    "        # Plot scatter points (only non-zero values)\n",
    "        scatter = ax[1].scatter(x_filtered, y_filtered, c=x_filtered, cmap=cmap, s=9, norm=norm, zorder=2)\n",
    "\n",
    "\n",
    "        # Panel C - Cumulative dh/dt\n",
    "        ax[2].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "        \n",
    "        # Plot cumulative values up to current date\n",
    "        ax[2].plot(x, np.cumsum(current_stationary_geom_calcs['stationary_outline_region_dh (m)']).astype(float), \n",
    "            color='lightgray', linestyle='solid', linewidth=1)\n",
    "        ax[2].plot(x, np.cumsum(current_evolving_geom_calcs['evolving_outlines_region_dh (m)']), \n",
    "            color='dimgray', linestyle='solid', linewidth=1)\n",
    "        ax[2].plot(x, np.cumsum(current_stationary_geom_calcs['stationary_outline_dh_corr (m)']), \n",
    "            color=stationary_color, linestyle='solid', linewidth=1)\n",
    "        ax[2].plot(x, np.cumsum(current_evolving_union_geom_calcs['stationary_outline_dh_corr (m)']), \n",
    "            color='k', linestyle='dashed', linewidth=1)\n",
    "        ax[2].plot(x, np.cumsum(current_evolving_geom_calcs['evolving_outlines_dh_corr (m)']-\n",
    "            current_stationary_geom_calcs['stationary_outline_dh_corr (m)']), \n",
    "            color='red', linestyle='solid', linewidth=1)\n",
    "        \n",
    "        # Plot multi-colored evolving outlines time series using LineCollection from points/segments\n",
    "        y = np.cumsum(current_evolving_geom_calcs['evolving_outlines_dh_corr (m)'])\n",
    "        points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "        segments = np.concatenate([points[:-1], points[1:]], axis=1) if len(points) > 1 else []\n",
    "        if len(segments) > 0:\n",
    "            lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "            lc.set_array(x)\n",
    "            lc.set_linewidth(2)\n",
    "            ax[2].add_collection(lc)\n",
    "        ax[2].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9, zorder=2)\n",
    "\n",
    "        # Set y-axis limits\n",
    "        ax[2].set_ylim(dh_min - 0.1 * dh_range, dh_max + 0.1 * dh_range)\n",
    "\n",
    "\n",
    "        # Panel D - Cumulative dV/dt\n",
    "        ax[3].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "        \n",
    "        ax[3].plot(x, np.divide(np.cumsum(current_stationary_geom_calcs['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "            color=stationary_color, linestyle='solid', linewidth=1)\n",
    "        ax[3].plot(x, np.divide(np.cumsum(current_evolving_union_geom_calcs['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "            color='k', linestyle='dashed', linewidth=1)\n",
    "        ax[3].plot(x, np.divide(np.cumsum(current_evolving_geom_calcs['evolving_outlines_dV_corr (m^3)']-\n",
    "            current_stationary_geom_calcs['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "            color='red', linestyle='solid', linewidth=1)\n",
    "\n",
    "        # Plot multi-colored evolving outlines time series using LineCollection from points/segments\n",
    "        y = np.cumsum(np.divide(current_evolving_geom_calcs['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "        points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "        segments = np.concatenate([points[:-1], points[1:]], axis=1) if len(points) > 1 else []\n",
    "        if len(segments) > 0:\n",
    "            lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "            lc.set_array(x)\n",
    "            lc.set_linewidth(2)\n",
    "            ax[3].add_collection(lc)\n",
    "        ax[3].scatter(x, y, c=x, cmap=cmap, norm=norm, s=10, zorder=2)\n",
    "\n",
    "        # Add legends\n",
    "        evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=1)\n",
    "        within_eval_line = plt.Line2D([], [], color='darkgray', linestyle='solid', linewidth=1)\n",
    "        stationary_region_patch = mpatches.Patch(color=stationary_color, alpha=0.2)\n",
    "        evolving_union_region_patch = mpatches.Patch(color='k', alpha=0.2)\n",
    "      \n",
    "        legend = ax[0].legend([tuple(onlake_lines),\n",
    "                               tuple(offlake_lines),\n",
    "                               evolving_union_line, \n",
    "                               stationary_line, \n",
    "                               within_eval_line,\n",
    "                               stationary_region_patch,\n",
    "                               evolving_union_region_patch], \n",
    "            [f'evolving outlines ({evolving_outlines_gdf.level[0]} m)',\n",
    "             'off-lake evolving outlines', \n",
    "             'evolving outlines union',\n",
    "             'stationary outline',\n",
    "             f'within evaluation line ({int(evolving_outlines_gdf.within_area_multiple[0])}x)',\n",
    "             'stationary region',\n",
    "             'evolving union region'],\n",
    "            handlelength=3,\n",
    "            handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "            loc='upper left',\n",
    "            bbox_to_anchor=(0, 1.5))\n",
    "        \n",
    "        legend = ax[1].legend([tuple(onlake_lines), \n",
    "                               evolving_union_line, \n",
    "                               stationary_line],\n",
    "            ['evolving outlines', \n",
    "             'evolving outlines union', \n",
    "             'stationary outline'], \n",
    "            handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "            fontsize='small', loc='upper left',\n",
    "            bbox_to_anchor=(0, 1.15))\n",
    "\n",
    "        evolving_region = plt.Line2D([], [], color='dimgray', linestyle='solid', linewidth=1)\n",
    "        stationary_region = plt.Line2D([], [], color='lightgray', linestyle='solid', linewidth=1)\n",
    "        evolving_union_line = plt.Line2D([], [], color='k', linestyle='dashed', linewidth=1)\n",
    "        bias = plt.Line2D([], [], color='red', linestyle='solid', linewidth=1)\n",
    "        legend = ax[2].legend(\n",
    "            [evolving_region,\n",
    "             stationary_region,\n",
    "             tuple(onlake_lines),\n",
    "             evolving_union_line,\n",
    "             stationary_line,  \n",
    "             bias],\n",
    "            ['evolving outlines region',\n",
    "             'stationary outline region',\n",
    "             'evolving outlines',\n",
    "             'evolving outlines union',\n",
    "             'stationary outline', \n",
    "             'bias (evolving - stationary)'],\n",
    "             handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "             fontsize='small', loc='upper left',\n",
    "             bbox_to_anchor=(0, 1.22))\n",
    "\n",
    "        legend = ax[3].legend([tuple(onlake_lines), \n",
    "                               evolving_union_line, \n",
    "                               stationary_line, \n",
    "                               bias],\n",
    "            ['evolving outlines',\n",
    "             'evolving outlines union',\n",
    "             'stationary outline',\n",
    "             'bias (evolving - stationary)'], \n",
    "            handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "            fontsize='small', loc='upper left',\n",
    "            bbox_to_anchor=(0, 1.15))\n",
    "\n",
    "        # Set y-axis limits\n",
    "        ax[3].set_ylim(dv_min - 0.1 * dv_range, dv_max + 0.1 * dv_range)\n",
    "\n",
    "        # # Set common attributes for time series panels\n",
    "        # for i in range(1, 4):\n",
    "        #     ax[i].set_xlim(mdates.date2num(np.min(cyc_start_dates)), mdates.date2num(np.max(cyc_end_dates)))\n",
    "        #     ax[i].xaxis.set_major_locator(mdates.YearLocator(base=2))\n",
    "        #     ax[i].xaxis.set_minor_locator(mdates.YearLocator(base=1))\n",
    "        #     ax[i].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "        #     ax[i].set_xlabel('year')\n",
    "        for i in range(1, ncols):\n",
    "            # Set x-axis limits\n",
    "            ax[i].set_xlim(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "            \n",
    "            # Set ticks for all years but labels only for even years (to match colorbar)\n",
    "            tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "            tick_labels = [date.strftime('%Y') if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "            ax[i].set_xticks(tick_locations)\n",
    "            ax[i].set_xticklabels(tick_labels)\n",
    "            \n",
    "            # Add minor ticks for quarters\n",
    "            ax[i].xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))\n",
    "            \n",
    "            ax[i].set_xlabel('year')\n",
    "\n",
    "        # Set titles\n",
    "        ax[0].set_title(f'{lake_name}\\nmid-cyc date: {current_date_pd.strftime(\"%Y-%m-%d\")}', size=12, y=1.5)\n",
    "        ax[1].set_title('active area [km$^2$]', size=12, y=1.15)\n",
    "        ax[2].set_title('cumulative dh [m]', size=12, y=1.22)\n",
    "        ax[3].set_title('cumulative dV [km$^3$]', size=12, y=1.15)\n",
    "\n",
    "        # Save and close\n",
    "        plt.savefig(os.path.join(sequential_dir, f'{lake_name}_{current_date_pd.strftime(\"%Y%m%d\")}.png'), \n",
    "                    dpi=300, bbox_inches='tight')\n",
    "\n",
    "        # Clean up to conserve memory\n",
    "        current_evolving_geom_calcs = None\n",
    "        current_stationary_geom_calcs = None\n",
    "        evolving_outlines_dt = None\n",
    "        offlake_outlines_dt = None\n",
    "        del current_dh, fig\n",
    "        if 'img' in locals():  # Only delete img if it exists\n",
    "            del img\n",
    "\n",
    "        # Explicitly clear the figure, close, and clear any reference to it\n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "\n",
    "        # Force garbage collection\n",
    "        gc.collect() \n",
    "        \n",
    "    # Convert images to video\n",
    "    try:\n",
    "        video_from_images_2(lake_gdf, output_dir=OUTPUT_DIR, \n",
    "            fps=1, img_extension='png')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating video for {lake_name}: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # Clear output\n",
    "    clear_output(wait=True)\n",
    "\n",
    "def video_from_images_2(lake_gdf, output_dir=OUTPUT_DIR, fps=1, img_extension='png'):\n",
    "    \"\"\"\n",
    "    Creates a video from still images stored in a folder based on the lake_gdf input, then deletes the images.\n",
    "    \n",
    "    Parameters:\n",
    "    - lake_gdf: GeoDataFrame containing lake information (expects a 'name' column).\n",
    "    - output_dir: Base directory where the images and video are stored/created.\n",
    "    - fps: Frames per second for the output video.\n",
    "    - img_extension: Extension of the images to look for in the folder.\n",
    "\n",
    "    # Example usage\n",
    "    video_from_images(lake_gdf, OUTPUT_DIR, fps=0.5, img_extension='png')\n",
    "    \"\"\"\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    print('Making video for', lake_name)\n",
    "\n",
    "    # Derive paths based on lake_gdf\n",
    "    images_folder = os.path.join(OUTPUT_DIR, f\"plot_evolving_and_stationary_comparison_sequential/{lake_name}\")\n",
    "    output_video_file = os.path.join(OUTPUT_DIR, \n",
    "        f\"plot_evolving_and_stationary_comparison_sequential/{lake_name}.mp4\")\n",
    "    \n",
    "    # Get all images in the folder with the specified extension\n",
    "    image_files = glob.glob(os.path.join(images_folder, f\"*.{img_extension}\"))\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_folder} with extension {img_extension}\")\n",
    "        return\n",
    "    \n",
    "    # Read the first image to determine the video size\n",
    "    frame = cv2.imread(image_files[0])\n",
    "    if frame is None:\n",
    "        print(f\"Could not read the image {image_files[0]}\")\n",
    "        return\n",
    "    height, width, layers = frame.shape\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(output_video_file, fourcc, fps, (width, height))\n",
    "\n",
    "    for image_file in sorted(image_files):\n",
    "        frame = cv2.imread(image_file)\n",
    "        if frame is not None:\n",
    "            video.write(frame)\n",
    "            # Clear the frame from memory\n",
    "            frame = None\n",
    "            gc.collect()\n",
    "\n",
    "    # Release the VideoWriter object\n",
    "    video.release()\n",
    "    video = None\n",
    "    gc.collect()\n",
    "    print(f\"Video file {output_video_file} created successfully.\")\n",
    "\n",
    "    # Delete the images in the directory\n",
    "    for image_file in image_files:\n",
    "        os.remove(image_file)\n",
    "    print(f\"Deleted {len(image_files)} image(s) from {images_folder}\")\n",
    "\n",
    "    # Force delete the folder and its contents\n",
    "    try:\n",
    "        shutil.rmtree(images_folder)\n",
    "        print(f\"Deleted folder and all contents: {images_folder}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not delete folder {images_folder}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f27227c0-08f3-49ac-b03a-ea27195fd984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_evolving_union(lake_ps, evolving_outlines_gdf, incl_stationary=True):\n",
    "    '''\n",
    "    Find the union of evolving outlines and optionally the corresponding stationary\n",
    "    outline and return as a GeoSeries.\n",
    "    \n",
    "    Args:\n",
    "    lake_ps: Pandas series of lake row from stationary lakes geodataframe\n",
    "    evolving_outlines_gdf: GeoDataFrame containing evolving outlines\n",
    "    incl_stationary: Boolean indicating whether to include stationary outline\n",
    "    Returns:\n",
    "    GeoSeries containing the union of the outlines, or None if an error occurs.\n",
    "    '''\n",
    "    lake_name = lake_ps['name']\n",
    "        \n",
    "    try:\n",
    "        if incl_stationary:\n",
    "            # Create a temporary GeoDataFrame for union\n",
    "            temp_gdf = gpd.GeoDataFrame(\n",
    "                geometry=[lake_ps['geometry']] + evolving_outlines_gdf['geometry'].tolist(),\n",
    "                crs=evolving_outlines_gdf.crs\n",
    "            )\n",
    "        else: \n",
    "            temp_gdf = evolving_outlines_gdf[['geometry']].copy()\n",
    "            \n",
    "        # Use union_all() method to find the union of outlines\n",
    "        outlines_union = temp_gdf.geometry.union_all()\n",
    "        \n",
    "        # Create a new GeoSeries with the lake name as index\n",
    "        result = gpd.GeoSeries([outlines_union], index=[lake_name], crs=evolving_outlines_gdf.crs)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating union for {lake_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ec15b-a338-457d-863f-233b4a703bb1",
   "metadata": {},
   "source": [
    "## process_continental_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1b602a77-1795-4485-b4d4-23f0ceb38735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_continental_sums(geom_calc_type):\n",
    "    \"\"\"\n",
    "    Process lake data for a specific comparison type directory.\n",
    "    \n",
    "    Args:\n",
    "        geom_calc_type (str): Type of comparison being performed\n",
    "    \"\"\"\n",
    "    # Define directory\n",
    "    directory = os.path.join('output/geometric_calcs', geom_calc_type)\n",
    "    \n",
    "    # Initialize lists for different lake categories\n",
    "    dfs_superset_IS2_lakes = []\n",
    "    dfs_subset_noCS2_IS2_lakes = []\n",
    "    dfs_subset_CS2_IS2_lakes_SARInPreExpansion = []\n",
    "    dfs_subset_CS2_IS2_lakes_SARInPostExpansion = []\n",
    "    \n",
    "    # Process each CSV file\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            lake_name = os.path.splitext(file)[0]\n",
    "            lake_row = revised_stationary_outlines_gdf[revised_stationary_outlines_gdf['name'] == lake_name]\n",
    "            \n",
    "            if not lake_row.empty:\n",
    "                dfs_superset_IS2_lakes.append(df)\n",
    "                \n",
    "                SARIn_date = lake_row['CS2_SARIn_start'].values[0]\n",
    "                if SARIn_date == '<NA>':\n",
    "                    dfs_subset_noCS2_IS2_lakes.append(df)\n",
    "                if SARIn_date in ['2010.5']:\n",
    "                    dfs_subset_CS2_IS2_lakes_SARInPreExpansion.append(df)\n",
    "                if SARIn_date in ['2010.5', '2013.75']:\n",
    "                    dfs_subset_CS2_IS2_lakes_SARInPostExpansion.append(df)\n",
    "    \n",
    "    # Process and save each subset\n",
    "    for subset_name, dfs_list in [\n",
    "        ('subset_noCS2_IS2_lakes', dfs_subset_noCS2_IS2_lakes),\n",
    "        ('subset_CS2_IS2_lakes_SARInPreExpansion', dfs_subset_CS2_IS2_lakes_SARInPreExpansion),\n",
    "        ('subset_CS2_IS2_lakes_SARInPostExpansion', dfs_subset_CS2_IS2_lakes_SARInPostExpansion)\n",
    "    ]:\n",
    "        if dfs_list:\n",
    "            df_concat = pd.concat(dfs_list, ignore_index=True)\n",
    "            df_sum = df_concat.groupby('midcyc_datetime').sum().reset_index()\n",
    "            output_path = os.path.join(directory, f'{subset_name}_sum.csv')\n",
    "            df_sum.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Process superset data\n",
    "    if dfs_superset_IS2_lakes:\n",
    "        superset_IS2_lakes = pd.concat(dfs_superset_IS2_lakes, ignore_index=True)\n",
    "        superset_IS2_lakes_sum = superset_IS2_lakes.groupby('midcyc_datetime').sum().reset_index()\n",
    "        superset_IS2_lakes_sum['midcyc_datetime'] = pd.to_datetime(superset_IS2_lakes_sum['midcyc_datetime'])\n",
    "        threshold = pd.Timestamp('2019-01-01 06:00:00')\n",
    "        superset_IS2_lakes_sum = superset_IS2_lakes_sum[\n",
    "            superset_IS2_lakes_sum['midcyc_datetime'] >= threshold\n",
    "        ].reset_index(drop=True)\n",
    "        output_path = os.path.join(directory, 'superset_IS2_lakes_sum.csv')\n",
    "        superset_IS2_lakes_sum.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8476ed5e-f7d9-43a2-8e21-e11ffbf0bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def even_year_formatter(x, pos):\n",
    "    '''\n",
    "    Create custom formatter that only labels even years\n",
    "    '''\n",
    "    date = mdates.num2date(x)\n",
    "    if date.year % 2 == 0:\n",
    "        return date.strftime('%Y')\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5e0e1c-b26e-4ee3-87eb-ceea82bc5d73",
   "metadata": {},
   "source": [
    "# Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "48d1542f-fb79-4965-a13f-3c84a3fa69fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import subglacial lake outlines \n",
    "stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/stationary_outlines_gdf.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1720b14b-ed5a-4870-b3f9-e9429a07e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CryoSat-2 SARIn mode mask\n",
    "# See 0_preprocess_data.ipynb for data source and pre-processing steps\n",
    "gdf_SARIn_3_1 = gpd.read_file('output/CS2_SARIn_mode_masks/gdf_SARIn_3_1.geojson')\n",
    "gdf_SARIn_3_1_3_6_diff= gpd.read_file('output/CS2_SARIn_mode_masks/gdf_SARIn_3_1_3_6_diff.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c3c97b1e-bbd7-4f54-a26d-21d2aa3c9c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODIS MOA 2014 coastline and grounding line\n",
    "# https://nsidc.org/data/nsidc-0730/versions/1\n",
    "shp = DATA_DIR + '/boundaries/MODIS_MOA/2014/moa2014_coastline_v01.shp' \n",
    "moa_2014_coastline = gpd.read_file(shp)\n",
    "shp = DATA_DIR + '/boundaries/MODIS_MOA/2014/moa2014_grounding_line_v01.shp' \n",
    "moa_2014_groundingline = gpd.read_file(shp)\n",
    "# moa_2014_groundingline['geometry'] = moa_2014_groundingline.buffer(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2dc7100d-ceb8-441c-9fe2-5988420e3d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Authenticate with Earthdata Login\n",
    "# auth = earthaccess.login()\n",
    "\n",
    "# # Find MEaSUREs MODIS Mosaic of Antarctica 2013-2014 (MOA2014) Image Map, Version 1\n",
    "# results = earthaccess.search_data(\n",
    "#     doi='10.5067/RNF17BP824UM',\n",
    "#     version=1,\n",
    "#     # bounding_box=(180, -90, -180, -60),  # (lower_left_lon, lower_left_lat , upper_right_lon, upper_right_lat))\n",
    "#     cloud_hosted=True,\n",
    "# )\n",
    "\n",
    "# # Open data granules as s3 files to stream\n",
    "# files = earthaccess.open(results)\n",
    "\n",
    "# # Struggling to find stream; opting for local copy for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e44c162d-b4d2-419f-81cd-945373956006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MODIS Mosaic of Antarctica (MOA) surface imagery\n",
    "# https://nsidc.org/data/nsidc-0730/versions/1\n",
    "\n",
    "moa_highres = DATA_DIR + '/surface_imagery/MODIS_MOA/2014/moa125_2014_hp1_v01.tif'\n",
    "moa_highres_da = rioxarray.open_rasterio(moa_highres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b7fd6faa-738f-4689-abe0-38324f66d6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "html[data-theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 14GB\n",
       "Dimensions:      (y: 4451, x: 5451, time: 35)\n",
       "Coordinates:\n",
       "  * y            (y) float64 36kB -2.185e+06 -2.184e+06 ... 2.264e+06 2.265e+06\n",
       "  * x            (x) float64 44kB -2.665e+06 -2.664e+06 ... 2.784e+06 2.785e+06\n",
       "    spatial_ref  int64 8B 0\n",
       "  * time         (time) datetime64[ns] 280B 2010-07-02T15:00:00 ... 2019-01-01\n",
       "Data variables:\n",
       "    mask         (y, x) float64 194MB ...\n",
       "    delta_h      (time, y, x) float64 7GB ...\n",
       "    data_count   (time, y, x) float64 7GB ...\n",
       "Attributes:\n",
       "    fileName:                mos_2010.5_2021.5.h5\n",
       "    shortName:               CS2-Smith-2017\n",
       "    identifier_product_DOI:  doi:10.5194/tc-11-451-2017</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-9ae2057c-deb5-4697-b61e-4f370d031b14' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-9ae2057c-deb5-4697-b61e-4f370d031b14' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>y</span>: 4451</li><li><span class='xr-has-index'>x</span>: 5451</li><li><span class='xr-has-index'>time</span>: 35</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-09ebf02f-7482-495c-b950-2e5f5c4b1e71' class='xr-section-summary-in' type='checkbox'  checked><label for='section-09ebf02f-7482-495c-b950-2e5f5c4b1e71' class='xr-section-summary' >Coordinates: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>y</span></div><div class='xr-var-dims'>(y)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-2.185e+06 -2.184e+06 ... 2.265e+06</div><input id='attrs-f0738b0c-7594-405b-b8d0-6a426e394143' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-f0738b0c-7594-405b-b8d0-6a426e394143' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-fb07679c-b71f-49a1-b757-644facb27d07' class='xr-var-data-in' type='checkbox'><label for='data-fb07679c-b71f-49a1-b757-644facb27d07' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([-2185000., -2184000., -2183000., ...,  2263000.,  2264000.,  2265000.])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>x</span></div><div class='xr-var-dims'>(x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-2.665e+06 -2.664e+06 ... 2.785e+06</div><input id='attrs-e6434c35-4eb1-4021-b836-28e72c67b2bf' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-e6434c35-4eb1-4021-b836-28e72c67b2bf' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-772326be-5e7b-487e-8501-a9744ffb3064' class='xr-var-data-in' type='checkbox'><label for='data-772326be-5e7b-487e-8501-a9744ffb3064' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([-2665000., -2664000., -2663000., ...,  2783000.,  2784000.,  2785000.])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>spatial_ref</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0</div><input id='attrs-98c98fb7-ae11-4412-b582-60ad60647fd2' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-98c98fb7-ae11-4412-b582-60ad60647fd2' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-866903c6-dcbe-4867-ad24-462d8a6d45f9' class='xr-var-data-in' type='checkbox'><label for='data-866903c6-dcbe-4867-ad24-462d8a6d45f9' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>crs_wkt :</span></dt><dd>PROJCS[&quot;WGS 84 / Antarctic Polar Stereographic&quot;,GEOGCS[&quot;WGS 84&quot;,DATUM[&quot;WGS_1984&quot;,SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563,AUTHORITY[&quot;EPSG&quot;,&quot;7030&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;6326&quot;]],PRIMEM[&quot;Greenwich&quot;,0,AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]],UNIT[&quot;degree&quot;,0.0174532925199433,AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]],PROJECTION[&quot;Polar_Stereographic&quot;],PARAMETER[&quot;latitude_of_origin&quot;,-71],PARAMETER[&quot;central_meridian&quot;,0],PARAMETER[&quot;false_easting&quot;,0],PARAMETER[&quot;false_northing&quot;,0],UNIT[&quot;metre&quot;,1,AUTHORITY[&quot;EPSG&quot;,&quot;9001&quot;]],AXIS[&quot;Easting&quot;,NORTH],AXIS[&quot;Northing&quot;,NORTH],AUTHORITY[&quot;EPSG&quot;,&quot;3031&quot;]]</dd><dt><span>semi_major_axis :</span></dt><dd>6378137.0</dd><dt><span>semi_minor_axis :</span></dt><dd>6356752.314245179</dd><dt><span>inverse_flattening :</span></dt><dd>298.257223563</dd><dt><span>reference_ellipsoid_name :</span></dt><dd>WGS 84</dd><dt><span>longitude_of_prime_meridian :</span></dt><dd>0.0</dd><dt><span>prime_meridian_name :</span></dt><dd>Greenwich</dd><dt><span>geographic_crs_name :</span></dt><dd>WGS 84</dd><dt><span>horizontal_datum_name :</span></dt><dd>World Geodetic System 1984</dd><dt><span>projected_crs_name :</span></dt><dd>WGS 84 / Antarctic Polar Stereographic</dd><dt><span>grid_mapping_name :</span></dt><dd>polar_stereographic</dd><dt><span>standard_parallel :</span></dt><dd>-71.0</dd><dt><span>straight_vertical_longitude_from_pole :</span></dt><dd>0.0</dd><dt><span>false_easting :</span></dt><dd>0.0</dd><dt><span>false_northing :</span></dt><dd>0.0</dd><dt><span>spatial_ref :</span></dt><dd>PROJCS[&quot;WGS 84 / Antarctic Polar Stereographic&quot;,GEOGCS[&quot;WGS 84&quot;,DATUM[&quot;WGS_1984&quot;,SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563,AUTHORITY[&quot;EPSG&quot;,&quot;7030&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;6326&quot;]],PRIMEM[&quot;Greenwich&quot;,0,AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]],UNIT[&quot;degree&quot;,0.0174532925199433,AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]],PROJECTION[&quot;Polar_Stereographic&quot;],PARAMETER[&quot;latitude_of_origin&quot;,-71],PARAMETER[&quot;central_meridian&quot;,0],PARAMETER[&quot;false_easting&quot;,0],PARAMETER[&quot;false_northing&quot;,0],UNIT[&quot;metre&quot;,1,AUTHORITY[&quot;EPSG&quot;,&quot;9001&quot;]],AXIS[&quot;Easting&quot;,NORTH],AXIS[&quot;Northing&quot;,NORTH],AUTHORITY[&quot;EPSG&quot;,&quot;3031&quot;]]</dd></dl></div><div class='xr-var-data'><pre>array(0)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2010-07-02T15:00:00 ... 2019-01-01</div><input id='attrs-dbb21a77-b4be-424f-aa85-08ac9e469b69' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-dbb21a77-b4be-424f-aa85-08ac9e469b69' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-537337fb-74fe-4498-903d-407cde1bddf3' class='xr-var-data-in' type='checkbox'><label for='data-537337fb-74fe-4498-903d-407cde1bddf3' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>description :</span></dt><dd>Time for each node</dd></dl></div><div class='xr-var-data'><pre>array([&#x27;2010-07-02T15:00:00.000000000&#x27;, &#x27;2010-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2011-01-01T00:00:00.000000000&#x27;, &#x27;2011-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2011-07-02T15:00:00.000000000&#x27;, &#x27;2011-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2012-01-01T00:00:00.000000000&#x27;, &#x27;2012-04-01T07:30:00.000000000&#x27;,\n",
       "       &#x27;2012-07-01T15:00:00.000000000&#x27;, &#x27;2012-09-30T22:30:00.000000000&#x27;,\n",
       "       &#x27;2013-01-01T00:00:00.000000000&#x27;, &#x27;2013-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2013-07-02T15:00:00.000000000&#x27;, &#x27;2013-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2014-01-01T00:00:00.000000000&#x27;, &#x27;2014-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2014-07-02T15:00:00.000000000&#x27;, &#x27;2014-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2015-01-01T00:00:00.000000000&#x27;, &#x27;2015-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2015-07-02T15:00:00.000000000&#x27;, &#x27;2015-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2016-01-01T00:00:00.000000000&#x27;, &#x27;2016-04-01T07:30:00.000000000&#x27;,\n",
       "       &#x27;2016-07-01T15:00:00.000000000&#x27;, &#x27;2016-09-30T22:30:00.000000000&#x27;,\n",
       "       &#x27;2017-01-01T00:00:00.000000000&#x27;, &#x27;2017-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2017-07-02T15:00:00.000000000&#x27;, &#x27;2017-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2018-01-01T00:00:00.000000000&#x27;, &#x27;2018-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2018-07-02T15:00:00.000000000&#x27;, &#x27;2018-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2019-01-01T00:00:00.000000000&#x27;], dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-a7130b3b-ef60-4515-8d13-9014a92443ca' class='xr-section-summary-in' type='checkbox'  checked><label for='section-a7130b3b-ef60-4515-8d13-9014a92443ca' class='xr-section-summary' >Data variables: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>mask</span></div><div class='xr-var-dims'>(y, x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-0e24e76c-b63a-48b5-9bf7-3b0a5cc4cc02' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-0e24e76c-b63a-48b5-9bf7-3b0a5cc4cc02' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-ead82b22-3ea4-4573-9cfd-2811daa1595d' class='xr-var-data-in' type='checkbox'><label for='data-ead82b22-3ea4-4573-9cfd-2811daa1595d' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>description: :</span></dt><dd>Data mask. 0: unknown, 1: unknown, nan: nan</dd></dl></div><div class='xr-var-data'><pre>[24262401 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>delta_h</span></div><div class='xr-var-dims'>(time, y, x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-65aa59b5-b8fd-41c5-998b-c4a727d94d0e' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-65aa59b5-b8fd-41c5-998b-c4a727d94d0e' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-6a00d684-9763-4ad1-88c7-4ab8aaaf9769' class='xr-var-data-in' type='checkbox'><label for='data-6a00d684-9763-4ad1-88c7-4ab8aaaf9769' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>description: :</span></dt><dd>Height change relative to the datum (2016) surface</dd><dt><span>units :</span></dt><dd>m</dd></dl></div><div class='xr-var-data'><pre>[849184035 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>data_count</span></div><div class='xr-var-dims'>(time, y, x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-c6dc59db-682d-4a85-a5fb-3c2836ad415e' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-c6dc59db-682d-4a85-a5fb-3c2836ad415e' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-8b3edbc7-d7e2-4a21-b2fa-7913c9bca506' class='xr-var-data-in' type='checkbox'><label for='data-8b3edbc7-d7e2-4a21-b2fa-7913c9bca506' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>description: :</span></dt><dd>Data mask. 0: bare ground or ocean?, 1: ice?, nan: nan</dd></dl></div><div class='xr-var-data'><pre>[849184035 values with dtype=float64]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-9043c678-9861-4e57-8703-14234c4fd8ce' class='xr-section-summary-in' type='checkbox'  ><label for='section-9043c678-9861-4e57-8703-14234c4fd8ce' class='xr-section-summary' >Indexes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>y</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-e8d7da84-5479-4a73-a790-c0ca68f72cad' class='xr-index-data-in' type='checkbox'/><label for='index-e8d7da84-5479-4a73-a790-c0ca68f72cad' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([-2185000.0, -2184000.0, -2183000.0, -2182000.0, -2181000.0, -2180000.0,\n",
       "       -2179000.0, -2178000.0, -2177000.0, -2176000.0,\n",
       "       ...\n",
       "        2256000.0,  2257000.0,  2258000.0,  2259000.0,  2260000.0,  2261000.0,\n",
       "        2262000.0,  2263000.0,  2264000.0,  2265000.0],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;y&#x27;, length=4451))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>x</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-b6ce073c-c27d-49b7-8ba4-c53d57f4ddeb' class='xr-index-data-in' type='checkbox'/><label for='index-b6ce073c-c27d-49b7-8ba4-c53d57f4ddeb' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([-2665000.0, -2664000.0, -2663000.0, -2662000.0, -2661000.0, -2660000.0,\n",
       "       -2659000.0, -2658000.0, -2657000.0, -2656000.0,\n",
       "       ...\n",
       "        2776000.0,  2777000.0,  2778000.0,  2779000.0,  2780000.0,  2781000.0,\n",
       "        2782000.0,  2783000.0,  2784000.0,  2785000.0],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;x&#x27;, length=5451))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>time</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-c553c89a-f3f5-4fa0-a2de-18227f16e1a2' class='xr-index-data-in' type='checkbox'/><label for='index-c553c89a-f3f5-4fa0-a2de-18227f16e1a2' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(DatetimeIndex([&#x27;2010-07-02 15:00:00&#x27;, &#x27;2010-10-01 22:30:00&#x27;,\n",
       "               &#x27;2011-01-01 00:00:00&#x27;, &#x27;2011-04-02 07:30:00&#x27;,\n",
       "               &#x27;2011-07-02 15:00:00&#x27;, &#x27;2011-10-01 22:30:00&#x27;,\n",
       "               &#x27;2012-01-01 00:00:00&#x27;, &#x27;2012-04-01 07:30:00&#x27;,\n",
       "               &#x27;2012-07-01 15:00:00&#x27;, &#x27;2012-09-30 22:30:00&#x27;,\n",
       "               &#x27;2013-01-01 00:00:00&#x27;, &#x27;2013-04-02 07:30:00&#x27;,\n",
       "               &#x27;2013-07-02 15:00:00&#x27;, &#x27;2013-10-01 22:30:00&#x27;,\n",
       "               &#x27;2014-01-01 00:00:00&#x27;, &#x27;2014-04-02 07:30:00&#x27;,\n",
       "               &#x27;2014-07-02 15:00:00&#x27;, &#x27;2014-10-01 22:30:00&#x27;,\n",
       "               &#x27;2015-01-01 00:00:00&#x27;, &#x27;2015-04-02 07:30:00&#x27;,\n",
       "               &#x27;2015-07-02 15:00:00&#x27;, &#x27;2015-10-01 22:30:00&#x27;,\n",
       "               &#x27;2016-01-01 00:00:00&#x27;, &#x27;2016-04-01 07:30:00&#x27;,\n",
       "               &#x27;2016-07-01 15:00:00&#x27;, &#x27;2016-09-30 22:30:00&#x27;,\n",
       "               &#x27;2017-01-01 00:00:00&#x27;, &#x27;2017-04-02 07:30:00&#x27;,\n",
       "               &#x27;2017-07-02 15:00:00&#x27;, &#x27;2017-10-01 22:30:00&#x27;,\n",
       "               &#x27;2018-01-01 00:00:00&#x27;, &#x27;2018-04-02 07:30:00&#x27;,\n",
       "               &#x27;2018-07-02 15:00:00&#x27;, &#x27;2018-10-01 22:30:00&#x27;,\n",
       "               &#x27;2019-01-01 00:00:00&#x27;],\n",
       "              dtype=&#x27;datetime64[ns]&#x27;, name=&#x27;time&#x27;, freq=None))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-e50e6110-d09e-405e-b7ad-ac422979ed5d' class='xr-section-summary-in' type='checkbox'  checked><label for='section-e50e6110-d09e-405e-b7ad-ac422979ed5d' class='xr-section-summary' >Attributes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>fileName :</span></dt><dd>mos_2010.5_2021.5.h5</dd><dt><span>shortName :</span></dt><dd>CS2-Smith-2017</dd><dt><span>identifier_product_DOI :</span></dt><dd>doi:10.5194/tc-11-451-2017</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset> Size: 14GB\n",
       "Dimensions:      (y: 4451, x: 5451, time: 35)\n",
       "Coordinates:\n",
       "  * y            (y) float64 36kB -2.185e+06 -2.184e+06 ... 2.264e+06 2.265e+06\n",
       "  * x            (x) float64 44kB -2.665e+06 -2.664e+06 ... 2.784e+06 2.785e+06\n",
       "    spatial_ref  int64 8B 0\n",
       "  * time         (time) datetime64[ns] 280B 2010-07-02T15:00:00 ... 2019-01-01\n",
       "Data variables:\n",
       "    mask         (y, x) float64 194MB ...\n",
       "    delta_h      (time, y, x) float64 7GB ...\n",
       "    data_count   (time, y, x) float64 7GB ...\n",
       "Attributes:\n",
       "    fileName:                mos_2010.5_2021.5.h5\n",
       "    shortName:               CS2-Smith-2017\n",
       "    identifier_product_DOI:  doi:10.5194/tc-11-451-2017"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Smith and others, 2017, TC method CryoSat-2 SARIn height and dh data (closed source aquired from Ben Smith)\n",
    "CS2_Smith2017 = xr.open_dataset(DATA_DIR + '/altimetry/CryoSat2/CS2_SARIn_Smith2017method/mos_2010.5_2019.0.nc')\n",
    "\n",
    "# Assign CRS\n",
    "CS2_Smith2017.rio.write_crs(\"EPSG:3031\", inplace=True)\n",
    "CS2_Smith2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8d5dd4c7-cb4d-4e98-a6e5-2396278c2647",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55e2a05623e4eb0bc598a1277b046ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2319ae797ee54ad691831aaa089c9793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54217b43e6f4bfca1cb9834cb860cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Authenticate with Earthdata Login\n",
    "# auth = earthaccess.login()\n",
    "earthaccess.login()\n",
    "\n",
    "# Find ICESat-2 ATL15 v004 data granules\n",
    "results = earthaccess.search_data(\n",
    "    doi='10.5067/ATLAS/ATL15.004',\n",
    "    bounding_box=(180, -90, -180, -60),  # (lower_left_lon, lower_left_lat , upper_right_lon, upper_right_lat))\n",
    "    cloud_hosted=True,\n",
    ")\n",
    "\n",
    "# Open data granules as s3 files to stream\n",
    "files = earthaccess.open(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "21ffb0a5-9651-474e-b47b-02340c93c87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A1_0324_01km_004_04.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A2_0324_01km_004_04.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A3_0324_01km_004_04.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A4_0324_01km_004_04.nc>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter to 1-km resolution data sets\n",
    "filtered_files = [f for f in files if '01km' in str(f)]\n",
    "\n",
    "# Filter to the specific revision number\n",
    "filtered_files = [f for f in filtered_files if '_04' in str(f)]\n",
    "\n",
    "# Delete intermediary objects for memory conservation\n",
    "del results, files\n",
    "\n",
    "# Sort alphabetically by the data set file name\n",
    "filtered_files.sort(key=lambda x: str(x).split('/')[-1])\n",
    "\n",
    "# Display filtered list\n",
    "filtered_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "69c4b4f9-6096-40de-92bb-12a1d9688890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Specify the variables to keep\n",
    "# variables_to_keep = ['time', 'y', 'x', 'delta_h', 'data_count']\n",
    "\n",
    "# # List of xarray datasets\n",
    "# datasets = [ATL15_A1, ATL15_A2, ATL15_A3, ATL15_A4]\n",
    "\n",
    "# # Function to drop variables not in variables_to_keep from a dataset\n",
    "# def drop_unwanted_variables(dataset):\n",
    "#     variables_to_drop = [var for var in dataset.variables if var not in variables_to_keep]\n",
    "#     return dataset.drop_vars(variables_to_drop)\n",
    "\n",
    "# # Apply the function to each dataset\n",
    "# ATL15_A1, ATL15_A2, ATL15_A3, ATL15_A4 = [drop_unwanted_variables(ds) for ds in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dae55ad9-3130-429d-af7c-bab068eaa9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset 1/4: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A1_0324_01km_004_04.nc>\n",
      "Opening file: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A1_0324_01km_004_04.nc> (Attempt 1)\n",
      "Successfully opened file: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A1_0324_01km_004_04.nc>\n",
      "Dropping unwanted variables from: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A1_0324_01km_004_04.nc>\n",
      "Finished processing dataset 1/4\n",
      "Processing dataset 2/4: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A2_0324_01km_004_04.nc>\n",
      "Opening file: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A2_0324_01km_004_04.nc> (Attempt 1)\n",
      "Successfully opened file: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A2_0324_01km_004_04.nc>\n",
      "Dropping unwanted variables from: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A2_0324_01km_004_04.nc>\n",
      "Finished processing dataset 2/4\n",
      "Processing dataset 3/4: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A3_0324_01km_004_04.nc>\n",
      "Opening file: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A3_0324_01km_004_04.nc> (Attempt 1)\n",
      "Successfully opened file: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A3_0324_01km_004_04.nc>\n",
      "Dropping unwanted variables from: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A3_0324_01km_004_04.nc>\n",
      "Finished processing dataset 3/4\n",
      "Processing dataset 4/4: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A4_0324_01km_004_04.nc>\n",
      "Opening file: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A4_0324_01km_004_04.nc> (Attempt 1)\n",
      "Successfully opened file: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A4_0324_01km_004_04.nc>\n",
      "Dropping unwanted variables from: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A4_0324_01km_004_04.nc>\n",
      "Finished processing dataset 4/4\n",
      "Dataset assigned to variable: ATL15_A1\n",
      "Dataset assigned to variable: ATL15_A2\n",
      "Dataset assigned to variable: ATL15_A3\n",
      "Dataset assigned to variable: ATL15_A4\n",
      "All datasets processed and assigned.\n"
     ]
    }
   ],
   "source": [
    "# Specify the variables to keep\n",
    "variables_to_keep = ['time', 'y', 'x', 'delta_h', 'data_count']\n",
    "\n",
    "# List of filtered file paths (output of the filtered_files)\n",
    "datasets_files = filtered_files\n",
    "\n",
    "# Function to drop variables not in variables_to_keep from a dataset\n",
    "def drop_unwanted_variables(dataset):\n",
    "    variables_to_drop = [var for var in dataset.variables if var not in variables_to_keep]\n",
    "    return dataset.drop_vars(variables_to_drop)\n",
    "\n",
    "# Function to open dataset safely with retries\n",
    "def safe_open_and_filter(file, group='delta_h', retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            print(f\"Opening file: {file} (Attempt {attempt + 1})\")\n",
    "            ds = xr.open_dataset(file, group=group)\n",
    "            print(f\"Successfully opened file: {file}\")\n",
    "            # Drop unwanted variables\n",
    "            print(f\"Dropping unwanted variables from: {file}\")\n",
    "            return drop_unwanted_variables(ds)\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt+1} for file {file} failed: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                print(\"Retrying...\")\n",
    "            else:\n",
    "                print(f\"Failed to open file: {file} after {retries} attempts.\")\n",
    "                raise e\n",
    "\n",
    "# Dynamically open, retry, and filter datasets\n",
    "datasets = []\n",
    "for i, file in enumerate(datasets_files):\n",
    "    print(f\"Processing dataset {i+1}/{len(datasets_files)}: {file}\")\n",
    "    datasets.append(safe_open_and_filter(file))\n",
    "    print(f\"Finished processing dataset {i+1}/{len(datasets_files)}\")\n",
    "\n",
    "# Assign filtered datasets to variables dynamically\n",
    "for i, ds in enumerate(datasets):\n",
    "    dataset_name = f\"ATL15_A{i+1}\"\n",
    "    globals()[dataset_name] = ds\n",
    "    print(f\"Dataset assigned to variable: {dataset_name}\")\n",
    "\n",
    "# Check datasets (optional)\n",
    "print(\"All datasets processed and assigned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "da23c068-f494-45bc-814e-e76e22b4729b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Open locally stored files when NSIDC cloud access isn't working\n",
    "# ATL15_A1 = xr.open_dataset(DATA_DIR + '/altimetry/ICESat2/ATL15.004-Ant/ATL15_A1_0322_01km_004_02.nc', group='delta_h')\n",
    "# ATL15_A2 = xr.open_dataset(DATA_DIR + '/altimetry/ICESat2/ATL15.004-Ant/ATL15_A2_0322_01km_004_02.nc', group='delta_h')\n",
    "# ATL15_A3 = xr.open_dataset(DATA_DIR + '/altimetry/ICESat2/ATL15.004-Ant/ATL15_A3_0322_01km_004_02.nc', group='delta_h')\n",
    "# ATL15_A4 = xr.open_dataset(DATA_DIR + '/altimetry/ICESat2/ATL15.004-Ant/ATL15_A4_0322_01km_004_02.nc', group='delta_h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f875fb41-ec6f-4e03-a659-e7e7273b98df",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use xarray concatenation to stitch two quadrants togethers\n",
    "# Use xarray index selecting to occlude the duplicated x=0 vector of data\n",
    "ATL15_A12 = xr.concat([ATL15_A2.isel(x=slice(0,-1)), ATL15_A1], dim=\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "372eec1e-3eb5-42d8-94ee-a2de74316803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use xarray concatenation to stitch two quadrants togethers\n",
    "# Use xarray index selecting to occlude the duplicated x=0 vector of data\n",
    "ATL15_A34 = xr.concat([ATL15_A3.isel(x=slice(0,-1)), ATL15_A4], dim='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0d14e491-fd5e-424b-ae91-75ba59381ec2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use xarray concatenation to stitch two quadrants togethers\n",
    "# Use xarray index selecting to occlude the duplicated x=0 vector of data\n",
    "ATL15_dh = xr.concat([ATL15_A34.isel(y=slice(0,-1)), ATL15_A12], dim='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7d62934f-4e13-404d-b2b3-4338f498cfd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete variables to reduce memory consumption\n",
    "del ATL15_A1, ATL15_A12, ATL15_A2, ATL15_A3, ATL15_A34, ATL15_A4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e80b88e5-6957-4f63-b9d9-70591d18af2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "html[data-theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 5GB\n",
       "Dimensions:     (time: 24, y: 4461, x: 5461)\n",
       "Coordinates:\n",
       "  * time        (time) datetime64[ns] 192B 2019-01-01T06:00:00 ... 2024-10-01...\n",
       "  * x           (x) float64 44kB -2.67e+06 -2.669e+06 ... 2.789e+06 2.79e+06\n",
       "  * y           (y) float64 36kB -2.19e+06 -2.189e+06 ... 2.269e+06 2.27e+06\n",
       "Data variables:\n",
       "    delta_h     (time, y, x) float32 2GB nan nan nan nan nan ... nan nan nan nan\n",
       "    data_count  (time, y, x) float32 2GB nan nan nan nan nan ... nan nan nan nan\n",
       "Attributes:\n",
       "    description:             delta_h group includes variables describing heig...\n",
       "    identifier_product_DOI:  doi:10.5067/ATLAS/ATL15.004\n",
       "    shortName:               ATL15</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-cdb9d664-5083-4f9f-9d6b-34eb1d502b65' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-cdb9d664-5083-4f9f-9d6b-34eb1d502b65' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>time</span>: 24</li><li><span class='xr-has-index'>y</span>: 4461</li><li><span class='xr-has-index'>x</span>: 5461</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-308bc2a6-b38d-4d0c-9119-5f650fa17f19' class='xr-section-summary-in' type='checkbox'  checked><label for='section-308bc2a6-b38d-4d0c-9119-5f650fa17f19' class='xr-section-summary' >Coordinates: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2019-01-01T06:00:00 ... 2024-10-...</div><input id='attrs-328a499e-1cec-417f-9d66-d6b21f158705' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-328a499e-1cec-417f-9d66-d6b21f158705' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-739c5fb5-6872-47ef-a5a5-bd98f263b565' class='xr-var-data-in' type='checkbox'><label for='data-739c5fb5-6872-47ef-a5a5-bd98f263b565' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>dimensions :</span></dt><dd>time</dd><dt><span>datatype :</span></dt><dd>float64</dd><dt><span>least_significant_digit :</span></dt><dd>None</dd><dt><span>description :</span></dt><dd>Time for each node, in days since 2018-01-01:T00.00.00 UTC</dd><dt><span>long_name :</span></dt><dd>quarterly h(t) time</dd><dt><span>source :</span></dt><dd>ATBD section 4.2</dd></dl></div><div class='xr-var-data'><pre>array([&#x27;2019-01-01T06:00:00.000000000&#x27;, &#x27;2019-04-02T13:30:00.000000000&#x27;,\n",
       "       &#x27;2019-07-02T21:00:00.000000000&#x27;, &#x27;2019-10-02T04:30:00.000000000&#x27;,\n",
       "       &#x27;2020-01-01T12:00:00.000000000&#x27;, &#x27;2020-04-01T19:30:00.000000000&#x27;,\n",
       "       &#x27;2020-07-02T03:00:00.000000000&#x27;, &#x27;2020-10-01T10:30:00.000000000&#x27;,\n",
       "       &#x27;2020-12-31T18:00:00.000000000&#x27;, &#x27;2021-04-02T01:30:00.000000000&#x27;,\n",
       "       &#x27;2021-07-02T09:00:00.000000000&#x27;, &#x27;2021-10-01T16:30:00.000000000&#x27;,\n",
       "       &#x27;2022-01-01T00:00:00.000000000&#x27;, &#x27;2022-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2022-07-02T15:00:00.000000000&#x27;, &#x27;2022-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2023-01-01T06:00:00.000000000&#x27;, &#x27;2023-04-02T13:30:00.000000000&#x27;,\n",
       "       &#x27;2023-07-02T21:00:00.000000000&#x27;, &#x27;2023-10-02T04:30:00.000000000&#x27;,\n",
       "       &#x27;2024-01-01T12:00:00.000000000&#x27;, &#x27;2024-04-01T19:30:00.000000000&#x27;,\n",
       "       &#x27;2024-07-02T03:00:00.000000000&#x27;, &#x27;2024-10-01T10:30:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>x</span></div><div class='xr-var-dims'>(x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-2.67e+06 -2.669e+06 ... 2.79e+06</div><input id='attrs-84a3b798-8b03-46e7-a48f-54ace1ff48cd' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-84a3b798-8b03-46e7-a48f-54ace1ff48cd' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-ca91cf01-df3b-45f1-8ae8-c7b6b482e9d4' class='xr-var-data-in' type='checkbox'><label for='data-ca91cf01-df3b-45f1-8ae8-c7b6b482e9d4' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>meters</dd><dt><span>dimensions :</span></dt><dd>x</dd><dt><span>datatype :</span></dt><dd>float64</dd><dt><span>least_significant_digit :</span></dt><dd>None</dd><dt><span>description :</span></dt><dd>x coordinate of the 1-km cell centers, in projected coordinates</dd><dt><span>long_name :</span></dt><dd>polar stereographic x at 1km</dd><dt><span>source :</span></dt><dd>ATBD section 3.2</dd><dt><span>grid_mapping :</span></dt><dd>Polar_Stereographic</dd><dt><span>standard_name :</span></dt><dd>projection_x_coordinate</dd></dl></div><div class='xr-var-data'><pre>array([-2670000., -2669000., -2668000., ...,  2788000.,  2789000.,  2790000.])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>y</span></div><div class='xr-var-dims'>(y)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-2.19e+06 -2.189e+06 ... 2.27e+06</div><input id='attrs-dafdb30e-4c4f-46b2-b3e4-1d06a3f9b068' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-dafdb30e-4c4f-46b2-b3e4-1d06a3f9b068' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-9920d6e5-f71d-47fe-b7bc-08c0cab3d5b6' class='xr-var-data-in' type='checkbox'><label for='data-9920d6e5-f71d-47fe-b7bc-08c0cab3d5b6' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>meters</dd><dt><span>dimensions :</span></dt><dd>y</dd><dt><span>datatype :</span></dt><dd>float64</dd><dt><span>least_significant_digit :</span></dt><dd>None</dd><dt><span>description :</span></dt><dd>y coordinate of the 1-km cell centers, in projected coordinates</dd><dt><span>long_name :</span></dt><dd>polar stereographic y at 1km</dd><dt><span>source :</span></dt><dd>ATBD section 3.2</dd><dt><span>grid_mapping :</span></dt><dd>Polar_Stereographic</dd><dt><span>standard_name :</span></dt><dd>projection_y_coordinate</dd></dl></div><div class='xr-var-data'><pre>array([-2190000., -2189000., -2188000., ...,  2268000.,  2269000.,  2270000.])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-3dee8c55-6dfc-4c47-afef-49bb9ee3f669' class='xr-section-summary-in' type='checkbox'  checked><label for='section-3dee8c55-6dfc-4c47-afef-49bb9ee3f669' class='xr-section-summary' >Data variables: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>delta_h</span></div><div class='xr-var-dims'>(time, y, x)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>nan nan nan nan ... nan nan nan nan</div><input id='attrs-43ee1e72-1a58-44c9-8afa-d5991028e4b2' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-43ee1e72-1a58-44c9-8afa-d5991028e4b2' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-370578d8-75c8-4768-bd0c-f0b4c06a2f06' class='xr-var-data-in' type='checkbox'><label for='data-370578d8-75c8-4768-bd0c-f0b4c06a2f06' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>least_significant_digit :</span></dt><dd>4</dd><dt><span>units :</span></dt><dd>meters</dd><dt><span>dimensions :</span></dt><dd>time,y,x</dd><dt><span>datatype :</span></dt><dd>float32</dd><dt><span>description :</span></dt><dd>Height change relative to the datum (Jan 1, 2020) surface</dd><dt><span>long_name :</span></dt><dd>height change  at 1 km</dd><dt><span>source :</span></dt><dd>ATBD section 3.4</dd><dt><span>grid_mapping :</span></dt><dd>Polar_Stereographic</dd></dl></div><div class='xr-var-data'><pre>array([[[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "...\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>data_count</span></div><div class='xr-var-dims'>(time, y, x)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>nan nan nan nan ... nan nan nan nan</div><input id='attrs-b8aaca19-ca1d-4482-8dae-850e5d944d27' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-b8aaca19-ca1d-4482-8dae-850e5d944d27' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-9964a372-93d6-4b62-93ef-3516f07ea592' class='xr-var-data-in' type='checkbox'><label for='data-9964a372-93d6-4b62-93ef-3516f07ea592' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>least_significant_digit :</span></dt><dd>4</dd><dt><span>units :</span></dt><dd>counts</dd><dt><span>dimensions :</span></dt><dd>time,y,x</dd><dt><span>datatype :</span></dt><dd>float32</dd><dt><span>description :</span></dt><dd>Weighted number of data contributing to each node in the 1-km height-change grid</dd><dt><span>long_name :</span></dt><dd>data count </dd><dt><span>source :</span></dt><dd>ATBD section 5.2.4.4</dd><dt><span>grid_mapping :</span></dt><dd>Polar_Stereographic</dd></dl></div><div class='xr-var-data'><pre>array([[[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "...\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-12d5b894-5064-4a49-8b67-57f568e4e386' class='xr-section-summary-in' type='checkbox'  ><label for='section-12d5b894-5064-4a49-8b67-57f568e4e386' class='xr-section-summary' >Indexes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>time</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-677f452e-cccc-4d95-96dd-94c3b5c4a3de' class='xr-index-data-in' type='checkbox'/><label for='index-677f452e-cccc-4d95-96dd-94c3b5c4a3de' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(DatetimeIndex([&#x27;2019-01-01 06:00:00&#x27;, &#x27;2019-04-02 13:30:00&#x27;,\n",
       "               &#x27;2019-07-02 21:00:00&#x27;, &#x27;2019-10-02 04:30:00&#x27;,\n",
       "               &#x27;2020-01-01 12:00:00&#x27;, &#x27;2020-04-01 19:30:00&#x27;,\n",
       "               &#x27;2020-07-02 03:00:00&#x27;, &#x27;2020-10-01 10:30:00&#x27;,\n",
       "               &#x27;2020-12-31 18:00:00&#x27;, &#x27;2021-04-02 01:30:00&#x27;,\n",
       "               &#x27;2021-07-02 09:00:00&#x27;, &#x27;2021-10-01 16:30:00&#x27;,\n",
       "               &#x27;2022-01-01 00:00:00&#x27;, &#x27;2022-04-02 07:30:00&#x27;,\n",
       "               &#x27;2022-07-02 15:00:00&#x27;, &#x27;2022-10-01 22:30:00&#x27;,\n",
       "               &#x27;2023-01-01 06:00:00&#x27;, &#x27;2023-04-02 13:30:00&#x27;,\n",
       "               &#x27;2023-07-02 21:00:00&#x27;, &#x27;2023-10-02 04:30:00&#x27;,\n",
       "               &#x27;2024-01-01 12:00:00&#x27;, &#x27;2024-04-01 19:30:00&#x27;,\n",
       "               &#x27;2024-07-02 03:00:00&#x27;, &#x27;2024-10-01 10:30:00&#x27;],\n",
       "              dtype=&#x27;datetime64[ns]&#x27;, name=&#x27;time&#x27;, freq=None))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>x</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-fbd1e278-8ea9-41c5-bf7f-0a0e065fe613' class='xr-index-data-in' type='checkbox'/><label for='index-fbd1e278-8ea9-41c5-bf7f-0a0e065fe613' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([-2670000.0, -2669000.0, -2668000.0, -2667000.0, -2666000.0, -2665000.0,\n",
       "       -2664000.0, -2663000.0, -2662000.0, -2661000.0,\n",
       "       ...\n",
       "        2781000.0,  2782000.0,  2783000.0,  2784000.0,  2785000.0,  2786000.0,\n",
       "        2787000.0,  2788000.0,  2789000.0,  2790000.0],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;x&#x27;, length=5461))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>y</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-7fbc13cf-a5e2-4705-ba37-cd58f97404c4' class='xr-index-data-in' type='checkbox'/><label for='index-7fbc13cf-a5e2-4705-ba37-cd58f97404c4' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([-2190000.0, -2189000.0, -2188000.0, -2187000.0, -2186000.0, -2185000.0,\n",
       "       -2184000.0, -2183000.0, -2182000.0, -2181000.0,\n",
       "       ...\n",
       "        2261000.0,  2262000.0,  2263000.0,  2264000.0,  2265000.0,  2266000.0,\n",
       "        2267000.0,  2268000.0,  2269000.0,  2270000.0],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;y&#x27;, length=4461))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-7ecbf42d-ddfa-401e-a805-3390812bed97' class='xr-section-summary-in' type='checkbox'  checked><label for='section-7ecbf42d-ddfa-401e-a805-3390812bed97' class='xr-section-summary' >Attributes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>description :</span></dt><dd>delta_h group includes variables describing height differences between the model surface at any time and the DEM surface at a resolution of 1 km.</dd><dt><span>identifier_product_DOI :</span></dt><dd>doi:10.5067/ATLAS/ATL15.004</dd><dt><span>shortName :</span></dt><dd>ATL15</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset> Size: 5GB\n",
       "Dimensions:     (time: 24, y: 4461, x: 5461)\n",
       "Coordinates:\n",
       "  * time        (time) datetime64[ns] 192B 2019-01-01T06:00:00 ... 2024-10-01...\n",
       "  * x           (x) float64 44kB -2.67e+06 -2.669e+06 ... 2.789e+06 2.79e+06\n",
       "  * y           (y) float64 36kB -2.19e+06 -2.189e+06 ... 2.269e+06 2.27e+06\n",
       "Data variables:\n",
       "    delta_h     (time, y, x) float32 2GB nan nan nan nan nan ... nan nan nan nan\n",
       "    data_count  (time, y, x) float32 2GB nan nan nan nan nan ... nan nan nan nan\n",
       "Attributes:\n",
       "    description:             delta_h group includes variables describing heig...\n",
       "    identifier_product_DOI:  doi:10.5067/ATLAS/ATL15.004\n",
       "    shortName:               ATL15"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add datasets attributes\n",
    "ATL15_dh.attrs['identifier_product_DOI'] = 'doi:10.5067/ATLAS/ATL15.004'\n",
    "ATL15_dh.attrs['shortName'] = 'ATL15'\n",
    "\n",
    "# View data set\n",
    "ATL15_dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1b0bae96-66b2-4c2e-8c49-5623c3fde835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scripps Grounding Line\n",
    "# https://doi.pangaea.de/10.1594/PANGAEA.819147\n",
    "Scripps_gl = gpd.read_file(DATA_DIR + \n",
    "    '/boundaries/Depoorter2013/Antarctica_masks/scripps_antarctica_polygons_v1.shp')\n",
    "\n",
    "# Isolate only land ice\n",
    "Scripps_landice = Scripps_gl[Scripps_gl['Id_text'] == 'Grounded ice or land']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "df19fcb6-5ea0-4d70-946d-53e960a9bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip altimetry datasets to grounding line to limit analysis to below grounded ice\n",
    "CS2_Smith2017.rio.write_crs(3031, inplace=True)\n",
    "CS2_Smith2017 = CS2_Smith2017.rio.clip(Scripps_landice.geometry.values, Scripps_landice.crs, drop=False)\n",
    "ATL15_dh.rio.write_crs(3031, inplace=True)\n",
    "ATL15_dh = ATL15_dh.rio.clip(Scripps_landice.geometry.values, Scripps_landice.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "955c5399-d3c1-47b0-90df-ba9ecaa637a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cyc_start_dates</th>\n",
       "      <th>mid_cyc_dates</th>\n",
       "      <th>cyc_end_dates</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-07-02 15:00:00</td>\n",
       "      <td>2010-08-17 06:45:00</td>\n",
       "      <td>2010-10-01 22:30:00</td>\n",
       "      <td>CS2_Smith2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-10-01 22:30:00</td>\n",
       "      <td>2010-11-16 11:15:00</td>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>CS2_Smith2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>2011-02-15 15:45:00</td>\n",
       "      <td>2011-04-02 07:30:00</td>\n",
       "      <td>CS2_Smith2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-04-02 07:30:00</td>\n",
       "      <td>2011-05-17 23:15:00</td>\n",
       "      <td>2011-07-02 15:00:00</td>\n",
       "      <td>CS2_Smith2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-07-02 15:00:00</td>\n",
       "      <td>2011-08-17 06:45:00</td>\n",
       "      <td>2011-10-01 22:30:00</td>\n",
       "      <td>CS2_Smith2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cyc_start_dates       mid_cyc_dates       cyc_end_dates        dataset\n",
       "0 2010-07-02 15:00:00 2010-08-17 06:45:00 2010-10-01 22:30:00  CS2_Smith2017\n",
       "1 2010-10-01 22:30:00 2010-11-16 11:15:00 2011-01-01 00:00:00  CS2_Smith2017\n",
       "2 2011-01-01 00:00:00 2011-02-15 15:45:00 2011-04-02 07:30:00  CS2_Smith2017\n",
       "3 2011-04-02 07:30:00 2011-05-17 23:15:00 2011-07-02 15:00:00  CS2_Smith2017\n",
       "4 2011-07-02 15:00:00 2011-08-17 06:45:00 2011-10-01 22:30:00  CS2_Smith2017"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import mid_cyc_dates\n",
    "cyc_dates = pd.read_csv('output/cycle_dates.csv', parse_dates=['cyc_start_dates', 'mid_cyc_dates', 'cyc_end_dates'])\n",
    "\n",
    "# View dates\n",
    "cyc_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0ad65559-91e2-4419-86ca-66b37ac09980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store the cyc_dates columns as a np array with datetime64[ns] data type\n",
    "cyc_start_dates = [np.datetime64(ts) for ts in cyc_dates['cyc_start_dates']]\n",
    "mid_cyc_dates = [np.datetime64(ts) for ts in cyc_dates['mid_cyc_dates']]\n",
    "cyc_end_dates = [np.datetime64(ts) for ts in cyc_dates['cyc_end_dates']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596962e3-e1e3-47c9-b19e-62d368182982",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analysis of previously identified lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ebdf9a-2a12-4a78-aa9f-c72ced985c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find evolving outlines at optimal levels at various within_evaluation boundaries for each lake\n",
    "\n",
    "# Get list of remaining lakes left to process that don't have an output file in the folder_path\n",
    "folder_path = OUTPUT_DIR + '/levels'\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "remaining_lakes = filter_gdf_by_folder_contents(stationary_outlines_gdf, folder_path)\n",
    "\n",
    "if remaining_lakes.empty:\n",
    "    print(\"All lakes processed.\")\n",
    "else:\n",
    "    for i in range(len(remaining_lakes)):\n",
    "        print(len(remaining_lakes), 'lake(s) remain.')\n",
    "        # Process the lake\n",
    "        lake_gdf = remaining_lakes.iloc[0:1]\n",
    "        find_and_save_optimal_parameters(lake_gdf)\n",
    "        visualize_and_save_evolving_outlines(lake_gdf)\n",
    "\n",
    "        # Recheck which lakes still need processing\n",
    "        remaining_lakes = filter_gdf_by_folder_contents(stationary_outlines_gdf, folder_path)\n",
    "        \n",
    "        if remaining_lakes.empty:\n",
    "            print(\"All lakes processed.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fef049-b6b2-489c-a1c0-2119aef45a12",
   "metadata": {},
   "source": [
    "## Extended analysis at lakes with no evolving outlines found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2fc45-e536-4969-8082-d579f3c21892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry lakes with no evolving outlines found at greater within_area_multiples\n",
    "\n",
    "# Find evolving outlines at various search extents and levels for each lake\n",
    "\n",
    "# Get list of remaining lakes left to process that don't have an output file in the folder_path\n",
    "folder_path = OUTPUT_DIR + '/levels'\n",
    "remaining_lakes = filter_gdf_by_folder_contents(stationary_outlines_gdf, folder_path, exclude=False, file_extension='txt')\n",
    "\n",
    "if remaining_lakes.empty:\n",
    "    print(\"All lakes processed.\")\n",
    "else:\n",
    "    for i in range(len(remaining_lakes)):\n",
    "        print(len(remaining_lakes), 'lake(s) remain.')\n",
    "        \n",
    "        # Process the lake\n",
    "        lake_gdf = remaining_lakes.iloc[i:i+1]\n",
    "        find_and_save_optimal_parameters(lake_gdf, within_area_multiples=range(16, 21))\n",
    "        visualize_and_save_evolving_outlines(lake_gdf)\n",
    "\n",
    "        if remaining_lakes.empty:\n",
    "            print(\"All lakes processed.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bb5a49-3397-4460-9a78-503fb52994a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any \"no outlines\" TXT files when there is a levels CSV file in two directories\n",
    "cleanup_duplicate_files(directory_path=OUTPUT_DIR + '/levels',\n",
    "    keep_extension='csv', delete_extension='txt')\n",
    "\n",
    "cleanup_duplicate_files(directory_path='output/lake_outlines/evolving_outlines/',\n",
    "    keep_extension='geojson', delete_extension='txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6177f68b-afdb-40c7-9ee2-358008ab9b27",
   "metadata": {},
   "source": [
    "## Ensure outlines were visualized and saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "71468bef-565c-4ee1-bd71-281c249e8b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 lakes remain.\n",
      "Visualizing outlines for Site_B\n",
      "No levels file found for Site_B. Skipping lake.\n",
      "1 lakes remain.\n",
      "Visualizing outlines for Site_C\n",
      "No levels file found for Site_C. Skipping lake.\n",
      "All lakes processed.\n"
     ]
    }
   ],
   "source": [
    "# Verify all lakes were processed using the visualize_and_save_evolving_outlines func\n",
    "\n",
    "# First set of lakes: lakes that don't have a txt or geojson file generated\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "remaining_lakes_set1 = filter_gdf_by_folder_contents(stationary_outlines_gdf, folder_path, exclude=True)\n",
    "\n",
    "# Second set of lakes: lakes with evolving outlines saved as geojson but without mp4 visualizations\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "remaining_lakes_set2 = filter_gdf_by_folder_contents(stationary_outlines_gdf, folder_path, exclude=False, file_extension='.geojson')\n",
    "\n",
    "# Further filter the second set to include only lakes without mp4 visualizations\n",
    "folder_path = OUTPUT_DIR + '/find_evolving_outlines'\n",
    "remaining_lakes_set2 = filter_gdf_by_folder_contents(\n",
    "    remaining_lakes_set2, \n",
    "    folder_path,\n",
    "    exclude=True,\n",
    "    suffix_pattern=r'_\\d+-idx_\\d+\\.\\d+m-level_\\d+x-within',\n",
    "    file_extension='mp4'\n",
    ")\n",
    "\n",
    "# Second set of lakes: lakes with evolving outlines saved as geojson but without png visualizations\n",
    "# Do this only on lake with evolving outlines saved as geojson file in evolving_outlines directory\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "remaining_lakes = filter_gdf_by_folder_contents(stationary_outlines_gdf, folder_path, exclude=False, file_extension='.geojson')\n",
    "\n",
    "# Visualization png's saved to plot_evolving_outlines_time_series directory\n",
    "folder_path = OUTPUT_DIR + '/plot_evolving_outlines_time_series'\n",
    "remaining_lakes = filter_gdf_by_folder_contents(\n",
    "    remaining_lakes, \n",
    "    folder_path,\n",
    "    suffix_pattern=r'_\\d+-idx_\\d+\\.\\d+m-level_\\d+x-within',\n",
    "    file_extension='png'\n",
    ")\n",
    "\n",
    "# Combine both sets of lakes (no need to drop duplicates)\n",
    "remaining_lakes = pd.concat([remaining_lakes_set1, remaining_lakes_set2], ignore_index=True)\n",
    "\n",
    "# Reprocess remaining lakes\n",
    "if remaining_lakes.empty:\n",
    "    print(\"All lakes processed.\")\n",
    "    \n",
    "else:\n",
    "    total_lakes = len(remaining_lakes)\n",
    "    processed_lakes = 0\n",
    "\n",
    "    try:\n",
    "        for i in range(len(remaining_lakes)):\n",
    "            remaining_count = total_lakes - processed_lakes\n",
    "            print(f\"{remaining_count} lakes remain.\")\n",
    "        \n",
    "            # Process the lake\n",
    "            lake_gdf = remaining_lakes.iloc[i:i+1]\n",
    "            visualize_and_save_evolving_outlines(lake_gdf)\n",
    "\n",
    "            # Increment processing counter\n",
    "            processed_lakes += 1\n",
    "\n",
    "            # # Clear output of each index\n",
    "            # clear_output(wait=True)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing lake at index {i}: {str(e)}\")\n",
    "            \n",
    "    print(\"All lakes processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dfbf9f-4639-470f-beea-8e1ba10a4358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "71bd85b1-9dea-4126-99bd-1a3aff0c28f5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All lakes processed.\n"
     ]
    }
   ],
   "source": [
    "# # Verify all lakes were processed using the visualize_and_save_evolving_outlines func\n",
    "\n",
    "# # Do this only on lake with evolving outlines saved as geojson file in evolving_outlines directory\n",
    "# folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "# remaining_lakes = filter_gdf_by_folder_contents(stationary_outlines_gdf, folder_path, exclude=False, file_extension='.geojson')\n",
    "\n",
    "# # Verify all lakes were processed using the visualize_and_save_evolving_outlines func\n",
    "# # Visualization mp4's saved to find_evolving_outlines directory\n",
    "# folder_path = OUTPUT_DIR + '/find_evolving_outlines'\n",
    "# remaining_lakes = filter_gdf_by_folder_contents(\n",
    "#     remaining_lakes, \n",
    "#     folder_path,\n",
    "#     suffix_pattern=r'_\\d+-idx_\\d+\\.\\d+m-level_\\d+x-within',\n",
    "#     file_extension='mp4'\n",
    "# )\n",
    "\n",
    "# # Reprocess remaining lakes\n",
    "# if remaining_lakes.empty:\n",
    "#     print(\"All lakes processed.\")\n",
    "    \n",
    "# else:\n",
    "#     total_lakes = len(remaining_lakes)\n",
    "#     processed_lakes = 0\n",
    "\n",
    "#     try:\n",
    "#         for i in range(len(remaining_lakes)):\n",
    "#             remaining_count = total_lakes - processed_lakes\n",
    "#             print(f\"{remaining_count} lakes remain.\")\n",
    "        \n",
    "#             # Process the lake\n",
    "#             lake_gdf = remaining_lakes.iloc[i:i+1]\n",
    "#             visualize_and_save_evolving_outlines(lake_gdf)\n",
    "\n",
    "#             # Increment processing counter\n",
    "#             processed_lakes += 1\n",
    "\n",
    "#             # Clear output of each index\n",
    "#             clear_output(wait=True)\n",
    "                \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing lake at index {i}: {str(e)}\")\n",
    "            \n",
    "#     print(\"All lakes processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9a7ddd-da3f-49e8-895b-bd474c5776cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify all lakes were processed using the plot_evolving_outlines_time_series func \n",
    "# # (which is nested within the visualize_and_save_evolving_outlines func)\n",
    "\n",
    "# # Do this only on lake with evolving outlines saved as geojson file in evolving_outlines directory\n",
    "# folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "# remaining_lakes = filter_gdf_by_folder_contents(stationary_outlines_gdf, folder_path, exclude=False, file_extension='.geojson')\n",
    "\n",
    "# # Visualization png's saved to plot_evolving_outlines_time_series directory\n",
    "# folder_path = OUTPUT_DIR + '/plot_evolving_outlines_time_series'\n",
    "# remaining_lakes = filter_gdf_by_folder_contents(\n",
    "#     remaining_lakes, \n",
    "#     folder_path,\n",
    "#     suffix_pattern=r'_\\d+-idx_\\d+\\.\\d+m-level_\\d+x-within',\n",
    "#     file_extension='png'\n",
    "# )\n",
    "\n",
    "# # Reprocess remaining lakes\n",
    "# if remaining_lakes.empty:\n",
    "#     print(\"All lakes processed.\")\n",
    "    \n",
    "# else:\n",
    "#     total_lakes = len(remaining_lakes)\n",
    "#     processed_lakes = 0\n",
    "\n",
    "#     try:\n",
    "#         for i in range(len(remaining_lakes)):\n",
    "#             remaining_count = total_lakes - processed_lakes\n",
    "#             print(f\"{remaining_count} lakes remain.\")\n",
    "        \n",
    "#             # Process the lake\n",
    "#             lake_gdf = remaining_lakes.iloc[i:i+1]\n",
    "#             lake_name = lake_gdf['name'].iloc[0]\n",
    "#             evolving_outlines_gdf = gpd.read_file(os.path.join(OUTPUT_DIR_GIT + f'/lake_outlines/evolving_outlines/{lake_name}.geojson'))\n",
    "#             offlake_outlines_gdf = gpd.read_file(os.path.join(OUTPUT_DIR + f'/find_evolving_outlines//offlake_outlines/{lake_name}.geojson'))\n",
    "#             plot_evolving_outlines_time_series(lake_gdf, evolving_outlines_gdf, offlake_outlines_gdf)\n",
    "            \n",
    "#             # Increment processing counter\n",
    "#             processed_lakes += 1\n",
    "\n",
    "#             # Clear output of each index\n",
    "#             clear_output(wait=True)\n",
    "                \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing lake at index {i}: {str(e)}\")\n",
    "            \n",
    "#     print(\"All lakes processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d9f9d-2b9b-497e-bc61-6d9a13894d3b",
   "metadata": {},
   "source": [
    "# Lake groups\n",
    "\n",
    "From reviewing the evolving outlines, we observed lakes that have neighbor lake and appear to interact with that neighbor, so we analyze those lake groupings as lake systems where two or more lakes are analyzed together see if perhaps the lakes should be considered as one lake or remain as separate lakes. Additionally The upper Thwaites lakes are close neighbors we attempted to group them to see if a more optimal level could be obtained when analyzed simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2c9c41-c404-45a6-bf71-ed8ffb6303d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define lake groups\n",
    "lake_groups = [\n",
    "    ['Mac1', 'Mac2'],\n",
    "    ['Site_B', 'Site_C'],\n",
    "    ['Slessor_4', 'Slessor_5'],\n",
    "    ['Thw_70', 'Thw_124', 'Thw_142', 'Thw_170'],\n",
    "    ['Thw_124', 'Thw_142', 'Thw_170'],\n",
    "    ['Thw_124', 'Thw_142'],\n",
    "    ['Thw_142', 'Thw_170']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2978f041-1e33-44ed-b579-db0bbd74545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reverse for processing backwards using a second server\n",
    "# lake_groups.reverse()\n",
    "\n",
    "# Process each group\n",
    "for lake_group in lake_groups:\n",
    "\n",
    "    # Create combined lake group GeoDataFrame row\n",
    "    group_single_gdf = prepare_group_gdf(stationary_outlines_gdf, lake_group)\n",
    "    \n",
    "    # First find search extents and levels for the group\n",
    "    find_and_save_optimal_parameters(group_single_gdf, within_area_multiples=range(5, 16))\n",
    "        \n",
    "    # Then finalize the evolving outlines using these parameters\n",
    "    visualize_and_save_evolving_outlines(group_single_gdf)\n",
    "\n",
    "del lake_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde5fc9-103b-4a4e-886d-06a44f0e891c",
   "metadata": {},
   "source": [
    "After reviewing the results of the lake groups analysis, I found:\n",
    "* Mac1_Mac2 have distinct activity but there is an interesting dh expression between the lakes during the CryoSat-2 era that may be indicative of lake migration; however, analyzing as a group does not add new information as the intereting feature was found over Mac2 in the individual lake analysis and the level found analyzing as a lake system (0.4 m) was higher than analyzing separately: Mac1 (0.30 m) and Mac2 (0.36 m).\n",
    "* Site_B_Site_C have two dh espressions that both overlap with locations of the two lakes; it's unclear if either of the dh expressions belong to one of the lakes or the other because the two dh expressions are nearly centered between the two lakes with some lateral offset. This is an improvement from analyzing the two lakes separately where Site C had a lowest optimal level of 1.27 m and Site B had no evolving outlines found compared to analyzed as lake group had a lowest optimal level of 1.2 m.\n",
    "* Slessor_4_Slessor_5 have unconvincing evidence of being one lake system: there is one time slice where there dh expression covering both lakes, 2019-07-02 to 2019-10-02, but several other time slices have dh expressions of opposite sign (similar to observed at Slessor_23 by Siegfried and Fricker, 2018).\n",
    "* Thw_70_Thw_124_Thw_142_Thw_170 does not improve analysis from individual lakes because there is no overlapping activity and the level (1.37 m) is higher than two out of four of the lakes analyzed individually.\n",
    "* Thw_124_Thw_142_Thw_170 has some potential as the there overlapping activity over the three lakes and a lower level (0.53 m) than two of the three lakes; however, there are many off-lake outlines identified, so we will plot the second most optimal level of this lake grouping.\n",
    "* Thw_124_Thw_142 appears to have some potential in identifying overlapping outlines shared between these lakes; however, Thw_170 is included in the found outlines, so it makes sense to use that grouping instead.\n",
    "* Thw_142_Thw_170 was not useful because of the high level (1.86 m) found to be most optimal for this grouping.\n",
    "\n",
    "Based on this, we will delete the evolving outlines generated and saved to geojson files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf520436-96e4-4e51-8a2f-ae867a909e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Site_B_Site_C to Site_BC to follow naming convention used for at past lake unions\n",
    "# Lake_78 and Slessor_23\n",
    "old_name = 'output/lake_outlines/evolving_outlines/' + 'Site_B_Site_C.geojson'\n",
    "new_name = 'output/lake_outlines/evolving_outlines/' + 'Site_BC.geojson'\n",
    "\n",
    "if os.path.exists(old_name):\n",
    "    os.rename(old_name, new_name)\n",
    "    print(f'Successfully renamed {old_name} to {new_name}')\n",
    "else:\n",
    "    print(f'File {old_name} does not exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaedcc3-0194-43f9-8a13-3063da885840",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_names = ['output/lake_outlines/evolving_outlines/Site_B_Site_C.geojson', \n",
    "             OUTPUT_DIR + '/levels/Site_B_Site_C.csv',\n",
    "             OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines/Site_B_Site_C.geojson']\n",
    "new_names = ['output/lake_outlines/evolving_outlines/Site_BC.geojson',\n",
    "            OUTPUT_DIR + '/levels/Site_BC.csv',\n",
    "            OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines/Site_BC.geojson']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1cc6d-780f-432b-83a7-99e695501fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through both lists simultaneously\n",
    "for old_name, new_name in zip(old_names, new_names):\n",
    "    # Try to rename file\n",
    "    if os.path.exists(old_name):\n",
    "        os.rename(old_name, new_name)\n",
    "        print(f'Successfully renamed {old_name} to {new_name}')\n",
    "    else:\n",
    "        print(f'File not found: {old_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1def7d8-0fb3-4630-a6fc-06b075dd41a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_outputs_to_delete = [\n",
    "    'Mac1_Mac2',\n",
    "    'Site_B', 'Site_C',\n",
    "    'Slessor_4_Slessor_5',\n",
    "    'Thw_70_Thw_124_Thw_142_Thw_170',\n",
    "    'Thw_142_Thw_170'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5727487d-b1fa-4a1a-bc92-dfd4b1b4ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in lakes_outputs_to_delete:\n",
    "    dir = OUTPUT_DIR + '/levels/'\n",
    "    file_path = os.path.join(dir, f\"{filename}.csv\")\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Deleted: {file_path}\")\n",
    "    else:\n",
    "        print(f\"Not found: {file_path}\")\n",
    "\n",
    "    dir = 'output/lake_outlines/evolving_outlines/'\n",
    "    for ext in ('.geojson', '.txt'):\n",
    "        file_path = os.path.join(dir, f\"{filename}{ext}\")\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")\n",
    "        else:\n",
    "            print(f\"Not found: {file_path}\")\n",
    "\n",
    "del dir, lakes_outputs_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39f9066-a790-451e-a701-68284a467bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write no outlines txt file to two lakes that will be replaced by the combination lake\n",
    "dir = 'output/lake_outlines/evolving_outlines/'\n",
    "\n",
    "for lake in ['Site_B', 'Site_C']:\n",
    "    write_no_outlines(dir + lake)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253a34c5-5457-4a21-b6ac-179a9ccb2325",
   "metadata": {},
   "source": [
    "Next we address lakes that have evolving outlines that appear flawed because of the number of off-lake outlines that make it appear that the lowest level selected using the algorithm perhaps was too low. We address this by selecting the next most optimal level/within_area_multiple combination contained in the levels csv file for these lakes in the first row (instead of the default zeroth row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a01d67-6acc-4c00-a449-4cbdffacb083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lake groups\n",
    "lake_groups_analyze_row_1 = [\n",
    "    ['Thw_124', 'Thw_142', 'Thw_170'],\n",
    "    ['Thw_124', 'Thw_142'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c07ad-d580-4c10-a600-1ee4fe92b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lake_group in lake_groups_analyze_row_1:\n",
    "\n",
    "    # Create combined lake group GeoDataFrame row\n",
    "    group_single_gdf = prepare_group_gdf(stationary_outlines_gdf, lake_group)\n",
    "    \n",
    "    # Visualize second level stored in first row\n",
    "    visualize_and_save_evolving_outlines(group_single_gdf, row_index=1)\n",
    "\n",
    "del lake_groups_analyze_row_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4697db-30cc-4438-9237-653d15f00c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lake groups\n",
    "lake_groups_analyze_row_2 = [\n",
    "    # ['Thw_124', 'Thw_142', 'Thw_170'],\n",
    "    ['Thw_124', 'Thw_142']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd29d3d3-8648-4eb7-9936-6a1e3129ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lake_group in lake_groups_analyze_row_2:\n",
    "\n",
    "    # Create combined lake group GeoDataFrame row\n",
    "    group_single_gdf = prepare_group_gdf(stationary_outlines_gdf, lake_group)\n",
    "    \n",
    "    # Visualize second level stored in first row\n",
    "    visualize_and_save_evolving_outlines(group_single_gdf, row_index=2)\n",
    "\n",
    "del lake_groups_analyze_row_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb897137-e826-49d5-a436-7c11ef13ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lake groups\n",
    "lake_groups_analyze_row_3 = [\n",
    "    ['Thw_124', 'Thw_142', 'Thw_170'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db9a661-5115-4354-8d0c-195ca9b85dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lake_group in lake_groups_analyze_row_3:\n",
    "\n",
    "    # Create combined lake group GeoDataFrame row\n",
    "    group_single_gdf = prepare_group_gdf(stationary_outlines_gdf, lake_group)\n",
    "    \n",
    "    # Visualize second level stored in first row\n",
    "    visualize_and_save_evolving_outlines(group_single_gdf, row_index=3)\n",
    "\n",
    "del lake_groups_analyze_row_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ea689b-3ab5-4fb7-a0ea-8f125f41ac90",
   "metadata": {},
   "source": [
    "The various levels of Thw_124_Thw_142_Thw_170 did not provide better outlines than analyzing the lakes individually, but instead Thw_124_Thw_142 is able to capture the lobing activity of Thw_142 and more of Thw_124's activity at a lower level then when each lake is analyzed separately. However, their activity appears spatial distinct, so we will:\n",
    "\n",
    "1) Use evolving outlines generated in the analysis of lake group, Thw_124_Thw_142, for the individual lake evolving outlines for each respective lake by separating them spatially.\n",
    "\n",
    "2) We will delete the Thw_124_Thw_142_Thw_170 geojson file as it did not prove useful analyzing these three lakes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef041aa-ac94-4636-a8f1-cfca74ea6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "evolving_outlines_path = os.path.join(OUTPUT_DIR_GIT + '/lake_outlines/evolving_outlines/Thw_124_Thw_142.geojson')\n",
    "output_dir = os.path.join('output/lake_outlines/evolving_outlines')\n",
    "\n",
    "# Read the evolving outlines\n",
    "Thw_124_Thw_142_evolving_outlines_gdf = gpd.read_file(evolving_outlines_path)\n",
    "\n",
    "# Get the stationary outlines for each lake\n",
    "Thw_124_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == 'Thw_124']\n",
    "Thw_142_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == 'Thw_142']\n",
    "\n",
    "if Thw_124_gdf.empty or Thw_142_gdf.empty:\n",
    "    raise ValueError(f\"Could not find one or both lakes in stationary_outlines_gdf: {Thw_124_gdf['name']}, {Thw_142_gdf['name']}\")\n",
    "\n",
    "# Extract outlines that intersect with each lake\n",
    "Thw_124_outlines, Thw_124_offlake_outlines = extract_intersecting_polygons_recursive(Thw_124_Thw_142_evolving_outlines_gdf, Thw_124_gdf.geometry.iloc[0])\n",
    "Thw_142_outlines, Thw_142_offlake_outlines = extract_intersecting_polygons_recursive(Thw_124_Thw_142_evolving_outlines_gdf, Thw_142_gdf.geometry.iloc[0])\n",
    "\n",
    "if Thw_124_outlines is not None and not Thw_124_outlines.empty:\n",
    "    lake_name = 'Thw_124'\n",
    "    Thw_124_outlines.to_file(\n",
    "        os.path.join(OUTPUT_DIR, f\"{lake_name}.geojson\"),\n",
    "        driver='GeoJSON'\n",
    "    )\n",
    "    print(f\"Saved outlines for {lake_name}\")\n",
    "\n",
    "if Thw_142_outlines is not None and not Thw_142_outlines.empty:\n",
    "    lake_name = 'Thw_142'\n",
    "    Thw_142_outlines.to_file(\n",
    "        os.path.join(OUTPUT_DIR, f\"{lake_name}.geojson\"),\n",
    "        driver='GeoJSON'\n",
    "    )\n",
    "    print(f\"Saved outlines for {lake_name}\")\n",
    "\n",
    "del evolving_outlines_path, output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd132a0-c1a5-4c6a-9c30-002940f74f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick visualization to ensure outlines were split properly\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Get bounds for plot extent\n",
    "total_bounds = Thw_124_Thw_142_evolving_outlines_gdf.total_bounds\n",
    "x_min, y_min, x_max, y_max = total_bounds\n",
    "buffer_factor = 0.2\n",
    "x_buffer = (x_max - x_min) * buffer_factor\n",
    "y_buffer = (y_max - y_min) * buffer_factor\n",
    "\n",
    "# Plot MOA background\n",
    "mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "moa_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "ax.imshow(moa_subset[0,:,:], cmap=\"gray\", clim=[14000, 17000], \n",
    "         extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "\n",
    "# Plot the outlines\n",
    "Thw_124_outlines_imported = gpd.read_file('output/lake_outlines/evolving_outlines/Thw_124.geojson')\n",
    "Thw_142_outlines_imported = gpd.read_file('output/lake_outlines/evolving_outlines/Thw_142.geojson')\n",
    "Thw_124_outlines_imported.boundary.plot(ax=ax, color='red', linewidth=1, label='Thw_124 evolving')\n",
    "Thw_142_outlines_imported.boundary.plot(ax=ax, color='blue', linewidth=1, label='Thw_142 evolving')\n",
    "    \n",
    "# Plot stationary outlines\n",
    "Thw_124_gdf.boundary.plot(ax=ax, color='darkred', linestyle='--', linewidth=2, label=f\"{Thw_124_gdf['name'].iloc[0]} stationary\")\n",
    "Thw_142_gdf.boundary.plot(ax=ax, color='darkblue', linestyle='--', linewidth=2, label=f\"{Thw_142_gdf['name'].iloc[0]} stationary\")\n",
    "\n",
    "# Add inset map\n",
    "axins = ax.inset_axes([0.05, 0.05, 0.3, 0.3])\n",
    "axins.set_aspect('equal')\n",
    "moa_2014_coastline.plot(ax=axins, color='gray', edgecolor='k', linewidth=0.1)\n",
    "moa_2014_groundingline.plot(ax=axins, color='ghostwhite', edgecolor='k', linewidth=0.1)\n",
    "center_x = (x_min + x_max) / 2\n",
    "center_y = (y_min + y_max) / 2\n",
    "axins.scatter(center_x, center_y, c='red', marker='*', s=100, zorder=5)\n",
    "axins.axis('off')\n",
    "\n",
    "# Format main plot\n",
    "km_scale = 1e3\n",
    "ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "ax.set_xlabel('x [km]')\n",
    "ax.set_ylabel('y [km]')\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2)\n",
    "ax.set_title('Split Evolving Lake Outlines')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfa50b-245a-433c-bfce-ccfcc9d9296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot evolving outlines time series for Thw_124 and Thw_142\n",
    "Thw_124_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == 'Thw_124']\n",
    "Thw_142_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == 'Thw_142']\n",
    "plot_evolving_outlines_time_series(Thw_124_gdf, Thw_124_outlines, Thw_124_offlake_outlines)\n",
    "plot_evolving_outlines_time_series(Thw_142_gdf, Thw_142_outlines, Thw_142_offlake_outlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8e019-df5f-45b7-9f98-36bd7f39a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del Thw_124_gdf, Thw_124_outlines, Thw_124_offlake_outlines\n",
    "del Thw_142_gdf, Thw_142_outlines, Thw_142_offlake_outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c1b497-8580-48e0-9bc1-c9958cfc6c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_outputs_to_delete = [\n",
    "    'Thw_124_Thw_142_Thw_170',\n",
    "    'Thw_124_Thw_142'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d8f9dc-1458-41d6-8ddb-a7876954a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in lakes_outputs_to_delete:\n",
    "    dir = OUTPUT_DIR + '/levels/'\n",
    "    file_path = os.path.join(dir, f\"{filename}.csv\")\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Deleted: {file_path}\")\n",
    "    else:\n",
    "        print(f\"Not found: {file_path}\")\n",
    "\n",
    "    dir = 'output/lake_outlines/evolving_outlines/'\n",
    "    for ext in ('.geojson', '.txt'):\n",
    "        file_path = os.path.join(dir, f\"{filename}{ext}\")\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")\n",
    "        else:\n",
    "            print(f\"Not found: {file_path}\")\n",
    "\n",
    "del dir, lakes_outputs_to_delete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d89cea-4589-4838-a74f-ab12eeab3fa7",
   "metadata": {},
   "source": [
    "# Revise stationary_outlines_gdf\n",
    "\n",
    "We will revise stationary_outlines_gdf to not have Site_B and Site_C as individual lakes, but instead have Site_BC as a combined lake group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc837b4d-ea77-402a-8e68-ff7672c6ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of stationary_outlines_gdf\n",
    "revised_stationary_outlines_gdf = stationary_outlines_gdf.copy(deep=True)\n",
    "\n",
    "# Create combined Site_B_Site_C row\n",
    "site_bc_row = prepare_group_gdf(revised_stationary_outlines_gdf, ['Site_B', 'Site_C'])\n",
    "\n",
    "# Copy the citation to new Site_B_Site_C row if Sites B and C have the same citation\n",
    "if (revised_stationary_outlines_gdf[revised_stationary_outlines_gdf['name'] == 'Site_B']['cite'].iloc[0] == \n",
    "    revised_stationary_outlines_gdf[revised_stationary_outlines_gdf['name'] == 'Site_C']['cite'].iloc[0]):\n",
    "    site_bc_row['cite'] = (revised_stationary_outlines_gdf[revised_stationary_outlines_gdf['name'] == 'Site_B']\n",
    "                          ['cite'].iloc[0])\n",
    "\n",
    "# Drop individual lakes we are replacing\n",
    "revised_stationary_outlines_gdf = revised_stationary_outlines_gdf.drop(\n",
    "    revised_stationary_outlines_gdf[revised_stationary_outlines_gdf['name'].isin(['Site_B', 'Site_C'])].index)\n",
    "\n",
    "# Get evolving outlines and calculate area for Site_BC\n",
    "try:\n",
    "    lake_name = 'Site_BC'\n",
    "    \n",
    "    evolving_outlines_gdf = gpd.read_file('output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name))\n",
    "    \n",
    "    # Get evolving outlines union\n",
    "    evolving_union_gs = find_evolving_union(site_bc_row.iloc[0], evolving_outlines_gdf, incl_stationary=False)\n",
    "    \n",
    "    if evolving_union_gs is not None:\n",
    "        # Create temporary GeoDataFrame with the union\n",
    "        temp_gdf = gpd.GeoDataFrame(\n",
    "            index=[0], \n",
    "            crs='EPSG:3031', \n",
    "            geometry=[evolving_union_gs.iloc[0]])\n",
    "        \n",
    "        # Convert to 4326 for area calculation\n",
    "        temp_gdf = temp_gdf.to_crs('4326')\n",
    "        \n",
    "        # Calculate area\n",
    "        area = calculate_geodesic_area(temp_gdf['geometry'].iloc[0])\n",
    "        \n",
    "        # Update site_bc_row with calculated area and geometry\n",
    "        site_bc_row['area (m^2)'] = area if area is not None else None\n",
    "        site_bc_row['geometry'] = evolving_union_gs.iloc[0]\n",
    "\n",
    "\n",
    "    # Rename to follow combination naming convention used for at passed lakes Lake_78 and Slessor_23\n",
    "    site_bc_row['name'] = 'Site_BC'\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error processing {lake_name}: {str(e)}\")\n",
    "\n",
    "# Ensure that new entry isn't already in inventory before adding to avoid duplicate entry\n",
    "gdf_diff = site_bc_row[~site_bc_row['name'].isin(revised_stationary_outlines_gdf['name'])]\n",
    "\n",
    "# Add the new row to stationary_outlines_gdf\n",
    "revised_stationary_outlines_gdf = pd.concat([revised_stationary_outlines_gdf, gdf_diff], ignore_index=True)\n",
    "\n",
    "# Sort the GeoDataFrame alphabetically by the 'name' column; reset the index after sorting\n",
    "revised_stationary_outlines_gdf = revised_stationary_outlines_gdf.sort_values('name').reset_index(drop=True)\n",
    "\n",
    "# Print processing confirmation\n",
    "print(f\"\\nProcessed {lake_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e71d1a-29a5-4349-92a7-40ac75485980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View new row to ensure worked properly\n",
    "revised_stationary_outlines_gdf[revised_stationary_outlines_gdf['name'] == 'Site_BC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b428502e-ed81-480f-9a62-90f9b9a560ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "revised_stationary_outlines_gdf.to_file(\n",
    "    'output/lake_outlines/stationary_outlines/revised_stationary_outlines_gdf.geojson',\n",
    "    driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a146a886-92a4-4525-8e8e-6278af3caea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "revised_stationary_outlines_gdf = gpd.read_file(\n",
    "    'output/lake_outlines/stationary_outlines/revised_stationary_outlines_gdf.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d725d06-64e9-4c64-b3ee-24502c1cc4b4",
   "metadata": {},
   "source": [
    "# Review evolving outlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc765b3-b2cc-4c81-bd06-ebc0cd663c7d",
   "metadata": {},
   "source": [
    "In your OUTPUT_DIR/FigS1_lake_reexamination_methods, navigate to the `plot_evolving_outlines_time_series` folder. There you will see the time series of evolving outlines plotted in aggregate for each lake. Some lakes will have very few evolving outlines that don't appear much different from the off-lake outlines generated. We additionally looking at the data_counts, dh, and evolving outline video for each time lake in the `find_evolving_outlines` folder for each lake.\n",
    "\n",
    "In these cases we cannot be certain the evolving outlines are indicative of lake behavior or just background height anomalies. So we delete these evolving outlines geojson files and conclude there were no evolving outlines found for these lakes.\n",
    "\n",
    "Some deletions are due to a lake's evolving outlines being those of a close neighbor, (e.g., Mac5 evolving outlines were those of Mac4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e5db4fb7-676b-4c87-8644-0a2792ab0fe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lake_outlines_to_discard = [\n",
    "    'Bindschadler_1',\n",
    "    'Bindschadler_3',\n",
    "    'Bindschadler_4',\n",
    "    'Bindschadler_5',\n",
    "    'Bindschadler_6',\n",
    "    'Byrd_s1',\n",
    "    'Byrd_s5',\n",
    "    'Byrd_s7',\n",
    "    'Byrd_s14',\n",
    "    'Cook_E1',\n",
    "    'David_s4',\n",
    "    'David_s5',\n",
    "    'EAP_3',\n",
    "    'EAP_5',\n",
    "    'EAP_6',\n",
    "    'EAP_7',\n",
    "    'EAP_8',\n",
    "    'EAP_9',\n",
    "    'Foundation_2',\n",
    "    'Foundation_4',\n",
    "    'Foundation_9',\n",
    "    'Foundation_14',\n",
    "    'Institute_W1',\n",
    "    'JG_Combined_D2_b_E1',\n",
    "    'JG_D1_a',\n",
    "    'JG_D2_a',\n",
    "    'JG_F1',\n",
    "    'Kamb_1',\n",
    "    'Kamb_2',\n",
    "    'Kamb_3',\n",
    "    'Kamb_4',\n",
    "    'Kamb_9',\n",
    "    'Kamb_11',\n",
    "    'Lambert_1',\n",
    "    'LennoxKing_1',\n",
    "    'Mac5',\n",
    "    'Mac6',\n",
    "    'Mertz_1',\n",
    "    'Rec5',\n",
    "    'Rec10',\n",
    "    'Slessor_5',\n",
    "    'Slessor_6',\n",
    "    'Slessor_7',\n",
    "    'TL122',\n",
    "    'U1',\n",
    "    'U3',\n",
    "    'V1',\n",
    "    'Whillans_8',\n",
    "    'Wilkes_1',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "811cb148-3575-4e49-a2ad-4aab0592b6d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bindschadler_1': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Bindschadler_3': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Bindschadler_4': (False,\n",
       "  'Source file does not exist: output/lake_outlines/evolving_outlines/Bindschadler_4.geojson'),\n",
       " 'Bindschadler_5': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Bindschadler_6': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Byrd_s1': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Byrd_s5': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Byrd_s7': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Byrd_s14': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Cook_E1': (True, 'Successfully moved and replaced existing file'),\n",
       " 'David_s4': (True, 'Successfully moved and replaced existing file'),\n",
       " 'David_s5': (True, 'Successfully moved and replaced existing file'),\n",
       " 'EAP_3': (True, 'Successfully moved and replaced existing file'),\n",
       " 'EAP_5': (True, 'Successfully moved and replaced existing file'),\n",
       " 'EAP_6': (False,\n",
       "  'Source file does not exist: output/lake_outlines/evolving_outlines/EAP_6.geojson'),\n",
       " 'EAP_7': (True, 'Successfully moved and replaced existing file'),\n",
       " 'EAP_8': (True, 'Successfully moved and replaced existing file'),\n",
       " 'EAP_9': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Foundation_2': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Foundation_4': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Foundation_9': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Foundation_14': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Institute_W1': (True, 'Successfully moved and replaced existing file'),\n",
       " 'JG_Combined_D2_b_E1': (True,\n",
       "  'Successfully moved and replaced existing file'),\n",
       " 'JG_D1_a': (True, 'Successfully moved and replaced existing file'),\n",
       " 'JG_D2_a': (True, 'Successfully moved and replaced existing file'),\n",
       " 'JG_F1': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Kamb_1': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Kamb_2': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Kamb_3': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Kamb_4': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Kamb_9': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Kamb_11': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Lambert_1': (True, 'Successfully moved and replaced existing file'),\n",
       " 'LennoxKing_1': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Mac5': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Mac6': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Mertz_1': (False,\n",
       "  'Source file does not exist: output/lake_outlines/evolving_outlines/Mertz_1.geojson'),\n",
       " 'Rec5': (False,\n",
       "  'Source file does not exist: output/lake_outlines/evolving_outlines/Rec5.geojson'),\n",
       " 'Rec10': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Slessor_5': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Slessor_6': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Slessor_7': (True, 'Successfully moved and replaced existing file'),\n",
       " 'TL122': (True, 'Successfully moved and replaced existing file'),\n",
       " 'U1': (True, 'Successfully moved and replaced existing file'),\n",
       " 'U3': (False,\n",
       "  'Source file does not exist: output/lake_outlines/evolving_outlines/U3.geojson'),\n",
       " 'V1': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Whillans_8': (True, 'Successfully moved and replaced existing file'),\n",
       " 'Wilkes_1': (True, 'Successfully moved and replaced existing file')}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discard outlines by moving out of git repo and into non-git repo\n",
    "discard_lake_outlines(\n",
    "    lake_outlines_to_discard=lake_outlines_to_discard,\n",
    "    source_dir='output/lake_outlines/evolving_outlines',\n",
    "    dest_dir=OUTPUT_DIR + '/find_evolving_outlines/discarded_outlines'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e05fcb-1dcf-41ae-bc12-f65c3912f31a",
   "metadata": {},
   "source": [
    "Similar to the lake groups, we try the next highest level for evolving outlines that appear flawed because of the number of off-lake outlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "639cd0a4-5fce-43e3-9d5e-060a07c51d81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakes_analyze_row_1 = [\n",
    "    'Byrd_s10',\n",
    "    'Byrd_s11',\n",
    "    'Byrd_s13',\n",
    "    'Byrd_s15',\n",
    "    # 'ConwaySubglacialLake',\n",
    "    # 'EngelhardtSubglacialLake',\n",
    "    # 'Foundation_N1',\n",
    "    'Foundation_N3',\n",
    "    'KT2',\n",
    "    'KT3',\n",
    "    'L1',\n",
    "    # 'Lake78',\n",
    "    # 'Mac1',\n",
    "    'Nimrod_2',\n",
    "    # 'R1',\n",
    "    # 'Rec1',\n",
    "    # 'Rec2',\n",
    "    'Rec6',\n",
    "    # 'Slessor_4',\n",
    "    # 'Slessor_23',\n",
    "    # 'Thw_170',\n",
    "    'UpperSubglacialLakeConway',\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6927714-3424-40a8-98e7-0e8681b1f085",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing outlines for UpperSubglacialLakeConway\n",
      "Parameters: row_index=1, within_area_multiple=12, level=0.29, doi(s)=doi:10.5194/tc-11-451-2017, doi:10.5067/ATLAS/ATL15.004\n",
      "Saved outlines to: /home/jovyan/1_evolving_lakes/output/FigS1_lake_reexamination_methods/find_evolving_outlines/offlake_outlines/UpperSubglacialLakeConway.geojson\n",
      "Making video for UpperSubglacialLakeConway\n",
      "\n",
      "Attempt 1 of 3\n",
      "Validating 57 images...\n",
      "Video created successfully on attempt 1\n",
      "Cleaned up folder: /home/jovyan/1_evolving_lakes/output/FigS1_lake_reexamination_methods/find_evolving_outlines/UpperSubglacialLakeConway\n",
      "Creating evolving outlines time series plot for lake: UpperSubglacialLakeConway\n",
      "Parameters: row_index=1, within_area_multiple=12, level=0.29\n"
     ]
    }
   ],
   "source": [
    "for lake in lakes_analyze_row_1:\n",
    "    # Process the lake at the next highest level\n",
    "    lake_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == lake]\n",
    "    visualize_and_save_evolving_outlines(lake_gdf, row_index=1)\n",
    "\n",
    "del lakes_analyze_row_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a9fd6e-9147-401a-ba2b-2275872c9d1f",
   "metadata": {},
   "source": [
    "We assess these results (in `OUTPUT_DIR/FigS1_lake_reexamination_methods/plot_evolving_outlines_time_series` folder) and decide if 1) lakes analyzed at next highest dh level are satisfactory, or 2) outlines that can be deleted because they are too similar to background off-lake activity or 2) lakes that need to be visualized at the next level because they are too similar to background off-lake activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fa1921-4f69-46a8-afbd-02a9423b8107",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_analyze_row_2 = [\n",
    "    # 'ConwaySubglacialLake',\n",
    "    # 'David_1',\n",
    "    'EngelhardtSubglacialLake',\n",
    "    # 'Foundation_N1',\n",
    "    'Lake78',\n",
    "    'Mac1',\n",
    "    # 'R1',\n",
    "    'Rec1',\n",
    "    # 'Rec2',\n",
    "    # 'Slessor_4',\n",
    "    'Slessor_23',\n",
    "    # 'Thw_170',\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5edc11-5dae-4269-a938-c41a24fd4968",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lake in lakes_analyze_row_2:\n",
    "    # Process the lake at all other levels\n",
    "    lake_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == lake]\n",
    "    visualize_and_save_evolving_outlines(lake_gdf, row_index=2)\n",
    "    \n",
    "del lakes_analyze_row_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb22059f-5f46-4815-a04e-f24898a545bc",
   "metadata": {},
   "source": [
    "We assess these results (in `OUTPUT_DIR/FigS1_lake_reexamination_methods/plot_evolving_outlines_time_series` folder) and decide if 1) lakes analyzed at next highest dh level are satisfactory, or 2) outlines that can be deleted because they are too similar to background off-lake activity or 2) lakes that need to be visualized at the next level because they are too similar to background off-lake activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b124741c-3323-4b76-a4d4-a8b9ec34a4e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lake_outlines_to_discard = [\n",
    "    'David_1',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0bf38a47-b021-4719-b296-69567f2e59f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'David_1': (True, 'Successfully moved and replaced existing file')}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discard outlines by moving out of git repo and into non-git repo\n",
    "discard_lake_outlines(\n",
    "    lake_outlines_to_discard=lake_outlines_to_discard,\n",
    "    source_dir='output/lake_outlines/evolving_outlines',\n",
    "    dest_dir=OUTPUT_DIR + '/find_evolving_outlines/discarded_outlines'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bcdbc72d-f295-44f6-9392-ac8782df1ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_analyze_row_3 = [\n",
    "    # 'ConwaySubglacialLake',\n",
    "    'Foundation_N1',\n",
    "    'R1',\n",
    "    # 'Rec2',\n",
    "    # 'Slessor_4',\n",
    "    'Thw_170'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "50beaef3-0a7e-46a8-bca7-1eddb2623a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing outlines for Thw_170\n",
      "Parameters: row_index=3, within_area_multiple=11, level=0.7, doi(s)=doi:10.5194/tc-11-451-2017, doi:10.5067/ATLAS/ATL15.004\n",
      "Saved outlines to: /home/jovyan/1_evolving_lakes/output/FigS1_lake_reexamination_methods/find_evolving_outlines/offlake_outlines/Thw_170.geojson\n",
      "Making video for Thw_170\n",
      "\n",
      "Attempt 1 of 3\n",
      "Validating 57 images...\n",
      "Video created successfully on attempt 1\n",
      "Cleaned up folder: /home/jovyan/1_evolving_lakes/output/FigS1_lake_reexamination_methods/find_evolving_outlines/Thw_170\n",
      "Creating evolving outlines time series plot for lake: Thw_170\n",
      "Parameters: row_index=3, within_area_multiple=11, level=0.7\n",
      "Successfully saved plot to: /home/jovyan/1_evolving_lakes/output/FigS1_lake_reexamination_methods/plot_evolving_outlines_time_series/Thw_170_3-idx_0.7m-level_11x-within.png\n"
     ]
    }
   ],
   "source": [
    "for lake in lakes_analyze_row_3:\n",
    "    # Process the lake at all other levels\n",
    "    lake_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == lake]\n",
    "    visualize_and_save_evolving_outlines(lake_gdf, row_index=3)\n",
    "    \n",
    "del lakes_analyze_row_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f6bca-c64d-43df-91d3-51460230218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_analyze_row_4 = [\n",
    "    'ConwaySubglacialLake',\n",
    "    'Rec2',\n",
    "    'Slessor_4',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783c5c2c-bf6c-4dfa-84cd-bc9a1fb90af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lake in lakes_analyze_row_4:\n",
    "    # Process the lake at all other levels\n",
    "    lake_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == lake]\n",
    "    visualize_and_save_evolving_outlines(lake_gdf, row_index=4)\n",
    "    \n",
    "del lakes_analyze_row_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "923f9fdd-63c7-4bbd-8fe3-60fa9ffba6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_analyze_row_5 = [\n",
    "    'ConwaySubglacialLake',\n",
    "    'Rec2',\n",
    "    'Slessor_4',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "36661c41-6b4d-4aea-9864-4b8f6bf1abdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing outlines for Slessor_4\n",
      "Parameters: row_index=5, within_area_multiple=5, level=0.57, doi(s)=doi:10.5067/ATLAS/ATL15.004\n",
      "Saved outlines to: /home/jovyan/1_evolving_lakes/output/FigS1_lake_reexamination_methods/find_evolving_outlines/offlake_outlines/Slessor_4.geojson\n",
      "Making video for Slessor_4\n",
      "\n",
      "Attempt 1 of 3\n",
      "Validating 23 images...\n",
      "Video created successfully on attempt 1\n",
      "Cleaned up folder: /home/jovyan/1_evolving_lakes/output/FigS1_lake_reexamination_methods/find_evolving_outlines/Slessor_4\n",
      "Creating evolving outlines time series plot for lake: Slessor_4\n",
      "Parameters: row_index=5, within_area_multiple=5, level=0.57\n",
      "Successfully saved plot to: /home/jovyan/1_evolving_lakes/output/FigS1_lake_reexamination_methods/plot_evolving_outlines_time_series/Slessor_4_5-idx_0.57m-level_5x-within.png\n"
     ]
    }
   ],
   "source": [
    "for lake in lakes_analyze_row_5:\n",
    "    # Process the lake at all other levels\n",
    "    lake_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == lake]\n",
    "    visualize_and_save_evolving_outlines(lake_gdf, row_index=5)\n",
    "\n",
    "del lakes_analyze_row_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c239c59-5973-4bf0-afa0-31f90f24e054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there are no lakes with both geojson and txt file for evolving outlines\n",
    "dir = OUTPUT_DIR + '/levels'\n",
    "file_dict = defaultdict(list)\n",
    "\n",
    "# Get all files in directory\n",
    "for file_path in Path(dir).glob('**/*'):\n",
    "    if file_path.is_file():\n",
    "        # Get base name without extension\n",
    "        base_name = file_path.stem\n",
    "        # Add full filename to list under base name\n",
    "        file_dict[base_name].append(file_path.name)\n",
    "\n",
    "# Filter to only files with duplicates\n",
    "duplicates = {k: v for k, v in file_dict.items() if len(v) > 1}\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1912df48-84f8-489b-bf30-e190a856d389",
   "metadata": {},
   "source": [
    "# Union outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca8bc5-fd0c-494e-aa96-68a57e6e2b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two new lakes geodataframe that are the union of \n",
    "# 1) the evolving outlines for each lake found to have evolving outlines\n",
    "# 2) the evolving outlines and the stationary outline for lakes with activity (found to have evolving outlines)\n",
    "# 3) the evolving outlines and the stationary outline for all lakes\n",
    "\n",
    "# List all folders in target directory\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "\n",
    "# Filter stationary_outlines_gdf to only include lakes that have evolving outlines found at them\n",
    "stationary_outlines_gdf_evolving_lakes = filter_gdf_by_folder_contents(revised_stationary_outlines_gdf, folder_path, file_extension='geojson', exclude=False)\n",
    "\n",
    "# Remove Site A, B, C, LSLM, and LSLC because their outlines were perfect circles created using their \n",
    "# point locations and approx. areas so should not be part of the union\n",
    "exclude_list = ['Site_A', 'LowerConwaySubglacialLake', 'LowerMercerSubglacialLake']\n",
    "stationary_outlines_gdf_evolving_lakes = stationary_outlines_gdf_evolving_lakes[~stationary_outlines_gdf_evolving_lakes['name'].isin(exclude_list)]\n",
    "stationary_outlines_gdf_all_lakes = revised_stationary_outlines_gdf[~revised_stationary_outlines_gdf['name'].isin(exclude_list)]\n",
    "\n",
    "# Create initial GDFs\n",
    "evolving_outlines_union_gdf = stationary_outlines_gdf_evolving_lakes.copy(deep=True)\n",
    "evolving_stationary_outlines_union_evolving_lakes_gdf = stationary_outlines_gdf_evolving_lakes.copy(deep=True)\n",
    "evolving_stationary_outlines_union_all_lakes_gdf = stationary_outlines_gdf_all_lakes.copy(deep=True)\n",
    "\n",
    "# First process lakes with evolving outlines\n",
    "for idx, row in stationary_outlines_gdf_evolving_lakes.iterrows():\n",
    "    try:\n",
    "        lake_ps = stationary_outlines_gdf_evolving_lakes.loc[idx]\n",
    "        lake_name = lake_ps['name']\n",
    "        \n",
    "        try:\n",
    "            evolving_outlines_gdf = gpd.read_file('output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping because evolving outlines geojson file not found for {lake_name}.\")\n",
    "            continue\n",
    "            \n",
    "        # Process evolving outlines\n",
    "        evolving_union_gs = find_evolving_union(lake_ps, evolving_outlines_gdf, incl_stationary=False)\n",
    "        evolving_stationary_outlines_union_gs = find_evolving_union(lake_ps, evolving_outlines_gdf, incl_stationary=True)\n",
    "\n",
    "        if evolving_stationary_outlines_union_gs is None or evolving_union_gs is None:\n",
    "            print(f\"Skipping {lake_name}: Could not create union of outlines\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Store polygon from geoseries in geodataframes with CRS\n",
    "            evolving_outlines_union_gdf_idx = gpd.GeoDataFrame(\n",
    "                index=[0], crs='EPSG:3031', geometry=[evolving_union_gs.iloc[0]])\n",
    "            evolving_stationary_outlines_union_evolving_lakes_gdf_idx = gpd.GeoDataFrame(\n",
    "                index=[0], crs='EPSG:3031', geometry=[evolving_stationary_outlines_union_gs.iloc[0]])\n",
    "            \n",
    "            # Convert GeoDataFrames to EPSG:4326 CRS for geodesic area calculation\n",
    "            evolving_outlines_union_gdf_idx = evolving_outlines_union_gdf_idx.to_crs('4326')\n",
    "            evolving_stationary_outlines_union_evolving_lakes_gdf_idx = evolving_stationary_outlines_union_evolving_lakes_gdf_idx.to_crs('4326')\n",
    "            \n",
    "            # Update geometries in union GDFs using the correct index\n",
    "            evolving_outlines_union_gdf.loc[idx, 'geometry'] = evolving_union_gs.iloc[0]\n",
    "            evolving_stationary_outlines_union_evolving_lakes_gdf.loc[idx, 'geometry'] = evolving_stationary_outlines_union_gs.iloc[0]\n",
    "            \n",
    "            # Calculate and store areas\n",
    "            area1 = calculate_geodesic_area(evolving_outlines_union_gdf_idx['geometry'].iloc[0])\n",
    "            area2 = calculate_geodesic_area(evolving_stationary_outlines_union_evolving_lakes_gdf_idx['geometry'].iloc[0])\n",
    "            \n",
    "            if area1 is not None:\n",
    "                evolving_outlines_union_gdf.loc[idx, 'area (m^2)'] = area1\n",
    "            if area2 is not None:\n",
    "                evolving_stationary_outlines_union_evolving_lakes_gdf.loc[idx, 'area (m^2)'] = area2\n",
    "                \n",
    "            # Update the all_lakes GDF for this lake\n",
    "            mask = evolving_stationary_outlines_union_all_lakes_gdf['name'] == lake_name\n",
    "            if mask.any():\n",
    "                evolving_stationary_outlines_union_all_lakes_gdf.loc[mask, 'geometry'] = evolving_stationary_outlines_union_gs.iloc[0]\n",
    "                evolving_stationary_outlines_union_all_lakes_gdf.loc[mask, 'area (m^2)'] = area2\n",
    "                \n",
    "        except (IndexError, AttributeError) as e:\n",
    "            print(f\"Error processing geometries for {lake_name}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error processing {lake_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Then process remaining lakes for all_lakes GDF\n",
    "remaining_lakes = set(stationary_outlines_gdf_all_lakes['name']) - set(stationary_outlines_gdf_evolving_lakes['name'])\n",
    "for lake_name in remaining_lakes:\n",
    "    try:\n",
    "        # Get the lake's data using boolean indexing\n",
    "        mask = evolving_stationary_outlines_union_all_lakes_gdf['name'] == lake_name\n",
    "        if not mask.any():\n",
    "            print(f\"Lake {lake_name} not found in all_lakes GDF\")\n",
    "            continue\n",
    "            \n",
    "        lake_geom = evolving_stationary_outlines_union_all_lakes_gdf.loc[mask, 'geometry'].iloc[0]\n",
    "        \n",
    "        # Convert to 4326 for area calculation\n",
    "        temp_gdf = gpd.GeoDataFrame(\n",
    "            index=[0], crs='EPSG:3031', geometry=[lake_geom])\n",
    "        temp_gdf = temp_gdf.to_crs('4326')\n",
    "        area = calculate_geodesic_area(temp_gdf['geometry'].iloc[0])\n",
    "        \n",
    "        if area is not None:\n",
    "            evolving_stationary_outlines_union_all_lakes_gdf.loc[mask, 'area (m^2)'] = area\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing non-evolving lake {lake_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Delete intermediary GDFs\n",
    "del stationary_outlines_gdf_evolving_lakes, stationary_outlines_gdf_all_lakes\n",
    "\n",
    "# Make additional_lakes_gdf for Site A, LSLM, and LSLC whose stationary outlines were removed \n",
    "# because they were approximations using point location and reported area instead of an actual outline\n",
    "initial_lakes = ['Site_A', 'LowerConwaySubglacialLake', 'LowerEngelhardtSubglacialLake', 'LowerMercerSubglacialLake']\n",
    "additional_lakes_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'].isin(initial_lakes)].copy()\n",
    "\n",
    "# Add these additional lakes as rows to the union gdf's\n",
    "for idx, row in additional_lakes_gdf.iterrows():\n",
    "    try:\n",
    "        lake_ps = additional_lakes_gdf.loc[idx]\n",
    "        lake_name = lake_ps['name']\n",
    "        \n",
    "        try:\n",
    "            evolving_outlines_gdf = gpd.read_file('output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping because evolving outlines geojson file not found for {lake_name}.\")\n",
    "            continue\n",
    "           \n",
    "        # Only get evolving outlines union since there's no stationary outline\n",
    "        evolving_union_gs = find_evolving_union(lake_ps, evolving_outlines_gdf, incl_stationary=False)\n",
    "        \n",
    "        if evolving_union_gs is None:\n",
    "            print(f\"Skipping {lake_name}: Could not create union of outlines\")\n",
    "            continue\n",
    "           \n",
    "        try:\n",
    "            # Create temporary GeoDataFrame with the union\n",
    "            temp_gdf = gpd.GeoDataFrame(\n",
    "                index=[0], \n",
    "                crs='EPSG:3031', \n",
    "                geometry=[evolving_union_gs.iloc[0]])\n",
    "            \n",
    "            # Convert to 4326 for area calculation\n",
    "            temp_gdf = temp_gdf.to_crs('4326')\n",
    "            \n",
    "            # Calculate area\n",
    "            area = calculate_geodesic_area(temp_gdf['geometry'].iloc[0])\n",
    "            \n",
    "            # Create new row from the current lake's data\n",
    "            new_row_gdf = gpd.GeoDataFrame([{\n",
    "                'name': lake_name,\n",
    "                'area (m^2)': area if area is not None else None,\n",
    "                'cite': lake_ps['cite'],\n",
    "                'CS2_SARIn_start': lake_ps['CS2_SARIn_start'],\n",
    "                'geometry': evolving_union_gs.iloc[0]\n",
    "            }], crs='EPSG:3031')\n",
    "\n",
    "            # For evolving_outlines_union_gdf\n",
    "            if not lake_name in evolving_outlines_union_gdf['name'].values:\n",
    "                evolving_outlines_union_gdf = pd.concat([\n",
    "                    evolving_outlines_union_gdf, \n",
    "                    new_row_gdf\n",
    "                ], ignore_index=True)\n",
    "            else:\n",
    "                print(f'Skipping {lake_name} for evolving_outlines_union_gdf - already exists')\n",
    "                \n",
    "            # Check if the lake is already in either GeoDataFrame and only append if it's new\n",
    "            # For evolving_stationary_outlines_union_evolving_lakes_gdf\n",
    "            if not lake_name in evolving_stationary_outlines_union_evolving_lakes_gdf['name'].values:\n",
    "                evolving_stationary_outlines_union_evolving_lakes_gdf = pd.concat([\n",
    "                    evolving_stationary_outlines_union_evolving_lakes_gdf, \n",
    "                    new_row_gdf], ignore_index=True)\n",
    "            else:\n",
    "                print(f'Skipping {lake_name} for evolving_stationary_outlines_union_evolving_lakes_gdf - already exists')\n",
    "\n",
    "            # Check if the lake is already in either GeoDataFrame and only append if it's new\n",
    "            # For evolving_stationary_outlines_union_evolving_lakes_gdf\n",
    "            if not lake_name in evolving_stationary_outlines_union_all_lakes_gdf['name'].values:\n",
    "                evolving_stationary_outlines_union_all_lakes_gdf = pd.concat([\n",
    "                    evolving_stationary_outlines_union_all_lakes_gdf, \n",
    "                    new_row_gdf], ignore_index=True)\n",
    "            else:\n",
    "                print(f'Skipping {lake_name} for evolving_stationary_outlines_union_evolving_lakes_gdf - already exists')\n",
    "\n",
    "           \n",
    "        except (IndexError, AttributeError) as e:\n",
    "            print(f\"Error processing geometries for {lake_name}: {str(e)}\")\n",
    "            continue\n",
    "           \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error processing {lake_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Sort the GeoDataFrame alphabetically by the 'name' column; Reset the index after sorting; Reproject GeoDataFrame to EPSG:3031\n",
    "evolving_outlines_union_gdf = evolving_outlines_union_gdf.sort_values('name').reset_index(drop=True).set_crs('3031')\n",
    "evolving_stationary_outlines_union_evolving_lakes_gdf = evolving_stationary_outlines_union_evolving_lakes_gdf.sort_values('name').reset_index(drop=True)\n",
    "evolving_stationary_outlines_union_all_lakes_gdf = evolving_stationary_outlines_union_all_lakes_gdf.sort_values('name').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c94a8-b762-437b-8e93-be5a0b098df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows where processing failed (geometry is None or invalid)\n",
    "evolving_outlines_union_gdf = evolving_outlines_union_gdf[\n",
    "    evolving_outlines_union_gdf.geometry.notna() & \n",
    "    evolving_outlines_union_gdf.geometry.is_valid]\n",
    "\n",
    "evolving_stationary_outlines_union_evolving_lakes_gdf = evolving_stationary_outlines_union_evolving_lakes_gdf[\n",
    "    evolving_stationary_outlines_union_evolving_lakes_gdf.geometry.notna() & \n",
    "    evolving_stationary_outlines_union_evolving_lakes_gdf.geometry.is_valid]\n",
    "\n",
    "evolving_stationary_outlines_union_all_lakes_gdf = evolving_stationary_outlines_union_all_lakes_gdf[\n",
    "    evolving_stationary_outlines_union_all_lakes_gdf.geometry.notna() & \n",
    "    evolving_stationary_outlines_union_all_lakes_gdf.geometry.is_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d9d33b-b0c0-4abc-90e2-0141c34cd589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print lengths of GDFs to ensure everything worked properly\n",
    "print(\"Length of GDF's for all lakes:\")\n",
    "print(len(revised_stationary_outlines_gdf))\n",
    "print(len(evolving_stationary_outlines_union_all_lakes_gdf))\n",
    "\n",
    "# List all folders in target directory\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "\n",
    "# Filter stationary_outlines_gdf to only include lakes that have evolving outlines found at them\n",
    "stationary_outlines_gdf_evolving_lakes = filter_gdf_by_folder_contents(revised_stationary_outlines_gdf, folder_path, file_extension='geojson', exclude=False)\n",
    "print(\"Length of GDF's for lakes that were found to have evolving outlines:\")\n",
    "print(len(stationary_outlines_gdf_evolving_lakes))\n",
    "print(len(evolving_outlines_union_gdf))\n",
    "print(len(evolving_stationary_outlines_union_evolving_lakes_gdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920832b9-a1f2-43a6-9948-2ece19b69777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_names(df1, df2):\n",
    "    \"\"\"\n",
    "    Compare name columns between two dataframes and find unique entries.\n",
    "    \n",
    "    Parameters:\n",
    "    df1, df2: pandas DataFrames containing a 'name' column\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (names_only_in_df1, names_only_in_df2)\n",
    "    \"\"\"\n",
    "    # Find names that are in df1 but not in df2\n",
    "    only_in_df1 = df1[~df1['name'].isin(df2['name'])]['name']\n",
    "    \n",
    "    # Find names that are in df2 but not in df1\n",
    "    only_in_df2 = df2[~df2['name'].isin(df1['name'])]['name']\n",
    "    \n",
    "    return only_in_df1, only_in_df2\n",
    "\n",
    "# Example usage:\n",
    "names_df1_only, names_df2_only = compare_names(revised_stationary_outlines_gdf, evolving_stationary_outlines_union_all_lakes_gdf)\n",
    "print(\"Names only in first dataframe:\", names_df1_only.tolist())\n",
    "print(\"Names only in second dataframe:\", names_df2_only.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f100e4c8-4e31-41d3-837a-a49b6dbd3b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "stationary_outlines_gdf.boundary.plot(ax=ax, color='blue')\n",
    "evolving_outlines_union_gdf.boundary.plot(ax=ax, color='red')\n",
    "evolving_stationary_outlines_union_evolving_lakes_gdf.boundary.plot(ax=ax, color='purple')\n",
    "evolving_stationary_outlines_union_all_lakes_gdf.boundary.plot(ax=ax, color='k', linestyle='dashed')\n",
    "Scripps_landice.boundary.plot(ax=ax, lw=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5512d2-a3ea-4bb3-80fb-3c1fe8e3aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "evolving_outlines_union_gdf.to_file('output/lake_outlines/evolving_outlines_union_gdf.geojson', driver='GeoJSON')\n",
    "evolving_stationary_outlines_union_evolving_lakes_gdf.to_file('output/lake_outlines/evolving_stationary_outlines_union_evolving_lakes_gdf.geojson', driver='GeoJSON')\n",
    "evolving_stationary_outlines_union_all_lakes_gdf.to_file('output/lake_outlines/evolving_stationary_outlines_union_all_lakes_gdf.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d35df-ec1a-4713-9a9b-9e3c3d0c32ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import to ensure saved properly\n",
    "evolving_outlines_union_gdf = gpd.read_file(\n",
    "    'output/lake_outlines/evolving_outlines_union_gdf.geojson')\n",
    "evolving_stationary_outlines_union_evolving_lakes_gdf = gpd.read_file(\n",
    "    'output/lake_outlines/evolving_stationary_outlines_union_evolving_lakes_gdf.geojson')\n",
    "evolving_stationary_outlines_union_all_lakes_gdf = gpd.read_file(\n",
    "    'output/lake_outlines/evolving_stationary_outlines_union_all_lakes_gdf.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ae52fc-aa5a-4fb8-9028-f5000bc2bae5",
   "metadata": {},
   "source": [
    "# Forward fill\n",
    "To prepare for geometric calculations, we will make copies of the evolving outlines geojson files that foward fill  time steps with no evolving outlines with the most recent observation when available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c13c230-45b9-4702-bfd7-649e52b0b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_foward_fill_files(input_dir, output_dir, mid_cyc_dates):\n",
    "    \"\"\"\n",
    "    Generate forward fill or Last Observation Carried Forward (LOCF) versions of lake outline GeoJSON files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dir : str\n",
    "        Directory containing the original lake outline GeoJSON files\n",
    "    output_dir : str\n",
    "        Directory where the forward filled GeoJSON files will be saved\n",
    "    mid_cyc_dates : list\n",
    "        List of all dates (as datetime objects or strings) to use for the complete time series\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Ensure mid_cyc_dates are datetime objects and sorted\n",
    "    mid_cyc_dates = [pd.to_datetime(date) for date in mid_cyc_dates]\n",
    "    mid_cyc_dates = sorted(mid_cyc_dates)\n",
    "    \n",
    "    # Get list of all GeoJSON files in input directory\n",
    "    geojson_files = [f for f in os.listdir(input_dir) if f.endswith('.geojson')]\n",
    "    \n",
    "    for file in geojson_files:\n",
    "        lake_name = os.path.splitext(file)[0]\n",
    "        print(f\"Processing lake: {lake_name}\")\n",
    "        \n",
    "        # Read the original GeoDataFrame\n",
    "        input_file = os.path.join(input_dir, file)\n",
    "        try:\n",
    "            gdf = gpd.read_file(input_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {input_file}: {e}\")\n",
    "            continue\n",
    "            \n",
    "        if gdf.empty:\n",
    "            print(f\"Empty GeoDataFrame for {lake_name}, skipping.\")\n",
    "            continue\n",
    "            \n",
    "        # Sort by midcyc_datetime to ensure chronological order\n",
    "        if 'midcyc_datetime' in gdf.columns:\n",
    "            gdf['midcyc_datetime'] = pd.to_datetime(gdf['midcyc_datetime'])\n",
    "            gdf = gdf.sort_values('midcyc_datetime')\n",
    "            \n",
    "            # Create a new empty GeoDataFrame with the same structure as the input\n",
    "            # This ensures consistent dtypes from the beginning\n",
    "            forward_fill_rows = []\n",
    "            \n",
    "            last_valid_row = None\n",
    "            \n",
    "            # Use the provided mid_cyc_dates for forward fill processing\n",
    "            for date in mid_cyc_dates:\n",
    "                # Get rows for the current date\n",
    "                current_date_rows = gdf[gdf['midcyc_datetime'] == date]\n",
    "                \n",
    "                if not current_date_rows.empty:\n",
    "                    # If we have data for this date, add all rows\n",
    "                    for _, row in current_date_rows.iterrows():\n",
    "                        forward_fill_rows.append(row)\n",
    "                    \n",
    "                    # Update the last valid observation\n",
    "                    last_valid_row = current_date_rows.iloc[-1].copy()\n",
    "                elif last_valid_row is not None:\n",
    "                    # If no data for this date but we have a previous observation, carry it forward\n",
    "                    new_row = last_valid_row.copy()\n",
    "                    new_row['midcyc_datetime'] = date\n",
    "                    forward_fill_rows.append(new_row)\n",
    "                    \n",
    "                    print(f\"  Carried forward observation from {last_valid_row['midcyc_datetime']} to {date}\")\n",
    "            \n",
    "            # Create the result GeoDataFrame from the collected rows\n",
    "            if forward_fill_rows:\n",
    "                forward_fill_gdf = gpd.GeoDataFrame(forward_fill_rows, crs=gdf.crs)\n",
    "                \n",
    "                # Save the forward fill GeoDataFrame\n",
    "                output_file = os.path.join(output_dir, file)\n",
    "                forward_fill_gdf.to_file(output_file, driver='GeoJSON')\n",
    "                print(f\"  Saved forward fill file to {output_file}\")\n",
    "            else:\n",
    "                print(f\"  No rows after forward fill for {lake_name}\")\n",
    "\n",
    "            # Clear output of each index\n",
    "            clear_output(wait=True)\n",
    "\n",
    "        else:\n",
    "            print(f\"  No 'midcyc_datetime' column found in {lake_name}, copying original file.\")\n",
    "            # If no datetime column, just copy the original file\n",
    "            gdf.to_file(os.path.join(output_dir, file), driver='GeoJSON')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "029e9dd3-bcab-48fc-9c54-7c417e3dbd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lake: M1\n",
      "  Carried forward observation from 2021-08-17 00:45:00 to 2021-11-16 08:15:00\n",
      "  Carried forward observation from 2021-08-17 00:45:00 to 2022-02-15 15:45:00\n",
      "  Carried forward observation from 2021-08-17 00:45:00 to 2022-05-17 23:15:00\n",
      "  Carried forward observation from 2021-08-17 00:45:00 to 2022-08-17 06:45:00\n",
      "  Carried forward observation from 2021-08-17 00:45:00 to 2022-11-16 14:15:00\n",
      "  Carried forward observation from 2021-08-17 00:45:00 to 2023-02-15 21:45:00\n",
      "  Carried forward observation from 2021-08-17 00:45:00 to 2023-05-18 05:15:00\n",
      "  Carried forward observation from 2021-08-17 00:45:00 to 2023-08-17 12:45:00\n",
      "  Carried forward observation from 2021-08-17 00:45:00 to 2023-11-16 20:15:00\n",
      "  Carried forward observation from 2021-08-17 00:45:00 to 2024-02-16 03:45:00\n",
      "  Carried forward observation from 2021-08-17 00:45:00 to 2024-05-17 11:15:00\n",
      "  Carried forward observation from 2021-08-17 00:45:00 to 2024-08-16 18:45:00\n",
      "  Carried forward observation from 2021-08-17 00:45:00 to 2024-11-16 02:15:00\n",
      "  Saved forward fill file to /home/jovyan/1_evolving_lakes/Sauthoff-202X-evolving-lakes/output/lake_outlines/evolving_outlines/forward_fill/M1.geojson\n"
     ]
    }
   ],
   "source": [
    "generate_foward_fill_files(input_dir=OUTPUT_DIR_GIT + '/lake_outlines/evolving_outlines', \n",
    "                    output_dir=OUTPUT_DIR_GIT + '/lake_outlines/evolving_outlines/forward_fill', \n",
    "                    mid_cyc_dates=mid_cyc_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39d56f87-8122-483f-a6df-e28374f2ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_evolving_outlines_versions(filename, mid_cyc_dates, output_dir_git):\n",
    "    \"\"\"\n",
    "    Compare dates from mid_cyc_dates list with dates from the same GeoJSON file in two different directories.\n",
    "    Also includes the geometries associated with each date.\n",
    "    \n",
    "    Parameters:\n",
    "    filename (str): The name of the GeoJSON file to compare\n",
    "    mid_cyc_dates (list): List of dates to compare against\n",
    "    output_dir_git (str): Base directory path\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the comparison of dates and associated geometries\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    from datetime import datetime\n",
    "    import re\n",
    "    \n",
    "    # Define the two paths\n",
    "    path1 = os.path.join(output_dir_git, 'lake_outlines/evolving_outlines', f\"{filename}.geojson\")\n",
    "    path2 = os.path.join(output_dir_git, 'lake_outlines/evolving_outlines/forward_fill', f\"{filename}.geojson\")\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not os.path.exists(path1):\n",
    "        raise FileNotFoundError(f\"File not found: {path1}\")\n",
    "    if not os.path.exists(path2):\n",
    "        raise FileNotFoundError(f\"File not found: {path2}\")\n",
    "    \n",
    "    # Initialize variables to store dates and geometries\n",
    "    dates1 = []\n",
    "    dates2 = []\n",
    "    geometries1 = []\n",
    "    geometries2 = []\n",
    "    \n",
    "    # Read GeoJSON files using geopandas\n",
    "    try:\n",
    "        gdf1 = gpd.read_file(path1)\n",
    "        gdf2 = gpd.read_file(path2)\n",
    "        \n",
    "        # Store the full GeoDataFrames for later geometry extraction\n",
    "        gdf1_full = gdf1.copy()\n",
    "        gdf2_full = gdf2.copy()\n",
    "    except Exception as e:\n",
    "        # Fallback to manual JSON parsing if geopandas fails\n",
    "        print(f\"Falling back to manual JSON parsing: {e}\")\n",
    "        try:\n",
    "            with open(path1, 'r') as f1:\n",
    "                data1 = json.load(f1)\n",
    "            with open(path2, 'r') as f2:\n",
    "                data2 = json.load(f2)\n",
    "                \n",
    "            # Extract properties and geometries from features\n",
    "            dates1 = []\n",
    "            dates2 = []\n",
    "            geometries1 = []\n",
    "            geometries2 = []\n",
    "            \n",
    "            # Process file 1\n",
    "            if 'features' in data1:\n",
    "                for feature in data1['features']:\n",
    "                    if 'properties' in feature and 'mid_cyc_dates' in feature['properties']:\n",
    "                        dates1.append(feature['properties']['mid_cyc_dates'])\n",
    "                        geometries1.append(feature.get('geometry', None))\n",
    "            elif 'properties' in data1 and 'mid_cyc_dates' in data1['properties']:\n",
    "                # Handle case when the file is a single feature\n",
    "                dates1.append(data1['properties']['mid_cyc_dates'])\n",
    "                geometries1.append(data1.get('geometry', None))\n",
    "                \n",
    "            # Process file 2\n",
    "            if 'features' in data2:\n",
    "                for feature in data2['features']:\n",
    "                    if 'properties' in feature and 'mid_cyc_dates' in feature['properties']:\n",
    "                        dates2.append(feature['properties']['mid_cyc_dates'])\n",
    "                        geometries2.append(feature.get('geometry', None))\n",
    "            elif 'properties' in data2 and 'mid_cyc_dates' in data2['properties']:\n",
    "                # Handle case when the file is a single feature\n",
    "                dates2.append(data2['properties']['mid_cyc_dates'])\n",
    "                geometries2.append(data2.get('geometry', None))\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse GeoJSON file: {e}\")\n",
    "    else:\n",
    "        # If geopandas succeeded, extract the midcyc_datetime column and geometries\n",
    "        date_col_name = None\n",
    "        \n",
    "        # Determine which column contains the dates\n",
    "        if 'midcyc_datetime' in gdf1.columns:\n",
    "            date_col_name = 'midcyc_datetime'\n",
    "        elif 'properties' in gdf1.columns and isinstance(gdf1['properties'].iloc[0], dict):\n",
    "            # Handle case where properties are stored as a dictionary in a column\n",
    "            date_col_name = 'properties'\n",
    "        else:\n",
    "            # Try to find the column in properties\n",
    "            for col in gdf1.columns:\n",
    "                if 'midcyc_datetime' in col:\n",
    "                    date_col_name = col\n",
    "                    break\n",
    "            else:\n",
    "                # Try alternative column names\n",
    "                for col in gdf1.columns:\n",
    "                    if 'mid_cyc_dates' in col:\n",
    "                        date_col_name = col\n",
    "                        break\n",
    "                else:\n",
    "                    raise ValueError(\"Could not find midcyc_datetime or mid_cyc_dates in the first file\")\n",
    "        \n",
    "        # Extract dates and geometries from file 1\n",
    "        if date_col_name == 'properties':\n",
    "            # Extract from properties dictionary\n",
    "            dates1_with_idx = []\n",
    "            for idx, prop in enumerate(gdf1['properties']):\n",
    "                if prop and 'midcyc_datetime' in prop:\n",
    "                    dates1_with_idx.append((idx, prop.get('midcyc_datetime')))\n",
    "                elif prop and 'mid_cyc_dates' in prop:\n",
    "                    dates1_with_idx.append((idx, prop.get('mid_cyc_dates')))\n",
    "            \n",
    "            dates1 = [date for _, date in dates1_with_idx]\n",
    "            geometries1 = [gdf1.iloc[idx]['geometry'] for idx, _ in dates1_with_idx]\n",
    "        else:\n",
    "            # Extract directly from columns\n",
    "            dates1 = gdf1[date_col_name].tolist()\n",
    "            geometries1 = gdf1['geometry'].tolist()\n",
    "        \n",
    "        # Same for file 2\n",
    "        if 'midcyc_datetime' in gdf2.columns:\n",
    "            date_col_name = 'midcyc_datetime'\n",
    "        elif 'properties' in gdf2.columns and isinstance(gdf2['properties'].iloc[0], dict):\n",
    "            date_col_name = 'properties'\n",
    "        else:\n",
    "            # Try to find the column in properties\n",
    "            for col in gdf2.columns:\n",
    "                if 'midcyc_datetime' in col:\n",
    "                    date_col_name = col\n",
    "                    break\n",
    "            else:\n",
    "                # Try alternative column names\n",
    "                for col in gdf2.columns:\n",
    "                    if 'mid_cyc_dates' in col:\n",
    "                        date_col_name = col\n",
    "                        break\n",
    "                else:\n",
    "                    raise ValueError(\"Could not find midcyc_datetime or mid_cyc_dates in the second file\")\n",
    "        \n",
    "        # Extract dates and geometries from file 2\n",
    "        if date_col_name == 'properties':\n",
    "            # Extract from properties dictionary\n",
    "            dates2_with_idx = []\n",
    "            for idx, prop in enumerate(gdf2['properties']):\n",
    "                if prop and 'midcyc_datetime' in prop:\n",
    "                    dates2_with_idx.append((idx, prop.get('midcyc_datetime')))\n",
    "                elif prop and 'mid_cyc_dates' in prop:\n",
    "                    dates2_with_idx.append((idx, prop.get('mid_cyc_dates')))\n",
    "            \n",
    "            dates2 = [date for _, date in dates2_with_idx]\n",
    "            geometries2 = [gdf2.iloc[idx]['geometry'] for idx, _ in dates2_with_idx]\n",
    "        else:\n",
    "            # Extract directly from columns\n",
    "            dates2 = gdf2[date_col_name].tolist()\n",
    "            geometries2 = gdf2['geometry'].tolist()\n",
    "    \n",
    "    # Print debug info\n",
    "    print(f\"Found {len(dates1)} dates in first file\")\n",
    "    print(f\"Found {len(dates2)} dates in second file\")\n",
    "    print(f\"Number of dates in mid_cyc_dates: {len(mid_cyc_dates)}\")\n",
    "    \n",
    "    # Function to standardize date formats\n",
    "    def standardize_date(date_str):\n",
    "        if not date_str:\n",
    "            return None\n",
    "            \n",
    "        # Convert to string if it's not already\n",
    "        if not isinstance(date_str, str):\n",
    "            date_str = str(date_str)\n",
    "            \n",
    "        # Remove microseconds if present\n",
    "        date_str = re.sub(r'\\.(\\d+)', '', date_str)\n",
    "        \n",
    "        # Standardize T separator and space\n",
    "        date_str = date_str.replace('T', ' ')\n",
    "        \n",
    "        # Only keep date and time, remove timezone info if present\n",
    "        if '+' in date_str:\n",
    "            date_str = date_str.split('+')[0].strip()\n",
    "        \n",
    "        # Ensure we have seconds\n",
    "        if len(date_str.split(':')) == 2:\n",
    "            date_str += ':00'\n",
    "            \n",
    "        # Try to parse and re-format to ensure consistency\n",
    "        try:\n",
    "            # Parse the date with different formats\n",
    "            for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M']:\n",
    "                try:\n",
    "                    parsed_date = datetime.strptime(date_str, fmt)\n",
    "                    # Return in a standardized format\n",
    "                    return parsed_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            \n",
    "            # If none of the formats worked, return the original string\n",
    "            return date_str\n",
    "        except Exception:\n",
    "            # If parsing fails, return the cleaned string\n",
    "            return date_str\n",
    "    \n",
    "    # Standardize all dates\n",
    "    mid_cyc_dates_std = [standardize_date(date) for date in mid_cyc_dates]\n",
    "    dates1_std = [standardize_date(date) for date in dates1]\n",
    "    dates2_std = [standardize_date(date) for date in dates2]\n",
    "    \n",
    "    # Create dictionaries to map standardized dates to their geometries\n",
    "    date1_to_geom = {std_date: geom for std_date, geom in zip(dates1_std, geometries1) if std_date}\n",
    "    date2_to_geom = {std_date: geom for std_date, geom in zip(dates2_std, geometries2) if std_date}\n",
    "    \n",
    "    # Create a set of all unique standardized dates across all sources\n",
    "    all_dates = set(mid_cyc_dates_std) | set(dates1_std) | set(dates2_std)\n",
    "    all_dates = [date for date in all_dates if date]  # Filter out None values\n",
    "    \n",
    "    # Create comparison DataFrame with both original and standardized dates\n",
    "    comparison = pd.DataFrame({\n",
    "        'Date': sorted(list(all_dates)),\n",
    "        'In mid_cyc_dates': [date in mid_cyc_dates_std for date in sorted(list(all_dates))],\n",
    "        'In regular file': [date in dates1_std for date in sorted(list(all_dates))],\n",
    "        'In forward fill file': [date in dates2_std for date in sorted(list(all_dates))],\n",
    "        'Regular file geometry': [date1_to_geom.get(date, None) for date in sorted(list(all_dates))],\n",
    "        'forward fill file geometry': [date2_to_geom.get(date, None) for date in sorted(list(all_dates))],\n",
    "    })\n",
    "    \n",
    "    # Add original formats for reference\n",
    "    comparison['Dates in mid_cyc_dates'] = [\n",
    "        next((d for d in mid_cyc_dates if standardize_date(d) == date), None) \n",
    "        for date in comparison['Date']\n",
    "    ]\n",
    "    comparison['Original in regular file'] = [\n",
    "        next((d for d in dates1 if standardize_date(d) == date), None) \n",
    "        for date in comparison['Date']\n",
    "    ]\n",
    "    comparison['Original in forward fill file'] = [\n",
    "        next((d for d in dates2 if standardize_date(d) == date), None) \n",
    "        for date in comparison['Date']\n",
    "    ]\n",
    "    \n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c9b05235-02cd-4432-8c78-06d3b33b93f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 dates in first file\n",
      "Found 61 dates in second file\n",
      "Number of dates in mid_cyc_dates: 59\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>In mid_cyc_dates</th>\n",
       "      <th>In regular file</th>\n",
       "      <th>In forward fill file</th>\n",
       "      <th>Regular file geometry</th>\n",
       "      <th>forward fill file geometry</th>\n",
       "      <th>Dates in mid_cyc_dates</th>\n",
       "      <th>Original in regular file</th>\n",
       "      <th>Original in forward fill file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-08-17 06:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>POLYGON ((1985987.275371355 -1220812.280201133...</td>\n",
       "      <td>POLYGON ((1985987.275371355 -1220812.280201133...</td>\n",
       "      <td>2010-08-17 06:45:00</td>\n",
       "      <td>2010-08-17 06:45:00</td>\n",
       "      <td>2010-08-17 06:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-11-16 11:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>POLYGON ((1985987.275371355 -1219016.983478922...</td>\n",
       "      <td>POLYGON ((1985987.275371355 -1219016.983478922...</td>\n",
       "      <td>2010-11-16 11:15:00</td>\n",
       "      <td>2010-11-16 11:15:00</td>\n",
       "      <td>2010-11-16 11:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-02-15 15:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2011-02-15 15:45:00</td>\n",
       "      <td>2011-02-15 15:45:00</td>\n",
       "      <td>2011-02-15 15:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-05-17 23:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2011-05-17 23:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2011-05-17 23:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-08-17 06:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2011-08-17 06:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2011-08-17 06:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2011-11-16 11:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2011-11-16 11:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2011-11-16 11:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2012-02-15 15:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2012-02-15 15:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2012-02-15 15:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2012-05-16 23:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2012-05-16 23:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2012-05-16 23:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2012-08-16 06:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2012-08-16 06:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2012-08-16 06:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2012-11-15 23:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2012-11-15 23:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2012-11-15 23:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2013-02-15 15:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2013-02-15 15:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2013-02-15 15:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2013-05-17 23:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2013-05-17 23:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2013-05-17 23:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2013-08-17 06:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2013-08-17 06:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2013-08-17 06:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2013-11-16 11:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2013-11-16 11:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2013-11-16 11:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2014-02-15 15:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2014-02-15 15:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2014-02-15 15:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2014-05-17 23:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2014-05-17 23:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2014-05-17 23:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2014-08-17 06:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2014-08-17 06:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2014-08-17 06:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2014-11-16 11:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2014-11-16 11:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2014-11-16 11:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2015-02-15 15:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2015-02-15 15:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2015-02-15 15:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2015-05-17 23:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2015-05-17 23:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2015-05-17 23:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2015-08-17 06:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2015-08-17 06:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2015-08-17 06:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2015-11-16 11:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2015-11-16 11:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2015-11-16 11:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2016-02-15 15:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2016-02-15 15:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2016-02-15 15:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2016-05-16 23:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2016-05-16 23:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2016-05-16 23:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2016-08-16 06:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2016-08-16 06:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2016-08-16 06:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2016-11-15 23:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2016-11-15 23:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2016-11-15 23:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2017-02-15 15:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2017-02-15 15:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-02-15 15:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2017-05-17 23:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2017-05-17 23:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-05-17 23:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2017-08-17 06:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2017-08-17 06:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-08-17 06:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2017-11-16 11:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2017-11-16 11:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-11-16 11:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2018-02-15 15:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2018-02-15 15:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2018-02-15 15:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2018-05-17 23:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2018-05-17 23:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2018-05-17 23:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2018-08-17 06:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2018-08-17 06:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2018-08-17 06:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2018-11-16 11:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2018-11-16 11:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2018-11-16 11:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2019-02-15 12:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2019-02-15 12:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-02-15 12:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2019-02-15 21:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2019-02-15 21:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-02-15 21:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2019-05-18 05:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2019-05-18 05:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-05-18 05:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2019-08-17 12:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2019-08-17 12:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-08-17 12:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2019-11-16 20:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2019-11-16 20:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-11-16 20:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2020-02-16 03:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2020-02-16 03:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-02-16 03:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2020-05-17 11:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2020-05-17 11:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-05-17 11:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2020-08-16 18:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2020-08-16 18:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-08-16 18:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2020-11-16 02:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2020-11-16 02:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-11-16 02:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2021-02-15 09:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2021-02-15 09:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2021-02-15 09:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2021-05-17 17:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2021-05-17 17:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2021-05-17 17:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2021-08-17 00:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2021-08-17 00:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2021-08-17 00:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2021-11-16 08:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2021-11-16 08:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2021-11-16 08:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2022-02-15 15:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2022-02-15 15:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-02-15 15:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2022-05-17 23:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1986983.4010221758 -1220069.96177957...</td>\n",
       "      <td>2022-05-17 23:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-05-17 23:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2022-08-17 06:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>POLYGON ((1989971.7779746382 -1223813.24378515...</td>\n",
       "      <td>POLYGON ((1989971.7779746382 -1223813.24378515...</td>\n",
       "      <td>2022-08-17 06:45:00</td>\n",
       "      <td>2022-08-17 06:45:00</td>\n",
       "      <td>2022-08-17 06:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2022-11-16 14:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1989971.7779746382 -1223813.24378515...</td>\n",
       "      <td>2022-11-16 14:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-11-16 14:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2023-02-15 21:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1989971.7779746382 -1223813.24378515...</td>\n",
       "      <td>2023-02-15 21:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023-02-15 21:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2023-05-18 05:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1989971.7779746382 -1223813.24378515...</td>\n",
       "      <td>2023-05-18 05:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023-05-18 05:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2023-08-17 12:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1989971.7779746382 -1223813.24378515...</td>\n",
       "      <td>2023-08-17 12:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023-08-17 12:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2023-11-16 20:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1989971.7779746382 -1223813.24378515...</td>\n",
       "      <td>2023-11-16 20:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023-11-16 20:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2024-02-16 03:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1989971.7779746382 -1223813.24378515...</td>\n",
       "      <td>2024-02-16 03:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2024-02-16 03:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2024-05-17 11:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1989971.7779746382 -1223813.24378515...</td>\n",
       "      <td>2024-05-17 11:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2024-05-17 11:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2024-08-16 18:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1989971.7779746382 -1223813.24378515...</td>\n",
       "      <td>2024-08-16 18:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2024-08-16 18:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2024-11-16 02:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1989971.7779746382 -1223813.24378515...</td>\n",
       "      <td>2024-11-16 02:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2024-11-16 02:15:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Date  In mid_cyc_dates  In regular file  \\\n",
       "0   2010-08-17 06:45:00              True             True   \n",
       "1   2010-11-16 11:15:00              True             True   \n",
       "2   2011-02-15 15:45:00              True             True   \n",
       "3   2011-05-17 23:15:00              True            False   \n",
       "4   2011-08-17 06:45:00              True            False   \n",
       "5   2011-11-16 11:15:00              True            False   \n",
       "6   2012-02-15 15:45:00              True            False   \n",
       "7   2012-05-16 23:15:00              True            False   \n",
       "8   2012-08-16 06:45:00              True            False   \n",
       "9   2012-11-15 23:15:00              True            False   \n",
       "10  2013-02-15 15:45:00              True            False   \n",
       "11  2013-05-17 23:15:00              True            False   \n",
       "12  2013-08-17 06:45:00              True            False   \n",
       "13  2013-11-16 11:15:00              True            False   \n",
       "14  2014-02-15 15:45:00              True            False   \n",
       "15  2014-05-17 23:15:00              True            False   \n",
       "16  2014-08-17 06:45:00              True            False   \n",
       "17  2014-11-16 11:15:00              True            False   \n",
       "18  2015-02-15 15:45:00              True            False   \n",
       "19  2015-05-17 23:15:00              True            False   \n",
       "20  2015-08-17 06:45:00              True            False   \n",
       "21  2015-11-16 11:15:00              True            False   \n",
       "22  2016-02-15 15:45:00              True            False   \n",
       "23  2016-05-16 23:15:00              True            False   \n",
       "24  2016-08-16 06:45:00              True            False   \n",
       "25  2016-11-15 23:15:00              True            False   \n",
       "26  2017-02-15 15:45:00              True            False   \n",
       "27  2017-05-17 23:15:00              True            False   \n",
       "28  2017-08-17 06:45:00              True            False   \n",
       "29  2017-11-16 11:15:00              True            False   \n",
       "30  2018-02-15 15:45:00              True            False   \n",
       "31  2018-05-17 23:15:00              True            False   \n",
       "32  2018-08-17 06:45:00              True            False   \n",
       "33  2018-11-16 11:15:00              True            False   \n",
       "34  2019-02-15 12:45:00              True            False   \n",
       "35  2019-02-15 21:45:00              True            False   \n",
       "36  2019-05-18 05:15:00              True            False   \n",
       "37  2019-08-17 12:45:00              True            False   \n",
       "38  2019-11-16 20:15:00              True            False   \n",
       "39  2020-02-16 03:45:00              True            False   \n",
       "40  2020-05-17 11:15:00              True            False   \n",
       "41  2020-08-16 18:45:00              True            False   \n",
       "42  2020-11-16 02:15:00              True            False   \n",
       "43  2021-02-15 09:45:00              True            False   \n",
       "44  2021-05-17 17:15:00              True            False   \n",
       "45  2021-08-17 00:45:00              True            False   \n",
       "46  2021-11-16 08:15:00              True            False   \n",
       "47  2022-02-15 15:45:00              True            False   \n",
       "48  2022-05-17 23:15:00              True            False   \n",
       "49  2022-08-17 06:45:00              True             True   \n",
       "50  2022-11-16 14:15:00              True            False   \n",
       "51  2023-02-15 21:45:00              True            False   \n",
       "52  2023-05-18 05:15:00              True            False   \n",
       "53  2023-08-17 12:45:00              True            False   \n",
       "54  2023-11-16 20:15:00              True            False   \n",
       "55  2024-02-16 03:45:00              True            False   \n",
       "56  2024-05-17 11:15:00              True            False   \n",
       "57  2024-08-16 18:45:00              True            False   \n",
       "58  2024-11-16 02:15:00              True            False   \n",
       "\n",
       "    In forward fill file                              Regular file geometry  \\\n",
       "0                   True  POLYGON ((1985987.275371355 -1220812.280201133...   \n",
       "1                   True  POLYGON ((1985987.275371355 -1219016.983478922...   \n",
       "2                   True  POLYGON ((1986983.4010221758 -1220069.96177957...   \n",
       "3                   True                                               None   \n",
       "4                   True                                               None   \n",
       "5                   True                                               None   \n",
       "6                   True                                               None   \n",
       "7                   True                                               None   \n",
       "8                   True                                               None   \n",
       "9                   True                                               None   \n",
       "10                  True                                               None   \n",
       "11                  True                                               None   \n",
       "12                  True                                               None   \n",
       "13                  True                                               None   \n",
       "14                  True                                               None   \n",
       "15                  True                                               None   \n",
       "16                  True                                               None   \n",
       "17                  True                                               None   \n",
       "18                  True                                               None   \n",
       "19                  True                                               None   \n",
       "20                  True                                               None   \n",
       "21                  True                                               None   \n",
       "22                  True                                               None   \n",
       "23                  True                                               None   \n",
       "24                  True                                               None   \n",
       "25                  True                                               None   \n",
       "26                  True                                               None   \n",
       "27                  True                                               None   \n",
       "28                  True                                               None   \n",
       "29                  True                                               None   \n",
       "30                  True                                               None   \n",
       "31                  True                                               None   \n",
       "32                  True                                               None   \n",
       "33                  True                                               None   \n",
       "34                  True                                               None   \n",
       "35                  True                                               None   \n",
       "36                  True                                               None   \n",
       "37                  True                                               None   \n",
       "38                  True                                               None   \n",
       "39                  True                                               None   \n",
       "40                  True                                               None   \n",
       "41                  True                                               None   \n",
       "42                  True                                               None   \n",
       "43                  True                                               None   \n",
       "44                  True                                               None   \n",
       "45                  True                                               None   \n",
       "46                  True                                               None   \n",
       "47                  True                                               None   \n",
       "48                  True                                               None   \n",
       "49                  True  POLYGON ((1989971.7779746382 -1223813.24378515...   \n",
       "50                  True                                               None   \n",
       "51                  True                                               None   \n",
       "52                  True                                               None   \n",
       "53                  True                                               None   \n",
       "54                  True                                               None   \n",
       "55                  True                                               None   \n",
       "56                  True                                               None   \n",
       "57                  True                                               None   \n",
       "58                  True                                               None   \n",
       "\n",
       "                           forward fill file geometry Dates in mid_cyc_dates  \\\n",
       "0   POLYGON ((1985987.275371355 -1220812.280201133...    2010-08-17 06:45:00   \n",
       "1   POLYGON ((1985987.275371355 -1219016.983478922...    2010-11-16 11:15:00   \n",
       "2   POLYGON ((1986983.4010221758 -1220069.96177957...    2011-02-15 15:45:00   \n",
       "3   POLYGON ((1986983.4010221758 -1220069.96177957...    2011-05-17 23:15:00   \n",
       "4   POLYGON ((1986983.4010221758 -1220069.96177957...    2011-08-17 06:45:00   \n",
       "5   POLYGON ((1986983.4010221758 -1220069.96177957...    2011-11-16 11:15:00   \n",
       "6   POLYGON ((1986983.4010221758 -1220069.96177957...    2012-02-15 15:45:00   \n",
       "7   POLYGON ((1986983.4010221758 -1220069.96177957...    2012-05-16 23:15:00   \n",
       "8   POLYGON ((1986983.4010221758 -1220069.96177957...    2012-08-16 06:45:00   \n",
       "9   POLYGON ((1986983.4010221758 -1220069.96177957...    2012-11-15 23:15:00   \n",
       "10  POLYGON ((1986983.4010221758 -1220069.96177957...    2013-02-15 15:45:00   \n",
       "11  POLYGON ((1986983.4010221758 -1220069.96177957...    2013-05-17 23:15:00   \n",
       "12  POLYGON ((1986983.4010221758 -1220069.96177957...    2013-08-17 06:45:00   \n",
       "13  POLYGON ((1986983.4010221758 -1220069.96177957...    2013-11-16 11:15:00   \n",
       "14  POLYGON ((1986983.4010221758 -1220069.96177957...    2014-02-15 15:45:00   \n",
       "15  POLYGON ((1986983.4010221758 -1220069.96177957...    2014-05-17 23:15:00   \n",
       "16  POLYGON ((1986983.4010221758 -1220069.96177957...    2014-08-17 06:45:00   \n",
       "17  POLYGON ((1986983.4010221758 -1220069.96177957...    2014-11-16 11:15:00   \n",
       "18  POLYGON ((1986983.4010221758 -1220069.96177957...    2015-02-15 15:45:00   \n",
       "19  POLYGON ((1986983.4010221758 -1220069.96177957...    2015-05-17 23:15:00   \n",
       "20  POLYGON ((1986983.4010221758 -1220069.96177957...    2015-08-17 06:45:00   \n",
       "21  POLYGON ((1986983.4010221758 -1220069.96177957...    2015-11-16 11:15:00   \n",
       "22  POLYGON ((1986983.4010221758 -1220069.96177957...    2016-02-15 15:45:00   \n",
       "23  POLYGON ((1986983.4010221758 -1220069.96177957...    2016-05-16 23:15:00   \n",
       "24  POLYGON ((1986983.4010221758 -1220069.96177957...    2016-08-16 06:45:00   \n",
       "25  POLYGON ((1986983.4010221758 -1220069.96177957...    2016-11-15 23:15:00   \n",
       "26  POLYGON ((1986983.4010221758 -1220069.96177957...    2017-02-15 15:45:00   \n",
       "27  POLYGON ((1986983.4010221758 -1220069.96177957...    2017-05-17 23:15:00   \n",
       "28  POLYGON ((1986983.4010221758 -1220069.96177957...    2017-08-17 06:45:00   \n",
       "29  POLYGON ((1986983.4010221758 -1220069.96177957...    2017-11-16 11:15:00   \n",
       "30  POLYGON ((1986983.4010221758 -1220069.96177957...    2018-02-15 15:45:00   \n",
       "31  POLYGON ((1986983.4010221758 -1220069.96177957...    2018-05-17 23:15:00   \n",
       "32  POLYGON ((1986983.4010221758 -1220069.96177957...    2018-08-17 06:45:00   \n",
       "33  POLYGON ((1986983.4010221758 -1220069.96177957...    2018-11-16 11:15:00   \n",
       "34  POLYGON ((1986983.4010221758 -1220069.96177957...    2019-02-15 12:45:00   \n",
       "35  POLYGON ((1986983.4010221758 -1220069.96177957...    2019-02-15 21:45:00   \n",
       "36  POLYGON ((1986983.4010221758 -1220069.96177957...    2019-05-18 05:15:00   \n",
       "37  POLYGON ((1986983.4010221758 -1220069.96177957...    2019-08-17 12:45:00   \n",
       "38  POLYGON ((1986983.4010221758 -1220069.96177957...    2019-11-16 20:15:00   \n",
       "39  POLYGON ((1986983.4010221758 -1220069.96177957...    2020-02-16 03:45:00   \n",
       "40  POLYGON ((1986983.4010221758 -1220069.96177957...    2020-05-17 11:15:00   \n",
       "41  POLYGON ((1986983.4010221758 -1220069.96177957...    2020-08-16 18:45:00   \n",
       "42  POLYGON ((1986983.4010221758 -1220069.96177957...    2020-11-16 02:15:00   \n",
       "43  POLYGON ((1986983.4010221758 -1220069.96177957...    2021-02-15 09:45:00   \n",
       "44  POLYGON ((1986983.4010221758 -1220069.96177957...    2021-05-17 17:15:00   \n",
       "45  POLYGON ((1986983.4010221758 -1220069.96177957...    2021-08-17 00:45:00   \n",
       "46  POLYGON ((1986983.4010221758 -1220069.96177957...    2021-11-16 08:15:00   \n",
       "47  POLYGON ((1986983.4010221758 -1220069.96177957...    2022-02-15 15:45:00   \n",
       "48  POLYGON ((1986983.4010221758 -1220069.96177957...    2022-05-17 23:15:00   \n",
       "49  POLYGON ((1989971.7779746382 -1223813.24378515...    2022-08-17 06:45:00   \n",
       "50  POLYGON ((1989971.7779746382 -1223813.24378515...    2022-11-16 14:15:00   \n",
       "51  POLYGON ((1989971.7779746382 -1223813.24378515...    2023-02-15 21:45:00   \n",
       "52  POLYGON ((1989971.7779746382 -1223813.24378515...    2023-05-18 05:15:00   \n",
       "53  POLYGON ((1989971.7779746382 -1223813.24378515...    2023-08-17 12:45:00   \n",
       "54  POLYGON ((1989971.7779746382 -1223813.24378515...    2023-11-16 20:15:00   \n",
       "55  POLYGON ((1989971.7779746382 -1223813.24378515...    2024-02-16 03:45:00   \n",
       "56  POLYGON ((1989971.7779746382 -1223813.24378515...    2024-05-17 11:15:00   \n",
       "57  POLYGON ((1989971.7779746382 -1223813.24378515...    2024-08-16 18:45:00   \n",
       "58  POLYGON ((1989971.7779746382 -1223813.24378515...    2024-11-16 02:15:00   \n",
       "\n",
       "   Original in regular file Original in forward fill file  \n",
       "0       2010-08-17 06:45:00           2010-08-17 06:45:00  \n",
       "1       2010-11-16 11:15:00           2010-11-16 11:15:00  \n",
       "2       2011-02-15 15:45:00           2011-02-15 15:45:00  \n",
       "3                       NaT           2011-05-17 23:15:00  \n",
       "4                       NaT           2011-08-17 06:45:00  \n",
       "5                       NaT           2011-11-16 11:15:00  \n",
       "6                       NaT           2012-02-15 15:45:00  \n",
       "7                       NaT           2012-05-16 23:15:00  \n",
       "8                       NaT           2012-08-16 06:45:00  \n",
       "9                       NaT           2012-11-15 23:15:00  \n",
       "10                      NaT           2013-02-15 15:45:00  \n",
       "11                      NaT           2013-05-17 23:15:00  \n",
       "12                      NaT           2013-08-17 06:45:00  \n",
       "13                      NaT           2013-11-16 11:15:00  \n",
       "14                      NaT           2014-02-15 15:45:00  \n",
       "15                      NaT           2014-05-17 23:15:00  \n",
       "16                      NaT           2014-08-17 06:45:00  \n",
       "17                      NaT           2014-11-16 11:15:00  \n",
       "18                      NaT           2015-02-15 15:45:00  \n",
       "19                      NaT           2015-05-17 23:15:00  \n",
       "20                      NaT           2015-08-17 06:45:00  \n",
       "21                      NaT           2015-11-16 11:15:00  \n",
       "22                      NaT           2016-02-15 15:45:00  \n",
       "23                      NaT           2016-05-16 23:15:00  \n",
       "24                      NaT           2016-08-16 06:45:00  \n",
       "25                      NaT           2016-11-15 23:15:00  \n",
       "26                      NaT           2017-02-15 15:45:00  \n",
       "27                      NaT           2017-05-17 23:15:00  \n",
       "28                      NaT           2017-08-17 06:45:00  \n",
       "29                      NaT           2017-11-16 11:15:00  \n",
       "30                      NaT           2018-02-15 15:45:00  \n",
       "31                      NaT           2018-05-17 23:15:00  \n",
       "32                      NaT           2018-08-17 06:45:00  \n",
       "33                      NaT           2018-11-16 11:15:00  \n",
       "34                      NaT           2019-02-15 12:45:00  \n",
       "35                      NaT           2019-02-15 21:45:00  \n",
       "36                      NaT           2019-05-18 05:15:00  \n",
       "37                      NaT           2019-08-17 12:45:00  \n",
       "38                      NaT           2019-11-16 20:15:00  \n",
       "39                      NaT           2020-02-16 03:45:00  \n",
       "40                      NaT           2020-05-17 11:15:00  \n",
       "41                      NaT           2020-08-16 18:45:00  \n",
       "42                      NaT           2020-11-16 02:15:00  \n",
       "43                      NaT           2021-02-15 09:45:00  \n",
       "44                      NaT           2021-05-17 17:15:00  \n",
       "45                      NaT           2021-08-17 00:45:00  \n",
       "46                      NaT           2021-11-16 08:15:00  \n",
       "47                      NaT           2022-02-15 15:45:00  \n",
       "48                      NaT           2022-05-17 23:15:00  \n",
       "49      2022-08-17 06:45:00           2022-08-17 06:45:00  \n",
       "50                      NaT           2022-11-16 14:15:00  \n",
       "51                      NaT           2023-02-15 21:45:00  \n",
       "52                      NaT           2023-05-18 05:15:00  \n",
       "53                      NaT           2023-08-17 12:45:00  \n",
       "54                      NaT           2023-11-16 20:15:00  \n",
       "55                      NaT           2024-02-16 03:45:00  \n",
       "56                      NaT           2024-05-17 11:15:00  \n",
       "57                      NaT           2024-08-16 18:45:00  \n",
       "58                      NaT           2024-11-16 02:15:00  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'Wilkes_2'\n",
    "compare_evolving_outlines_versions(filename, mid_cyc_dates, OUTPUT_DIR_GIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38187942-2948-4996-a955-18782ee8a52a",
   "metadata": {},
   "source": [
    "# Geometric calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fcfcbcb3-cec0-4bb4-9e07-5e918fdcdaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load geodataframes needed for these geometric calculations\n",
    "revised_stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/revised_stationary_outlines_gdf.geojson')\n",
    "evolving_outlines_union_gdf = gpd.read_file('output/lake_outlines/evolving_outlines_union_gdf.geojson')\n",
    "evolving_stationary_outlines_union_evolving_lakes_gdf = gpd.read_file('output/lake_outlines/evolving_stationary_outlines_union_evolving_lakes_gdf.geojson')\n",
    "evolving_stationary_outlines_union_all_lakes_gdf = gpd.read_file('output/lake_outlines/evolving_stationary_outlines_union_all_lakes_gdf.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd862020-3685-4bf3-9f99-ad008c0326fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories before processing any lakes\n",
    "os.makedirs('output/geometric_calcs/evolving_outlines_geom_calc', exist_ok=True)\n",
    "os.makedirs('output/geometric_calcs/evolving_outlines_geom_calc/forward_fill', exist_ok=True)\n",
    "os.makedirs('output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes', exist_ok=True)\n",
    "os.makedirs('output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes', exist_ok=True)\n",
    "os.makedirs('output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes', exist_ok=True)\n",
    "os.makedirs('output/geometric_calcs/stationary_outline_geom_calc/evolving_stationary_union_at_evolving_lakes', exist_ok=True)\n",
    "os.makedirs('output/geometric_calcs/stationary_outline_geom_calc/evolving_stationary_union_at_all_lakes', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c966f417-1d75-41c5-8cef-f7e50a3f49c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All remaining lakes have been processed.\n"
     ]
    }
   ],
   "source": [
    "# Process geometric calculations at lakes where evolving outlines were found\n",
    "\n",
    "# Get list of remaining lakes left to process based on\n",
    "\n",
    "# 1) Filter out lakes that have evolving outlines\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "evolving_lakes_gdf = filter_gdf_by_folder_contents(revised_stationary_outlines_gdf, \n",
    "    folder_path, exclude=True, file_extension='txt')\n",
    "\n",
    "# 2) Filter out lakes that have already been processed in each directory\n",
    "folder_paths = [\n",
    "    OUTPUT_DIR_GIT + '/geometric_calcs/evolving_outlines_geom_calc/',\n",
    "    OUTPUT_DIR_GIT + '/geometric_calcs/evolving_outlines_geom_calc/forward_fill',\n",
    "    OUTPUT_DIR_GIT + '/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/',\n",
    "    OUTPUT_DIR_GIT + '/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes/',\n",
    "    OUTPUT_DIR_GIT + '/geometric_calcs/stationary_outline_geom_calc/evolving_stationary_union_at_evolving_lakes/',\n",
    "]\n",
    "\n",
    "# Create a list to store filtered GeoDataFrames\n",
    "filtered_gdfs = []\n",
    "\n",
    "# Filter the original GeoDataFrame for each directory and add to list\n",
    "for folder_path in folder_paths:\n",
    "    filtered_gdf = filter_gdf_by_folder_contents(evolving_lakes_gdf, folder_path)\n",
    "    if not filtered_gdf.empty:\n",
    "        filtered_gdfs.append(filtered_gdf)\n",
    "\n",
    "# Concatenate all filtered GeoDataFrames and drop duplicates\n",
    "if filtered_gdfs:\n",
    "    remaining_lakes = pd.concat(filtered_gdfs, ignore_index=True)\n",
    "    remaining_lakes = remaining_lakes.drop_duplicates(subset='name').reset_index(drop=True)\n",
    "else:\n",
    "    remaining_lakes = gpd.GeoDataFrame(columns=evolving_lakes_gdf.columns)\n",
    "\n",
    "print(f\"{len(remaining_lakes)} lake(s) to process.\")\n",
    "\n",
    "# Process geometric calculations on lakes\n",
    "total_lakes = len(remaining_lakes)\n",
    "processed_lakes = 0\n",
    "\n",
    "for i in remaining_lakes.index:\n",
    "    remaining_count = total_lakes - processed_lakes\n",
    "    print(f\"{remaining_count} lakes remain.\")\n",
    "    \n",
    "    try:\n",
    "        # Isolate lake from remaining_lakes as geodataframe\n",
    "        stationary_outline_gdf = remaining_lakes.loc[[i]]\n",
    "        lake_name = stationary_outline_gdf['name'].iloc[0]\n",
    "        \n",
    "        if stationary_outline_gdf.empty:\n",
    "            print(\"Skipping empty lake entry\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare data sets\n",
    "        dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(stationary_outline_gdf, 30)\n",
    "    \n",
    "        # Check if lake has evolving outlines\n",
    "        has_evolving_outlines = os.path.exists(os.path.join(\n",
    "            os.getcwd(), 'output/lake_outlines/evolving_outlines', f\"{stationary_outline_gdf['name'].iloc[0]}.geojson\"))\n",
    "        \n",
    "        if has_evolving_outlines:\n",
    "            # Calculate active area, dh, and dV for lakes with evolving outlines\n",
    "            evolving_outlines_geom_calc(stationary_outline_gdf=stationary_outline_gdf,\n",
    "                dataset1=dataset1_masked, dataset2=dataset2_masked)\n",
    "\n",
    "            # Calculate active area, dh, and dV for lakes with evolving outlines with forward fill\n",
    "            evolving_outlines_geom_calc(stationary_outline_gdf=stationary_outline_gdf,\n",
    "                dataset1=dataset1_masked, dataset2=dataset2_masked, forward_fill=True)\n",
    "\n",
    "            # And for stationary outlines (at evolving lakes only)\n",
    "            stationary_outline_geom_calc(stationary_outline_gdf=stationary_outline_gdf,\n",
    "               dataset1=dataset1_masked, dataset2=dataset2_masked, sub_dir='stationary_outlines_at_evolving_lakes')\n",
    "            \n",
    "            # And for evolving outlines union (at evolving lakes only)\n",
    "            evolving_union_gdf = evolving_outlines_union_gdf[\n",
    "                evolving_outlines_union_gdf['name'] == lake_name]\n",
    "\n",
    "            if not evolving_union_gdf.empty:\n",
    "                stationary_outline_geom_calc(stationary_outline_gdf=evolving_union_gdf,\n",
    "                    dataset1=dataset1_masked, dataset2=dataset2_masked, sub_dir='evolving_union_at_evolving_lakes')\n",
    "            else:\n",
    "                print(f\"No evolving union outline found for lake: {lake_name}\")\n",
    "            \n",
    "            # And for evolving-stationary outlines union (at evolving lakes only)\n",
    "            evolving_stationary_union_gdf = evolving_stationary_outlines_union_evolving_lakes_gdf[\n",
    "                evolving_stationary_outlines_union_evolving_lakes_gdf['name'] == lake_name]\n",
    "\n",
    "            if not evolving_stationary_union_gdf.empty:\n",
    "                stationary_outline_geom_calc(stationary_outline_gdf=evolving_stationary_union_gdf,\n",
    "                    dataset1=dataset1_masked, dataset2=dataset2_masked, sub_dir='evolving_stationary_union_at_evolving_lakes')\n",
    "            else:\n",
    "                print(f\"No evolving-stationary union outline found for lake: {lake_name}\")\n",
    "            \n",
    "        # Increment processing counter\n",
    "        processed_lakes += 1\n",
    "        \n",
    "        # Clear output of each index\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing lake '{lake_name}' at index {i}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "if processed_lakes == total_lakes:\n",
    "    print(\"All remaining lakes have been processed.\")\n",
    "else:\n",
    "    print(f\"Processing complete. {total_lakes - processed_lakes} lakes were skipped due to errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "222fc432-78dc-4a64-9655-a3a67e155748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All remaining lakes have been processed.\n"
     ]
    }
   ],
   "source": [
    "# Process geometric calculations at all previously identified lakes using stationary outlines\n",
    "\n",
    "# Get list of remaining lakes left to process based on\n",
    "\n",
    "# Filter out lakes that have already been processed in each directory\n",
    "folder_paths = [\n",
    "    OUTPUT_DIR_GIT + '/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes/',\n",
    "    OUTPUT_DIR_GIT + '/geometric_calcs/stationary_outline_geom_calc/evolving_stationary_union_at_all_lakes/'\n",
    "]\n",
    "\n",
    "# Create a list to store filtered GeoDataFrames\n",
    "filtered_gdfs = []\n",
    "\n",
    "# Filter the original GeoDataFrame for each directory and add to list\n",
    "for folder_path in folder_paths:\n",
    "    filtered_gdf = filter_gdf_by_folder_contents(revised_stationary_outlines_gdf, folder_path)\n",
    "    if not filtered_gdf.empty:\n",
    "        filtered_gdfs.append(filtered_gdf)\n",
    "\n",
    "# Concatenate all filtered GeoDataFrames and drop duplicates\n",
    "if filtered_gdfs:\n",
    "    remaining_lakes = pd.concat(filtered_gdfs, ignore_index=True)\n",
    "    remaining_lakes = remaining_lakes.drop_duplicates(subset='name').reset_index(drop=True)\n",
    "else:\n",
    "    remaining_lakes = gpd.GeoDataFrame(columns=revised_stationary_outlines_gdf.columns)\n",
    "\n",
    "print(f\"{len(remaining_lakes)} lake(s) to process.\")\n",
    "\n",
    "# Process geometric calculations on lakes\n",
    "total_lakes = len(remaining_lakes)\n",
    "processed_lakes = 0\n",
    "\n",
    "for i in remaining_lakes.index:\n",
    "    remaining_count = total_lakes - processed_lakes\n",
    "    print(f\"{remaining_count} lakes remain.\")\n",
    "    \n",
    "    try:\n",
    "        # Isolate lake from remaining_lakes as geodataframe\n",
    "        stationary_outline_gdf = remaining_lakes.loc[[i]]\n",
    "        lake_name = stationary_outline_gdf['name'].iloc[0]\n",
    "        \n",
    "        if stationary_outline_gdf.empty:\n",
    "            print(\"Skipping empty lake entry\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare data sets\n",
    "        dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(stationary_outline_gdf, 30)\n",
    "\n",
    "        # And for stationary outlines (at all lakes)\n",
    "        stationary_outline_geom_calc(stationary_outline_gdf=stationary_outline_gdf,\n",
    "           dataset1=dataset1_masked, dataset2=dataset2_masked, sub_dir='stationary_outlines_at_all_lakes')\n",
    "\n",
    "        # And for evolving-stationary outlines union (at all lakes)\n",
    "        evolving_stationary_union_gdf = evolving_stationary_outlines_union_all_lakes_gdf[\n",
    "            evolving_stationary_outlines_union_all_lakes_gdf['name'] == lake_name]\n",
    "        if not evolving_stationary_union_gdf.empty:\n",
    "            stationary_outline_geom_calc(stationary_outline_gdf=evolving_stationary_union_gdf,\n",
    "                dataset1=dataset1_masked, dataset2=dataset2_masked, sub_dir='evolving_stationary_union_at_all_lakes')\n",
    "        else:\n",
    "            print(f\"No evolving-stationary union outline found for lake: {lake_name}\")\n",
    "            \n",
    "        # Increment processing counter\n",
    "        processed_lakes += 1\n",
    "        # Clear output of each index\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing lake '{lake_name}' at index {i}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "if processed_lakes == total_lakes:\n",
    "    print(\"All remaining lakes have been processed.\")\n",
    "else:\n",
    "    print(f\"Processing complete. {total_lakes - processed_lakes} lakes were skipped due to errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e438f2-91c7-42c3-9d8b-b4b967af55f0",
   "metadata": {},
   "source": [
    "# Visualizations of geometric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7dc31d79-fba8-4065-9f4b-2862db687bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 lake(s) remain.\n",
      "Processing lake: M1\n",
      "Error processing lake M1: 'midcyc_datetime'\n",
      "43 lake(s) remain.\n",
      "Processing lake: M1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# plot_evolving_and_stationary_comparison(lake_gdf)\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[43mplot_evolving_and_stationary_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlake_gdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing lake \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlake_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[60], line 129\u001b[0m, in \u001b[0;36mplot_evolving_and_stationary_comparison\u001b[0;34m(lake_gdf, forward_fill)\u001b[0m\n\u001b[1;32m    126\u001b[0m mask_y \u001b[38;5;241m=\u001b[39m (moa_highres_da\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m y_min\u001b[38;5;241m-\u001b[39my_buffer) \u001b[38;5;241m&\u001b[39m (moa_highres_da\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m y_max\u001b[38;5;241m+\u001b[39my_buffer)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Plot stationary and evolving outlines onto MOA surface imagery\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m moa_highres_da_subset \u001b[38;5;241m=\u001b[39m \u001b[43mmoa_highres_da\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_x\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m ax[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(moa_highres_da_subset[\u001b[38;5;241m0\u001b[39m,:,:], cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m\"\u001b[39m, clim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m14000\u001b[39m, \u001b[38;5;241m17000\u001b[39m], extent\u001b[38;5;241m=\u001b[39m[x_min\u001b[38;5;241m-\u001b[39mx_buffer, x_max\u001b[38;5;241m+\u001b[39mx_buffer, y_min\u001b[38;5;241m-\u001b[39my_buffer, y_max\u001b[38;5;241m+\u001b[39my_buffer])\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Plot evolving outlines\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/core/common.py:1189\u001b[0m, in \u001b[0;36mDataWithCoords.where\u001b[0;34m(self, cond, other, drop)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cond, (Dataset, DataArray)):\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1186\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcond argument is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcond\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m but must be a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDataset\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDataArray\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m (or a callable than returns one).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1187\u001b[0m     )\n\u001b[0;32m-> 1189\u001b[0m \u001b[38;5;28mself\u001b[39m, cond \u001b[38;5;241m=\u001b[39m \u001b[43malign\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dataarray_indexer\u001b[39m(dim: Hashable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataArray:\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cond\u001b[38;5;241m.\u001b[39many(dim\u001b[38;5;241m=\u001b[39m(d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m cond\u001b[38;5;241m.\u001b[39mdims \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;241m!=\u001b[39m dim))\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/core/alignment.py:882\u001b[0m, in \u001b[0;36malign\u001b[0;34m(join, copy, indexes, exclude, fill_value, *objects)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03mGiven any number of Dataset and/or DataArray objects, returns new\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;124;03mobjects with aligned indexes and dimension sizes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    872\u001b[0m \n\u001b[1;32m    873\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    874\u001b[0m aligner \u001b[38;5;241m=\u001b[39m Aligner(\n\u001b[1;32m    875\u001b[0m     objects,\n\u001b[1;32m    876\u001b[0m     join\u001b[38;5;241m=\u001b[39mjoin,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    880\u001b[0m     fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[1;32m    881\u001b[0m )\n\u001b[0;32m--> 882\u001b[0m \u001b[43maligner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m aligner\u001b[38;5;241m.\u001b[39mresults\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/core/alignment.py:582\u001b[0m, in \u001b[0;36mAligner.align\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 582\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/core/alignment.py:557\u001b[0m, in \u001b[0;36mAligner.reindex_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreindex_all\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 557\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_one(obj, matching_indexes)\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m obj, matching_indexes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    560\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects_matching_indexes\n\u001b[1;32m    561\u001b[0m         )\n\u001b[1;32m    562\u001b[0m     )\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/core/alignment.py:558\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreindex_all\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m--> 558\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatching_indexes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m obj, matching_indexes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    560\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects_matching_indexes\n\u001b[1;32m    561\u001b[0m         )\n\u001b[1;32m    562\u001b[0m     )\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/core/alignment.py:546\u001b[0m, in \u001b[0;36mAligner._reindex_one\u001b[0;34m(self, obj, matching_indexes)\u001b[0m\n\u001b[1;32m    543\u001b[0m new_indexes, new_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_indexes_and_vars(obj, matching_indexes)\n\u001b[1;32m    544\u001b[0m dim_pos_indexers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dim_pos_indexers(matching_indexes)\n\u001b[0;32m--> 546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_callback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_pos_indexers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_indexes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclude_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclude_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/core/dataarray.py:1909\u001b[0m, in \u001b[0;36mDataArray._reindex_callback\u001b[0;34m(self, aligner, dim_pos_indexers, variables, indexes, fill_value, exclude_dims, exclude_vars)\u001b[0m\n\u001b[1;32m   1906\u001b[0m         fill_value[_THIS_ARRAY] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m   1908\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_temp_dataset()\n\u001b[0;32m-> 1909\u001b[0m reindexed \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_callback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1910\u001b[0m \u001b[43m    \u001b[49m\u001b[43maligner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_pos_indexers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1919\u001b[0m da \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_temp_dataset(reindexed)\n\u001b[1;32m   1920\u001b[0m da\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/core/dataset.py:3517\u001b[0m, in \u001b[0;36mDataset._reindex_callback\u001b[0;34m(self, aligner, dim_pos_indexers, variables, indexes, fill_value, exclude_dims, exclude_vars)\u001b[0m\n\u001b[1;32m   3515\u001b[0m         reindexed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overwrite_indexes(new_indexes, new_variables)\n\u001b[1;32m   3516\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3517\u001b[0m         reindexed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maligner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3519\u001b[0m     to_reindex \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3520\u001b[0m         k: v\n\u001b[1;32m   3521\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   3522\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m variables \u001b[38;5;129;01mand\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude_vars\n\u001b[1;32m   3523\u001b[0m     }\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/core/dataset.py:1366\u001b[0m, in \u001b[0;36mDataset.copy\u001b[0;34m(self, deep, data)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m, deep: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, data: DataVars \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a copy of this dataset.\u001b[39;00m\n\u001b[1;32m   1271\u001b[0m \n\u001b[1;32m   1272\u001b[0m \u001b[38;5;124;03m    If `deep=True`, a deep copy is made of each of the component variables.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;124;03m    pandas.DataFrame.copy\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/core/dataset.py:1402\u001b[0m, in \u001b[0;36mDataset._copy\u001b[0;34m(self, deep, data, memo)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         variables[k] \u001b[38;5;241m=\u001b[39m index_vars[k]\n\u001b[1;32m   1401\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1402\u001b[0m         variables[k] \u001b[38;5;241m=\u001b[39m \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1404\u001b[0m attrs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attrs, memo) \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;28;01melse\u001b[39;00m copy\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attrs)\n\u001b[1;32m   1405\u001b[0m encoding \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1406\u001b[0m     copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encoding, memo) \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;28;01melse\u001b[39;00m copy\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encoding)\n\u001b[1;32m   1407\u001b[0m )\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/core/variable.py:925\u001b[0m, in \u001b[0;36mVariable._copy\u001b[0;34m(self, deep, data, memo)\u001b[0m\n\u001b[1;32m    922\u001b[0m         ndata \u001b[38;5;241m=\u001b[39m indexing\u001b[38;5;241m.\u001b[39mMemoryCachedArray(data_old\u001b[38;5;241m.\u001b[39marray)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 925\u001b[0m         ndata \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mndata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m     ndata \u001b[38;5;241m=\u001b[39m as_compatible_data(data)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    151\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ce815424404b07b5df4fc5a2fc013e",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "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",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,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' width=1600.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5bc1e5957744d61a316f7d9de61cb0f",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "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",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,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' width=1600.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load datasets necessary for plotting\n",
    "revised_stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/revised_stationary_outlines_gdf.geojson')\n",
    "evolving_outlines_union_gdf = gpd.read_file('output/lake_outlines/evolving_outlines_union_gdf.geojson')\n",
    "evolving_stationary_outlines_union_evolving_lakes_gdf = gpd.read_file('output/lake_outlines/evolving_stationary_outlines_union_evolving_lakes_gdf.geojson')\n",
    "evolving_stationary_outlines_union_all_lakes_gdf = gpd.read_file('output/lake_outlines/evolving_stationary_outlines_union_all_lakes_gdf.geojson')\n",
    "\n",
    "# Get list of remaining lakes left to process based on\n",
    "# 1) lakes that have evolving outlines\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "remaining_lakes = filter_gdf_by_folder_contents(revised_stationary_outlines_gdf, \n",
    "    folder_path, exclude=True, file_extension='txt')\n",
    "\n",
    "# 2) lakes that have already be processed using the desired func\n",
    "folder_path = OUTPUT_DIR + '/plot_evolving_and_stationary_comparison/forward_fill'\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "remaining_lakes = filter_gdf_by_folder_contents(remaining_lakes, folder_path, exclude=True)\n",
    "\n",
    "# Reset index to avoid indexing issues\n",
    "remaining_lakes = remaining_lakes.reset_index(drop=True)\n",
    "\n",
    "if remaining_lakes.empty:\n",
    "    print(\"All lakes processed.\")\n",
    "else:\n",
    "    while not remaining_lakes.empty:\n",
    "        print(f\"{len(remaining_lakes)} lake(s) remain.\")\n",
    "        \n",
    "        # Always get the first lake to process\n",
    "        lake_gdf = remaining_lakes.iloc[[0]]  # Always take the first row\n",
    "        lake_name = lake_gdf['name'].iloc[0]\n",
    "\n",
    "        if lake_gdf.empty:\n",
    "            print(\"Warning: Empty lake_gdf. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # plot_evolving_and_stationary_comparison(lake_gdf)\n",
    "            plot_evolving_and_stationary_comparison(lake_gdf, forward_fill=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing lake {lake_name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        # Recheck remaining lakes after processing\n",
    "        remaining_lakes = filter_gdf_by_folder_contents(remaining_lakes, folder_path)\n",
    "\n",
    "        if remaining_lakes is None or remaining_lakes.empty:\n",
    "            print(\"No lakes remain after recheck.\")\n",
    "            break\n",
    "\n",
    "        # Reset index to prevent index misalignment\n",
    "        remaining_lakes = remaining_lakes.reset_index(drop=True)\n",
    "\n",
    "        # Clear output of each index\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(\"All lakes processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1c6fceb9-a3f3-4cb0-baeb-f7557a0b6933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get list of remaining lakes left to process based on\n",
    "\n",
    "# # 1) lakes that have evolving outlines\n",
    "# folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "# remaining_lakes = filter_gdf_by_folder_contents(revised_stationary_outlines_gdf, \n",
    "#     folder_path, file_extension='txt')\n",
    "\n",
    "# # 2) lakes that have already be processed\n",
    "# folder_path = OUTPUT_DIR + '/plot_evolving_and_stationary_comparison_sequential'\n",
    "# os.makedirs(folder_path, exist_ok=True)\n",
    "# remaining_lakes = filter_gdf_by_folder_contents(remaining_lakes, folder_path, file_extension='mp4')\n",
    "\n",
    "# if remaining_lakes.empty:\n",
    "#     print(\"All lakes processed.\")\n",
    "# else:\n",
    "#     for i in range(len(remaining_lakes)):\n",
    "#         print(len(remaining_lakes), 'lake(s) remain.')\n",
    "        \n",
    "#         # Process the lake\n",
    "#         lake_gdf = remaining_lakes.iloc[0:1]\n",
    "#         plot_evolving_and_stationary_comparison_sequential(lake_gdf)\n",
    "\n",
    "#         # Recheck which lakes still need processing\n",
    "#         remaining_lakes = filter_gdf_by_folder_contents(remaining_lakes, folder_path)\n",
    "\n",
    "#         # Clear output of each index\n",
    "#         clear_output(wait=True)\n",
    "\n",
    "#         if remaining_lakes.empty:\n",
    "#             print(\"All lakes processed.\")\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78b888c-22cd-47a7-8c12-d206ac090d7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TESTING: help me to combine these two code blocks into one that will check both plot_evolving_and_stationary_comparison and plot_evolving_and_stationary_comparison_sequential folders, and if the lake has not been processed in both, then run both functions, plot_evolving_and_stationary_comparison and plot_evolving_and_stationary_comparison_sequential.\n",
    "\n",
    "# Generate visualizations of geometric data\n",
    "\n",
    "# Load geodataframes needed\n",
    "revised_stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/revised_stationary_outlines_gdf.geojson')\n",
    "evolving_outlines_union_gdf = gpd.read_file('output/lake_outlines/evolving_outlines_union_gdf.geojson')\n",
    "evolving_stationary_outlines_union_evolving_lakes_gdf = gpd.read_file('output/lake_outlines/evolving_stationary_outlines_union_evolving_lakes_gdf.geojson')\n",
    "# evolving_stationary_outlines_union_all_lakes_gdf = gpd.read_file('output/lake_outlines/evolving_stationary_outlines_union_all_lakes_gdf.geojson')\n",
    "\n",
    "# Get list of remaining lakes left to process based on\n",
    "# 1) lakes that have evolving outlines\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "remaining_lakes = filter_gdf_by_folder_contents(revised_stationary_outlines_gdf, \n",
    "    folder_path, exclude=True, file_extension='txt')\n",
    "\n",
    "# 2) Check which lakes need processing in either of the two output folders\n",
    "comparison_folder = OUTPUT_DIR + '/plot_evolving_and_stationary_comparison'\n",
    "sequential_folder = OUTPUT_DIR + '/plot_evolving_and_stationary_comparison_sequential'\n",
    "\n",
    "# Make sure both output directories exist\n",
    "os.makedirs(comparison_folder, exist_ok=True)\n",
    "os.makedirs(sequential_folder, exist_ok=True)\n",
    "\n",
    "# Reset index to avoid indexing issues\n",
    "remaining_lakes = remaining_lakes.reset_index(drop=True)\n",
    "\n",
    "if remaining_lakes.empty:\n",
    "    print(\"All lakes processed.\")\n",
    "else:\n",
    "    while not remaining_lakes.empty:\n",
    "        print(f\"{len(remaining_lakes)} lake(s) remain.\")\n",
    "        \n",
    "        # Always get the first lake to process\n",
    "        lake_gdf = remaining_lakes.iloc[[0]]  # Always take the first row\n",
    "        lake_name = lake_gdf['name'].iloc[0]\n",
    "        \n",
    "        if lake_gdf.empty:\n",
    "            print(f\"Warning: Empty lake_gdf for {lake_name}. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Check if the lake needs processing in the comparison folder\n",
    "        needs_comparison = not lake_gdf['name'].iloc[0] in [\n",
    "            os.path.splitext(f)[0] for f in os.listdir(comparison_folder)\n",
    "        ]\n",
    "        \n",
    "        # Check if the lake needs processing in the sequential folder\n",
    "        needs_sequential = not lake_gdf['name'].iloc[0] in [\n",
    "            os.path.splitext(f)[0] for f in os.listdir(sequential_folder) \n",
    "            if f.endswith('.mp4')\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            # Process with the first function if needed\n",
    "            if needs_comparison:\n",
    "                print(f\"Processing {lake_name} with plot_evolving_and_stationary_comparison\")\n",
    "                plot_evolving_and_stationary_comparison(lake_gdf)\n",
    "                plot_evolving_and_stationary_comparison(lake_gdf, forward_fill=True)\n",
    "            \n",
    "            # Process with the second function if needed\n",
    "            if needs_sequential:\n",
    "                print(f\"Processing {lake_name} with plot_evolving_and_stationary_comparison_sequential\")\n",
    "                plot_evolving_and_stationary_comparison_sequential(lake_gdf)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing lake {lake_name}: {str(e)}\")\n",
    "            continue\n",
    "        \n",
    "        # Recheck remaining lakes after processing\n",
    "        # A lake is considered processed if it exists in both folders\n",
    "        processed_in_comparison = filter_gdf_by_folder_contents(\n",
    "            remaining_lakes, comparison_folder, exclude=False)\n",
    "        processed_in_sequential = filter_gdf_by_folder_contents(\n",
    "            remaining_lakes, sequential_folder, exclude=False, file_extension='mp4')\n",
    "        \n",
    "        # Lakes that have been processed in both folders\n",
    "        processed_lakes = pd.merge(\n",
    "            processed_in_comparison, processed_in_sequential, \n",
    "            on=list(processed_in_comparison.columns)\n",
    "        )\n",
    "        \n",
    "        # Remove processed lakes from remaining_lakes\n",
    "        if not processed_lakes.empty:\n",
    "            remaining_lakes = remaining_lakes[\n",
    "                ~remaining_lakes['name'].isin(processed_lakes['name'])\n",
    "            ]\n",
    "        \n",
    "        # Reset index to prevent index misalignment\n",
    "        remaining_lakes = remaining_lakes.reset_index(drop=True)\n",
    "        \n",
    "        # Clear output of each index\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        if remaining_lakes.empty:\n",
    "            print(\"All lakes processed.\")\n",
    "            break\n",
    "            \n",
    "    print(\"All lakes processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e55cf8a-3eea-4bf5-8daa-3f199633f87b",
   "metadata": {},
   "source": [
    "# Final check that all lakes have been processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92df904-9b3d-4471-8bf0-d12206e32e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load geodataframe needed\n",
    "revised_stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/revised_stationary_outlines_gdf.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ff17b42-ea5b-4e56-b44a-0daf34850c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis done on all previously identified lakes\n",
      "157 lakes reanalyzed\n",
      "156 lakes analyzed using find_and_save_optimal_paraters func\n",
      "138 lakes found to have evolving outlines\n",
      "18 lakes found to have no evolving outlines\n",
      "156 sum of lakes found with and without evolving outlines\n",
      "\n",
      "Analysis done only on previously identified lakes found to have evolving outlines\n",
      "156 lakes analyzed in revised inventory due to Site_B and Site_C being combined into Site_BC\n",
      "156 lakes analyzed using stationary_outline_geom_calc/stationary_outlines_at_all_lakes\n",
      "156 lakes analyzed using stationary_outline_geom_calc/evolving_stationary_union_at_all_lakes\n",
      "94 lakes analyzed using evolving_outlines_geom_calc\n",
      "93 lakes analyzed using stationary_outline_geom_calc/evolving_union_at_evolving_lakes\n",
      "93 lakes analyzed using stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes\n",
      "93 lakes analyzed using stationary_outline_geom_calc/evolving_stationary_union_at_evolving_lakes\n",
      "49 lakes analyzed using plot_evolving_and_stationary_comparison\n",
      "49 lakes analyzed using plot_evolving_and_stationary_comparison_sequential\n"
     ]
    }
   ],
   "source": [
    "# Ensure all lakes were reanalyzed\n",
    "print('Analysis done on all previously identified lakes')\n",
    "\n",
    "print(len(stationary_outlines_gdf), 'lakes reanalyzed')\n",
    "\n",
    "dir = OUTPUT_DIR + '/levels'\n",
    "print(len([f for f in os.listdir(dir) if os.path.isfile(os.path.join(dir, f))]), \n",
    "      'lakes analyzed using find_and_save_optimal_paraters func')\n",
    "\n",
    "# Breakdown of lakes where evolving outlines were found vs. not\n",
    "print(len([f for f in os.listdir('output/lake_outlines/evolving_outlines') if f.endswith('.geojson')]),\n",
    "      'lakes found to have evolving outlines')\n",
    "print(len([f for f in os.listdir('output/lake_outlines/evolving_outlines') if f.endswith('.txt')]),\n",
    "      'lakes found to have no evolving outlines')\n",
    "print(len([f for f in os.listdir('output/lake_outlines/evolving_outlines') if f.endswith('.geojson')]) +\n",
    "      len([f for f in os.listdir('output/lake_outlines/evolving_outlines') if f.endswith('.txt')]), \n",
    "      'sum of lakes found with and without evolving outlines')\n",
    "\n",
    "# Analysis done on evolving lakes\n",
    "print('\\nAnalysis done only on previously identified lakes found to have evolving outlines')\n",
    "\n",
    "# These directories will have four more csv files because of continental sums files for four sets of lakes\n",
    "# so if this code is rerun after the 'Continental summations' section, the counts will be higher than expected\n",
    "\n",
    "print(len(revised_stationary_outlines_gdf), \n",
    "    'lakes analyzed in revised inventory due to Site_B and Site_C being combined into Site_BC')\n",
    "\n",
    "dir = 'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.csv')]),\n",
    "      'lakes analyzed using stationary_outline_geom_calc/stationary_outlines_at_all_lakes')\n",
    "\n",
    "dir = 'output/geometric_calcs/stationary_outline_geom_calc/evolving_stationary_union_at_all_lakes'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.csv')]),\n",
    "      'lakes analyzed using stationary_outline_geom_calc/evolving_stationary_union_at_all_lakes')\n",
    "\n",
    "dir = 'output/geometric_calcs/evolving_outlines_geom_calc'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.csv') and not Path(f).stem.endswith('_sum')]),\n",
    "      'lakes analyzed using evolving_outlines_geom_calc')\n",
    "\n",
    "dir = 'output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.csv') and not Path(f).stem.endswith('_sum')]),\n",
    "      'lakes analyzed using stationary_outline_geom_calc/evolving_union_at_evolving_lakes')\n",
    "\n",
    "dir = 'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.csv') and not Path(f).stem.endswith('_sum')]),\n",
    "      'lakes analyzed using stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes')\n",
    "\n",
    "dir = 'output/geometric_calcs/stationary_outline_geom_calc/evolving_stationary_union_at_evolving_lakes'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.csv') and not Path(f).stem.endswith('_sum')]),\n",
    "      'lakes analyzed using stationary_outline_geom_calc/evolving_stationary_union_at_evolving_lakes')\n",
    "\n",
    "dir = OUTPUT_DIR + '/plot_evolving_and_stationary_comparison'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.png')]),\n",
    "      'lakes analyzed using plot_evolving_and_stationary_comparison')\n",
    "\n",
    "dir = OUTPUT_DIR + '/plot_evolving_and_stationary_comparison_sequential'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.mp4')]),\n",
    "      'lakes analyzed using plot_evolving_and_stationary_comparison_sequential')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5be2863-8295-476c-8929-1f809318ca49",
   "metadata": {},
   "source": [
    "# Continental summations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e265cdf-dfb8-407b-80d0-4d9d12d94bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load geodataframe needed\n",
    "revised_stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/revised_stationary_outlines_gdf.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ba2ece-c90f-4879-9835-e7f6df453e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of comparison types\n",
    "geom_calc_types = [\n",
    "    'stationary_outline_geom_calc/stationary_outlines_at_all_lakes',\n",
    "    'stationary_outline_geom_calc/evolving_stationary_union_at_all_lakes',\n",
    "    'evolving_outlines_geom_calc',\n",
    "    'stationary_outline_geom_calc/evolving_union_at_evolving_lakes',\n",
    "    'stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes',\n",
    "    'stationary_outline_geom_calc/evolving_stationary_union_at_evolving_lakes',\n",
    "]\n",
    "\n",
    "# Process each comparison type\n",
    "for geom_calc_type in geom_calc_types:\n",
    "    process_continental_sums(geom_calc_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442a61ae-b4a1-41c2-970d-c2a3f41d926a",
   "metadata": {},
   "source": [
    "## Explaining continental sum trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4306138-4da3-4898-8a31-b1dd1d621476",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'output/geometric_calcs/evolving_outlines_geom_calc'\n",
    "\n",
    "def create_interactive_plot(directory, stationary_outlines_gdf):\n",
    "    # Load and process all lake data\n",
    "    dfs = []\n",
    "    lake_names = []\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            lake_name = os.path.splitext(file)[0]\n",
    "            \n",
    "            # Process each lake's data\n",
    "            df['lake_name'] = lake_name\n",
    "            df['datetime'] = pd.to_datetime(df['midcyc_datetime'])\n",
    "            df['cumsum_vol'] = np.cumsum(df['evolving_outlines_dV_corr (m^3)'])\n",
    "            dfs.append(df)\n",
    "            lake_names.append(lake_name)\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Create plot using hvplot\n",
    "    plot = combined_df.hvplot.line(\n",
    "        x='datetime',\n",
    "        y='cumsum_vol',\n",
    "        by='lake_name',\n",
    "        width=800,\n",
    "        height=400,\n",
    "        title='Lake Volume Changes Over Time',\n",
    "        xlabel='Date',\n",
    "        ylabel='Cumulative Volume Change (m³)',\n",
    "        grid=True,\n",
    "        legend=False\n",
    "    )\n",
    "    \n",
    "    return plot\n",
    "\n",
    "# Usage:\n",
    "plot = create_interactive_plot(directory, stationary_outlines_gdf)\n",
    "plot  # Display in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03af0cc7-93fe-43ce-b036-1594040f8ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory = 'output/geometric_calcs/evolving_outlines_geom_calc'\n",
    "\n",
    "def create_interactive_plot(directory, stationary_outlines_gdf):\n",
    "    # Load and process all lake data\n",
    "    dfs_subset_CS2_IS2_lakes = []\n",
    "    lake_names = []\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            lake_name = os.path.splitext(file)[0]\n",
    "            \n",
    "            # Process each lake's data\n",
    "            df['lake_name'] = lake_name\n",
    "            df['datetime'] = pd.to_datetime(df['midcyc_datetime'])\n",
    "            df['cumsum_vol'] = np.cumsum(np.divide(df['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "            dfs_subset_CS2_IS2_lakes.append(df)\n",
    "            lake_names.append(lake_name)\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(dfs_subset_CS2_IS2_lakes, ignore_index=True)\n",
    "    \n",
    "    # Create the plot using Dataset and Curve\n",
    "    dataset = hv.Dataset(combined_df)\n",
    "    curves = dataset.to(hv.Curve, \n",
    "                       kdims=['datetime'], \n",
    "                       vdims=['cumsum_vol', 'lake_name'],\n",
    "                       groupby='lake_name')\n",
    "    \n",
    "    # Apply options to the plot\n",
    "    plot = curves.opts(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        tools=['hover'],\n",
    "        title='Lake Volume Changes Over Time',\n",
    "        xlabel='Date',\n",
    "        ylabel='Cumulative Volume Change (m³)',\n",
    "        show_grid=True,\n",
    "        toolbar='above'\n",
    "    )\n",
    "    \n",
    "    return plot\n",
    "\n",
    "# Usage:\n",
    "plot = create_interactive_plot(directory, stationary_outlines_gdf)\n",
    "plot  # Display in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed060331-f121-4702-910c-e3c852aab6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e92875-6e7b-4a5d-8032-503997712fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the lakes driving the deviation of evolving and stationary \n",
    "\n",
    "evolving_directory = 'output/geometric_calcs/evolving_outlines_geom_calc'\n",
    "stationary_directory = 'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes'\n",
    "\n",
    "def process_lake_data(directory, is_evolving=True):\n",
    "    dfs = {}\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            lake_name = os.path.splitext(file)[0]\n",
    "            \n",
    "            # Process each lake's data\n",
    "            df['lake_name'] = lake_name\n",
    "            df['datetime'] = pd.to_datetime(df['midcyc_datetime'])\n",
    "            \n",
    "            # Calculate cumulative volume based on directory type\n",
    "            if is_evolving:\n",
    "                df['cumsum_vol'] = np.cumsum(df['evolving_outlines_dV_corr (m^3)'])\n",
    "            else:\n",
    "                df['cumsum_vol'] = np.cumsum(df['stationary_outline_dV_corr (m^3)'])\n",
    "            \n",
    "            dfs[lake_name] = df\n",
    "    \n",
    "    return dfs\n",
    "\n",
    "def create_interactive_plot(evolving_directory, stationary_directory):\n",
    "    # Load data from both directories\n",
    "    evolving_dfs = process_lake_data(evolving_directory, is_evolving=True)\n",
    "    stationary_dfs = process_lake_data(stationary_directory, is_evolving=False)\n",
    "    \n",
    "    # Initialize lists to store processed dataframes\n",
    "    plot_dfs = []\n",
    "    \n",
    "    # Process common lakes\n",
    "    common_lakes = set(evolving_dfs.keys()) & set(stationary_dfs.keys())\n",
    "    for lake_name in common_lakes:\n",
    "        evolving_df = evolving_dfs[lake_name].copy()\n",
    "        stationary_df = stationary_dfs[lake_name].copy()\n",
    "        \n",
    "        # Calculate difference (evolving - stationary)\n",
    "        merged_df = pd.merge(\n",
    "            evolving_df[['datetime', 'cumsum_vol']], \n",
    "            stationary_df[['datetime', 'cumsum_vol']], \n",
    "            on='datetime', \n",
    "            suffixes=('_evolving', '_stationary')\n",
    "        )\n",
    "        merged_df['cumsum_vol'] = merged_df['cumsum_vol_evolving'] - merged_df['cumsum_vol_stationary']\n",
    "        merged_df['lake_name'] = lake_name + '_difference'\n",
    "        plot_dfs.append(merged_df[['datetime', 'cumsum_vol', 'lake_name']])\n",
    "    \n",
    "    # Process lakes only in stationary directory\n",
    "    stationary_only = set(stationary_dfs.keys()) - set(evolving_dfs.keys())\n",
    "    for lake_name in stationary_only:\n",
    "        df = stationary_dfs[lake_name].copy()\n",
    "        df['lake_name'] = lake_name + '_stationary'\n",
    "        plot_dfs.append(df[['datetime', 'cumsum_vol', 'lake_name']])\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(plot_dfs, ignore_index=True)\n",
    "    \n",
    "    # Create plot using hvplot\n",
    "    plot = combined_df.hvplot.line(\n",
    "        x='datetime',\n",
    "        y='cumsum_vol',\n",
    "        by='lake_name',\n",
    "        width=800,\n",
    "        height=400,\n",
    "        title='Lake Volume Changes Over Time',\n",
    "        xlabel='Date',\n",
    "        ylabel='Cumulative Volume Change (m³)',\n",
    "        grid=True,\n",
    "        legend=False\n",
    "    )\n",
    "    \n",
    "    return plot\n",
    "\n",
    "# Usage:\n",
    "plot = create_interactive_plot(evolving_directory, stationary_directory)\n",
    "plot  # Display in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0d9ed4-5dc6-45a6-8af0-e51da56cae4d",
   "metadata": {},
   "source": [
    "# Fig. S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c602343-390f-48b2-a908-b932c7a4c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add something along the lines of code below to illustrate lake area/outline for evolving outline union boundary/area:\n",
    "# # Plot polygons in the GeoDataFrame\n",
    "# gdf.plot(ax=ax, color='lightblue', edgecolor='black', linewidth=1, label='Lake area')\n",
    "# gdf.boundary.plot(ax=ax, color='red', linewidth=2, label='Lake outline')\n",
    "\n",
    "# Add off-lake, lake (no corr.) to dh plot to illustrate correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277eb44-f225-4553-ad0c-62452e3b3eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.family': 'DejaVu Sans',\n",
    "    'font.size': 9,\n",
    "    'axes.labelsize': 10,\n",
    "    'axes.titlesize': 9,\n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 9,\n",
    "    'legend.fontsize': 9,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e971a37-5db7-46e3-9b7f-a9ee01739403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets necessary for plotting\n",
    "revised_stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/revised_stationary_outlines_gdf.geojson')\n",
    "evolving_outlines_union_gdf = gpd.read_file('output/lake_outlines/evolving_outlines_union_gdf.geojson')\n",
    "evolving_stationary_outlines_union_evolving_lakes_gdf = gpd.read_file('output/lake_outlines/evolving_stationary_outlines_union_evolving_lakes_gdf.geojson')\n",
    "evolving_stationary_outlines_union_all_lakes_gdf = gpd.read_file('output/lake_outlines/evolving_stationary_outlines_union_all_lakes_gdf.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a5ff1-334d-494b-9dc2-16d32c89d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make output directory for figures\n",
    "# os.makedirs(OUTPUT_DIR + '/figures', exist_ok=True)\n",
    "\n",
    "# # Select lake\n",
    "# lake_gdf = revised_stationary_outlines_gdf[revised_stationary_outlines_gdf['name'] == 'Institute_E1']\n",
    "\n",
    "# # Define lake name and polygon\n",
    "# lake_name = lake_gdf['name'].iloc[0]\n",
    "# lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "\n",
    "# # Load evolving outlines as geodataframe\n",
    "# try:\n",
    "#     onlake_outlines_gdf = gpd.read_file(os.path.join(\n",
    "#         os.getcwd(), 'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)))\n",
    "# except fiona.errors.DriverError:\n",
    "#     print(f\"File for {lake_name} not found.\")\n",
    "\n",
    "# # Load off-lake evolving outlines as geodataframe\n",
    "# try:\n",
    "#     offlake_outlines_gdf = gpd.read_file(OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines/{}.geojson'.format(lake_name))\n",
    "# except fiona.errors.DriverError:\n",
    "#     print(f\"File for {lake_name} not found.\")\n",
    "\n",
    "# # Ensure the result is a GeoDataFrame with proper geometry\n",
    "# evolving_outlines_gdf = gpd.GeoDataFrame(\n",
    "#     pd.concat([onlake_outlines_gdf, offlake_outlines_gdf], ignore_index=True),\n",
    "#     geometry='geometry', crs=onlake_outlines_gdf.crs)\n",
    "\n",
    "# # Load evolving outlines union\n",
    "# evolving_stationary_union_gdf = evolving_stationary_outlines_union_evolving_lakes_gdf[evolving_stationary_outlines_union_evolving_lakes_gdf['name'] == lake_name]\n",
    "# evolving_union_gdf = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]\n",
    "\n",
    "# # Load evolving outlines search parameters\n",
    "# row_index=evolving_outlines_gdf['row_index'][0]\n",
    "# within_area_multiple=evolving_outlines_gdf['within_area_multiple'][0]\n",
    "# level=evolving_outlines_gdf['level'][0]\n",
    "\n",
    "# # Attempt to open the geometric calculations CSV files\n",
    "# try:\n",
    "#     evolving_geom_calcs_df = pd.read_csv(os.path.join(\n",
    "#         os.getcwd(), 'output/geometric_calcs/evolving_outlines_geom_calc/{}.csv'.format(lake_name)))\n",
    "#     stationary_geom_calcs_df = pd.read_csv(os.path.join(\n",
    "#         os.getcwd(), \n",
    "#         'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes/{}.csv'.format(lake_name)))\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"CSV files for {lake_name} not found.\")\n",
    "\n",
    "# # Convert of strings to datetime\n",
    "# evolving_geom_calcs_df['midcyc_datetime'] = pd.to_datetime(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "# stationary_geom_calcs_df['midcyc_datetime'] = pd.to_datetime(stationary_geom_calcs_df['midcyc_datetime'])\n",
    "\n",
    "# # Prepare datasets - using larger buffer for initial masking\n",
    "# dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(lake_gdf, 25)\n",
    "\n",
    "# dataset1_dh = dataset1_masked['delta_h'].diff('time')\n",
    "# dataset2_dh = dataset2_masked['delta_h'].diff('time')\n",
    "\n",
    "# # Select time step to show for CryoSat-2 (CS2) and ICESat-2 (IS2) eras\n",
    "# CS2_i = 1  \n",
    "# IS2_i = 12\n",
    "\n",
    "# # Isolate dh time steps\n",
    "# CS2_timestep = dataset1_masked['time'][CS2_i].values\n",
    "# IS2_timestep = dataset2_masked['time'][IS2_i].values\n",
    "\n",
    "# # Convert date_list to numpy array\n",
    "# CS2_cyc_start_dates_npa = np.array(cyc_start_dates, dtype='datetime64')\n",
    "# IS2_cyc_start_dates_npa = np.array(cyc_start_dates, dtype='datetime64')\n",
    "\n",
    "# # Find matching cyc_dates index\n",
    "# CS2_cyc_dates_idx = np.where(CS2_cyc_start_dates_npa == CS2_timestep)[0][0]\n",
    "# IS2_cyc_dates_idx = np.where(IS2_cyc_start_dates_npa == IS2_timestep)[0][0]\n",
    "\n",
    "# # Initialize empty lists for height anomalies\n",
    "# height_anom_pos = []\n",
    "# height_anom_neg = []\n",
    "\n",
    "# # Get height anomalies for CryoSat-2 timestep (i = 20)\n",
    "# if dataset1_masked is not None:\n",
    "#     if np.any(~np.isnan(dataset1_dh[CS2_i])):\n",
    "#         height_anom_pos.append(np.nanmax(dataset1_dh[CS2_i]))\n",
    "#         height_anom_neg.append(np.nanmin(dataset1_dh[CS2_i]))\n",
    "\n",
    "# # Get height anomalies for ICESat-2 timestep (i = 12)\n",
    "# if np.any(~np.isnan(dataset2_dh[IS2_i])):\n",
    "#     height_anom_pos.append(np.nanmax(dataset2_dh[IS2_i]))\n",
    "#     height_anom_neg.append(np.nanmin(dataset2_dh[IS2_i]))\n",
    "\n",
    "# # Find max height anomalies across both time slices\n",
    "# max_height_anom_pos = max(height_anom_pos)\n",
    "# max_height_anom_neg = min(height_anom_neg)\n",
    "# max_anom = max([max_height_anom_pos, abs(max_height_anom_neg)])\n",
    "# del height_anom_pos, height_anom_neg\n",
    "\n",
    "# # Create the diverging normalization for the colormap\n",
    "# divnorm = colors.TwoSlopeNorm(vmin=-max_anom, vcenter=0., vmax=max_anom)\n",
    "# del max_height_anom_pos, max_height_anom_neg\n",
    "\n",
    "# # Establish x_min, x_max, y_min, y_max\n",
    "# ROI_poly = area_multiple_buffer(lake_poly, 25)\n",
    "# x_min, y_min, x_max, y_max = ROI_poly.bounds\n",
    "# x_buffer, y_buffer = abs(x_max-x_min)*0.02, abs(y_max-y_min)*0.02\n",
    "\n",
    "# # Subsetting dataset\n",
    "# dataset1 = CS2_Smith2017\n",
    "# dataset2 = ATL15_dh\n",
    "\n",
    "# # Prepare datasets\n",
    "# dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(lake_gdf, 25)\n",
    "\n",
    "# # Create figure with GridSpec - modified to include timeline panel\n",
    "# fig = plt.figure(figsize=(10,16))  # Increased height to accommodate new panel\n",
    "# gs = fig.add_gridspec(4, 2, height_ratios=[0.24, 1, 1, 1], hspace=0.3, wspace=0.3)\n",
    "\n",
    "# # Create remaining axes array excluding timeline row\n",
    "# axs = np.array([[fig.add_subplot(gs[i,j]) for j in range(2)] for i in range(1,4)])\n",
    "\n",
    "\n",
    "# # Panel - Satellite era timeline\n",
    "\n",
    "# # Add timeline panel spanning both columns\n",
    "# timeline_ax = fig.add_subplot(gs[0, :])\n",
    "\n",
    "# # Create timeline visualization\n",
    "# timeline_ax.set_xlim(cyc_start_dates[0], cyc_end_dates[-1])\n",
    "# timeline_ax.xaxis.set_major_locator(mdates.YearLocator(base=1))\n",
    "# timeline_ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "# # Plot colored bars for different satellite eras\n",
    "# for i, date in enumerate(cyc_dates['mid_cyc_dates']):\n",
    "#     dataset = cyc_dates['dataset'][i]\n",
    "#     color = 'lightblue' if dataset == 'CS2_Smith2017' else 'lightgreen'\n",
    "#     label = 'CryoSat-2' if dataset == 'CS2_Smith2017' else 'ICESat-2'\n",
    "    \n",
    "#     if i == 0:  # Only add label for first occurrence of each dataset\n",
    "#         timeline_ax.axvspan(cyc_dates['cyc_start_dates'][i], cyc_dates['cyc_end_dates'][i], \n",
    "#                           color=color, alpha=0.3, label=label)\n",
    "#     else:\n",
    "#         # Check if dataset changed from previous\n",
    "#         if cyc_dates['dataset'][i] != cyc_dates['dataset'][i-1]:\n",
    "#             timeline_ax.axvspan(cyc_dates['cyc_start_dates'][i], cyc_dates['cyc_end_dates'][i], \n",
    "#                               color=color, alpha=0.3, label=label)\n",
    "#         else:\n",
    "#             timeline_ax.axvspan(cyc_dates['cyc_start_dates'][i], cyc_dates['cyc_end_dates'][i], \n",
    "#                               color=color, alpha=0.3)\n",
    "\n",
    "# # Get the dates for our timesteps shown in top panels\n",
    "# # For CryoSat-2\n",
    "# cs2_start_date = cyc_start_dates[CS2_cyc_dates_idx]\n",
    "# cs2_end_date = cyc_end_dates[CS2_cyc_dates_idx]\n",
    "\n",
    "# # For ICESat-2\n",
    "# is2_start_date = cyc_start_dates[IS2_cyc_dates_idx]\n",
    "# is2_end_date = cyc_end_dates[IS2_cyc_dates_idx]\n",
    "\n",
    "# # Add vertical spans for the timesteps shown in top panels\n",
    "# # For CryoSat-2\n",
    "# timeline_ax.axvspan(mdates.date2num(cs2_start_date), \n",
    "#                               mdates.date2num(cs2_end_date),\n",
    "#                               color='gray', alpha=0.3)\n",
    "\n",
    "# # For ICESat-2\n",
    "# timeline_ax.axvspan(mdates.date2num(is2_start_date),\n",
    "#                               mdates.date2num(is2_end_date),\n",
    "#                               color='gray', alpha=0.3)\n",
    "# # Create patches for legend\n",
    "# cs2_patch = mpatches.Patch(color='lightblue', alpha=0.3, label='CryoSat-2')\n",
    "# is2_patch = mpatches.Patch(color='lightgreen', alpha=0.3, label='ICESat-2')\n",
    "# timestep_patch = mpatches.Patch(color='gray', alpha=0.3, label='displayed time step')\n",
    "\n",
    "# # Add legend with all patches\n",
    "# timeline_ax.legend(handles=[cs2_patch, is2_patch, timestep_patch], \n",
    "#                   loc='upper right')\n",
    "\n",
    "# # Customize timeline appearance\n",
    "# # timeline_ax.legend(loc='upper right')\n",
    "# timeline_ax.set_title('Multi-mission satellite timeline')\n",
    "\n",
    "# # Remove y-axis ticks and labels\n",
    "# timeline_ax.set_yticks([])\n",
    "\n",
    "\n",
    "# # Panel - CryoSat-2 dh time step with evolving outlines and area multiple within evaluation lines\n",
    "\n",
    "# # Plot dh time step\n",
    "# img = axs[0,0].imshow(dataset1_dh[CS2_i], cmap='RdBu', norm=divnorm, \n",
    "#                       origin='lower', extent=[x_min, x_max, y_min, y_max])\n",
    "\n",
    "# dt = mid_cyc_dates[CS2_cyc_dates_idx]\n",
    "# evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "\n",
    "# if not evolving_outlines_gdf.empty:\n",
    "#     # Split into positive and negative dh values\n",
    "#     positive_outlines = evolving_outlines_dt[evolving_outlines_dt['dh (m)'] > 0]\n",
    "#     negative_outlines = evolving_outlines_dt[evolving_outlines_dt['dh (m)'] < 0]\n",
    "    \n",
    "#     # Plot positive dh in blue\n",
    "#     if not positive_outlines.empty:\n",
    "#         positive_outlines.boundary.plot(ax=axs[0,0], color='blue', linewidth=1)\n",
    "    \n",
    "#     # Plot negative dh in red\n",
    "#     if not negative_outlines.empty:\n",
    "#         negative_outlines.boundary.plot(ax=axs[0,0], color='red', linewidth=1)\n",
    "\n",
    "# # Plot inset map\n",
    "# axIns = axs[0,0].inset_axes([0.02, 0, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "# axIns.set_aspect('equal')\n",
    "# moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1)\n",
    "# moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1)\n",
    "# axIns.axis('off')\n",
    "# # Plot red star to indicate location\n",
    "# axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "#     linewidth=1, color='k', s=75)\n",
    "\n",
    "# # Set a title for the axes\n",
    "# title_text = f'Height change from \\n{date_to_quarter_year(mid_cyc_dates[CS2_cyc_dates_idx])} to {date_to_quarter_year(mid_cyc_dates[CS2_cyc_dates_idx+1])}'\n",
    "# axs[0,0].set_title(title_text, y=1)\n",
    "\n",
    "# # Create lines for legend\n",
    "# stationary_color = 'darkturquoise'\n",
    "# stationary_line = plt.Line2D([], [], color=stationary_color, linestyle='solid', linewidth=2)\n",
    "# within_eval_lines = plt.Line2D([], [], color='gray', linestyle='dotted', linewidth=2)\n",
    "# optimal_within_eval_line = plt.Line2D([], [], color='gray', linestyle='solid', linewidth=2)\n",
    "\n",
    "# # Plot legend\n",
    "# legend = axs[0,0].legend([stationary_line, \n",
    "#                           within_eval_lines,\n",
    "#                           optimal_within_eval_line], \n",
    "#     ['stationary outline', \n",
    "#      'within evaluation boundaries',\n",
    "#      'optimal within boundary'],\n",
    "#     handlelength=3,\n",
    "#     handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#     loc='upper right')\n",
    "\n",
    "\n",
    "# # Panel - ICESat-2 dh time step with evolving outlines and area multiple within evaluation lines  \n",
    "\n",
    "# # Plot dh time step\n",
    "# img = axs[0,1].imshow(dataset2_dh[IS2_i], cmap='RdBu', norm=divnorm, \n",
    "#                       origin='lower', extent=[x_min, x_max, y_min, y_max])\n",
    "\n",
    "# dt = mid_cyc_dates[IS2_cyc_dates_idx]\n",
    "# evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "\n",
    "# if not evolving_outlines_gdf.empty:\n",
    "#     # Split into positive and negative dh values\n",
    "#     positive_outlines = evolving_outlines_dt[evolving_outlines_dt['dh (m)'] > 0]\n",
    "#     negative_outlines = evolving_outlines_dt[evolving_outlines_dt['dh (m)'] < 0]\n",
    "    \n",
    "#     # Plot positive dh in blue\n",
    "#     if not positive_outlines.empty:\n",
    "#         positive_outlines.boundary.plot(ax=axs[0,1], color='blue', linewidth=1)\n",
    "    \n",
    "#     # Plot negative dh in red\n",
    "#     if not negative_outlines.empty:\n",
    "#         negative_outlines.boundary.plot(ax=axs[0,1], color='red', linewidth=1)\n",
    "\n",
    "# # Add colorbar space to both axes for consistent sizing\n",
    "# for ax in [axs[0,0], axs[0,1]]:\n",
    "#     divider = make_axes_locatable(ax)\n",
    "#     cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "#     if ax == axs[0,1]:  # Only add the actual colorbar to the second plot\n",
    "#         cb = fig.colorbar(img, cax=cax)\n",
    "#         cb.set_label('dh [m quarter$^{-1}$]')\n",
    "#     else:\n",
    "#         # Hide the empty axis for the first plot\n",
    "#         cax.set_visible(False)\n",
    "\n",
    "# # Set a title for the axes\n",
    "# title_text = f'Height change from \\n{date_to_quarter_year(mid_cyc_dates[IS2_cyc_dates_idx])} to {date_to_quarter_year(mid_cyc_dates[IS2_cyc_dates_idx+1])}'\n",
    "# axs[0,1].set_title(title_text, y=1)\n",
    "\n",
    "# for ax in [axs[0,0], axs[0,1]]:\n",
    "#     ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "# axs[0,1].sharey(axs[0,0])\n",
    "\n",
    "# # Create lines for legend\n",
    "# pos_dh_anom = plt.Line2D([], [], color='blue', linestyle='solid', linewidth=2)\n",
    "# neg_dh_anom = plt.Line2D([], [], color='red', linestyle='solid', linewidth=2)\n",
    "\n",
    "# # Plot legend\n",
    "# legend = axs[0,1].legend([pos_dh_anom, \n",
    "#                           neg_dh_anom], \n",
    "#     [f'pos. dh anomaly (>+{level} m)', \n",
    "#      f'neg. dh anomaly (<−{level} m)'],\n",
    "#     handlelength=3,\n",
    "#     handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#     loc='upper right')\n",
    "\n",
    "\n",
    "# # Panel - Evolving outlines time series plot ---------------------------------------------\n",
    "\n",
    "# # Set up colormap\n",
    "# cmap = plt.get_cmap('plasma', len(mid_cyc_dates)-1)\n",
    "\n",
    "# # Norm to time variable\n",
    "# norm = plt.Normalize(mdates.date2num(cyc_dates['mid_cyc_dates'].iloc[0]), \n",
    "#                      mdates.date2num(cyc_dates['mid_cyc_dates'].iloc[-1]))\n",
    "\n",
    "# # Zoom in slightly to bounds of optimal within evaluation boundary\n",
    "# # Establish x_min, x_max, y_min, y_max\n",
    "# optimal_within_eval_poly = area_multiple_buffer(lake_poly, within_area_multiple)\n",
    "# x_min, y_min, x_max, y_max = optimal_within_eval_poly.bounds\n",
    "# x_buffer, y_buffer = abs(x_max-x_min)*0.01, abs(y_max-y_min)*0.01\n",
    "\n",
    "# # Plot MOA surface imagery\n",
    "# mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "# mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "# moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "# axs[1,0].imshow(moa_highres_da_subset[0,:,:], cmap='gray', clim=[14000, 17000], \n",
    "#           extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "\n",
    "# # Plot evolving outlines\n",
    "# onlake_lines, offlake_lines = [], []\n",
    "# for i, dt in enumerate(mid_cyc_dates):\n",
    "#     x, y = 1, 1\n",
    "#     onlake_line, = axs[1,0].plot(x, y, color=cmap(norm(mdates.date2num(mid_cyc_dates[i]))), linewidth=2)\n",
    "#     onlake_lines.append(onlake_line)\n",
    "#     offlake_line, = axs[1,0].plot(x, y, color=cmap(norm(date_to_quarter_year(mid_cyc_dates[i]))), linewidth=2, alpha=0.2)\n",
    "#     offlake_lines.append(offlake_line)\n",
    "    \n",
    "#     onlake_outlines_dt = onlake_outlines_gdf[onlake_outlines_gdf['midcyc_datetime'] == dt]\n",
    "#     offlake_outlines_dt = offlake_outlines_gdf[offlake_outlines_gdf['midcyc_datetime'] == dt]\n",
    "    \n",
    "#     if not onlake_outlines_dt.empty:\n",
    "#         onlake_outlines_dt.boundary.plot(ax=axs[1,0], \n",
    "#             color=cmap(norm(mdates.date2num(mid_cyc_dates[i]))), \n",
    "#             linewidth=1)\n",
    "#     if not offlake_outlines_dt.empty:\n",
    "#         offlake_outlines_dt.boundary.plot(ax=axs[1,0], \n",
    "#             color=cmap(norm(mdates.date2num(mid_cyc_dates[i]))), \n",
    "#             linewidth=1, alpha=0.5)\n",
    "\n",
    "# # Plot evolving outlines unary union\n",
    "# evolving_union_gdf.boundary.plot(ax=axs[1,0], color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "# # Create stationary region and evolving outlines region and plot\n",
    "# stationary_region = area_multiple_buffer(lake_poly, 2)\n",
    "# stationary_region = stationary_region.difference(lake_poly)\n",
    "# evolving_union_region = area_multiple_buffer(evolving_union_gdf['geometry'], 2)\n",
    "# evolving_union_region = evolving_union_region.difference(evolving_union_gdf['geometry'].iloc[0])\n",
    "# gpd.GeoDataFrame(geometry=[stationary_region], crs='3031').plot(ax=axs[1,0], color=stationary_color, linewidth=2, alpha=0.2)\n",
    "# gpd.GeoDataFrame(geometry=[evolving_union_region], crs='3031').plot(ax=axs[1,0], color='k', linewidth=2, alpha=0.2)\n",
    "\n",
    "# # Create colorbar\n",
    "# m = plt.cm.ScalarMappable(cmap=cmap)\n",
    "# m.set_array(np.array([mdates.date2num(date) for date in mid_cyc_dates]))\n",
    "# divider = make_axes_locatable(axs[1,0])\n",
    "# cax = divider.append_axes('bottom', size='3%', pad=0.6)\n",
    "\n",
    "# # Define major and minor years\n",
    "# # major_years = [2012, 2014, 2016, 2018, 2020, 2022, 2024]\n",
    "# major_years = [2012, 2016, 2020, 2024]\n",
    "# minor_years = list(range(2011, 2025))\n",
    "# major_dates = [mdates.date2num(datetime.datetime(year, 1, 1)) for year in major_years]\n",
    "# minor_dates = [mdates.date2num(datetime.datetime(year, 1, 1)) for year in minor_years]\n",
    "\n",
    "# cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "# cbar.set_ticks(major_dates)\n",
    "# cbar.set_ticklabels(major_years)\n",
    "# cbar.ax.xaxis.set_minor_locator(ticker.FixedLocator(minor_dates))\n",
    "# cbar.set_label('evolving outline year', labelpad=5)\n",
    "\n",
    "# axs[1,0].set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "    \n",
    "# # Emphasize zeroth row_index within evaluation line selected for particular lake\n",
    "# gpd.GeoSeries(area_multiple_buffer(lake_poly, within_area_multiple)).boundary.plot(ax=ax, linestyle='solid', color='gray')\n",
    "\n",
    "# # Plot legend\n",
    "# evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=2)\n",
    "# stationary_region_patch = mpatches.Patch(color=stationary_color, alpha=0.2)\n",
    "# evolving_union_region_patch = mpatches.Patch(color='k', alpha=0.2)\n",
    "\n",
    "# legend = axs[1,0].legend([stationary_region_patch,\n",
    "#                        evolving_union_region_patch,\n",
    "#                        tuple(onlake_lines),\n",
    "#                        # tuple(offlake_lines),\n",
    "#                        evolving_union_line], \n",
    "#     ['stationary region',\n",
    "#      'evolving union region',\n",
    "#      'evolving outlines',\n",
    "#      # f'evolving outlines ({evolving_outlines_gdf.level[0]} m)',\n",
    "#      # 'off-lake evolving outlines', \n",
    "#      'evolving outlines union'],\n",
    "#     handlelength=3,\n",
    "#     handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#     loc='upper right')\n",
    "\n",
    "\n",
    "# # Panel - active area ---------------------------------------------\n",
    "    \n",
    "# # Plot horizontal zero line for reference\n",
    "# axs[1,1].axhline(0, color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "# # Plot stationary outline and evolving outlines unary union areas\n",
    "# axs[1,1].axhline(np.divide(lake_gdf['area (m^2)'], 1e6).values, \n",
    "#                  color=stationary_color, linestyle='solid', linewidth=2)\n",
    "# axs[1,1].axhline(np.divide(evolving_union_gdf['area (m^2)'], 1e6).values, \n",
    "#                  color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "# # Plot multi-colored line and scatter for data points\n",
    "# x = mdates.date2num(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "# y = np.divide(evolving_geom_calcs_df['evolving_outlines_area (m^2)'], 1e6)\n",
    "\n",
    "# # Plot evolving outlines time series using LineCollection from points/segments to plot multi-colored line\n",
    "# points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "# segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "# lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "# lc.set_array(x)  # Set the values used for colormapping, using matplotlib dates for colors\n",
    "# lc.set_linewidth(2)\n",
    "# line = axs[1,1].add_collection(lc)\n",
    "# scatter = axs[1,1].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "# # Add legend\n",
    "# legend = axs[1,1].legend([stationary_line, \n",
    "#                           evolving_union_line, \n",
    "#                           tuple(onlake_lines)], \n",
    "#     ['stationary outline',\n",
    "#      'evolving outlines union',\n",
    "#      'evolving outlines',],\n",
    "#      handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#      loc='upper right')\n",
    "\n",
    "\n",
    "# # Panel - dh/dt -------------------------------------------------------\n",
    "\n",
    "# # Plot horizontal zero line for reference\n",
    "# axs[2,0].axhline(0, color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "# # Plot stationary outlines off-lake region dh\n",
    "# axs[2,0].plot(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "#     np.cumsum(stationary_geom_calcs_df['stationary_outline_region_dh (m)']),\n",
    "#     color='lightgray', linestyle='solid', linewidth=2)\n",
    "# axs[2,0].scatter(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "#     np.cumsum(stationary_geom_calcs_df['stationary_outline_region_dh (m)']),\n",
    "#     color='lightgray', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# # Plot evolving outlines off-lake region dh\n",
    "# axs[2,0].plot(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']), \n",
    "#     np.cumsum(evolving_geom_calcs_df['evolving_outlines_region_dh (m)']), \n",
    "#     color='dimgray', linestyle='solid', linewidth=2)\n",
    "# axs[2,0].scatter(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']), \n",
    "#     np.cumsum(evolving_geom_calcs_df['evolving_outlines_region_dh (m)']),\n",
    "#     color='dimgray', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# # Plot stationary outline time series\n",
    "# axs[2,0].plot(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "#     np.cumsum(stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "#     color=stationary_color, linestyle='solid', linewidth=2)\n",
    "# axs[2,0].scatter(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "#     np.cumsum(stationary_geom_calcs_df['stationary_outline_dh_corr (m)']),\n",
    "#     color=stationary_color, linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# # Plot multi-colored line and scatter for data points\n",
    "# x = mdates.date2num(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "# y = np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'])\n",
    "\n",
    "# # Plot evolving outlines time series using multi-colored LineCollection from points/segments\n",
    "# points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "# segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "# lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "# lc.set_array(x)\n",
    "# lc.set_linewidth(2)\n",
    "# line = axs[2,0].add_collection(lc)\n",
    "# scatter = axs[2,0].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "# # Plot bias\n",
    "# axs[2,0].plot(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']),\n",
    "#     np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'] -\n",
    "#               stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "#     color='red', linestyle='solid', linewidth=2)\n",
    "# axs[2,0].scatter(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']),\n",
    "#     np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'] -\n",
    "#               stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "#     color='red', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# # Add legend\n",
    "# evolving_region = plt.Line2D([], [], color='dimgray', linestyle='solid', linewidth=2)\n",
    "# stationary_region = plt.Line2D([], [], color='lightgray', linestyle='solid', linewidth=2)\n",
    "# bias = plt.Line2D([], [], color='red', linestyle='solid', linewidth=2)\n",
    "# legend = axs[2,0].legend(\n",
    "#     [evolving_region,\n",
    "#      stationary_region,\n",
    "#      tuple(onlake_lines),\n",
    "#      stationary_line,  \n",
    "#      bias],\n",
    "#     ['evolving outlines regional',\n",
    "#      'stationary outline regional',\n",
    "#      'evolving outlines',\n",
    "#      'stationary outline', \n",
    "#      'bias (evolving − stationary)'],\n",
    "#      handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#      loc='upper right')\n",
    "\n",
    "\n",
    "# # Panel - dV/dt --------------------------------------------------\n",
    "\n",
    "# # Plot horizontal line at zero for reference\n",
    "# axs[2,1].axhline(0, color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "# # Plot stationary outline time series\n",
    "# axs[2,1].plot(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "#     np.divide(np.cumsum(stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "#     color=stationary_color, linestyle='solid', linewidth=2)\n",
    "# axs[2,1].scatter(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "#     np.divide(np.cumsum(stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "#     color=stationary_color, linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# # Plot multi-colored line and scatter for data points\n",
    "# x = mdates.date2num(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "# y = np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "\n",
    "# # Plot evolving outlines time series using LineCollection from points/segments to plot multi-colored line\n",
    "# points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "# segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "# lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "# lc.set_array(x)\n",
    "# lc.set_linewidth(2)\n",
    "# line = axs[2,1].add_collection(lc)\n",
    "# scatter = axs[2,1].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "# # Plot bias\n",
    "# axs[2,1].plot(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']),\n",
    "#     np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "#                         stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)'], 1e9)), \n",
    "#     color='red', linestyle='solid', linewidth=2)\n",
    "# axs[2,1].scatter(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']),\n",
    "#     np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "#                         stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)'], 1e9)), \n",
    "#     color='red', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# # Add legend\n",
    "# legend = axs[2,1].legend([tuple(onlake_lines), stationary_line, bias],\n",
    "#     ['evolving outlines', \n",
    "#      'stationary outline',\n",
    "#      'bias (evolving − stationary)'], \n",
    "#     handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#     loc='upper right')\n",
    "\n",
    "# # Adjust y-axis limits to avoid legend/data overlap\n",
    "# axs[1,1].set_ylim(-25, 625)\n",
    "# axs[2,0].set_ylim(-1, 8)\n",
    "# axs[2,1].set_ylim(-0.4, 0.4)\n",
    "\n",
    "# # Get the dates for our timesteps shown in top panels\n",
    "# # For CryoSat-2\n",
    "# cs2_start_date = cyc_start_dates[CS2_cyc_dates_idx]\n",
    "# cs2_end_date = cyc_end_dates[CS2_cyc_dates_idx]\n",
    "\n",
    "# # For ICESat-2\n",
    "# is2_start_date = cyc_start_dates[IS2_cyc_dates_idx]\n",
    "# is2_end_date = cyc_end_dates[IS2_cyc_dates_idx]\n",
    "\n",
    "# # Add vertical spans to time series plots\n",
    "# for ax in [axs[1,1], axs[2,0], axs[2,1]]:\n",
    "#     # Add vertical spans for both timesteps\n",
    "#     ax.axvspan(mdates.date2num(cs2_start_date), \n",
    "#                          mdates.date2num(cs2_end_date),\n",
    "#                          color='gray', alpha=0.1)\n",
    "    \n",
    "#     ax.axvspan(mdates.date2num(is2_start_date),\n",
    "#                          mdates.date2num(is2_end_date),\n",
    "#                          color='gray', alpha=0.1)\n",
    "\n",
    "# # Label the timeline panel\n",
    "# timeline_ax.text(0.01, 0.98, 'a', transform=timeline_ax.transAxes, \n",
    "#                 fontsize=20, va='top', ha='left')\n",
    "\n",
    "# # Plot elements common to more than one ax object\n",
    "# for i in range(axs.shape[0]):\n",
    "#     for j in range(axs.shape[1]):\n",
    "#         # Set common tick size for all axes\n",
    "#         axs[i,j].tick_params(axis='both')\n",
    "        \n",
    "#         # Add subplot labels (a, b, c, etc.)\n",
    "#         axs[i,j].text(0.02, 0.98, chr(97 + i*2 + j + 1), transform=axs[i,j].transAxes, \n",
    "#                      fontsize=16, va='top', ha='left')\n",
    "\n",
    "#         # Special formatting for map plots (top row and [1,0])\n",
    "#         if (i == 0) or (i == 1 and j == 0):\n",
    "#             # Set x and y labels for map plots\n",
    "#             axs[i,j].set_xlabel('x [km]')\n",
    "#             if j == 0:  # Only for first column\n",
    "#                 axs[i,j].set_ylabel('y [km]')\n",
    "\n",
    "#             # Add map-specific elements\n",
    "#             revised_stationary_outlines_gdf.boundary.plot(ax=axs[i,j], \n",
    "#                 edgecolor=stationary_color, linestyle='solid', linewidth=2, zorder=0)\n",
    "            \n",
    "#             # Convert meters to kilometers\n",
    "#             km_scale = 1e3\n",
    "#             ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "#             ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "#             axs[i,j].xaxis.set_major_formatter(ticks_x)\n",
    "#             axs[i,j].yaxis.set_major_formatter(ticks_y)\n",
    "\n",
    "#             # Add evaluation lines\n",
    "#             for within_area_multiple_i in range(2,16):\n",
    "#                 gpd.GeoSeries(area_multiple_buffer(lake_poly, within_area_multiple_i)).boundary.plot(\n",
    "#                     ax=axs[i,j], linestyle='dotted', color='gray')\n",
    "            \n",
    "#             # Add emphasized evaluation line\n",
    "#             gpd.GeoSeries(area_multiple_buffer(lake_poly, within_area_multiple)).boundary.plot(\n",
    "#                 ax=axs[i,j], linestyle='solid', color='gray')\n",
    "\n",
    "#         # Format time series plots\n",
    "#         else:\n",
    "#             axs[i,j].set_xlabel('year')\n",
    "            \n",
    "#             # Set y labels for time series plots\n",
    "#             if j == 0:  # First column\n",
    "#                 if i == 2:\n",
    "#                     axs[i,j].set_ylabel('cumulative dh [m]')\n",
    "#             elif j == 1:  # Second column\n",
    "#                 if i == 1:\n",
    "#                     axs[i,j].set_ylabel('active area [km$^2$]')\n",
    "#                 elif i == 2:\n",
    "#                     axs[i,j].set_ylabel('cumulative dV [km$^3$]')\n",
    "            \n",
    "#             # Add date formatting for specific time series plots\n",
    "#             if (i == 1 and j == 1) or (i == 2):  # axs[1,1], axs[2,0], and axs[2,1]\n",
    "#                 axs[i,j].xaxis.set_minor_locator(mdates.YearLocator(base=1))\n",
    "#                 axs[i,j].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "#                 axs[i,j].set_xlim(cyc_start_dates[0], cyc_end_dates[-1])\n",
    "\n",
    "# # Save and close plot\n",
    "# plt.savefig(OUTPUT_DIR + '/figures/Figure_S1.png',\n",
    "#     dpi=300, bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15678ca9-d0e5-4ae7-946f-a337e514e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make output directory for figures\n",
    "os.makedirs(OUTPUT_DIR + '/figures', exist_ok=True)\n",
    "\n",
    "# Select lake\n",
    "lake_gdf = revised_stationary_outlines_gdf[revised_stationary_outlines_gdf['name'] == 'Institute_E1']\n",
    "\n",
    "# Define lake name and polygon\n",
    "lake_name = lake_gdf['name'].iloc[0]\n",
    "lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "\n",
    "# Load evolving outlines as geodataframe\n",
    "try:\n",
    "    onlake_outlines_gdf = gpd.read_file(os.path.join(\n",
    "        os.getcwd(), 'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)))\n",
    "except fiona.errors.DriverError:\n",
    "    print(f\"File for {lake_name} not found.\")\n",
    "\n",
    "# Load off-lake evolving outlines as geodataframe\n",
    "try:\n",
    "    offlake_outlines_gdf = gpd.read_file(OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines/{}.geojson'.format(lake_name))\n",
    "except fiona.errors.DriverError:\n",
    "    print(f\"File for {lake_name} not found.\")\n",
    "\n",
    "# Ensure the result is a GeoDataFrame with proper geometry\n",
    "evolving_outlines_gdf = gpd.GeoDataFrame(\n",
    "    pd.concat([onlake_outlines_gdf, offlake_outlines_gdf], ignore_index=True),\n",
    "    geometry='geometry', crs=onlake_outlines_gdf.crs)\n",
    "\n",
    "# Load evolving outlines union\n",
    "evolving_stationary_union_gdf = evolving_stationary_outlines_union_evolving_lakes_gdf[evolving_stationary_outlines_union_evolving_lakes_gdf['name'] == lake_name]\n",
    "evolving_union_gdf = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]\n",
    "\n",
    "# Load evolving outlines search parameters\n",
    "row_index=evolving_outlines_gdf['row_index'][0]\n",
    "within_area_multiple=evolving_outlines_gdf['within_area_multiple'][0]\n",
    "level=evolving_outlines_gdf['level'][0]\n",
    "\n",
    "# Attempt to open the geometric calculations CSV files\n",
    "try:\n",
    "    evolving_geom_calcs_df = pd.read_csv(os.path.join(\n",
    "        os.getcwd(), 'output/geometric_calcs/evolving_outlines_geom_calc/{}.csv'.format(lake_name)))\n",
    "    stationary_geom_calcs_df = pd.read_csv(os.path.join(\n",
    "        os.getcwd(), \n",
    "        'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes/{}.csv'.format(lake_name)))\n",
    "except FileNotFoundError:\n",
    "    print(f\"CSV files for {lake_name} not found.\")\n",
    "\n",
    "# Convert of strings to datetime\n",
    "evolving_geom_calcs_df['midcyc_datetime'] = pd.to_datetime(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "stationary_geom_calcs_df['midcyc_datetime'] = pd.to_datetime(stationary_geom_calcs_df['midcyc_datetime'])\n",
    "\n",
    "# Prepare datasets - using larger buffer for initial masking\n",
    "dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(lake_gdf, 25)\n",
    "\n",
    "dataset1_dh = dataset1_masked['delta_h'].diff('time')\n",
    "dataset2_dh = dataset2_masked['delta_h'].diff('time')\n",
    "\n",
    "# Select time step to show for CryoSat-2 (CS2) and ICESat-2 (IS2) eras\n",
    "CS2_i = 1  \n",
    "IS2_i = 12\n",
    "\n",
    "# Isolate dh time steps\n",
    "CS2_timestep = dataset1_masked['time'][CS2_i].values\n",
    "IS2_timestep = dataset2_masked['time'][IS2_i].values\n",
    "\n",
    "# Convert date_list to numpy array\n",
    "CS2_cyc_start_dates_npa = np.array(cyc_start_dates, dtype='datetime64')\n",
    "IS2_cyc_start_dates_npa = np.array(cyc_start_dates, dtype='datetime64')\n",
    "\n",
    "# Find matching cyc_dates index\n",
    "CS2_cyc_dates_idx = np.where(CS2_cyc_start_dates_npa == CS2_timestep)[0][0]\n",
    "IS2_cyc_dates_idx = np.where(IS2_cyc_start_dates_npa == IS2_timestep)[0][0]\n",
    "\n",
    "# Initialize empty lists for height anomalies\n",
    "height_anom_pos = []\n",
    "height_anom_neg = []\n",
    "\n",
    "# Get height anomalies for CryoSat-2 timestep (i = 20)\n",
    "if dataset1_masked is not None:\n",
    "    if np.any(~np.isnan(dataset1_dh[CS2_i])):\n",
    "        height_anom_pos.append(np.nanmax(dataset1_dh[CS2_i]))\n",
    "        height_anom_neg.append(np.nanmin(dataset1_dh[CS2_i]))\n",
    "\n",
    "# Get height anomalies for ICESat-2 timestep (i = 12)\n",
    "if np.any(~np.isnan(dataset2_dh[IS2_i])):\n",
    "    height_anom_pos.append(np.nanmax(dataset2_dh[IS2_i]))\n",
    "    height_anom_neg.append(np.nanmin(dataset2_dh[IS2_i]))\n",
    "\n",
    "# Find max height anomalies across both time slices\n",
    "max_height_anom_pos = max(height_anom_pos)\n",
    "max_height_anom_neg = min(height_anom_neg)\n",
    "max_anom = max([max_height_anom_pos, abs(max_height_anom_neg)])\n",
    "del height_anom_pos, height_anom_neg\n",
    "\n",
    "# Create the diverging normalization for the colormap\n",
    "divnorm = colors.TwoSlopeNorm(vmin=-max_anom, vcenter=0., vmax=max_anom)\n",
    "del max_height_anom_pos, max_height_anom_neg\n",
    "\n",
    "# Establish x_min, x_max, y_min, y_max\n",
    "ROI_poly = area_multiple_buffer(lake_poly, 25)\n",
    "x_min, y_min, x_max, y_max = ROI_poly.bounds\n",
    "x_buffer, y_buffer = abs(x_max-x_min)*0.02, abs(y_max-y_min)*0.02\n",
    "\n",
    "# Subsetting dataset\n",
    "dataset1 = CS2_Smith2017\n",
    "dataset2 = ATL15_dh\n",
    "\n",
    "# Prepare datasets\n",
    "dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(lake_gdf, 25)\n",
    "\n",
    "# Create figure with GridSpec - modified to include timeline panel\n",
    "fig = plt.figure(figsize=(10,16))  # Increased height to accommodate new panel\n",
    "gs = fig.add_gridspec(4, 2, height_ratios=[0.3, 1, 1, 1], hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Create remaining axes array excluding timeline row\n",
    "axs = np.array([[fig.add_subplot(gs[i,j]) for j in range(2)] for i in range(1,4)])\n",
    "\n",
    "\n",
    "# Panel - Satellite era timeline\n",
    "\n",
    "# Add timeline panel spanning both columns\n",
    "timeline_ax = fig.add_subplot(gs[0, :])\n",
    "\n",
    "# Create timeline visualization\n",
    "timeline_ax.set_xlim(cyc_start_dates[0], cyc_end_dates[-1])\n",
    "timeline_ax.xaxis.set_major_locator(mdates.YearLocator(base=1))\n",
    "timeline_ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "timeline_ax.xaxis.set_minor_locator(mdates.MonthLocator([1, 4, 7, 10]))\n",
    "\n",
    "# Plot colored bars for different satellite eras\n",
    "for i, date in enumerate(cyc_dates['mid_cyc_dates']):\n",
    "    dataset = cyc_dates['dataset'][i]   \n",
    "    color = 'lightblue' if dataset == 'CS2_Smith2017' else 'lightgreen'\n",
    "    label = 'CryoSat-2' if dataset == 'CS2_Smith2017' else 'ICESat-2'\n",
    "    \n",
    "    if i == 0:  # Only add label for first occurrence of each dataset\n",
    "        timeline_ax.axvspan(cyc_dates['cyc_start_dates'][i], cyc_dates['cyc_end_dates'][i], \n",
    "                          color='k', alpha=0.5, label='first cycle - no dh avail.')\n",
    "\n",
    "    elif i == 1:  # Only add label for first occurrence of each dataset\n",
    "        timeline_ax.axvspan(cyc_dates['cyc_start_dates'][i], cyc_dates['cyc_end_dates'][i], \n",
    "                          color=color, alpha=0.3, label=label)\n",
    "    else:\n",
    "        # Check if dataset changed from previous\n",
    "        if cyc_dates['dataset'][i] != cyc_dates['dataset'][i-1]:\n",
    "            timeline_ax.axvspan(cyc_dates['cyc_start_dates'][i], cyc_dates['cyc_end_dates'][i], \n",
    "                              color=color, alpha=0.3, label=label)\n",
    "        else:\n",
    "            timeline_ax.axvspan(cyc_dates['cyc_start_dates'][i], cyc_dates['cyc_end_dates'][i], \n",
    "                              color=color, alpha=0.3)\n",
    "\n",
    "# Get the dates for our timesteps shown in top panels\n",
    "# For CryoSat-2\n",
    "cs2_start_date = cyc_start_dates[CS2_cyc_dates_idx]\n",
    "cs2_end_date = cyc_end_dates[CS2_cyc_dates_idx]\n",
    "\n",
    "# For ICESat-2\n",
    "is2_start_date = cyc_start_dates[IS2_cyc_dates_idx]\n",
    "is2_end_date = cyc_end_dates[IS2_cyc_dates_idx]\n",
    "\n",
    "# Add vertical spans for the timesteps shown in top panels\n",
    "# For CryoSat-2\n",
    "timeline_ax.axvspan(mdates.date2num(cs2_start_date), \n",
    "                              mdates.date2num(cs2_end_date),\n",
    "                              color='gray', alpha=0.3)\n",
    "\n",
    "# For ICESat-2\n",
    "timeline_ax.axvspan(mdates.date2num(is2_start_date),\n",
    "                              mdates.date2num(is2_end_date),\n",
    "                              color='gray', alpha=0.3)\n",
    "# Create patches for legend\n",
    "cs2_patch = mpatches.Patch(color='lightblue', alpha=0.3, label='CryoSat-2')\n",
    "is2_patch = mpatches.Patch(color='lightgreen', alpha=0.3, label='ICESat-2')\n",
    "no_dh_patch = mpatches.Patch(color='darkgray', alpha=0.9, label='first cycle - no dh avail.')\n",
    "timestep_patch = mpatches.Patch(color='gray', alpha=0.3, label='displayed time step')\n",
    "\n",
    "# Add legend with all patches\n",
    "timeline_ax.legend(handles=[is2_patch, cs2_patch, no_dh_patch, timestep_patch], \n",
    "                  loc='upper right')\n",
    "\n",
    "# Customize timeline appearance\n",
    "# timeline_ax.legend(loc='upper right')\n",
    "timeline_ax.set_title('Multi-mission satellite timeline')\n",
    "\n",
    "# Remove y-axis ticks and labels\n",
    "timeline_ax.set_yticks([])\n",
    "\n",
    "\n",
    "# Panel - CryoSat-2 dh time step with evolving outlines and area multiple within evaluation lines\n",
    "\n",
    "# Plot dh time step\n",
    "img = axs[0,0].imshow(dataset1_dh[CS2_i], cmap='RdBu', norm=divnorm, \n",
    "                      origin='lower', extent=[x_min, x_max, y_min, y_max])\n",
    "\n",
    "dt = mid_cyc_dates[CS2_cyc_dates_idx]\n",
    "evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "\n",
    "if not evolving_outlines_gdf.empty:\n",
    "    # Split into positive and negative dh values\n",
    "    positive_outlines = evolving_outlines_dt[evolving_outlines_dt['dh (m)'] > 0]\n",
    "    negative_outlines = evolving_outlines_dt[evolving_outlines_dt['dh (m)'] < 0]\n",
    "    \n",
    "    # Plot positive dh in blue\n",
    "    if not positive_outlines.empty:\n",
    "        positive_outlines.boundary.plot(ax=axs[0,0], color='blue', linewidth=1)\n",
    "    \n",
    "    # Plot negative dh in red\n",
    "    if not negative_outlines.empty:\n",
    "        negative_outlines.boundary.plot(ax=axs[0,0], color='red', linewidth=1)\n",
    "\n",
    "# Plot inset map\n",
    "axIns = axs[0,0].inset_axes([0.02, 0, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "axIns.set_aspect('equal')\n",
    "moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1)\n",
    "moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1)\n",
    "axIns.axis('off')\n",
    "# Plot red star to indicate location\n",
    "axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "    linewidth=1, color='k', s=75)\n",
    "\n",
    "# Set a title for the axes\n",
    "title_text = f'Height change from \\n{date_to_quarter_year(mid_cyc_dates[CS2_cyc_dates_idx])} to {date_to_quarter_year(mid_cyc_dates[CS2_cyc_dates_idx+1])}'\n",
    "axs[0,0].set_title(title_text, y=1)\n",
    "\n",
    "# Create lines for legend\n",
    "stationary_color = 'darkturquoise'\n",
    "stationary_line = plt.Line2D([], [], color=stationary_color, linestyle='solid', linewidth=2)\n",
    "within_eval_lines = plt.Line2D([], [], color='gray', linestyle='dotted', linewidth=2)\n",
    "optimal_within_eval_line = plt.Line2D([], [], color='gray', linestyle='solid', linewidth=2)\n",
    "\n",
    "# Plot legend\n",
    "legend = axs[0,0].legend([stationary_line, \n",
    "                          within_eval_lines,\n",
    "                          optimal_within_eval_line], \n",
    "    ['stationary outline', \n",
    "     'within evaluation boundaries',\n",
    "     'optimal within boundary'],\n",
    "    handlelength=3,\n",
    "    handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper right')\n",
    "\n",
    "\n",
    "# Panel - ICESat-2 dh time step with evolving outlines and area multiple within evaluation lines  \n",
    "\n",
    "# Plot dh time step\n",
    "img = axs[0,1].imshow(dataset2_dh[IS2_i], cmap='RdBu', norm=divnorm, \n",
    "                      origin='lower', extent=[x_min, x_max, y_min, y_max])\n",
    "\n",
    "dt = mid_cyc_dates[IS2_cyc_dates_idx]\n",
    "evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "\n",
    "if not evolving_outlines_gdf.empty:\n",
    "    # Split into positive and negative dh values\n",
    "    positive_outlines = evolving_outlines_dt[evolving_outlines_dt['dh (m)'] > 0]\n",
    "    negative_outlines = evolving_outlines_dt[evolving_outlines_dt['dh (m)'] < 0]\n",
    "    \n",
    "    # Plot positive dh in blue\n",
    "    if not positive_outlines.empty:\n",
    "        positive_outlines.boundary.plot(ax=axs[0,1], color='blue', linewidth=1)\n",
    "    \n",
    "    # Plot negative dh in red\n",
    "    if not negative_outlines.empty:\n",
    "        negative_outlines.boundary.plot(ax=axs[0,1], color='red', linewidth=1)\n",
    "\n",
    "# Add colorbar space to both axes for consistent sizing\n",
    "for ax in [axs[0,0], axs[0,1]]:\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "    if ax == axs[0,1]:  # Only add the actual colorbar to the second plot\n",
    "        cb = fig.colorbar(img, cax=cax)\n",
    "        cb.set_label('dh [m quarter$^{-1}$]')\n",
    "    else:\n",
    "        # Hide the empty axis for the first plot\n",
    "        cax.set_visible(False)\n",
    "\n",
    "# Set a title for the axes\n",
    "title_text = f'Height change from \\n{date_to_quarter_year(mid_cyc_dates[IS2_cyc_dates_idx])} to {date_to_quarter_year(mid_cyc_dates[IS2_cyc_dates_idx+1])}'\n",
    "axs[0,1].set_title(title_text, y=1)\n",
    "\n",
    "for ax in [axs[0,0], axs[0,1]]:\n",
    "    ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "axs[0,1].sharey(axs[0,0])\n",
    "\n",
    "# Create lines for legend\n",
    "pos_dh_anom = plt.Line2D([], [], color='blue', linestyle='solid', linewidth=2)\n",
    "neg_dh_anom = plt.Line2D([], [], color='red', linestyle='solid', linewidth=2)\n",
    "\n",
    "# Plot legend\n",
    "legend = axs[0,1].legend([pos_dh_anom, \n",
    "                          neg_dh_anom], \n",
    "    [f'pos. dh anomaly (>+{level} m)', \n",
    "     f'neg. dh anomaly (<−{level} m)'],\n",
    "    handlelength=3,\n",
    "    handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper right')\n",
    "\n",
    "\n",
    "# Panel - Evolving outlines time series plot ---------------------------------------------\n",
    "\n",
    "# Set up colormap\n",
    "cmap = plt.get_cmap('plasma', len(mid_cyc_dates)-1)\n",
    "\n",
    "# Norm to time variable\n",
    "norm = plt.Normalize(mdates.date2num(cyc_dates['mid_cyc_dates'].iloc[0]), \n",
    "                     mdates.date2num(cyc_dates['mid_cyc_dates'].iloc[-1]))\n",
    "\n",
    "# Zoom in slightly to bounds of optimal within evaluation boundary\n",
    "# Establish x_min, x_max, y_min, y_max\n",
    "optimal_within_eval_poly = area_multiple_buffer(lake_poly, within_area_multiple)\n",
    "x_min, y_min, x_max, y_max = optimal_within_eval_poly.bounds\n",
    "x_buffer, y_buffer = abs(x_max-x_min)*0.01, abs(y_max-y_min)*0.01\n",
    "\n",
    "# Plot MOA surface imagery\n",
    "mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "axs[1,0].imshow(moa_highres_da_subset[0,:,:], cmap='gray', clim=[14000, 17000], \n",
    "          extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "\n",
    "# Plot evolving outlines\n",
    "onlake_lines, offlake_lines = [], []\n",
    "for i, dt in enumerate(mid_cyc_dates):\n",
    "    x, y = 1, 1\n",
    "    onlake_line, = axs[1,0].plot(x, y, color=cmap(norm(mdates.date2num(mid_cyc_dates[i]))), linewidth=2)\n",
    "    onlake_lines.append(onlake_line)\n",
    "    offlake_line, = axs[1,0].plot(x, y, color=cmap(norm(date_to_quarter_year(mid_cyc_dates[i]))), linewidth=2, alpha=0.2)\n",
    "    offlake_lines.append(offlake_line)\n",
    "    \n",
    "    onlake_outlines_dt = onlake_outlines_gdf[onlake_outlines_gdf['midcyc_datetime'] == dt]\n",
    "    offlake_outlines_dt = offlake_outlines_gdf[offlake_outlines_gdf['midcyc_datetime'] == dt]\n",
    "    \n",
    "    if not onlake_outlines_dt.empty:\n",
    "        onlake_outlines_dt.boundary.plot(ax=axs[1,0], \n",
    "            color=cmap(norm(mdates.date2num(mid_cyc_dates[i]))), \n",
    "            linewidth=1)\n",
    "    if not offlake_outlines_dt.empty:\n",
    "        offlake_outlines_dt.boundary.plot(ax=axs[1,0], \n",
    "            color=cmap(norm(mdates.date2num(mid_cyc_dates[i]))), \n",
    "            linewidth=1, alpha=0.5)\n",
    "\n",
    "# Plot evolving outlines unary union\n",
    "evolving_union_gdf.boundary.plot(ax=axs[1,0], color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "# Create stationary region and evolving outlines region and plot\n",
    "stationary_region = area_multiple_buffer(lake_poly, 2)\n",
    "stationary_region = stationary_region.difference(lake_poly)\n",
    "evolving_union_region = area_multiple_buffer(evolving_union_gdf['geometry'], 2)\n",
    "evolving_union_region = evolving_union_region.difference(evolving_union_gdf['geometry'].iloc[0])\n",
    "gpd.GeoDataFrame(geometry=[stationary_region], crs='3031').plot(ax=axs[1,0], color=stationary_color, linewidth=2, alpha=0.2)\n",
    "gpd.GeoDataFrame(geometry=[evolving_union_region], crs='3031').plot(ax=axs[1,0], color='k', linewidth=2, alpha=0.2)\n",
    "\n",
    "# Set up colormap\n",
    "min_date = pd.to_datetime(cyc_start_dates[1])\n",
    "max_date = pd.to_datetime(cyc_end_dates[-1])\n",
    "date_range = pd.date_range(min_date, max_date, periods=len(mid_cyc_dates[1:]))\n",
    "years = date_range.year.unique()\n",
    "years = pd.to_datetime(years, format='%Y')\n",
    "n_dates = len(mid_cyc_dates[1:])\n",
    "cmap = plt.get_cmap('plasma', n_dates)\n",
    "norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "m.set_array(np.linspace(mdates.date2num(min_date), mdates.date2num(max_date), n_dates))\n",
    "\n",
    "# Add colorbar\n",
    "divider = make_axes_locatable(axs[1,0])\n",
    "cax = divider.append_axes('bottom', size='3%', pad=0.6)\n",
    "cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "\n",
    "# Set ticks for all years but labels only for odd years\n",
    "tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "tick_labels = [date.strftime('%Y') if date.year % 4 == 0 else '' for date in years[1:]]\n",
    "cbar.set_ticks(tick_locations)\n",
    "cbar.set_ticklabels(tick_labels)\n",
    "\n",
    "# Add minor ticks for quarters\n",
    "cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Quarter intervals only\n",
    "cbar.set_label('year', size=12, labelpad=10)\n",
    "\n",
    "axs[1,0].set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "    \n",
    "# Emphasize zeroth row_index within evaluation line selected for particular lake\n",
    "gpd.GeoSeries(area_multiple_buffer(lake_poly, within_area_multiple)).boundary.plot(ax=ax, linestyle='solid', color='gray')\n",
    "\n",
    "# Plot legend\n",
    "evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=2)\n",
    "stationary_region_patch = mpatches.Patch(color=stationary_color, alpha=0.2)\n",
    "evolving_union_region_patch = mpatches.Patch(color='k', alpha=0.2)\n",
    "\n",
    "legend = axs[1,0].legend([stationary_region_patch,\n",
    "                       evolving_union_region_patch,\n",
    "                       tuple(onlake_lines),\n",
    "                       # tuple(offlake_lines),\n",
    "                       evolving_union_line], \n",
    "    ['stationary region',\n",
    "     'evolving union region',\n",
    "     'evolving outlines',\n",
    "     # f'evolving outlines ({evolving_outlines_gdf.level[0]} m)',\n",
    "     # 'off-lake evolving outlines', \n",
    "     'evolving outlines union'],\n",
    "    handlelength=3,\n",
    "    handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper right')\n",
    "\n",
    "\n",
    "# Panel - active area ---------------------------------------------\n",
    "    \n",
    "# Plot horizontal zero line for reference\n",
    "axs[1,1].axhline(0, color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "# Plot stationary outline and evolving outlines unary union areas\n",
    "axs[1,1].axhline(np.divide(lake_gdf['area (m^2)'], 1e6).values, \n",
    "                 color=stationary_color, linestyle='solid', linewidth=2)\n",
    "axs[1,1].axhline(np.divide(evolving_union_gdf['area (m^2)'], 1e6).values, \n",
    "                 color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "# Plot multi-colored line and scatter for data points\n",
    "x = mdates.date2num(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "y = np.divide(evolving_geom_calcs_df['evolving_outlines_area (m^2)'], 1e6)\n",
    "\n",
    "# Plot evolving outlines time series using LineCollection from points/segments to plot multi-colored line\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(x)  # Set the values used for colormapping, using matplotlib dates for colors\n",
    "lc.set_linewidth(2)\n",
    "line = axs[1,1].add_collection(lc)\n",
    "scatter = axs[1,1].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "# Add legend\n",
    "legend = axs[1,1].legend([stationary_line, \n",
    "                          evolving_union_line, \n",
    "                          tuple(onlake_lines)], \n",
    "    ['stationary outline',\n",
    "     'evolving outlines union',\n",
    "     'evolving outlines',],\n",
    "     handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "     loc='upper right')\n",
    "\n",
    "\n",
    "# Panel - dh/dt -------------------------------------------------------\n",
    "\n",
    "# Plot horizontal zero line for reference\n",
    "axs[2,0].axhline(0, color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "# Plot stationary outlines off-lake region dh\n",
    "axs[2,0].plot(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "    np.cumsum(stationary_geom_calcs_df['stationary_outline_region_dh (m)']),\n",
    "    color='lightgray', linestyle='solid', linewidth=2)\n",
    "axs[2,0].scatter(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "    np.cumsum(stationary_geom_calcs_df['stationary_outline_region_dh (m)']),\n",
    "    color='lightgray', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# Plot evolving outlines off-lake region dh\n",
    "axs[2,0].plot(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']), \n",
    "    np.cumsum(evolving_geom_calcs_df['evolving_outlines_region_dh (m)']), \n",
    "    color='dimgray', linestyle='solid', linewidth=2)\n",
    "axs[2,0].scatter(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']), \n",
    "    np.cumsum(evolving_geom_calcs_df['evolving_outlines_region_dh (m)']),\n",
    "    color='dimgray', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# Plot stationary outline time series\n",
    "axs[2,0].plot(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "    np.cumsum(stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "    color=stationary_color, linestyle='solid', linewidth=2)\n",
    "axs[2,0].scatter(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "    np.cumsum(stationary_geom_calcs_df['stationary_outline_dh_corr (m)']),\n",
    "    color=stationary_color, linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# Plot multi-colored line and scatter for data points\n",
    "x = mdates.date2num(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "y = np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'])\n",
    "\n",
    "# Plot evolving outlines time series using multi-colored LineCollection from points/segments\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(2)\n",
    "line = axs[2,0].add_collection(lc)\n",
    "scatter = axs[2,0].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "# Plot bias\n",
    "axs[2,0].plot(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']),\n",
    "    np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'] -\n",
    "              stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "    color='red', linestyle='solid', linewidth=2)\n",
    "axs[2,0].scatter(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']),\n",
    "    np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'] -\n",
    "              stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "    color='red', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# Add legend\n",
    "evolving_region = plt.Line2D([], [], color='dimgray', linestyle='solid', linewidth=2)\n",
    "stationary_region = plt.Line2D([], [], color='lightgray', linestyle='solid', linewidth=2)\n",
    "bias = plt.Line2D([], [], color='red', linestyle='solid', linewidth=2)\n",
    "legend = axs[2,0].legend(\n",
    "    [evolving_region,\n",
    "     stationary_region,\n",
    "     tuple(onlake_lines),\n",
    "     stationary_line,  \n",
    "     bias],\n",
    "    ['evolving outlines regional',\n",
    "     'stationary outline regional',\n",
    "     'evolving outlines',\n",
    "     'stationary outline', \n",
    "     'bias (evolving − stationary)'],\n",
    "     handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "     loc='upper right')\n",
    "\n",
    "\n",
    "# Panel - dV/dt --------------------------------------------------\n",
    "\n",
    "# Plot horizontal line at zero for reference\n",
    "axs[2,1].axhline(0, color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "# Plot stationary outline time series\n",
    "axs[2,1].plot(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "    np.divide(np.cumsum(stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "    color=stationary_color, linestyle='solid', linewidth=2)\n",
    "axs[2,1].scatter(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "    np.divide(np.cumsum(stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "    color=stationary_color, linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# Plot multi-colored line and scatter for data points\n",
    "x = mdates.date2num(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "y = np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "\n",
    "# Plot evolving outlines time series using LineCollection from points/segments to plot multi-colored line\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(2)\n",
    "line = axs[2,1].add_collection(lc)\n",
    "scatter = axs[2,1].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "# Plot bias\n",
    "axs[2,1].plot(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']),\n",
    "    np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "                        stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)'], 1e9)), \n",
    "    color='red', linestyle='solid', linewidth=2)\n",
    "axs[2,1].scatter(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']),\n",
    "    np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "                        stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)'], 1e9)), \n",
    "    color='red', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# Add legend\n",
    "legend = axs[2,1].legend([tuple(onlake_lines), stationary_line, bias],\n",
    "    ['evolving outlines', \n",
    "     'stationary outline',\n",
    "     'bias (evolving − stationary)'], \n",
    "    handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper right')\n",
    "\n",
    "# Adjust y-axis limits to avoid legend/data overlap\n",
    "axs[1,1].set_ylim(-25, 625)\n",
    "axs[2,0].set_ylim(-1, 8)\n",
    "axs[2,1].set_ylim(-0.4, 0.4)\n",
    "\n",
    "# Add vertical spans to time series plots\n",
    "for ax in [axs[1,1], axs[2,0], axs[2,1]]:\n",
    "    # Add vertical spans for both timesteps\n",
    "    ax.axvspan(mdates.date2num(cs2_start_date), \n",
    "                         mdates.date2num(cs2_end_date),\n",
    "                         color='gray', alpha=0.1)\n",
    "    \n",
    "    ax.axvspan(mdates.date2num(is2_start_date),\n",
    "                         mdates.date2num(is2_end_date),\n",
    "                         color='gray', alpha=0.1)\n",
    "\n",
    "# Label the timeline panel\n",
    "timeline_ax.text(0.01, 0.98, 'a', transform=timeline_ax.transAxes, \n",
    "                fontsize=20, va='top', ha='left')\n",
    "\n",
    "# Plot elements common to more than one ax object\n",
    "for i in range(axs.shape[0]):\n",
    "    for j in range(axs.shape[1]):\n",
    "        # Set common tick size for all axes\n",
    "        axs[i,j].tick_params(axis='both')\n",
    "        \n",
    "        # Add subplot labels (a, b, c, etc.)\n",
    "        axs[i,j].text(0.02, 0.98, chr(97 + i*2 + j + 1), transform=axs[i,j].transAxes, \n",
    "                     fontsize=16, va='top', ha='left')\n",
    "\n",
    "        # Special formatting for map plots (top row and [1,0])\n",
    "        if (i == 0) or (i == 1 and j == 0):\n",
    "            # Set x and y labels for map plots\n",
    "            axs[i,j].set_xlabel('x [km]')\n",
    "            if j == 0:  # Only for first column\n",
    "                axs[i,j].set_ylabel('y [km]')\n",
    "\n",
    "            # Add map-specific elements\n",
    "            revised_stationary_outlines_gdf.boundary.plot(ax=axs[i,j], \n",
    "                edgecolor=stationary_color, linestyle='solid', linewidth=2, zorder=0)\n",
    "            \n",
    "            # Convert meters to kilometers\n",
    "            km_scale = 1e3\n",
    "            ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "            ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "            axs[i,j].xaxis.set_major_formatter(ticks_x)\n",
    "            axs[i,j].yaxis.set_major_formatter(ticks_y)\n",
    "\n",
    "            # Add evaluation lines\n",
    "            for within_area_multiple_i in range(2,16):\n",
    "                gpd.GeoSeries(area_multiple_buffer(lake_poly, within_area_multiple_i)).boundary.plot(\n",
    "                    ax=axs[i,j], linestyle='dotted', color='gray')\n",
    "            \n",
    "            # Add emphasized evaluation line\n",
    "            gpd.GeoSeries(area_multiple_buffer(lake_poly, within_area_multiple)).boundary.plot(\n",
    "                ax=axs[i,j], linestyle='solid', color='gray')\n",
    "\n",
    "        # Format time series plots\n",
    "        else:\n",
    "            axs[i,j].set_xlabel('year')\n",
    "            \n",
    "            # Set y labels for time series plots\n",
    "            if j == 0:  # First column\n",
    "                if i == 2:\n",
    "                    axs[i,j].set_ylabel('cumulative dh [m]')\n",
    "            elif j == 1:  # Second column\n",
    "                if i == 1:\n",
    "                    axs[i,j].set_ylabel('active area [km$^2$]')\n",
    "                elif i == 2:\n",
    "                    axs[i,j].set_ylabel('cumulative dV [km$^3$]')\n",
    "            \n",
    "            # Add date formatting for specific time series plots\n",
    "            if (i == 1 and j == 1) or (i == 2):  # axs[1,1], axs[2,0], and axs[2,1]\n",
    "                axs[i,j].xaxis.set_major_formatter(ticker.FuncFormatter(even_year_formatter))\n",
    "                axs[i,j].xaxis.set_major_locator(mdates.YearLocator())\n",
    "                axs[i,j].xaxis.set_minor_locator(mdates.MonthLocator([1, 4, 7, 10]))  # Quarter year ticks (Jan, Apr, Jul, Oct)\n",
    "                axs[i,j].set_xlim(cyc_start_dates[1], cyc_end_dates[-1])\n",
    "\n",
    "# Save and preview plot\n",
    "plt.savefig(OUTPUT_DIR + '/figures/Figure_S1.png',\n",
    "    dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c404262a-4294-4c03-a7c3-b71bcc8f6edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0df42f8-a25a-4f86-b4ad-3f0c60e82611",
   "metadata": {},
   "source": [
    "# within_area_multiple, level distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6683e271-99bd-4d5c-8420-f1da2c55ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_first_rows(folder_path):\n",
    "#     \"\"\"\n",
    "#     Reads all CSV files in the specified folder and combines their first rows into a single DataFrame.\n",
    "    \n",
    "#     Parameters:\n",
    "#     folder_path (str): Path to the folder containing CSV files\n",
    "    \n",
    "#     Returns:\n",
    "#     pandas.DataFrame: DataFrame containing the first row from each CSV file\n",
    "#     \"\"\"\n",
    "#     # List to store first rows\n",
    "#     first_rows = []\n",
    "    \n",
    "#     # Iterate through all files in the folder\n",
    "#     for filename in os.listdir(folder_path):\n",
    "#         if filename.endswith('.csv'):\n",
    "#             file_path = os.path.join(folder_path, filename)\n",
    "#             try:\n",
    "#                 # Read the CSV file\n",
    "#                 df = pd.read_csv(file_path)\n",
    "                \n",
    "#                 # Get the first row and add filename as a column\n",
    "#                 if not df.empty:\n",
    "#                     first_row = df.iloc[0:1].copy()\n",
    "#                     first_row['source_file'] = filename\n",
    "#                     first_rows.append(first_row)\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing {filename}: {str(e)}\")\n",
    "    \n",
    "#     # Combine all first rows into a single DataFrame\n",
    "#     if first_rows:\n",
    "#         result = pd.concat(first_rows, ignore_index=True)\n",
    "#         return result\n",
    "#     else:\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "# # # Example usage:\n",
    "# # folder_path = \"/home/jovyan/1_evolving_lakes/output/FigS1_lake_reexamination_methods/levels\"\n",
    "# # result_df = combine_first_rows(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e45624d-3170-4b09-afcf-8554199dc56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_first_rows(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all CSV files in the specified folder and combines their first rows into a single DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    folder_path (str): Path to the folder containing CSV files\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame containing the first row from each CSV file\n",
    "    \"\"\"\n",
    "    # List to store first rows\n",
    "    first_rows = []\n",
    "    \n",
    "    # Iterate through all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Get the first row and add filename as a column\n",
    "                if not df.empty:\n",
    "                    first_row = df.iloc[0:1].copy()\n",
    "                    \n",
    "                    # Add full filename and filename without extension as columns\n",
    "                    first_row['name'] = os.path.splitext(filename)[0]\n",
    "                    \n",
    "                    first_rows.append(first_row)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {str(e)}\")\n",
    "    \n",
    "    # Combine all first rows into a single DataFrame\n",
    "    if first_rows:\n",
    "        result = pd.concat(first_rows, ignore_index=True)\n",
    "        return result\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b283216-6cb4-4077-a7ac-6073746f8af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/home/jovyan/1_evolving_lakes/output/FigS1_lake_reexamination_methods/levels\"\n",
    "result_df = combine_first_rows(folder_path)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e2a964-b4de-4716-a327-96042e68f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_level_histogram(df, column='level', bins=10):\n",
    "    \"\"\"\n",
    "    Creates a histogram of a specified column with enhanced styling\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the data\n",
    "    column (str): Name of the column to create histogram for\n",
    "    bins (int): Number of bins for the histogram\n",
    "    \"\"\"\n",
    "    # Create figure and axis objects with larger size\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create histogram\n",
    "    plt.hist(df[column], bins=bins, color='skyblue', edgecolor='black')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title(f'Distribution of {column}', pad=15, fontsize=14)\n",
    "    plt.xlabel(column, fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Rotate x-axis labels if needed\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_level_histogram(result_df, column='level', bins=35)\n",
    "# plot_level_histogram(result_df, column='within_area_multiple', bins=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29ab093-3ac1-4384-bbac-4afa727776d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_level_histogram(result_df, column='level', bins=35)\n",
    "plot_level_histogram(result_df, column='within_area_multiple', bins=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e555de-b55b-47b7-9d7f-aa535429e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[result_df['level'] > 0.3].sort_values('level', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad66e48-0282-4f66-8f5a-c279cef80145",
   "metadata": {},
   "source": [
    "# TODO: Export a final inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef7c08f-714d-4baf-9434-e476b210ca01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886a85e5-6c0d-4c11-90bf-da5e806cee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /home/jovyan/1_evolving_lakes/Sauthoff-202X-evolving-lakes/output/lake_outlines/evolving_outlines_union_gdf.geojson /home/jovyan/data/boundaries/evolving_outlines_union_gdf.geojson"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
