{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22381a81-8f1c-4d0d-9058-48c340a47d1a",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* add uncorr dh to Fig S1 and other output plots\n",
    "* add union of cumulative dh and dV panels of plot_evolving_and_stationary_comparison and plot_evolving_and_stationary_comparison_sequential functions\n",
    "* add kwarg that allows you to plot cumulative dh (vs cyc to cyc dh) for the output visualizations\n",
    "* \"/home/jovyan/1_evolving_lakes/Sauthoff-202X-evolving-lakes/output/lake_outlines/evolving_outlines_union_gdf.geojson\" appears to be the same as \"/home/jovyan/1_evolving_lakes/Sauthoff-202X-evolving-lakes/output/lake_outlines/stationary_outlines/stationary_outlines_gdf.geojson\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c199d3a-7236-40c4-b1ed-33f946685e07",
   "metadata": {},
   "source": [
    "Code to do data analysis of re-examined active subglacial lakes and create Fig. S1 in Sauthoff and others, 202X, _Journal_.\n",
    "\n",
    "Written 2023-07-11 by W. Sauthoff (wsauthoff.github.io)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff75638-a801-40a3-bd59-7d6536dbc054",
   "metadata": {},
   "source": [
    "# Set up computing environment\n",
    "\n",
    "This code runs continental-scale operations on multiple datasets and requires a ~64 GB server or local memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44cf437b-64b9-4534-b7a0-4604b1ede61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install earthaccess --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67b2a28f-a575-4f1e-b1c9-f601327149aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openpyxl --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ffcf4c9-ea9f-4c3c-b28a-e5b7c358bd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f802c6-6310-465e-9710-7d60a356cc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyogrio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad47cdb0-00aa-4268-8691-9e44b3bfda2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "  var py_version = '3.2.2'.replace('rc', '-rc.').replace('.dev', '-dev.');\n",
       "  var is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1;\n",
       "  var reloading = false;\n",
       "  var Bokeh = root.Bokeh;\n",
       "  var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n",
       "\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks;\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "    if (js_modules == null) js_modules = [];\n",
       "    if (js_exports == null) js_exports = {};\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    if (!reloading) {\n",
       "      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    }\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "    window._bokeh_on_load = on_load\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    var skip = [];\n",
       "    if (window.requirejs) {\n",
       "      window.requirejs.config({'packages': {}, 'paths': {'jspanel': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/jspanel', 'jspanel-modal': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal', 'jspanel-tooltip': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip', 'jspanel-hint': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint', 'jspanel-layout': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout', 'jspanel-contextmenu': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu', 'jspanel-dock': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@7.2.3/dist/gridstack-all', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'jspanel': {'exports': 'jsPanel'}, 'gridstack': {'exports': 'GridStack'}}});\n",
       "      require([\"jspanel\"], function(jsPanel) {\n",
       "\twindow.jsPanel = jsPanel\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-modal\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-tooltip\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-hint\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-layout\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-contextmenu\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-dock\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"gridstack\"], function(GridStack) {\n",
       "\twindow.GridStack = GridStack\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"notyf\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      root._bokeh_is_loading = css_urls.length + 9;\n",
       "    } else {\n",
       "      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n",
       "    }\n",
       "\n",
       "    var existing_stylesheets = []\n",
       "    var links = document.getElementsByTagName('link')\n",
       "    for (var i = 0; i < links.length; i++) {\n",
       "      var link = links[i]\n",
       "      if (link.href != null) {\n",
       "\texisting_stylesheets.push(link.href)\n",
       "      }\n",
       "    }\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      if (existing_stylesheets.indexOf(url) !== -1) {\n",
       "\ton_load()\n",
       "\tcontinue;\n",
       "      }\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }    if (((window['jsPanel'] !== undefined) && (!(window['jsPanel'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/jspanel.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/gridstack/gridstack@7.2.3/dist/gridstack-all.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    var existing_scripts = []\n",
       "    var scripts = document.getElementsByTagName('script')\n",
       "    for (var i = 0; i < scripts.length; i++) {\n",
       "      var script = scripts[i]\n",
       "      if (script.src != null) {\n",
       "\texisting_scripts.push(script.src)\n",
       "      }\n",
       "    }\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n",
       "\tif (!window.requirejs) {\n",
       "\t  on_load();\n",
       "\t}\n",
       "\tcontinue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    for (var i = 0; i < js_modules.length; i++) {\n",
       "      var url = js_modules[i];\n",
       "      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n",
       "\tif (!window.requirejs) {\n",
       "\t  on_load();\n",
       "\t}\n",
       "\tcontinue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      element.type = \"module\";\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    for (const name in js_exports) {\n",
       "      var url = js_exports[name];\n",
       "      if (skip.indexOf(url) >= 0 || root[name] != null) {\n",
       "\tif (!window.requirejs) {\n",
       "\t  on_load();\n",
       "\t}\n",
       "\tcontinue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.type = \"module\";\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      element.textContent = `\n",
       "      import ${name} from \"${url}\"\n",
       "      window.${name} = ${name}\n",
       "      window._bokeh_on_load()\n",
       "      `\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    if (!js_urls.length && !js_modules.length) {\n",
       "      on_load()\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.2.2.min.js\", \"https://cdn.holoviz.org/panel/1.2.3/dist/panel.min.js\"];\n",
       "  var js_modules = [];\n",
       "  var js_exports = {};\n",
       "  var css_urls = [];\n",
       "  var inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }\n",
       "      // Cache old bokeh versions\n",
       "      if (Bokeh != undefined && !reloading) {\n",
       "\tvar NewBokeh = root.Bokeh;\n",
       "\tif (Bokeh.versions === undefined) {\n",
       "\t  Bokeh.versions = new Map();\n",
       "\t}\n",
       "\tif (NewBokeh.version !== Bokeh.version) {\n",
       "\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n",
       "\t}\n",
       "\troot.Bokeh = Bokeh;\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    }\n",
       "    root._bokeh_is_initializing = false\n",
       "  }\n",
       "\n",
       "  function load_or_wait() {\n",
       "    // Implement a backoff loop that tries to ensure we do not load multiple\n",
       "    // versions of Bokeh and its dependencies at the same time.\n",
       "    // In recent versions we use the root._bokeh_is_initializing flag\n",
       "    // to determine whether there is an ongoing attempt to initialize\n",
       "    // bokeh, however for backward compatibility we also try to ensure\n",
       "    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n",
       "    // before older versions are fully initialized.\n",
       "    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n",
       "      root._bokeh_is_initializing = false;\n",
       "      root._bokeh_onload_callbacks = undefined;\n",
       "      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n",
       "      load_or_wait();\n",
       "    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n",
       "      setTimeout(load_or_wait, 100);\n",
       "    } else {\n",
       "      Bokeh = root.Bokeh;\n",
       "      bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n",
       "      root._bokeh_is_initializing = true\n",
       "      root._bokeh_onload_callbacks = []\n",
       "      if (!reloading && (!bokeh_loaded || is_dev)) {\n",
       "\troot.Bokeh = undefined;\n",
       "      }\n",
       "      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n",
       "\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "\trun_inline_js();\n",
       "      });\n",
       "    }\n",
       "  }\n",
       "  // Give older versions of the autoload script a head-start to ensure\n",
       "  // they initialize before we start loading newer version.\n",
       "  setTimeout(load_or_wait, 100)\n",
       "}(window));"
      ],
      "application/vnd.holoviews_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.2.2'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1;\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n  var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'jspanel': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/jspanel', 'jspanel-modal': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal', 'jspanel-tooltip': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip', 'jspanel-hint': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint', 'jspanel-layout': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout', 'jspanel-contextmenu': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu', 'jspanel-dock': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@7.2.3/dist/gridstack-all', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'jspanel': {'exports': 'jsPanel'}, 'gridstack': {'exports': 'GridStack'}}});\n      require([\"jspanel\"], function(jsPanel) {\n\twindow.jsPanel = jsPanel\n\ton_load()\n      })\n      require([\"jspanel-modal\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-tooltip\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-hint\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-layout\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-contextmenu\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-dock\"], function() {\n\ton_load()\n      })\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 9;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window['jsPanel'] !== undefined) && (!(window['jsPanel'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/jspanel.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/gridstack/gridstack@7.2.3/dist/gridstack-all.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.2.2.min.js\", \"https://cdn.holoviz.org/panel/1.2.3/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      Bokeh = root.Bokeh;\n      bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      if (!reloading && (!bokeh_loaded || is_dev)) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "if ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n",
       "  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n",
       "}\n",
       "\n",
       "\n",
       "    function JupyterCommManager() {\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n",
       "      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        comm_manager.register_target(comm_id, function(comm) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        });\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        });\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n",
       "          var messages = comm.messages[Symbol.asyncIterator]();\n",
       "          function processIteratorResult(result) {\n",
       "            var message = result.value;\n",
       "            console.log(message)\n",
       "            var content = {data: message.data, comm_id};\n",
       "            var buffers = []\n",
       "            for (var buffer of message.buffers || []) {\n",
       "              buffers.push(new DataView(buffer))\n",
       "            }\n",
       "            var metadata = message.metadata || {};\n",
       "            var msg = {content, buffers, metadata}\n",
       "            msg_handler(msg);\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "          return messages.next().then(processIteratorResult);\n",
       "        })\n",
       "      }\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n",
       "      if (comm_id in window.PyViz.comms) {\n",
       "        return window.PyViz.comms[comm_id];\n",
       "      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n",
       "        if (msg_handler) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        }\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n",
       "        comm.open();\n",
       "        if (msg_handler) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        }\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        var comm_promise = google.colab.kernel.comms.open(comm_id)\n",
       "        comm_promise.then((comm) => {\n",
       "          window.PyViz.comms[comm_id] = comm;\n",
       "          if (msg_handler) {\n",
       "            var messages = comm.messages[Symbol.asyncIterator]();\n",
       "            function processIteratorResult(result) {\n",
       "              var message = result.value;\n",
       "              var content = {data: message.data};\n",
       "              var metadata = message.metadata || {comm_id};\n",
       "              var msg = {content, metadata}\n",
       "              msg_handler(msg);\n",
       "              return messages.next().then(processIteratorResult);\n",
       "            }\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "        }) \n",
       "        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n",
       "          return comm_promise.then((comm) => {\n",
       "            comm.send(data, metadata, buffers, disposeOnDone);\n",
       "          });\n",
       "        };\n",
       "        var comm = {\n",
       "          send: sendClosure\n",
       "        };\n",
       "      }\n",
       "      window.PyViz.comms[comm_id] = comm;\n",
       "      return comm;\n",
       "    }\n",
       "    window.PyViz.comm_manager = new JupyterCommManager();\n",
       "    \n",
       "\n",
       "\n",
       "var JS_MIME_TYPE = 'application/javascript';\n",
       "var HTML_MIME_TYPE = 'text/html';\n",
       "var EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\n",
       "var CLASS_NAME = 'output';\n",
       "\n",
       "/**\n",
       " * Render data to the DOM node\n",
       " */\n",
       "function render(props, node) {\n",
       "  var div = document.createElement(\"div\");\n",
       "  var script = document.createElement(\"script\");\n",
       "  node.appendChild(div);\n",
       "  node.appendChild(script);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when a new output is added\n",
       " */\n",
       "function handle_add_output(event, handle) {\n",
       "  var output_area = handle.output_area;\n",
       "  var output = handle.output;\n",
       "  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "    return\n",
       "  }\n",
       "  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "  if (id !== undefined) {\n",
       "    var nchildren = toinsert.length;\n",
       "    var html_node = toinsert[nchildren-1].children[0];\n",
       "    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var scripts = [];\n",
       "    var nodelist = html_node.querySelectorAll(\"script\");\n",
       "    for (var i in nodelist) {\n",
       "      if (nodelist.hasOwnProperty(i)) {\n",
       "        scripts.push(nodelist[i])\n",
       "      }\n",
       "    }\n",
       "\n",
       "    scripts.forEach( function (oldScript) {\n",
       "      var newScript = document.createElement(\"script\");\n",
       "      var attrs = [];\n",
       "      var nodemap = oldScript.attributes;\n",
       "      for (var j in nodemap) {\n",
       "        if (nodemap.hasOwnProperty(j)) {\n",
       "          attrs.push(nodemap[j])\n",
       "        }\n",
       "      }\n",
       "      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n",
       "      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n",
       "      oldScript.parentNode.replaceChild(newScript, oldScript);\n",
       "    });\n",
       "    if (JS_MIME_TYPE in output.data) {\n",
       "      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n",
       "    }\n",
       "    output_area._hv_plot_id = id;\n",
       "    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n",
       "      window.PyViz.plot_index[id] = Bokeh.index[id];\n",
       "    } else {\n",
       "      window.PyViz.plot_index[id] = null;\n",
       "    }\n",
       "  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "    var bk_div = document.createElement(\"div\");\n",
       "    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var script_attrs = bk_div.children[0].attributes;\n",
       "    for (var i = 0; i < script_attrs.length; i++) {\n",
       "      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "    }\n",
       "    // store reference to server id on output_area\n",
       "    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when an output is cleared or removed\n",
       " */\n",
       "function handle_clear_output(event, handle) {\n",
       "  var id = handle.cell.output_area._hv_plot_id;\n",
       "  var server_id = handle.cell.output_area._bokeh_server_id;\n",
       "  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n",
       "  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n",
       "  if (server_id !== null) {\n",
       "    comm.send({event_type: 'server_delete', 'id': server_id});\n",
       "    return;\n",
       "  } else if (comm !== null) {\n",
       "    comm.send({event_type: 'delete', 'id': id});\n",
       "  }\n",
       "  delete PyViz.plot_index[id];\n",
       "  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n",
       "    var doc = window.Bokeh.index[id].model.document\n",
       "    doc.clear();\n",
       "    const i = window.Bokeh.documents.indexOf(doc);\n",
       "    if (i > -1) {\n",
       "      window.Bokeh.documents.splice(i, 1);\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle kernel restart event\n",
       " */\n",
       "function handle_kernel_cleanup(event, handle) {\n",
       "  delete PyViz.comms[\"hv-extension-comm\"];\n",
       "  window.PyViz.plot_index = {}\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle update_display_data messages\n",
       " */\n",
       "function handle_update_output(event, handle) {\n",
       "  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n",
       "  handle_add_output(event, handle)\n",
       "}\n",
       "\n",
       "function register_renderer(events, OutputArea) {\n",
       "  function append_mime(data, metadata, element) {\n",
       "    // create a DOM node to render to\n",
       "    var toinsert = this.create_output_subarea(\n",
       "    metadata,\n",
       "    CLASS_NAME,\n",
       "    EXEC_MIME_TYPE\n",
       "    );\n",
       "    this.keyboard_manager.register_events(toinsert);\n",
       "    // Render to node\n",
       "    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "    render(props, toinsert[0]);\n",
       "    element.append(toinsert);\n",
       "    return toinsert\n",
       "  }\n",
       "\n",
       "  events.on('output_added.OutputArea', handle_add_output);\n",
       "  events.on('output_updated.OutputArea', handle_update_output);\n",
       "  events.on('clear_output.CodeCell', handle_clear_output);\n",
       "  events.on('delete.Cell', handle_clear_output);\n",
       "  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n",
       "\n",
       "  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "    safe: true,\n",
       "    index: 0\n",
       "  });\n",
       "}\n",
       "\n",
       "if (window.Jupyter !== undefined) {\n",
       "  try {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  } catch(err) {\n",
       "  }\n",
       "}\n"
      ],
      "application/vnd.holoviews_load.v0+json": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"logo-block\">\n",
       "<img src='data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAYAAACqaXHeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAAB+wAAAfsBxc2miwAAABl0RVh0U29mdHdhcmUAd3d3Lmlua3NjYXBlLm9yZ5vuPBoAAA6zSURB\n",
       "VHic7ZtpeFRVmsf/5966taWqUlUJ2UioBBJiIBAwCZtog9IOgjqACsogKtqirT2ttt069nQ/zDzt\n",
       "tI4+CrJIREFaFgWhBXpUNhHZQoKBkIUASchWla1S+3ar7r1nPkDaCAnZKoQP/D7mnPOe9/xy76n3\n",
       "nFSAW9ziFoPFNED2LLK5wcyBDObkb8ZkxuaoSYlI6ZcOKq1eWFdedqNzGHQBk9RMEwFAASkk0Xw3\n",
       "ETacDNi2vtvc7L0ROdw0AjoSotQVkKSvHQz/wRO1lScGModBFbDMaNRN1A4tUBCS3lk7BWhQkgpD\n",
       "lG4852/+7DWr1R3uHAZVQDsbh6ZPN7CyxUrCzJMRouusj0ipRwD2uKm0Zn5d2dFwzX1TCGhnmdGo\n",
       "G62Nna+isiUqhkzuKrkQaJlPEv5mFl2fvGg2t/VnzkEV8F5ioioOEWkLG86fvbpthynjdhXYZziQ\n",
       "x1hC9J2NFyi8vCTt91Fh04KGip0AaG9zuCk2wQCVyoNU3Hjezee9bq92duzzTmxsRJoy+jEZZZYo\n",
       "GTKJ6SJngdJqAfRzpze0+jHreUtPc7gpBLQnIYK6BYp/uGhw9YK688eu7v95ysgshcg9qSLMo3JC\n",
       "4jqLKQFBgdKDPoQ+Pltb8dUyQLpeDjeVgI6EgLIQFT5tEl3rn2losHVsexbZ3EyT9wE1uGdkIPcy\n",
       "BGxn8QUq1QrA5nqW5i2tLqvrrM9NK6AdkVIvL9E9bZL/oyfMVd/jqvc8LylzRBKDJSzIExwhQzuL\n",
       "QYGQj4rHfFTc8mUdu3E7yoLtbTe9gI4EqVgVkug2i5+uXGo919ixbRog+3fTbQ8qJe4ZOYNfMoTI\n",
       "OoshUNosgO60AisX15aeI2PSIp5KiFLI9ubb1vV3Qb2ltwLakUCDAkWX7/nHKRmmGIl9VgYsUhJm\n",
       "2NXjKYADtM1ygne9QQDIXlk49FBstMKx66D1v4+XuQr7vqTe0VcBHQlRWiOCbmmSYe2SqtL6q5rJ\n",
       "zsTb7lKx3FKOYC4DoqyS/B5bvLPxvD9Qtf6saxYLQGJErmDOdOMr/zo96km1nElr8bmPOBwI9COv\n",
       "HnFPRIwmkSOv9kcAS4heRsidOkpeWBgZM+UBrTFAXNYL5Vf2ii9c1trNzpYdaoVil3WIc+wdk+gQ\n",
       "noie3ecCcxt9ITcLAPWt/laGEO/9U6PmzZkenTtsSMQ8uYywJVW+grCstAvCIaAdArAsIWkRDDs/\n",
       "KzLm2YcjY1Lv0UdW73HabE9n6V66cxSzfEmuJssTpKGVp+0vHq73FwL46eOjpMpbRAnNmJFrGJNu\n",
       "Ukf9Yrz+3rghiumCKNXXWPhLYcjxGsIpoCMsIRoFITkW8AuyM8jC1+/QLx4bozCEJIq38+1rtpR6\n",
       "V/yzb8eBlRb3fo5l783N0CWolAzJHaVNzkrTzlEp2bQ2q3TC5gn6wpnoQAmwSiGh2GitnTmVMc5O\n",
       "UyfKWUKCIsU7+fZDKwqdT6DDpvkzAX4/+AMFjk0tDp5GRXLpQ2MUmhgDp5gxQT8+Y7hyPsMi8uxF\n",
       "71H0oebujHALECjFKaW9Lm68n18wXp2kVzIcABytD5iXFzg+WVXkegpAsOOYziqo0OkK76GyquC3\n",
       "ltZAzMhhqlSNmmWTE5T6e3IN05ITFLM4GdN0vtZ3ob8Jh1NAKXFbm5PtLU/eqTSlGjkNAJjdgn/N\n",
       "aedXa0tdi7+t9G0FIF49rtMSEgAs1kDLkTPO7ebm4IUWeyh1bKomXqlgMG6kJmHcSM0clYLJ8XtR\n",
       "1GTnbV3F6I5wCGikAb402npp1h1s7LQUZZSMIfALFOuL3UUrfnS8+rez7v9qcold5tilgHbO1fjK\n",
       "9ubb17u9oshxzMiUBKXWqJNxd+fqb0tLVs4lILFnK71H0Ind7uiPgACVcFJlrb0tV6DzxqqTIhUM\n",
       "CwDf1/rrVhTa33/3pGPxJYdQ2l2cbgVcQSosdx8uqnDtbGjh9SlDVSMNWhlnilfqZk42Th2ZpLpf\n",
       "xrHec5e815zrr0dfBZSwzkZfqsv+1FS1KUknUwPARVvItfKUY+cn57yP7qv07UE3p8B2uhUwLk09\n",
       "e0SCOrK+hbdYHYLjRIl71wWzv9jpEoeOHhGRrJAzyEyNiJuUqX0g2sBN5kGK6y2Blp5M3lsB9Qh4\n",
       "y2Ja6x6+i0ucmKgwMATwhSjdUu49tKrQ/pvN5d53ml2CGwCmJipmKjgmyuaXzNeL2a0AkQ01Th5j\n",
       "2DktO3Jyk8f9vcOBQHV94OK+fPumJmvQHxJoWkaKWq9Vs+yUsbq0zGT1I4RgeH2b5wef7+c7bl8F\n",
       "eKgoHVVZa8ZPEORzR6sT1BzDUAD/d9F78e2Tzv99v8D+fLVTqAKAsbGamKey1Mt9Ann4eH3gTXTz\n",
       "idWtAJ8PQWOk7NzSeQn/OTHDuEikVF1R4z8BQCy+6D1aWRfY0tTGG2OM8rRoPaeIj5ZHzJxszElN\n",
       "VM8K8JS5WOfv8mzRnQAKoEhmt8gyPM4lU9SmBK1MCQBnW4KONT86v1hZ1PbwSXPw4JWussVjtH9Y\n",
       "NCoiL9UoH/6PSu8jFrfY2t36erQHXLIEakMi1SydmzB31h3GGXFDFNPaK8Rme9B79Ixrd0WN+1ij\n",
       "NRQ/doRmuFLBkHSTOm5GruG+pFjFdAmorG4IXH1Qua6ASniclfFtDYt+oUjKipPrCQB7QBQ2lrgP\n",
       "fFzm+9XWUtcqJ3/5vDLDpJ79XHZk3u8nGZ42qlj1+ydtbxysCezrydp6ugmipNJ7WBPB5tydY0jP\n",
       "HaVNzs3QzeE4ZpTbI+ZbnSFPbVOw9vsfnVvqWnirPyCNGD08IlqtYkh2hjZ5dErEQzoNm+6ykyOt\n",
       "Lt5/PQEuSRRKo22VkydK+vvS1XEKlhCJAnsqvcVvH7f/ZU2R67eXbMEGAMiIV5oWZWiWvz5Fv2xG\n",
       "sjqNJQRvn3Rs2lji/lNP19VjAQDgD7FHhujZB9OGqYxRkZxixgRDVlqS6uEOFaJUVu0rPFzctrnF\n",
       "JqijImVp8dEKVWyUXDk92zAuMZ6bFwpBU1HrOw6AdhQgUooChb0+ItMbWJitSo5Ws3IAOGEOtL53\n",
       "0vHZih9sC4vtofZ7Qu6523V/fmGcds1TY3V36pUsBwAbSlxnVh2xLfAD/IAIMDf7XYIkNmXfpp2l\n",
       "18rkAJAy9HKFaIr/qULkeQQKy9zf1JgDB2uaeFNGijo5QsUyacNUUTOnGO42xSnv4oOwpDi1zYkc\n",
       "efUc3I5Gk6PhyTuVKaOGyLUAYPGIoY9Pu/atL/L92+4q9wbflRJ2Trpm/jPjdBtfnqB/dIThcl8A\n",
       "KG7hbRuKnb8qsQsVvVlTrwQAQMUlf3kwJI24Z4JhPMtcfng5GcH49GsrxJpGvvHIaeem2ma+KSjQ\n",
       "lIwUdYyCY8j4dE1KzijNnIP2llF2wcXNnsoapw9XxsgYAl6k+KzUXbi2yP3KR2ecf6z3BFsBICdW\n",
       "nvnIaG3eHybqX7vbpEqUMT+9OL4Qpe8VON7dXuFd39v19FoAABRVePbGGuXTszO0P7tu6lghUonE\n",
       "llRdrhArLvmKdh9u29jcFiRRkfLUxBiFNiqSU9icoZQHo5mYBI1MBgBH6wMNb+U7Pnw337H4gi1Y\n",
       "ciWs+uks3Z9fztUvfzxTm9Ne8XXkvQLHNytOOZeiD4e0PgkAIAYCYknKUNUDSXEKzdWNpnil7r4p\n",
       "xqkjTarZMtk/K8TQ6Qve78qqvXurGwIJqcOUKfUWHsm8KGvxSP68YudXq4pcj39X49uOK2X142O0\n",
       "Tz5/u/7TVybqH0rSya6ZBwD21/gubbrgWdDgEOx9WUhfBaC2ibcEBYm7a7x+ukrBMNcEZggyR0TE\n",
       "T8zUPjikQ4VosQZbTpS4vqizBKvqmvjsqnpfzaZyx9JPiz1/bfGKdgD45XB1zoIMzYbfTdS/NClB\n",
       "Gct0USiY3YL/g0LHy/uq/Ef6uo5+n0R/vyhp17Klpge763f8rMu6YU/zrn2nml+2WtH+Z+5IAAFc\n",
       "2bUTdTDOSNa9+cQY7YLsOIXhevEkCvzph7a8laecz/Un/z4/Ae04XeL3UQb57IwU9ZDr9UuKVajv\n",
       "nxp1+1UVIo/LjztZkKH59fO3G/JemqCfmaCRqbqbd90ZZ8FfjtkfAyD0J/9+C2h1hDwsSxvGjNDc\n",
       "b4zk5NfrSwiQblLHzZhg+Jf4aPlUwpDqkQqa9nimbt1/TDH8OitGMaQnj+RJS6B1fbF7SY1TqO5v\n",
       "/v0WAADl1f7zokgS7s7VT2DZ7pegUjBM7mjtiDZbcN4j0YrHH0rXpCtY0qPX0cVL0rv5jv/ZXend\n",
       "0u/EESYBAFBU4T4Qa5TflZOhTe7pmKpaP8kCVUVw1+yhXfJWvn1P3hnXi33JsTN6PnP3hHZ8Z3/h\n",
       "aLHzmkNPuPj7Bc/F/Q38CwjTpSwQXgE4Vmwry9tpfq/ZFgqFMy4AVDtCvi8rvMvOmv0N4YwbVgEA\n",
       "sPM72/KVnzfspmH7HQGCRLG2yL1+z8XwvPcdCbsAANh+xPzstgMtxeGKt+6MK3/tacfvwhWvIwMi\n",
       "oKEBtm0H7W+UVfkc/Y1V0BhoPlDr/w1w/eu1vjIgAgDg22OtX6/eYfnEz/focrZTHAFR+PSs56/7\n",
       "q32nwpjazxgwAQCwcU/T62t3WL7r6/jVRa6/byp1rei+Z98ZUAEAhEPHPc8fKnTU9nbgtnOe8h0l\n",
       "9hcGIqmODLQAHCy2Xti6v/XNRivf43f4fFvIteu854+VHnR7q9tfBlwAAGz+pnndB9vM26UebAe8\n",
       "SLHujPOTPVW+rwY+sxskAAC2HrA8t2Vvc7ffP1r9o+vwR2dcr92InIAbKKC1FZ5tB1tf+/G8p8sv\n",
       "N/9Q5zd/XR34LYCwV5JdccMEAMDBk45DH243r/X4xGvqxFa/GNpS7n6rwOwNWwHVE26oAADYurf1\n",
       "zx/utOzt+DMKYM0p17YtZZ5VNzqfsB2HewG1WXE8PoZ7gOclbTIvynZf9JV+fqZtfgs/8F/Nu5rB\n",
       "EIBmJ+8QRMmpU7EzGRsf2FzuePqYRbzh/zE26EwdrT10f6r6o8HOYzCJB9Dpff8tbnGLG8L/A/WE\n",
       "roTBs2RqAAAAAElFTkSuQmCC'\n",
       "     style='height:25px; border-radius:12px; display: inline-block; float: left; vertical-align: middle'></img>\n",
       "\n",
       "\n",
       "  <img src='data:image/png;base64,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'\n",
       "       style='height:15px; border-radius:12px; display: inline-block; float: left'></img>\n",
       "  \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries\n",
    "from aiohttp import ClientResponseError\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "import datetime\n",
    "import earthaccess\n",
    "import fiona\n",
    "import gc\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "import hvplot.pandas\n",
    "from IPython.display import clear_output\n",
    "import logging\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Set backend to Agg to avoid NavigationToolbar issue\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pyogrio\n",
    "from pyproj import CRS, Geod, Transformer\n",
    "import re\n",
    "import rioxarray\n",
    "from rioxarray.exceptions import NoDataInBounds\n",
    "from shapely.geometry import box, MultiPolygon, Point, Polygon\n",
    "from shapely.ops import unary_union\n",
    "from shapely.validation import make_valid\n",
    "import shutil\n",
    "from skimage import measure\n",
    "import time\n",
    "import traceback\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "import xarray as xr\n",
    "\n",
    "# Magic functions\n",
    "%matplotlib widget\n",
    "# %matplotlib inline\n",
    "\n",
    "# Define data directories dependent on home environment\n",
    "# Replace with your directory file paths\n",
    "if os.getenv('HOME') == '/home/jovyan':\n",
    "    DATA_DIR = '/home/jovyan/data'\n",
    "    OUTPUT_DIR = '/home/jovyan/1_evolving_lakes/output/FigS1_lake_reexamination_methods'\n",
    "    OUTPUT_DIR_GIT = '/home/jovyan/1_evolving_lakes/Sauthoff-202X-evolving-lakes/output'\n",
    "\n",
    "# Define constants and coordinate transforms for the geodesic area calculation\n",
    "CRS_LL = \"EPSG:4326\" # wgs84 in lon,lat\n",
    "GEOD = CRS(CRS_LL).get_geod() # geod object for calculating geodesic area on defined ellipsoid\n",
    "CRS_XY = \"EPSG:3031\" # Antarctic Polar Stereographic in x, y\n",
    "XY_TO_LL = Transformer.from_crs(CRS_XY, CRS_LL, always_xy = True) # make coord transformer\n",
    "geod = Geod(ellps=\"WGS84\") # Create a Geod object for calculating area on the WGS84 ellipsoid\n",
    "\n",
    "# Change default font to increase font size\n",
    "plt.rcParams.update({'font.size': 8})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4adef9-57d4-4ab4-b066-ba4a4ebb38a5",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac48698c-9656-4c01-b62d-37c460a384e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_quarter_year(date):\n",
    "    \"\"\"Convert datetime64 to year.quarter.\"\"\"\n",
    "    if isinstance(date, np.datetime64):\n",
    "        date = pd.Timestamp(date)\n",
    "    \n",
    "    return date.year + (date.quarter - 1) * 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "495b08ae-6a2c-4f57-8a9c-1dd26c80b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_multiple_buffer(ref_polygon, area_multiple, precision=100, exclude_inner=False):\n",
    "    '''\n",
    "    This function takes a polygon and returns a polygon with a buffer such that the area of the buffered polygon\n",
    "    is approximately the specified multiple of the original polygon area. Optionally excludes the inner polygon\n",
    "    to create a ring-like shape.\n",
    "    \n",
    "    Inputs:\n",
    "    * param polygon: Shapely Polygon object\n",
    "    * param area_multiple: the multiple of the original polygon area you wish the buffered polygon to be\n",
    "    * param precision: Precision for the iterative process to find the buffer distance\n",
    "    * param exclude_inner: If True, returns the difference between the buffered area and original polygon\n",
    "    * return: Buffered Polygon or Ring-like Polygon (if exclude_inner=True)\n",
    "    '''\n",
    "    # Ensure we're working with a single geometry, not a Series\n",
    "    if hasattr(ref_polygon, 'iloc'):\n",
    "        ref_polygon = ref_polygon.iloc[0]\n",
    "        \n",
    "    original_area = ref_polygon.area\n",
    "    target_area = area_multiple * original_area\n",
    "    buffer_distance = 0\n",
    "    area_multiple_polygon = ref_polygon\n",
    "    \n",
    "    while area_multiple_polygon.area < target_area:\n",
    "        buffer_distance += precision\n",
    "        area_multiple_polygon = ref_polygon.buffer(buffer_distance)\n",
    "    \n",
    "    if exclude_inner:\n",
    "        # Return the difference between the buffered polygon and the original polygon\n",
    "        return area_multiple_polygon.difference(ref_polygon)\n",
    "    else:\n",
    "        return area_multiple_polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c858ca39-7f32-483a-a037-81ebf05b20d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_gdf_by_folder_contents(gdf, folder_path, exclude=True, prefix=None, suffix=None, suffix_pattern=None, file_extension=None):\n",
    "    '''\n",
    "    Filter the GeoDataFrame based on processed lake names from the folder contents.\n",
    "    \n",
    "    Args:\n",
    "    gdf: GeoDataFrame to be filtered.\n",
    "    folder_path: Path to the directory containing files and/or subdirectories.\n",
    "    exclude: If True, excludes gdf rows where the 'name' is in the folder_path directories or files.\n",
    "             If False, includes only gdf rows where the 'name' is in the folder_path directories or files.\n",
    "    prefix: Optional string to remove from the beginning of filenames.\n",
    "    suffix: Optional string to remove from the end of filenames.\n",
    "    suffix_pattern: Optional regex pattern to remove from the end of filenames.\n",
    "    file_extension: Optional string specifying the file extension to filter (e.g., 'png', 'txt').\n",
    "    \n",
    "    Returns:\n",
    "    GeoDataFrame filtered based on the presence of 'name' in folder_path.\n",
    "\n",
    "    # Example usage:\n",
    "    remaining_lakes = filter_gdf_by_folder_contents(\n",
    "        stationary_outlines_gdf, \n",
    "        folder_path,\n",
    "        # prefix='plot_evolving_outlines_time_series_', \n",
    "        suffix_pattern=r'\\d+\\.\\d+m-level_\\d+x-with',\n",
    "        file_extension='txt'\n",
    "    )\n",
    "    '''\n",
    "    # Return empty GeoDataFrame if input is empty\n",
    "    if gdf is None or gdf.empty:\n",
    "        return gdf\n",
    "\n",
    "    def process_name(name):\n",
    "        \"\"\"Helper function to remove prefix and suffix from a name\"\"\"\n",
    "        processed_name = name\n",
    "        \n",
    "        # First strip the file extension if it exists\n",
    "        processed_name = os.path.splitext(processed_name)[0]\n",
    "        \n",
    "        # if prefix and processed_name.startswith(prefix):\n",
    "        #     processed_name = processed_name[len(prefix):]\n",
    "            \n",
    "        if suffix_pattern:\n",
    "            processed_name = re.sub(suffix_pattern + '$', '', processed_name)\n",
    "        elif suffix and processed_name.endswith(suffix):\n",
    "            processed_name = processed_name[:-len(suffix)]\n",
    "            \n",
    "        return processed_name.lower().strip()\n",
    "    \n",
    "    # Get all files and filter by extension if specified\n",
    "    all_files = os.listdir(folder_path)\n",
    "    if file_extension:\n",
    "        clean_extension = file_extension.lstrip('.')\n",
    "        all_files = [f for f in all_files if f.lower().endswith(f'.{clean_extension.lower()}')]\n",
    "    \n",
    "    # Process filenames to get lake names\n",
    "    names_in_folder = {\n",
    "        process_name(name)\n",
    "        for name in all_files\n",
    "    }\n",
    "    \n",
    "    # Filter without adding and then dropping a new column\n",
    "    gdf_filtered = gdf[gdf['name'].str.lower().str.strip().apply(\n",
    "        lambda x: (x not in names_in_folder) if exclude else (x in names_in_folder)\n",
    "    )]\n",
    "    \n",
    "    return gdf_filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b00b45-250f-4252-a8a7-3a6e77b05786",
   "metadata": {},
   "source": [
    "## find_and_save_optimal_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c09df69b-9a8d-4199-9b64-5298e6a70804",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_and_save_optimal_parameters(lake_gdf, within_area_multiples=range(2, 16)):\n",
    "    '''\n",
    "    Find and save optimal levels for each lake at various within evaluation boundaries.\n",
    "    '''\n",
    "    results = find_optimal_parameters(lake_gdf=lake_gdf, within_area_multiples=within_area_multiples)\n",
    "    save_search_results(lake_gdf, results)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "def find_optimal_parameters(lake_gdf, \n",
    "                          within_area_multiples=range(2, 16),\n",
    "                          initial_level=0.01,\n",
    "                          level_increment=0.01,\n",
    "                          within_fraction_target=0.95):\n",
    "    '''\n",
    "    Find optimal search extent and level parameters for a lake.     \n",
    "    '''\n",
    "    # Initialize results DataFrame\n",
    "    results_df = pd.DataFrame(\n",
    "        columns=['within_area_multiple', 'level', 'within_percent', 'dataset_dois'])\n",
    "\n",
    "    # Prepare datasets\n",
    "    dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(lake_gdf, 30)\n",
    "    \n",
    "    try:\n",
    "        for area_multiple in within_area_multiples:\n",
    "            print('Finding optimal levels at within evaluation boundaries for', lake_gdf['name'].iloc[0])\n",
    "            level = initial_level\n",
    "            # found_for_current_multiple = False  # Track if we found result for this multiple\n",
    "            # stop_incrementing = False\n",
    "            \n",
    "            # while level <= 2.0 and not found_for_current_multiple and not stop_incrementing:\n",
    "            while level <= 2.0:\n",
    "                outlines_gdf = find_evolving_outlines(\n",
    "                    lake_gdf=lake_gdf,\n",
    "                    within_area_multiple=area_multiple,\n",
    "                    level=level,\n",
    "                    dataset1_masked=dataset1_masked,\n",
    "                    dataset2_masked=dataset2_masked, \n",
    "                    search_extent_poly=search_extent_poly,\n",
    "                    plot=False\n",
    "                )\n",
    "\n",
    "                if outlines_gdf is not None and not outlines_gdf.empty:\n",
    "                    within_fraction = calculate_within_fraction(\n",
    "                        outlines_gdf, lake_gdf['geometry'], area_multiple)\n",
    "                    print(f\"within_area_multiple: {area_multiple}, level: {level}, within: {round(within_fraction*100)}%\")\n",
    "\n",
    "                    if within_fraction == 0.0:\n",
    "                        print(f\"Within fraction is 0.0, stopping search for area_multiple: {area_multiple}\")\n",
    "                        # stop_incrementing = True\n",
    "                        break\n",
    "\n",
    "                    if within_fraction >= within_fraction_target:\n",
    "                        onlake_outlines, offlake_outlines = extract_intersecting_polygons_recursive(\n",
    "                            outlines_gdf, lake_gdf['geometry'].iloc[0]\n",
    "                        )\n",
    "                        \n",
    "                        if not onlake_outlines.empty:\n",
    "                            # Add result\n",
    "                            dois = [doi for doi in [dataset1_doi, dataset2_doi] if doi is not None]\n",
    "                            results_df.loc[len(results_df)] = {\n",
    "                                'within_area_multiple': area_multiple,\n",
    "                                'level': level,\n",
    "                                'within_percent': within_fraction * 100,\n",
    "                                'dataset_dois': ', '.join(dois)\n",
    "                            }\n",
    "                            found_for_current_multiple = True  # Mark that we found result for this area_multiple\n",
    "                            break  # Found a good result for this area_multiple, move to next\n",
    "                        else:\n",
    "                            print(\"onlake_outlines is empty, continuing search\")\n",
    "                else:\n",
    "                    print(f\"within_area_multiple: {area_multiple}, level: {level}, within: no outlines found\")\n",
    "                    print(f\"No outlines found, stopping search for area_multiple: {area_multiple}\")\n",
    "                    # stop_incrementing = True  # Stop if no outlines found\n",
    "                    break\n",
    "\n",
    "                # Increment level\n",
    "                level = round(level + level_increment, 2)\n",
    "\n",
    "            # Clear output for each within_area_multiple\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing lake {lake_gdf['name']}: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "    finally:\n",
    "        gc.collect()\n",
    "\n",
    "    # Sort the DataFrame\n",
    "    results_df = results_df.sort_values(\n",
    "        by=['level', 'within_area_multiple'], \n",
    "        ascending=[True, True])\n",
    "\n",
    "    results_df = remove_higher_duplicates(results_df)\n",
    "        \n",
    "    return results_df\n",
    "\n",
    "def calculate_within_fraction(evolving_outlines_gdf, stationary_outline, area_multiple):\n",
    "    \"\"\"\n",
    "    Calculate fraction of evolving outlines that fall within the search extent.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    evolving_outlines_gdf : GeoDataFrame\n",
    "        The evolving outlines to analyze\n",
    "    stationary_outline : Geometry\n",
    "        The original lake outline\n",
    "    area_multiple : int\n",
    "        Area multiple for within evaluation boundary\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Fraction of outlines that are within the search extent (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    # Create search extent boundary\n",
    "    within_evaluation_poly = area_multiple_buffer(\n",
    "        ref_polygon=stationary_outline, \n",
    "        area_multiple=area_multiple)\n",
    "    within_evaluation_gdf = gpd.GeoDataFrame(\n",
    "        geometry=gpd.GeoSeries([within_evaluation_poly]), \n",
    "        crs=3031)\n",
    "\n",
    "    # Validate geometries\n",
    "    valid_outlines = evolving_outlines_gdf.loc[\n",
    "        evolving_outlines_gdf.is_valid & ~evolving_outlines_gdf.is_empty].copy()\n",
    "    valid_within_evaluation_poly = within_evaluation_gdf.loc[\n",
    "        within_evaluation_gdf.is_valid & ~within_evaluation_gdf.is_empty].copy()\n",
    "    \n",
    "    if valid_outlines.empty or valid_within_evaluation_poly.empty:\n",
    "        return 0.0\n",
    "\n",
    "    # Convert geometries to ensure they're polygons\n",
    "    valid_outlines.loc[:, 'geometry'] = valid_outlines['geometry'].apply(\n",
    "        lambda geom: geom if isinstance(geom, Polygon) else Polygon())\n",
    "    valid_within_evaluation_poly.loc[:, 'geometry'] = valid_within_evaluation_poly['geometry'].apply(\n",
    "        lambda geom: geom if isinstance(geom, Polygon) else Polygon())\n",
    "\n",
    "    # Perform spatial analysis\n",
    "    within = gpd.sjoin(valid_outlines, within_evaluation_gdf, predicate='within')\n",
    "    overlaps = gpd.sjoin(valid_outlines, within_evaluation_gdf, predicate='overlaps')\n",
    "\n",
    "    # Calculate fraction that are within\n",
    "    total = len(within) + len(overlaps)\n",
    "    if total > 0:\n",
    "        return round(len(within) / total, 2)\n",
    "    return 0.0\n",
    "\n",
    "def remove_higher_duplicates(df):\n",
    "    \"\"\"\n",
    "    Remove rows that have duplicate 'level' values at higher 'within_area_multiple' values.\n",
    "    Assumes the dataframe is already sorted by 'level' and 'within_area_multiple'.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame with 'level' and 'within_area_multiple' columns\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with duplicates removed\n",
    "    \"\"\"\n",
    "    # Keep track of levels we've seen\n",
    "    seen_levels = set()\n",
    "    # Create a boolean mask for rows to keep\n",
    "    mask = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # If we haven't seen this level before, keep the row\n",
    "        if row['level'] not in seen_levels:\n",
    "            mask.append(True)\n",
    "            seen_levels.add(row['level'])\n",
    "        else:\n",
    "            # If we have seen this level, don't keep the row\n",
    "            mask.append(False)\n",
    "    \n",
    "    # Return filtered dataframe\n",
    "    return df[mask]\n",
    "\n",
    "def save_search_results(lake_gdf, results_df):\n",
    "    '''\n",
    "    Save search results to files. Creates appropriate files for lakes with no results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lake_gdf : GeoDataFrame row\n",
    "        Single lake record containing name\n",
    "    results_df : DataFrame  \n",
    "        Results from find_optimal_parameters, may be empty\n",
    "    '''\n",
    "    os.makedirs(OUTPUT_DIR + '/levels', exist_ok=True)\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        results_df.to_csv(\n",
    "            OUTPUT_DIR + f'/levels/{lake_name}.csv',\n",
    "            index=False\n",
    "        )\n",
    "        print(f\"Saved optimal parameters for {lake_name}\")\n",
    "    else:\n",
    "        print(f\"No outlines found for {lake_name}\")\n",
    "        write_no_outlines(OUTPUT_DIR + f'/levels/{lake_name}.txt')\n",
    "        write_no_outlines(OUTPUT_DIR_GIT + f'/lake_outlines/evolving_outlines/{lake_name}.txt')\n",
    "\n",
    "def write_no_outlines(filepath):\n",
    "    \"\"\"Write file indicating no outlines found\"\"\"\n",
    "    try:\n",
    "        # Ensure directory exists\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        # Write file\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(\"There are no evolving outlines for this lake.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing no outlines file to {filepath}: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "def prepare_datasets(lake_gdf, area_multiple):\n",
    "    \"\"\"\n",
    "    Prepare masked datasets based on lake parameters\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lake_gdf : GeoDataFrame row\n",
    "        Single lake record containing CS2_SARIn_start and geometry\n",
    "    area_multiple : int\n",
    "        Area multiple for buffering\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked))\n",
    "    \"\"\"\n",
    "    # Extract CS2_SARIn_start as a single value, not a Series\n",
    "    if isinstance(lake_gdf, pd.Series):\n",
    "        CS2_SARIn_start = lake_gdf['CS2_SARIn_start']\n",
    "    else:\n",
    "        CS2_SARIn_start = lake_gdf.iloc[0]['CS2_SARIn_start']\n",
    "    \n",
    "    # Initialize dataset1\n",
    "    dataset1 = None\n",
    "    dataset1_doi = None\n",
    "    \n",
    "    # Check time period using proper null checking\n",
    "    if pd.isna(CS2_SARIn_start) or str(CS2_SARIn_start) == '<NA>':\n",
    "        dataset1 = None\n",
    "        dataset1_doi = None\n",
    "    elif CS2_SARIn_start == '2013.75':\n",
    "        dataset1 = CS2_Smith2017.sel(time=slice(np.datetime64('2013-10-01T22:30:00.000000000'), None))\n",
    "        dataset1_doi = dataset1.attrs.get('identifier_product_DOI', 'Unknown')\n",
    "    elif CS2_SARIn_start == '2010.5':\n",
    "        dataset1 = CS2_Smith2017\n",
    "        dataset1_doi = dataset1.attrs.get('identifier_product_DOI', 'Unknown')\n",
    "    \n",
    "    dataset2 = ATL15_dh\n",
    "    dataset2_doi = dataset2.attrs.get('identifier_product_DOI', 'Unknown')\n",
    "    \n",
    "    # Get geometry properly\n",
    "    geometry = lake_gdf['geometry'] if isinstance(lake_gdf, pd.Series) else lake_gdf.iloc[0]['geometry']\n",
    "    \n",
    "    # Mask datasets\n",
    "    search_extent_poly = area_multiple_buffer(geometry, area_multiple)\n",
    "    x_min, y_min, x_max, y_max = search_extent_poly.bounds\n",
    "    \n",
    "    dataset1_masked, dataset2_masked = mask_datasets(dataset1, dataset2, search_extent_poly, x_min, x_max, y_min, y_max)\n",
    "    \n",
    "    return dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked)\n",
    "    \n",
    "def mask_datasets(dataset1, dataset2, search_extent_poly, x_min, x_max, y_min, y_max):\n",
    "    \"\"\"Apply masks to both datasets\"\"\"\n",
    "    dataset1_masked = None\n",
    "    if dataset1 is not None:\n",
    "        dataset1_masked = apply_mask_to_dataset(dataset1, search_extent_poly, x_min, x_max, y_min, y_max)\n",
    "    \n",
    "    dataset2_masked = apply_mask_to_dataset(dataset2, search_extent_poly, x_min, x_max, y_min, y_max)\n",
    "    return dataset1_masked, dataset2_masked\n",
    "\n",
    "def apply_mask_to_dataset(dataset, search_extent_poly, x_min, x_max, y_min, y_max):\n",
    "    \"\"\"Apply mask to a single dataset\"\"\"\n",
    "    dataset_sub = dataset.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "    mask = np.array([[search_extent_poly.contains(Point(x, y)) \n",
    "                     for x in dataset_sub['x'].values] \n",
    "                     for y in dataset_sub['y'].values])\n",
    "    mask_da = xr.DataArray(mask, coords=[dataset_sub.y, dataset_sub.x], dims=[\"y\", \"x\"])\n",
    "    return dataset.where(mask_da, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a2b739-2388-4441-9815-34df44591a67",
   "metadata": {},
   "source": [
    "## find_evolving_outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77ea53f8-1207-4dbf-980c-c063334bba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_evolving_outlines(lake_gdf, \n",
    "                           within_area_multiple, \n",
    "                           level, \n",
    "                           dataset1_masked, \n",
    "                           dataset2_masked, \n",
    "                           search_extent_poly, \n",
    "                           plot=False): \n",
    "    '''\n",
    "    Create time-variable outlines using skimage contour to generate evolving outlines from surface height anomalies and optionally plot.\n",
    "    If plot=True, create planview dh/dt plots of ice surface height changes with evolving outlines found,\n",
    "    along with a subplot showing data counts used in the gridded dh data.\n",
    "\n",
    "    Inputs:\n",
    "    * lake_gdf: GeoDataFrame containing lake information\n",
    "    * within_area_multiple: Factor used elsewhere to multiply lake area to create a polygon used to calculate the within_fraction\n",
    "    * level: vertical dh in meters to delineate ice surface dh anomaly contour\n",
    "    * dataset1_masked: masked dataset1 to be analyzed\n",
    "    * dataset2_masked: masked dataset2 to be analyzed\n",
    "    * search_extent_poly: buffered polygon that is the extent of masked data available for making outlines\n",
    "    * plot: boolean, if True, create and save plots; default is False for faster production when searching for optimal levels at search extents\n",
    "    using find_and_save_optimal_parameters func\n",
    "    \n",
    "    Outputs: \n",
    "    * geopandas geodataframe of polygons created at each step\n",
    "    * If plot=True, sequence of planview dh visuals with variable ice surface dh contours \n",
    "    plotted to delineate evolving lake boundaries, along with data count subplot.\n",
    "\n",
    "    # Example usage\n",
    "    >>> outlines_gdf = find_evolving_outlines(lake_gdf=lake_gdf, within_area_multiple=2, level=0.1, \n",
    "        dataset1_masked=dataset1_masked, dataset2_masked=dataset2_masked, plot=False)\n",
    "    '''    \n",
    "    # Define lake name and polygon\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "\n",
    "    # Get the time period for this lake\n",
    "    time_period = lake_gdf['CS2_SARIn_start'].iloc[0]\n",
    "\n",
    "    # Establish x_min, x_max, y_min, y_max\n",
    "    x_min, y_min, x_max, y_max = search_extent_poly.bounds\n",
    "    x_buffer, y_buffer = abs(x_max-x_min)*0.05, abs(y_max-y_min)*0.05\n",
    "    \n",
    "    # Create empty lists to store polygons, areas, dh's, dV's and dates\n",
    "    polys = []\n",
    "    areas = []\n",
    "    dhs = []\n",
    "    dVs = []\n",
    "    midcyc_datetimes = []\n",
    "\n",
    "    # Calculate time steps and diffs for each dataset\n",
    "    if dataset1_masked is not None:\n",
    "        # Get dh values of cycle-to-cycle change instead of relative to datum for dataset1\n",
    "        dataset1_dh = dataset1_masked['delta_h'].diff('time')\n",
    "\n",
    "        # Calculate mid-cycle datetimes\n",
    "        dataset1_midcyc_times = []\n",
    "        dataset1_datetimes = dataset1_masked['time'].values\n",
    "        for i in range(1, len(dataset1_datetimes)):\n",
    "            midcyc_days = dataset1_datetimes[i] - dataset1_datetimes[i-1]\n",
    "            midcyc_date = dataset1_datetimes[i-1] + midcyc_days/2\n",
    "            dataset1_midcyc_times.append(midcyc_date)\n",
    "        dataset1_midcyc_times = np.array(dataset1_midcyc_times)\n",
    "\n",
    "        # Write CRS after diff operation\n",
    "        dataset1_dh.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "    else:\n",
    "        dataset1_dh = None\n",
    "        dataset1_midcyc_times = np.array([])\n",
    "\n",
    "    # Get dh values of cycle-to-cycle change instead of relative to datum for dataset2\n",
    "    dataset2_dh = dataset2_masked['delta_h'].diff('time')\n",
    "\n",
    "    # Calculate mid-cycle datetimes\n",
    "    dataset2_midcyc_times = []\n",
    "    dataset2_datetimes = dataset2_masked['time'].values    \n",
    "    for i in range(1, len(dataset2_datetimes)):\n",
    "        midcyc_days = dataset2_datetimes[i] - dataset2_datetimes[i-1]\n",
    "        midcyc_date = dataset2_datetimes[i-1] + midcyc_days/2\n",
    "        dataset2_midcyc_times.append(midcyc_date)\n",
    "    dataset2_midcyc_times = np.array(dataset2_midcyc_times)\n",
    "\n",
    "    # Write CRS after diff operation\n",
    "    dataset2_dh.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "    # Only proceed with plotting if plot=True\n",
    "    if plot:\n",
    "        # Find magnitude of dh for colorbar mapping across all time slices\n",
    "        height_anom_pos = []\n",
    "        height_anom_neg = []\n",
    "        max_counts = []\n",
    "\n",
    "        # Process dataset1 if available\n",
    "        if dataset1_masked is not None:\n",
    "            for dh_slice in dataset1_dh:\n",
    "                if np.any(~np.isnan(dh_slice)):\n",
    "                    pos = np.nanmax(dh_slice)\n",
    "                    neg = np.nanmin(dh_slice)\n",
    "                    height_anom_pos.append(pos)\n",
    "                    height_anom_neg.append(neg)\n",
    "                    max_counts.append(np.nanmax(dataset1_masked['data_count']))\n",
    "\n",
    "        # Process dataset2\n",
    "        for dh_slice in dataset2_dh:\n",
    "            if np.any(~np.isnan(dh_slice)):\n",
    "                pos = np.nanmax(dh_slice)\n",
    "                neg = np.nanmin(dh_slice)\n",
    "                height_anom_pos.append(pos)\n",
    "                height_anom_neg.append(neg)\n",
    "                max_counts.append(np.nanmax(dataset2_masked['data_count']))\n",
    "\n",
    "        if height_anom_pos:  # Check if we found any valid height anomalies\n",
    "            divnorm = colors.TwoSlopeNorm(vmin=min(height_anom_neg), \n",
    "                                        vcenter=0., \n",
    "                                        vmax=max(height_anom_pos))\n",
    "            countnorm = colors.Normalize(vmin=0, vmax=max(max_counts))\n",
    "        else:\n",
    "            print(\"No valid height anomalies found for plotting\")\n",
    "            return None\n",
    "\n",
    "        # Create plotting elements\n",
    "        stationary_lakes_color = 'darkturquoise'\n",
    "        stationary_line = plt.Line2D([], [], color=stationary_lakes_color, linestyle='solid', linewidth=2)\n",
    "        uplift = plt.Line2D([], [], color='blue', linestyle=(0, (3, 5, 1, 5)), linewidth=2)\n",
    "        subsidence = plt.Line2D([], [], color='red', linestyle=(0, (3, 5, 1, 5, 1, 5)), linewidth=2)\n",
    "        within_area_multiple_line = plt.Line2D([],[], color='dimgray', linestyle='solid', linewidth=2)\n",
    "\n",
    "    def process_timestep(dh, count, mid_cyc_time, dataset_name):\n",
    "        if np.any(~np.isnan(dh)):\n",
    "            x_conv = (x_max-x_min)/dh.shape[1]\n",
    "            y_conv = (y_max-y_min)/dh.shape[0]\n",
    "    \n",
    "            # Plot if requested\n",
    "            if plot:\n",
    "                create_and_save_plots(dh, count, mid_cyc_time, x_conv, y_conv)\n",
    "    \n",
    "            # Process contours\n",
    "            if np.any(~np.isnan(count)):\n",
    "                # Find positive contours\n",
    "                contours_pos = measure.find_contours(dh.values, level)\n",
    "                process_contours(contours_pos, x_conv, y_conv, dh, mid_cyc_time, is_positive=True)\n",
    "    \n",
    "                # Find negative contours\n",
    "                contours_neg = measure.find_contours(dh.values, -level)\n",
    "                process_contours(contours_neg, x_conv, y_conv, dh, mid_cyc_time, is_positive=False)\n",
    "    \n",
    "    \n",
    "    def create_and_save_plots(dh, count, mid_cyc_time, x_conv, y_conv):\n",
    "        \"\"\"Create and save plots for the current timestep\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    \n",
    "        # Plot data counts\n",
    "        img1 = ax1.imshow(count, extent=[x_min, x_max, y_min, y_max], \n",
    "            origin='lower', cmap='Greys', norm=countnorm)\n",
    "    \n",
    "        # Plot height change\n",
    "        img2 = ax2.imshow(dh, extent=[x_min, x_max, y_min, y_max], \n",
    "            origin='lower', cmap='coolwarm_r', norm=divnorm)\n",
    "    \n",
    "        # Find and plot contours\n",
    "        contour_pos = measure.find_contours(dh.values, level)\n",
    "        contour_neg = measure.find_contours(dh.values, -level)\n",
    "    \n",
    "        # Plot positive contours\n",
    "        for contour in contour_pos:\n",
    "            if not is_closed_contour(contour):\n",
    "                x = x_min + contour[:,1]*x_conv + (0.5*x_conv)\n",
    "                y = y_min + contour[:,0]*y_conv + (0.5*y_conv)\n",
    "                # Plot unclosed contours\n",
    "                ax1.plot(x, y, color='darkblue', linestyle='dashdot', linewidth=1)\n",
    "                ax2.plot(x, y, color='darkblue', linestyle='dashdot', linewidth=1)\n",
    "            else:\n",
    "                x = x_min + contour[:,1]*x_conv + (0.5*x_conv)\n",
    "                y = y_min + contour[:,0]*y_conv + (0.5*y_conv)\n",
    "                # Plot closed contours\n",
    "                ax1.plot(x, y, color='blue', linestyle=(0, (3, 5, 1, 5)), linewidth=1)\n",
    "                ax2.plot(x, y, color='blue', linestyle=(0, (3, 5, 1, 5)), linewidth=1)\n",
    "    \n",
    "        # Plot negative contours\n",
    "        for contour in contour_neg:\n",
    "            if not is_closed_contour(contour):\n",
    "                x = x_min + contour[:,1]*x_conv + (0.5*x_conv)\n",
    "                y = y_min + contour[:,0]*y_conv + (0.5*y_conv)\n",
    "                # Plot unclosed contours\n",
    "                ax1.plot(x, y, color='darkred', linestyle=(0, (3, 1, 1, 1)), linewidth=1)\n",
    "                ax2.plot(x, y, color='darkred', linestyle=(0, (3, 1, 1, 1)), linewidth=1)\n",
    "            else:\n",
    "                x = x_min + contour[:,1]*x_conv + (0.5*x_conv)\n",
    "                y = y_min + contour[:,0]*y_conv + (0.5*y_conv)\n",
    "                # Plot closed contours\n",
    "                ax1.plot(x, y, color='red', linestyle=(0, (3, 1, 1, 1)), linewidth=1)\n",
    "                ax2.plot(x, y, color='red', linestyle=(0, (3, 1, 1, 1)), linewidth=1)\n",
    "    \n",
    "        # Plot boundaries on both subplots\n",
    "        within_evaluation_poly = area_multiple_buffer(lake_poly, within_area_multiple)\n",
    "        gpd.GeoSeries(within_evaluation_poly).boundary.plot(ax=ax1, color='dimgray')\n",
    "        gpd.GeoSeries(within_evaluation_poly).boundary.plot(ax=ax2, color='dimgray')\n",
    "    \n",
    "        # Common plotting elements\n",
    "        for ax in [ax1, ax2]:\n",
    "            stationary_outlines_gdf.boundary.plot(ax=ax, edgecolor=stationary_lakes_color, linestyle='solid', linewidth=2)          \n",
    "            km_scale = 1e3\n",
    "            ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "            ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "            ax.set_xlabel('x [km]', size=15)\n",
    "            ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "    \n",
    "        # Additional ax1 elements\n",
    "        ax1.set_ylabel('y [km]', size=15)\n",
    "        axIns = ax1.inset_axes([0.01, -0.01, 0.2, 0.2])\n",
    "        axIns.set_aspect('equal')\n",
    "        moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        axIns.axis('off')\n",
    "        axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "            linewidth=1, edgecolor='k', facecolor='r', s=100, zorder=3)\n",
    "    \n",
    "        # Colorbars\n",
    "        divider1 = make_axes_locatable(ax1)\n",
    "        cax1 = divider1.append_axes('right', size='5%', pad=0.2)\n",
    "        fig.colorbar(img1, cax=cax1).set_label('data counts', size=15)\n",
    "    \n",
    "        divider2 = make_axes_locatable(ax2)\n",
    "        cax2 = divider2.append_axes('right', size='5%', pad=0.2)\n",
    "        fig.colorbar(img2, cax=cax2).set_label('height change (dh) [m]', size=15)\n",
    "    \n",
    "        # Legends\n",
    "        for ax, title in zip([ax1, ax2], ['Data counts', 'Height change']):\n",
    "            ax.legend([stationary_line, uplift, subsidence, within_area_multiple_line],\n",
    "                     ['stationary outline',\n",
    "                      f'evolving outline (+ {level} m)',\n",
    "                      f'evolving outline ( {level} m)',\n",
    "                      f'within evaluation line ({int(within_area_multiple)}x)'],\n",
    "                     loc='upper left')\n",
    "            ax.set_title(title)\n",
    "    \n",
    "        # Save plot\n",
    "        fig.suptitle(f'Mid-cycle date: {pd.Timestamp(mid_cyc_time)}', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{OUTPUT_DIR}/find_evolving_outlines/{lake_name}/find_evolving_outlines_{lake_name}_{level}m-level_{within_area_multiple}x-within_{pd.Timestamp(mid_cyc_time).strftime('%Y-%m-%d')}.png\", \n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "            \n",
    "    def process_contours(contours, x_conv, y_conv, dh, time, is_positive=True):\n",
    "        for contour in contours:\n",
    "            if is_closed_contour(contour):\n",
    "                x = x_min + contour[:,1]*x_conv + (0.5*x_conv)\n",
    "                y = y_min + contour[:,0]*y_conv + (0.5*y_conv)\n",
    "                \n",
    "                # Create and process polygon\n",
    "                poly = Polygon(list(zip(x, y)))\n",
    "                try:\n",
    "                    dhdt_poly = dh.rio.clip([poly])\n",
    "                    lon, lat = XY_TO_LL.transform(x,y)\n",
    "                    poly_area = abs(GEOD.polygon_area_perimeter(lon, lat)[0])\n",
    "                    \n",
    "                    if np.any(~np.isnan(dhdt_poly)):\n",
    "                        poly_dh = np.nanmean(dhdt_poly)\n",
    "                        poly_dV = poly_dh*poly_area\n",
    "                        polys.append(poly)\n",
    "                        areas.append(poly_area)\n",
    "                        dhs.append(poly_dh)\n",
    "                        dVs.append(poly_dV)\n",
    "                        midcyc_datetimes.append(time)\n",
    "                except NoDataInBounds:\n",
    "                    pass\n",
    "                except Exception as e:\n",
    "                    raise\n",
    "\n",
    "    # Process dataset1 if available\n",
    "    if dataset1_masked is not None:\n",
    "        for i, (dh_slice, mid_cyc_time) in enumerate(zip(dataset1_dh, dataset1_midcyc_times)):\n",
    "            process_timestep(dh_slice, dataset1_masked['data_count'][i], mid_cyc_time, 'dataset1')\n",
    "\n",
    "    # Process dataset2\n",
    "    for i, (dh_slice, mid_cyc_time) in enumerate(zip(dataset2_dh, dataset2_midcyc_times)):\n",
    "        process_timestep(dh_slice, dataset2_masked['data_count'][i], mid_cyc_time, 'dataset2')\n",
    "\n",
    "    # Return None if no polygons were found\n",
    "    if not polys:\n",
    "        return None\n",
    "\n",
    "    # Create GeoDataFrame if we found any polygons\n",
    "    gdf = gpd.GeoDataFrame({\n",
    "        'within_area_multiple': [within_area_multiple] * len(polys),\n",
    "        'level': [level] * len(polys),\n",
    "        'geometry': polys, \n",
    "        'area (m^2)': areas, \n",
    "        'dh (m)': dhs, \n",
    "        'vol (m^3)': dVs,\n",
    "        'midcyc_datetime': midcyc_datetimes\n",
    "    }, crs=\"EPSG:3031\")\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae1f1491-6ba7-4c8f-8ec0-0174fe1a8c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_closed_contour(contour, tolerance=1.0):\n",
    "    \"\"\"\n",
    "    Check if a contour is closed by comparing its start and end points.\n",
    "    \n",
    "    Args:\n",
    "        contour: numpy array of shape (N, 2) containing the contour points\n",
    "        tolerance: maximum distance between start and end points to consider contour closed;\n",
    "        default of 1.0 means start and end points must be within one pixel\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if contour is closed, False otherwise\n",
    "    \"\"\"\n",
    "    if len(contour) < 3:\n",
    "        return False\n",
    "        \n",
    "    # Get first and last points\n",
    "    start_point = contour[0]\n",
    "    end_point = contour[-1]\n",
    "    \n",
    "    # Calculate Euclidean distance between start and end points\n",
    "    distance = np.sqrt(np.sum((start_point - end_point) ** 2))\n",
    "    \n",
    "    # Consider contour closed if start and end points are within tolerance\n",
    "    return distance < tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a56f96c6-7d4a-44d3-a9b5-3578bb4ee54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_duplicate_files(directory_path, keep_extension, delete_extension):\n",
    "    \"\"\"\n",
    "    Search a directory for files with matching names but different extensions,\n",
    "    keeping files with one extension and deleting files with another.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to the directory to search\n",
    "        keep_extension (str): File extension to keep (e.g., 'csv')\n",
    "        delete_extension (str): File extension to delete (e.g., 'txt')\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (list of deleted files, list of errors encountered)\n",
    "    \n",
    "    Example usage:\n",
    "        directory = \"path/to/your/directory\"\n",
    "        deleted, errors = cleanup_duplicate_files(directory, \"csv\", \"txt\")\n",
    "        \n",
    "        if deleted:\n",
    "            print(\"\\nDeleted files:\")\n",
    "            for file in deleted:\n",
    "                print(f\"- {file}\")\n",
    "        \n",
    "        if errors:\n",
    "            print(\"\\nErrors encountered:\")\n",
    "            for error in errors:\n",
    "                print(f\"- {error}\")\n",
    "    \"\"\"\n",
    "    # Setup logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    deleted_files = []\n",
    "    errors = []\n",
    "    \n",
    "    try:\n",
    "        # Convert directory path to Path object\n",
    "        dir_path = Path(directory_path)\n",
    "        \n",
    "        # Validate inputs\n",
    "        if not dir_path.exists():\n",
    "            raise FileNotFoundError(f\"Directory {directory_path} does not exist\")\n",
    "            \n",
    "        if not isinstance(keep_extension, str) or not isinstance(delete_extension, str):\n",
    "            raise ValueError(\"File extensions must be strings\")\n",
    "            \n",
    "        # Normalize extensions (remove dots if present)\n",
    "        keep_extension = keep_extension.lstrip('.')\n",
    "        delete_extension = delete_extension.lstrip('.')\n",
    "        \n",
    "        if keep_extension == delete_extension:\n",
    "            raise ValueError(\"Keep and delete extensions must be different\")\n",
    "        \n",
    "        # Get all files of both types\n",
    "        files_to_keep = {f.stem: f for f in dir_path.glob(f\"*.{keep_extension}\")}\n",
    "        files_to_delete = {f.stem: f for f in dir_path.glob(f\"*.{delete_extension}\")}\n",
    "        \n",
    "        # Find matching files\n",
    "        for filename_stem in files_to_delete.keys():\n",
    "            if filename_stem in files_to_keep:\n",
    "                delete_path = files_to_delete[filename_stem]\n",
    "                try:\n",
    "                    delete_path.unlink()  # Delete the file\n",
    "                    deleted_files.append(str(delete_path))\n",
    "                    logger.info(f\"Deleted: {delete_path}\")\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Error deleting {delete_path}: {str(e)}\"\n",
    "                    errors.append(error_msg)\n",
    "                    logger.error(error_msg)\n",
    "        \n",
    "        # Summary\n",
    "        logger.info(f\"Processing complete. Deleted {len(deleted_files)} files. Encountered {len(errors)} errors.\")\n",
    "        \n",
    "        return deleted_files, errors\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Fatal error: {str(e)}\"\n",
    "        errors.append(error_msg)\n",
    "        logger.error(error_msg)\n",
    "        return deleted_files, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d9081-e42b-4ee5-90c2-fa0f5bc63ca2",
   "metadata": {},
   "source": [
    "## visualize_and_save_evolving_outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57775bed-cddb-4cdc-bd48-d2df79fead0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_save_evolving_outlines(lake_gdf, row_index=0):\n",
    "    '''\n",
    "    Visualize the evolving outlines for each stationary lake using optimal level and within_area_multiple combination.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    stationary_outlines_gdf : GeoDataFrame\n",
    "        The dataset of stationary lakes\n",
    "    row_index : int, optional (default=0)\n",
    "        Index of the row to use from the sorted levels_df dataframe.\n",
    "        0 gives the smallest level and corresponding within_area_multiple,\n",
    "        -1 gives the largest level and corresponding within_area_multiple found using the find_and_save_optimal_parameters function.\n",
    "    '''\n",
    "    # Store lake name and polygon\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "    print('Visualizing outlines for', lake_name)\n",
    "\n",
    "    try:\n",
    "        # Load levels dataframe\n",
    "        csv_path = OUTPUT_DIR + '/levels/{}.csv'.format(lake_name)\n",
    "        txt_path = OUTPUT_DIR + '/levels/{}.txt'.format(lake_name)\n",
    "        if os.path.exists(csv_path):\n",
    "            levels_df = pd.read_csv(csv_path)\n",
    "        elif os.path.exists(txt_path):\n",
    "            output_path = os.path.join(OUTPUT_DIR_GIT, f\"lake_outlines/evolving_outlines/{lake_name}.txt\")\n",
    "            print(f'Found no levels CSV file but found \"no outlines\" TXT file for {lake_name}. So writing \"no outlines\" TXT file to {output_path}.')\n",
    "            write_no_outlines(output_path)\n",
    "            return\n",
    "        else:\n",
    "            print(f'No levels file found for {lake_name}. Skipping lake.')\n",
    "            return\n",
    "        \n",
    "        # Select row based on provided index\n",
    "        if abs(row_index) >= len(levels_df):\n",
    "            print(f\"Warning: row_index {row_index} out of bounds for {lake_name}. Skipping.\")\n",
    "            return\n",
    "        else:\n",
    "            selected_row = levels_df.iloc[row_index]\n",
    "\n",
    "        # Print parameters\n",
    "        print(f\"Parameters: row_index={row_index}, within_area_multiple={selected_row['within_area_multiple']}, level={selected_row['level']}, doi(s)={selected_row['dataset_dois']}\")\n",
    "\n",
    "        # Prepare data sets\n",
    "        dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(lake_gdf, 25)\n",
    "\n",
    "        # Create output folders\n",
    "        os.makedirs(OUTPUT_DIR + f'/find_evolving_outlines/{lake_name}', exist_ok=True)\n",
    "        os.makedirs('output/lake_outlines/evolving_outlines', exist_ok=True)\n",
    "        os.makedirs(OUTPUT_DIR + '/plot_evolving_outlines_time_series', exist_ok=True)\n",
    "        os.makedirs(OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines', exist_ok=True)\n",
    "\n",
    "        # Find evolving outlines\n",
    "        evolving_outlines_gdf = find_evolving_outlines(\n",
    "            lake_gdf=lake_gdf, \n",
    "            within_area_multiple=selected_row['within_area_multiple'], \n",
    "            level=selected_row['level'], \n",
    "            dataset1_masked=dataset1_masked,\n",
    "            dataset2_masked=dataset2_masked,\n",
    "            search_extent_poly=search_extent_poly,\n",
    "            plot=True\n",
    "        )\n",
    "       \n",
    "        if evolving_outlines_gdf is None or evolving_outlines_gdf.empty:\n",
    "            print('No evolving outlines found.')\n",
    "            \n",
    "        try:\n",
    "            onlake_outlines, offlake_outlines = extract_intersecting_polygons_recursive(\n",
    "                evolving_outlines_gdf, \n",
    "                lake_gdf['geometry'].iloc[0]\n",
    "            )\n",
    "            \n",
    "            if onlake_outlines is None or onlake_outlines.empty:\n",
    "                print('No valid filtered outlines found this within_area_multiple and level. Deleting levels CSV and writing \"no outlines\" TXT file.')\n",
    "\n",
    "                # Delete levels CSV file and write \"no outlines\" TXT file\n",
    "                os.remove(OUTPUT_DIR + f'/levels/{lake_name}.csv')\n",
    "                write_no_outlines(OUTPUT_DIR + f'/levels/{lake_name}.txt')\n",
    "                write_no_outlines(OUTPUT_DIR_GIT + f'/lake_outlines/evolving_outlines/{lake_name}.txt')\n",
    "                \n",
    "                # Clean up generated images\n",
    "                img_extension = 'png'\n",
    "                images_folder = os.path.join(OUTPUT_DIR, f\"find_evolving_outlines/{lake_name}\")\n",
    "                image_files = sorted(glob.glob(os.path.join(images_folder, f\"*.{img_extension}\")))\n",
    "                for image_file in image_files:\n",
    "                    os.remove(image_file)\n",
    "                try:\n",
    "                    shutil.rmtree(images_folder)\n",
    "                    print(f\"Cleaned up folder: {images_folder}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error cleaning up folder for {lake_name}: {str(e)}\")\n",
    "                    \n",
    "                return\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting polygons for {lake_name}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            \n",
    "        # Save results and plot\n",
    "        if not onlake_outlines.empty:\n",
    "            try:\n",
    "                # Add metadata to onlake_outlines\n",
    "                onlake_outlines = onlake_outlines.copy()\n",
    "                onlake_outlines.loc[:, 'within_area_multiple'] = selected_row['within_area_multiple']\n",
    "                onlake_outlines.loc[:, 'level'] = selected_row['level']\n",
    "                onlake_outlines.loc[:, 'row_index'] = row_index\n",
    "                # onlake_outlines.loc[:, 'within_percent'] = selected_row['within_percent']\n",
    "\n",
    "                # offlake_outlines = offlake_outlines.copy()\n",
    "                offlake_outlines.loc[:, 'within_area_multiple'] = selected_row['within_area_multiple']\n",
    "                offlake_outlines.loc[:, 'level'] = selected_row['level']\n",
    "                offlake_outlines.loc[:, 'row_index'] = row_index\n",
    "                # offlake_outlines.loc[:, 'within_percent'] = selected_row['within_percent']\n",
    "                \n",
    "                # Export evolving outlines GeoDataFrame to GeoJSON\n",
    "                filepath = os.path.join(OUTPUT_DIR_GIT, f\"lake_outlines/evolving_outlines/{lake_name}.geojson\")\n",
    "                onlake_outlines.to_file(filename=filepath, driver='GeoJSON')\n",
    "                filepath = OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines/{}.geojson'.format(lake_name)\n",
    "                offlake_outlines.to_file(filename=filepath, driver='GeoJSON')\n",
    "                print(f\"Saved outlines to: {filepath}\")\n",
    "                \n",
    "                # Convert images to video\n",
    "                try:\n",
    "                    video_from_images(lake_gdf, output_dir=OUTPUT_DIR, \n",
    "                                      row_index=row_index, fps=0.25, img_extension='png')\n",
    "        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error creating video for {lake_name}: {str(e)}\")\n",
    "                    traceback.print_exc()\n",
    "                    \n",
    "                # Plot the outlines\n",
    "                try:\n",
    "                    plot_evolving_outlines_time_series(\n",
    "                        lake_gdf=lake_gdf,\n",
    "                        evolving_outlines_gdf=onlake_outlines,\n",
    "                        offlake_outlines_gdf=offlake_outlines\n",
    "                    )\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error creating evolving outlines time series plot for {lake_name}: {str(e)}\")\n",
    "                    traceback.print_exc()\n",
    "\n",
    "                # Clear output\n",
    "                clear_output(wait=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving results for {lake_name}: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "        else:\n",
    "            print(f\"No filtered outlines to save for {lake_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {lake_name}: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        cleanup_vars = [\n",
    "            'dataset1_masked', 'dataset2_masked', 'evolving_outlines_gdf', \n",
    "            'onlake_outlines', 'offlake_outlines', 'levels_df', 'selected_row',\n",
    "            'search_extent_poly'\n",
    "        ]\n",
    "        for var in cleanup_vars:\n",
    "            if var in locals():\n",
    "                del locals()[var]\n",
    "        gc.collect()\n",
    "\n",
    "def video_from_images(lake_gdf, output_dir=OUTPUT_DIR, row_index=0, fps=1, img_extension='png', max_retries=3):\n",
    "    \"\"\"\n",
    "    Creates a video from still images with additional validation and retry mechanism.\n",
    "    \n",
    "    Parameters:\n",
    "    - lake_gdf: GeoDataFrame containing lake information (expects a 'name' column)\n",
    "    - output_dir: Base directory where the images and video are stored/created\n",
    "    - row_index: row of optimal parameters used to generate evolving outlines\n",
    "    - fps: Frames per second for the output video\n",
    "    - img_extension: Extension of the images to look for in the folder\n",
    "    - max_retries: Maximum number of attempts to create the video\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if video creation was successful, False otherwise\n",
    "    \"\"\"\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    print('Making video for', lake_name)\n",
    "    \n",
    "    def validate_images(image_files):\n",
    "        \"\"\"Validate that all images are readable and have consistent dimensions\"\"\"\n",
    "        if not image_files:\n",
    "            return False, None\n",
    "            \n",
    "        reference_frame = cv2.imread(image_files[0])\n",
    "        if reference_frame is None:\n",
    "            return False, None\n",
    "            \n",
    "        height, width = reference_frame.shape[:2]\n",
    "        \n",
    "        for img_file in image_files:\n",
    "            frame = cv2.imread(img_file)\n",
    "            if frame is None:\n",
    "                print(f\"Could not read image: {img_file}\")\n",
    "                return False, None\n",
    "            if frame.shape[:2] != (height, width):\n",
    "                print(f\"Inconsistent dimensions in {img_file}\")\n",
    "                return False, None\n",
    "                \n",
    "        return True, (width, height)\n",
    "\n",
    "    def attempt_video_creation(attempt=1):\n",
    "        try:\n",
    "            # Load levels dataframe\n",
    "            levels_df = pd.read_csv(os.path.join(OUTPUT_DIR, 'levels', f'{lake_name}.csv'))\n",
    "            if levels_df.empty:\n",
    "                print('levels_df empty.')\n",
    "                return False\n",
    "\n",
    "            # Select row based on provided index\n",
    "            if abs(row_index) >= len(levels_df):\n",
    "                print(f\"Warning: row_index {row_index} out of bounds for {lake_name}. Using first row.\")\n",
    "                selected_row = levels_df.iloc[0]\n",
    "            else:\n",
    "                selected_row = levels_df.iloc[row_index]\n",
    "\n",
    "            # Derive paths\n",
    "            images_folder = os.path.join(OUTPUT_DIR, f\"find_evolving_outlines/{lake_name}\")\n",
    "            output_video_file = os.path.join(\n",
    "                output_dir,\n",
    "                f\"find_evolving_outlines/{lake_name}_{row_index}-idx_{selected_row['level']}m-level_{selected_row['within_area_multiple']}x-within.mp4\"\n",
    "            )\n",
    "\n",
    "            # Get and sort images\n",
    "            image_files = sorted(glob.glob(os.path.join(images_folder, f\"*.{img_extension}\")))\n",
    "            \n",
    "            # Validate images\n",
    "            print(f\"Validating {len(image_files)} images...\")\n",
    "            images_valid, dimensions = validate_images(image_files)\n",
    "            if not images_valid:\n",
    "                print(f\"Image validation failed on attempt {attempt}\")\n",
    "                return False\n",
    "\n",
    "            # Try different codecs if needed\n",
    "            codecs = ['mp4v', 'avc1', 'H264']\n",
    "            success = False\n",
    "            \n",
    "            for codec in codecs:\n",
    "                try:\n",
    "                    fourcc = cv2.VideoWriter_fourcc(*codec)\n",
    "                    video = cv2.VideoWriter(output_video_file, fourcc, fps, dimensions)\n",
    "                    \n",
    "                    if not video.isOpened():\n",
    "                        print(f\"Failed to open video writer with codec {codec}\")\n",
    "                        continue\n",
    "                        \n",
    "                    # Write frames\n",
    "                    for image_file in image_files:\n",
    "                        frame = cv2.imread(image_file)\n",
    "                        if frame is not None:\n",
    "                            video.write(frame)\n",
    "                    \n",
    "                    video.release()\n",
    "                    \n",
    "                    # Verify the video was created and is not empty\n",
    "                    if os.path.exists(output_video_file) and os.path.getsize(output_video_file) > 0:\n",
    "                        success = True\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"Video file empty or not created with codec {codec}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error with codec {codec}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            if success:\n",
    "                print(f\"Video created successfully on attempt {attempt}\")\n",
    "                # Clean up images only after successful video creation\n",
    "                for image_file in image_files:\n",
    "                    os.remove(image_file)\n",
    "                try:\n",
    "                    shutil.rmtree(images_folder)\n",
    "                    print(f\"Cleaned up folder: {images_folder}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not delete folder {images_folder}: {e}\")\n",
    "                return True\n",
    "            \n",
    "            return False\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error on attempt {attempt}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    # Main retry loop\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        print(f\"\\nAttempt {attempt} of {max_retries}\")\n",
    "        if attempt > 1:\n",
    "            print(\"Waiting 2 seconds before retry...\")\n",
    "            time.sleep(2)  # Add delay between attempts\n",
    "            \n",
    "        if attempt_video_creation(attempt):\n",
    "            return True\n",
    "            \n",
    "    print(f\"Failed to create video after {max_retries} attempts\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "008328b0-3f7c-4fe0-baa5-a6e787214f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_intersecting_polygons_recursive(gdf, reference_geometry):\n",
    "    \"\"\"\n",
    "    Extract and separate intersecting and non-intersecting polygons with topology validation and cleaning.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        Input GeoDataFrame containing polygons to analyze\n",
    "    reference_geometry : Geometry\n",
    "        Reference geometry to check for intersections\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple (GeoDataFrame, GeoDataFrame)\n",
    "        First GeoDataFrame contains intersecting polygons\n",
    "        Second GeoDataFrame contains non-intersecting polygons\n",
    "    \"\"\"\n",
    "    import shapely.geometry\n",
    "    from shapely.validation import make_valid\n",
    "    import geopandas as gpd\n",
    "    import pandas as pd\n",
    "    \n",
    "    empty_gdf = gpd.GeoDataFrame(geometry=[], crs=gdf.crs if gdf is not None else None)\n",
    "    \n",
    "    if gdf is None or gdf.empty:\n",
    "        return empty_gdf, empty_gdf\n",
    "        \n",
    "    try:\n",
    "        # Create a copy and clean geometries\n",
    "        gdf_copy = gdf.copy()\n",
    "        \n",
    "        # Clean reference geometry\n",
    "        if not reference_geometry.is_valid:\n",
    "            print(\"Cleaning reference geometry...\")\n",
    "            reference_geometry = make_valid(reference_geometry)\n",
    "            \n",
    "        # Clean all geometries in the GeoDataFrame\n",
    "        gdf_copy['geometry'] = gdf_copy['geometry'].apply(lambda geom: make_valid(geom) if not geom.is_valid else geom)\n",
    "        \n",
    "        # Find directly intersecting polygons\n",
    "        try:\n",
    "            directly_intersecting = gdf_copy.loc[gdf_copy.geometry.intersects(reference_geometry)].copy()\n",
    "        except Exception as e:\n",
    "            print(f\"Error in direct intersection: {str(e)}\")\n",
    "            return empty_gdf, empty_gdf\n",
    "            \n",
    "        if directly_intersecting.empty:\n",
    "            # If no direct intersections, return empty intersecting and full non-intersecting\n",
    "            return empty_gdf, gdf_copy\n",
    "            \n",
    "        # Initialize already_found with directly intersecting polygons\n",
    "        already_found = directly_intersecting.copy()\n",
    "        already_found_indices = set(already_found.index)\n",
    "        \n",
    "        # Recursive intersection\n",
    "        try:\n",
    "            while True:\n",
    "                # Create union with buffer to handle small topology issues\n",
    "                union_geom = already_found.geometry.union_all()\n",
    "                if not union_geom.is_valid:\n",
    "                    print(\"Cleaning union geometry...\")\n",
    "                    union_geom = make_valid(union_geom)\n",
    "                \n",
    "                # Find new intersecting polygons\n",
    "                new_intersecting = gdf_copy.loc[\n",
    "                    ~gdf_copy.index.isin(already_found_indices) & \n",
    "                    gdf_copy.geometry.intersects(union_geom)\n",
    "                ].copy()\n",
    "                \n",
    "                if new_intersecting.empty:\n",
    "                    break\n",
    "                    \n",
    "                # Add new indices to our set\n",
    "                already_found_indices.update(new_intersecting.index)\n",
    "                \n",
    "                # Combine results\n",
    "                already_found = gpd.GeoDataFrame(\n",
    "                    pd.concat([already_found, new_intersecting], ignore_index=False)\n",
    "                ).copy()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error in recursive intersection: {str(e)}\")\n",
    "            print(\"Returning directly intersecting polygons only\")\n",
    "            non_intersecting = gdf_copy[~gdf_copy.index.isin(directly_intersecting.index)].copy()\n",
    "            return directly_intersecting, non_intersecting\n",
    "        \n",
    "        # Get non-intersecting polygons using the set of indices we've collected\n",
    "        non_intersecting = gdf_copy[~gdf_copy.index.isin(already_found_indices)].copy()\n",
    "        return already_found, non_intersecting\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_intersecting_polygons_recursive: {str(e)}\")\n",
    "        return empty_gdf, empty_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add471d5-79dc-4caf-921d-3151164afa97",
   "metadata": {},
   "source": [
    "## plot_evolving_outlines_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9e96a18-e31b-4901-9617-09c2f60b4ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evolving_outlines_time_series(lake_gdf, evolving_outlines_gdf, offlake_outlines_gdf):\n",
    "    '''\n",
    "    Plot evolving outlines time series (on- and off-lake outlines) overlain on ice-surface imagery background.\n",
    "    '''\n",
    "    try:\n",
    "        # Define lake name and polygon\n",
    "        lake_name = lake_gdf['name'].iloc[0]\n",
    "        print(f\"Creating evolving outlines time series plot for lake: {lake_name}\")\n",
    "        \n",
    "        # Validate inputs\n",
    "        if evolving_outlines_gdf is None or evolving_outlines_gdf.empty:\n",
    "            print(f\"No evolving outlines provided for {lake_name}\")\n",
    "            return\n",
    "        \n",
    "        # Get parameters using iloc\n",
    "        within_area_multiple = evolving_outlines_gdf['within_area_multiple'].iloc[0]\n",
    "        level = evolving_outlines_gdf['level'].iloc[0]\n",
    "        row_index = evolving_outlines_gdf['row_index'].iloc[0]\n",
    "\n",
    "        print(f\"Parameters: row_index={row_index}, within_area_multiple={within_area_multiple}, level={level}\")\n",
    "        \n",
    "        stationary_outline = lake_gdf['geometry']\n",
    "        if stationary_outline is None:\n",
    "            print(f\"Error: No geometry found for {lake_name}\")\n",
    "            return\n",
    "\n",
    "        # Create search extent and within evaluation polygons\n",
    "        search_extent_poly = area_multiple_buffer(\n",
    "            stationary_outline, 25)\n",
    "        within_evaluation_poly = area_multiple_buffer(\n",
    "            stationary_outline, within_area_multiple)\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(5,5))\n",
    "        \n",
    "        # Plot search extent and within evaluation polygons\n",
    "        gpd.GeoDataFrame(geometry=[search_extent_poly]).boundary.plot(\n",
    "            ax=ax, edgecolor='k', facecolor='none', linewidth=1)\n",
    "        gpd.GeoDataFrame(geometry=[within_evaluation_poly]).boundary.plot(\n",
    "            ax=ax, edgecolor='dimgray', facecolor='none', linewidth=1)\n",
    "\n",
    "        # # Set up colormap\n",
    "        # min_date = pd.to_datetime(cyc_start_dates[1])\n",
    "        # max_date = pd.to_datetime(cyc_end_dates[-1])\n",
    "        # date_range = pd.date_range(min_date, max_date, periods=len(mid_cyc_dates[1:]))\n",
    "        # years = pd.date_range(min_date, max_date, freq='YE')\n",
    "\n",
    "        # # Get number of dates\n",
    "        # n_dates = len(mid_cyc_dates[1:])\n",
    "        # cmap = plt.get_cmap('plasma', n_dates)\n",
    "        # norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "        # m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        # m.set_array(np.linspace(mdates.date2num(min_date), mdates.date2num(max_date), n_dates))\n",
    "        \n",
    "        # # Add colorbar\n",
    "        # divider = make_axes_locatable(ax)\n",
    "        # cax = divider.append_axes('bottom', size='2.5%', pad=0.5)\n",
    "        # cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "\n",
    "        # # Set ticks for all years but labels only for odd years\n",
    "        # tick_locations = [mdates.date2num(date) for date in years]\n",
    "        # tick_labels = [date.strftime('%Y') if date.year % 2 == 1 else '' for date in years]\n",
    "        # cbar.set_ticks(tick_locations)\n",
    "        # cbar.set_ticklabels(tick_labels)\n",
    "\n",
    "        # # Add minor ticks for quarters\n",
    "        # cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Quarter intervals only\n",
    "        # cbar.set_label('year', size=10, labelpad=10)\n",
    "\n",
    "        # Set up colormap\n",
    "        min_date = pd.to_datetime(cyc_start_dates[1])\n",
    "        max_date = pd.to_datetime(cyc_end_dates[-1])\n",
    "        date_range = pd.date_range(min_date, max_date, periods=len(mid_cyc_dates[1:]))\n",
    "        years = date_range.year.unique()\n",
    "        years = pd.to_datetime(years, format='%Y')\n",
    "        n_dates = len(mid_cyc_dates[1:])\n",
    "        cmap = plt.get_cmap('plasma', n_dates)\n",
    "        norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "        m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        m.set_array(np.linspace(mdates.date2num(min_date), mdates.date2num(max_date), n_dates))\n",
    "        \n",
    "        # Add colorbar\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('bottom', size='2.5%', pad=0.5)\n",
    "        cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "    \n",
    "        # Set ticks for all years but labels only for odd years\n",
    "        tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "        tick_labels = [date.strftime('%Y') if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "        cbar.set_ticks(tick_locations)\n",
    "        cbar.set_ticklabels(tick_labels)\n",
    "        \n",
    "        # Add minor ticks for quarters\n",
    "        cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Quarter intervals only\n",
    "        cbar.set_label('year', size=10, labelpad=10)\n",
    "\n",
    "        # Set plot bounds\n",
    "        x_min, y_min, x_max, y_max = search_extent_poly.bounds\n",
    "        x_buffer = abs(x_max-x_min)*0.05\n",
    "        y_buffer = abs(y_max-y_min)*0.05\n",
    "        ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "        # Plot MOA surface imagery\n",
    "        mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "        mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "        moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "        ax.imshow(moa_highres_da_subset[0,:,:], cmap='gray', clim=[14000, 17000], \n",
    "                  extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "\n",
    "        # Plot stationary lakes\n",
    "        stationary_lake_color = 'darkturquoise'\n",
    "        stationary_outlines_gdf.boundary.plot(\n",
    "            ax=ax, facecolor=stationary_lake_color, linestyle='solid', linewidth=2, alpha=0.25)\n",
    "        stationary_outlines_gdf.boundary.plot(\n",
    "            ax=ax, edgecolor=stationary_lake_color, linestyle='solid', linewidth=2)\n",
    "\n",
    "        # Plot evolving outlines\n",
    "        onlake_lines, offlake_lines = [], []\n",
    "        for idx, dt in enumerate(mid_cyc_dates[1:]):\n",
    "            x, y = 1, 1\n",
    "            date_num = mdates.date2num(pd.to_datetime(dt))\n",
    "            onlake_line, = ax.plot(x, y, color=cmap(norm(date_num)), linewidth=2)\n",
    "            onlake_lines.append(onlake_line)\n",
    "            offlake_line, = ax.plot(x, y, color=cmap(norm(date_num)), linewidth=2, alpha=0.2)\n",
    "            offlake_lines.append(offlake_line)\n",
    "            \n",
    "            evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "            offlake_outlines_dt = offlake_outlines_gdf[offlake_outlines_gdf['midcyc_datetime'] == dt]\n",
    "            if not evolving_outlines_dt.empty:\n",
    "                evolving_outlines_dt.boundary.plot(ax=ax, \n",
    "                    color=cmap(norm(date_num)), \n",
    "                    linewidth=0.5)\n",
    "            if not offlake_outlines_dt.empty:\n",
    "                offlake_outlines_dt.boundary.plot(ax=ax, \n",
    "                    color=cmap(norm(date_num)), \n",
    "                    linewidth=0.5, alpha=0.25)\n",
    "\n",
    "        # Format axes\n",
    "        km_scale = 1e3\n",
    "        ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "        ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_xlabel('x [km]', size=10)\n",
    "        ax.set_ylabel('y [km]', size=10)\n",
    "        \n",
    "        # Add legend\n",
    "        stationary_line = plt.Line2D([],[], color=stationary_lake_color, linestyle='solid', linewidth=2)\n",
    "        within_area_multiple_line = plt.Line2D([],[], color='dimgray', linestyle='solid', linewidth=2)\n",
    "        search_extent_line = plt.Line2D([],[], color='black', linestyle='solid', linewidth=2)\n",
    "\n",
    "        ax.legend(\n",
    "            handles=[stationary_line, \n",
    "                     tuple(onlake_lines), \n",
    "                     tuple(offlake_lines),\n",
    "                     within_area_multiple_line, \n",
    "                     search_extent_line],\n",
    "            labels=['stationary outline', \n",
    "                    f'evolving outlines ({level} m)', \n",
    "                    'off-lake evolving outlines', \n",
    "                    f'within evaluation boundary ({int(within_area_multiple)}x)',\n",
    "                    'search extent'],\n",
    "            handlelength=3,\n",
    "            handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "            loc='upper left',\n",
    "            bbox_to_anchor=(0, 1.3))\n",
    "\n",
    "        # Add inset map\n",
    "        axIns = ax.inset_axes([0.02, 0.01, 0.25, 0.25])\n",
    "        axIns.set_aspect('equal')\n",
    "        moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        axIns.scatter(\n",
    "            ((x_max+x_min)/2), ((y_max+y_min)/2),\n",
    "            marker='*', linewidth=0.1, color='k', s=30, zorder=3\n",
    "        )\n",
    "        axIns.axis('off')\n",
    "\n",
    "        # Add title\n",
    "        ax.set_title(f'{lake_name}', size=12, y=1.3)\n",
    "\n",
    "        # Generate output filename and save\n",
    "        output_filename = os.path.join(OUTPUT_DIR, 'plot_evolving_outlines_time_series',\n",
    "            f'{lake_name}_{int(row_index)}-idx_{level}m-level_{int(within_area_multiple)}x-within.png'\n",
    "        )\n",
    "        \n",
    "        plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Successfully saved plot to: {output_filename}\")\n",
    "        \n",
    "        plt.close()\n",
    "\n",
    "        # Clear intermediate objects to conserve memory\n",
    "        try:\n",
    "            plt.close('all')\n",
    "            del moa_highres_da_subset\n",
    "            del onlake_lines, offlake_lines\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f\"Error cleaning up plot resources: {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in plot_evolving_outlines_time_series function for {lake_name}:\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        print(\"Error traceback:\")\n",
    "        traceback.print_exc()\n",
    "        plt.close('all')\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008bba3e-5361-40dc-8e05-5b1a863ebae9",
   "metadata": {},
   "source": [
    "## OUTDATED METHOD: finalize_evolving_outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "946880e3-7bc6-4bf7-b7fe-f4ce6c10dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def finalize_evolving_outlines(lake_gdf, row_index=0):\n",
    "#     '''\n",
    "#     Finalize the evolving outlines for each stationary lake using optimal search extent and level.\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     stationary_outlines_gdf : GeoDataFrame\n",
    "#         The dataset of stationary lakes\n",
    "#     row_index : int, optional (default=0)\n",
    "#         Index of the row to use from the sorted levels_df dataframe.\n",
    "#         0 gives the smallest level and its within_area_multiple,\n",
    "#         -1 gives the largest level and its within_area_multiple found using the find_and_save_optimal_parameters function.\n",
    "#     '''\n",
    "#     # Select lake\n",
    "#     lake_name = lake_gdf['name'].iloc[0]\n",
    "#     print('Finalizing outlines for', lake_name)\n",
    "\n",
    "#     try:\n",
    "#         # Load levels dataframe\n",
    "#         levels_df = pd.read_csv(OUTPUT_DIR + '/levels/{}.csv'.format(lake_name))\n",
    "#         if levels_df.empty:\n",
    "#             print('levels_df empty. Skipping lake.')\n",
    "        \n",
    "#         # Select row based on provided index\n",
    "#         if abs(row_index) >= len(levels_df):\n",
    "#             print(f\"Warning: row_index {row_index} out of bounds for {lake_name}. Skipping.\")\n",
    "#         else:\n",
    "#             selected_row = levels_df.iloc[row_index]\n",
    "#         print(f\"Using parameters from row {row_index}:\")\n",
    "#         print(selected_row)\n",
    "    \n",
    "#         # Prepare data sets\n",
    "#         dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(lake_gdf, 25)\n",
    "\n",
    "#         # Create output folders\n",
    "#         os.makedirs(OUTPUT_DIR + '/find_evolving_outlines', exist_ok=True)\n",
    "#         os.makedirs(OUTPUT_DIR + f'/find_evolving_outlines/{lake_name}', exist_ok=True)\n",
    "#         os.makedirs(OUTPUT_DIR + '/plot_evolving_outlines', exist_ok=True)\n",
    "#         os.makedirs(OUTPUT_DIR + '/lake_outlines/evolving_outlines', exist_ok=True)\n",
    "#         os.makedirs('output/lake_outlines/evolving_outlines', exist_ok=True)\n",
    "\n",
    "#         # Find evolving outlines\n",
    "#         evolving_outlines_gdf = find_evolving_outlines(\n",
    "#             lake_gdf=lake_gdf, \n",
    "#             within_area_multiple=selected_row['within_area_multiple'], \n",
    "#             level=selected_row['level'], \n",
    "#             dataset1_masked=dataset1_masked,\n",
    "#             dataset2_masked=dataset2_masked,\n",
    "#             search_extent_poly=search_extent_poly,\n",
    "#             plot=True\n",
    "#         )\n",
    "       \n",
    "#         if evolving_outlines_gdf is None or evolving_outlines_gdf.empty:\n",
    "#             print('No evolving outlines found. Skipping lake.')\n",
    "        \n",
    "#         # Clean geometries before filtering\n",
    "#         try:\n",
    "#             print(f\"Cleaning geometries for {lake_name}...\")\n",
    "#             # Clean evolving outlines geometries\n",
    "#             evolving_outlines_gdf = evolving_outlines_gdf.copy()\n",
    "#             evolving_outlines_gdf.loc[:, 'geometry'] = evolving_outlines_gdf['geometry'].apply(\n",
    "#                 lambda geom: make_valid(geom) if not geom.is_valid else geom\n",
    "#             )\n",
    "            \n",
    "#             # Clean reference geometry\n",
    "#             reference_geometry = make_valid(lake_gdf['geometry']) if not lake_gdf['geometry'].is_valid else lake_gdf['geometry']\n",
    "            \n",
    "#             # Extract intersecting polygons\n",
    "#             onlake_outlines, offlake_outlines = extract_intersecting_polygons_recursive(\n",
    "#                 evolving_outlines_gdf, \n",
    "#                 reference_geometry\n",
    "#             )\n",
    "            \n",
    "#             if onlake_outlines is None or onlake_outlines.empty:\n",
    "#                 print(\"No valid filtered outlines found this area_multiple and level\")\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error cleaning geometries for {lake_name}: {str(e)}\")\n",
    "#             print(\"Attempting to continue with original geometries...\")\n",
    "#             traceback.print_exc()\n",
    "            \n",
    "#             try:\n",
    "#                 onlake_outlines, offlake_outlines = extract_intersecting_polygons_recursive(\n",
    "#                     evolving_outlines_gdf, \n",
    "#                     lake_gdf['geometry']\n",
    "#                 )\n",
    "                \n",
    "#                 if onlake_outlines is None or onlake_outlines.empty:\n",
    "#                     print(\"No valid filtered outlines found this area_multiple and level\")\n",
    "                    \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error extracting polygons for {lake_name}: {str(e)}\")\n",
    "#                 traceback.print_exc()\n",
    "            \n",
    "#         # Save results and plot\n",
    "#         if not onlake_outlines.empty:\n",
    "#             try:\n",
    "#                 # Add metadata to onlake_outlines\n",
    "#                 onlake_outlines = onlake_outlines.copy()\n",
    "#                 onlake_outlines.loc[:, 'within_area_multiple'] = selected_row['within_area_multiple']\n",
    "#                 onlake_outlines.loc[:, 'level'] = selected_row['level']\n",
    "#                 # onlake_outlines.loc[:, 'within_percent'] = selected_row['within_percent'] if 'within_percent' in selected_row else 100.0\n",
    "\n",
    "#                 offlake_outlines = offlake_outlines.copy()\n",
    "#                 offlake_outlines.loc[:, 'within_area_multiple'] = selected_row['within_area_multiple']\n",
    "#                 offlake_outlines.loc[:, 'level'] = selected_row['level']\n",
    "                \n",
    "#                 # Export evolving outlines GeoDataFrame to GeoJSON\n",
    "#                 filepath = 'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)\n",
    "#                 onlake_outlines.to_file(filename=filepath, driver='GeoJSON')\n",
    "#                 filepath = OUTPUT_DIR + '/lake_outlines/evolving_outlines/{}_discarded.geojson'.format(lake_name)\n",
    "#                 offlake_outlines.to_file(filename=filepath, driver='GeoJSON')\n",
    "#                 print(f\"Saved outlines to: {filepath}\")\n",
    "                \n",
    "#                 # Plot the outlines\n",
    "#                 try:\n",
    "#                     plot_evolving_outlines(\n",
    "#                         lake_gdf=lake_gdf,\n",
    "#                         evolving_outlines_gdf=onlake_outlines,\n",
    "#                         offlake_outlines_gdf=offlake_outlines\n",
    "#                     )\n",
    "                \n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error plotting outlines for {lake_name}: {str(e)}\")\n",
    "#                     traceback.print_exc()\n",
    "\n",
    "#                 # Convert images to video\n",
    "#                 video_from_images(lake_gdf, output_dir=OUTPUT_DIR, row_index=row_index, fps=0.5, img_extension='png')\n",
    "        \n",
    "#                 # Clear output\n",
    "#                 clear_output(wait=True)\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error saving results for {lake_name}: {str(e)}\")\n",
    "#                 traceback.print_exc()\n",
    "#                 # write_no_outlines(lake_name)\n",
    "#         else:\n",
    "#             print(f\"No filtered outlines to save for {lake_name}\")\n",
    "#             # write_no_outlines(lake_name)\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {lake_name}: {str(e)}\")\n",
    "#         traceback.print_exc()\n",
    "    \n",
    "#     finally:\n",
    "#         # Clean up memory\n",
    "#         for var in ['dataset1_masked', 'dataset2_masked', 'evolving_outlines_gdf', 'onlake_outlines']:\n",
    "#             if var in locals():\n",
    "#                 del locals()[var]\n",
    "#         gc.collect()\n",
    "\n",
    "# def video_from_images(lake_gdf, output_dir=OUTPUT_DIR, row_index=0, fps=0.5, img_extension='png'):\n",
    "#     \"\"\"\n",
    "#     Creates a video from still images stored in a folder based on the lake_gdf input, then deletes the images.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - lake_gdf: GeoDataFrame containing lake information (expects a 'name' column).\n",
    "#     - output_dir: Base directory where the images and video are stored/created.\n",
    "#     - fps: Frames per second for the output video.\n",
    "#     - img_extension: Extension of the images to look for in the folder.\n",
    "\n",
    "#     # Example usage\n",
    "#     video_from_images(lake_gdf, OUTPUT_DIR, fps=0.5, img_extension='png')\n",
    "#     \"\"\"\n",
    "#     lake_name = lake_gdf['name'].iloc[0]\n",
    "#     print('Making video for', lake_name)\n",
    "    \n",
    "#     # Load levels dataframe\n",
    "#     levels_df = pd.read_csv(OUTPUT_DIR + '/levels/{}.csv'.format(lake_name))\n",
    "#     if levels_df.empty:\n",
    "#         print('levels_df empty.')\n",
    "    \n",
    "#     # Select row based on provided index\n",
    "#     if abs(row_index) >= len(levels_df):\n",
    "#         print(f\"Warning: row_index {row_index} out of bounds for {lake_name}. Using first row.\")\n",
    "#         selected_row = levels_df.iloc[0]\n",
    "#     else:\n",
    "#         selected_row = levels_df.iloc[row_index]\n",
    "#     print(f\"Parameters: row_index={row_index}, within_area_multiple={selected_row['within_area_multiple']}, level={selected_row['level']}, doi(s)={selected_row['dataset_dois']}\")\n",
    "\n",
    "#     # Derive paths based on lake_gdf\n",
    "#     images_folder = os.path.join(OUTPUT_DIR, f\"find_evolving_outlines/{lake_name}\")\n",
    "#     output_video_file = os.path.join(OUTPUT_DIR, \n",
    "#         f\"find_evolving_outlines/{lake_name}_{row_index}-idx_{selected_row['level']}m-level_{selected_row['within_area_multiple']}x-within.mp4\")\n",
    "    \n",
    "#     # Get all images in the folder with the specified extension\n",
    "#     image_files = glob.glob(os.path.join(images_folder, f\"*.{img_extension}\"))\n",
    "#     if not image_files:\n",
    "#         print(f\"No images found in {images_folder} with extension {img_extension}\")\n",
    "#         return\n",
    "    \n",
    "#     # Read the first image to determine the video size\n",
    "#     frame = cv2.imread(image_files[0])\n",
    "#     if frame is None:\n",
    "#         print(f\"Could not read the image {image_files[0]}\")\n",
    "#         return\n",
    "#     height, width, layers = frame.shape\n",
    "\n",
    "#     # Define the codec and create VideoWriter object\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "#     video = cv2.VideoWriter(output_video_file, fourcc, fps, (width, height))\n",
    "\n",
    "#     for image_file in sorted(image_files):\n",
    "#         frame = cv2.imread(image_file)\n",
    "#         if frame is not None:\n",
    "#             video.write(frame)\n",
    "\n",
    "#     # Release the VideoWriter object\n",
    "#     video.release()\n",
    "#     print(f\"Video file {output_video_file} created successfully.\")\n",
    "\n",
    "#     # Delete the images in the directory\n",
    "#     for image_file in image_files:\n",
    "#         os.remove(image_file)\n",
    "#     print(f\"Deleted {len(image_files)} image(s) from {images_folder}\")\n",
    "\n",
    "#     # Force delete the folder and its contents\n",
    "#     try:\n",
    "#         shutil.rmtree(images_folder)\n",
    "#         print(f\"Deleted folder and all contents: {images_folder}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Could not delete folder {images_folder}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9f3dcb-40f4-450f-9f76-62c482c07b46",
   "metadata": {},
   "source": [
    "## Function to analyze lake groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "182521e4-b89f-41b7-b1fe-92ec47350fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_group_gdf(stationary_outlines_gdf, lake_group):\n",
    "    '''\n",
    "    Prepare a GeoDataFrame row representing a group of lakes.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    stationary_outlines_gdf : GeoDataFrame\n",
    "        The complete dataset of stationary lakes\n",
    "    lake_group : list\n",
    "        List of lake names to be analyzed together\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    GeoDataFrame\n",
    "        Single-row GeoDataFrame with combined group properties\n",
    "    '''\n",
    "    try:\n",
    "        # Filter GeoDataFrame for lakes in the group\n",
    "        group_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'].isin(lake_group)].copy()\n",
    "        if group_gdf.empty:\n",
    "            print(f\"No lakes found for group: {lake_group}\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"Preparing group geodataframe for lake group: {lake_group}\")\n",
    "        \n",
    "        # Create a combined name for the group\n",
    "        group_name = \"_\".join(lake_group)\n",
    "        \n",
    "        # Create a union of all lake geometries for the group\n",
    "        group_stationary_outline = group_gdf.geometry.union_all()\n",
    "        if not group_stationary_outline.is_valid:\n",
    "            print(\"Cleaning group geometry...\")\n",
    "            group_stationary_outline = make_valid(group_stationary_outline)\n",
    "        \n",
    "        # Determine the group's time period\n",
    "        group_time_period = determine_group_time_period(group_gdf['CS2_SARIn_start'])\n",
    "        print(f\"Group CryoSat-2 SARIn time period determined as: {group_time_period}\")\n",
    "        \n",
    "        # Create a GeoDataFrame for the group\n",
    "        group_single_gdf = gpd.GeoDataFrame(\n",
    "            {\n",
    "                'name': [group_name],\n",
    "                'geometry': [group_stationary_outline],\n",
    "                'CS2_SARIn_start': [group_time_period]\n",
    "            },\n",
    "            crs=group_gdf.crs\n",
    "        )\n",
    "        \n",
    "        return group_single_gdf\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing group GeoDataFrame: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def determine_group_time_period(time_periods):\n",
    "    '''\n",
    "    Determine the most exclusive time period for a group of lakes.\n",
    "    \n",
    "    Rules:\n",
    "    - If any lake has <NA>, group gets <NA>\n",
    "    - If all lakes have '2010.5', group gets '2010.5'\n",
    "    - If all lakes have either '2013.75' or '2010.5', group gets '2013.75'\n",
    "    - Otherwise, group gets <NA>\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    time_periods : Series\n",
    "        Series of time periods from the group of lakes\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str or pd.NA\n",
    "        The determined time period for the group\n",
    "    '''\n",
    "    # If any lake has NA, group gets NA\n",
    "    if time_periods.isna().any():\n",
    "        return pd.NA\n",
    "        \n",
    "    # Convert to list and remove any NA values\n",
    "    periods = [p for p in time_periods if pd.notna(p)]\n",
    "    \n",
    "    # If all lakes have '2010.5'\n",
    "    if all(p == '2010.5' for p in periods):\n",
    "        return '2010.5'\n",
    "        \n",
    "    # If all lakes have either '2013.75' or '2010.5'\n",
    "    valid_periods = {'2013.75', '2010.5'}\n",
    "    if all(p in valid_periods for p in periods):\n",
    "        return '2013.75'\n",
    "        \n",
    "    # Default to NA for any other case\n",
    "    return pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a661a49-c0a1-41ed-a566-2cb87bef95c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard_lake_outlines(\n",
    "    lake_outlines_to_discard: List[str],\n",
    "    source_dir: str,\n",
    "    dest_dir: str\n",
    ") -> Dict[str, Tuple[bool, str]]:\n",
    "    \"\"\"\n",
    "    Move lake outlines from git repo to non-git repo and create indicator files.\n",
    "    Replaces existing files in destination directory.\n",
    "    \n",
    "    Args:\n",
    "        lake_outlines_to_discard: List of lake names to process\n",
    "        output_dir_git: Path to source git repository directory\n",
    "        output_dir: Path to destination non-git directory\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with lake names as keys and tuples of (success_bool, message) as values\n",
    "    \"\"\"\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    \n",
    "    results = {}\n",
    "    for lake in lake_outlines_to_discard:\n",
    "        source_filepath = os.path.join(source_dir, f\"{lake}.geojson\")\n",
    "        dest_filepath = os.path.join(dest_dir, f\"{lake}.geojson\")\n",
    "        txt_filepath = os.path.join(source_dir, f\"{lake}.txt\")\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(source_filepath):\n",
    "                results[lake] = (False, f\"Source file does not exist: {source_filepath}\")\n",
    "                continue\n",
    "                \n",
    "            # Remove check for existing destination file\n",
    "            # Use copy2 then remove original to ensure atomic operation\n",
    "            shutil.copy2(source_filepath, dest_filepath)\n",
    "            os.remove(source_filepath)\n",
    "            results[lake] = (True, \"Successfully moved and replaced existing file\")\n",
    "            \n",
    "            write_no_outlines(txt_filepath)\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[lake] = (False, f\"Error: {str(e)}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0c2849-f764-4f85-a073-d397fc8d0b3a",
   "metadata": {},
   "source": [
    "## OUTDATED METHOD: compare_evolving_and_stationary_outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96b199db-fe7f-42df-90be-508f998af556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare_evolving_and_stationary_outlines(stationary_outline_gdf, dataset1, dataset2): \n",
    "#     '''\n",
    "#     Create dataframe of active area, dh, dV calculations for evolving compared to stationary outlines\n",
    "#     '''\n",
    "#     # Define lake name and polygon and buffered polygon to use as off-lake region\n",
    "#     lake_name = stationary_outline_gdf['name'].iloc[0]\n",
    "\n",
    "#     # Open evolving outlines geodataframe\n",
    "#     try:\n",
    "#         evolving_outlines_gdf = gpd.read_file(os.path.join(\n",
    "#             os.getcwd(), 'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)))\n",
    "#     except fiona.errors.DriverError:\n",
    "#         print(f\"File for {lake_name} not found. Skipping...\")\n",
    "#         return  \n",
    "\n",
    "#     # Ensure there are outlines in outines_gdf\n",
    "#     if evolving_outlines_gdf.empty:\n",
    "#         print('There are no evolving outlines in geodataframe.')\n",
    "#         return  \n",
    "\n",
    "#     # Define region of interest for slicing from evolving and stationary outlines\n",
    "#     evolving_union = evolving_outlines_gdf.union_all()\n",
    "#     evolving_union_region = area_multiple_buffer(evolving_union, 2, exclude_inner=True)\n",
    "#     stationary_outline = stationary_outline_gdf['geometry'].iloc[0]\n",
    "#     stationary_region = area_multiple_buffer(stationary_outline, 2, exclude_inner=True)\n",
    "    \n",
    "#     # Combine stationary outline with evolving outlines \n",
    "#     ROI = unary_union(list(evolving_outlines_gdf.geometry) + [stationary_outline])\n",
    "#     x_min, y_min, x_max, y_max = ROI.bounds\n",
    "#     del ROI\n",
    "\n",
    "#     # Create masks for both datasets\n",
    "#     if dataset1 is not None:\n",
    "#         dataset1_ROI_subset = dataset1.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "\n",
    "#         # Convert xarray Dataset to DataArray\n",
    "#         dataset1_ROI_da = dataset1_ROI_subset['delta_h']\n",
    "        \n",
    "#         # Create and apply masks\n",
    "#         dataset1_evolving_region_mask = np.array([[evolving_union_region.contains(Point(x, y)) \n",
    "#                                                       for x in dataset1_ROI_da.x.values] \n",
    "#                                                       for y in dataset1_ROI_da.y.values])\n",
    "#         dataset1_evolving_region_masked = dataset1_ROI_da.where(xr.DataArray(dataset1_evolving_region_mask, \n",
    "#                                                         coords=[dataset1_ROI_da.y, dataset1_ROI_da.x], \n",
    "#                                                         dims=[\"y\", \"x\"]))\n",
    "        \n",
    "#         dataset1_stationary_mask = np.array([[stationary_outline.contains(Point(x, y)) \n",
    "#                                             for x in dataset1_ROI_da.x.values] \n",
    "#                                             for y in dataset1_ROI_da.y.values])\n",
    "#         dataset1_stationary_masked = dataset1_ROI_da.where(xr.DataArray(dataset1_stationary_mask, \n",
    "#                                                           coords=[dataset1_ROI_da.y, dataset1_ROI_da.x],\n",
    "#                                                           dims=[\"y\", \"x\"]))\n",
    "        \n",
    "#         dataset1_stationary_region_mask = np.array([[stationary_region.contains(Point(x, y)) \n",
    "#                                                       for x in dataset1_ROI_da.x.values] \n",
    "#                                                       for y in dataset1_ROI_da.y.values])\n",
    "#         dataset1_stationary_region_masked = dataset1_ROI_da.where(xr.DataArray(dataset1_stationary_region_mask, \n",
    "#                                                         coords=[dataset1_ROI_da.y, dataset1_ROI_da.x], \n",
    "#                                                         dims=[\"y\", \"x\"]))\n",
    "\n",
    "#     # Now dataset2\n",
    "#     dataset2_ROI_subset = dataset2.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "\n",
    "#     # Convert xarray Dataset to DataArray\n",
    "#     dataset2_ROI_da = dataset2_ROI_subset['delta_h']\n",
    "    \n",
    "#     # Create and apply masks\n",
    "#     dataset2_evolving_region_mask = np.array([[evolving_union_region.contains(Point(x, y)) \n",
    "#                                                   for x in dataset2_ROI_da.x.values] \n",
    "#                                                   for y in dataset2_ROI_da.y.values])\n",
    "#     dataset2_evolving_region_masked = dataset2_ROI_da.where(xr.DataArray(dataset2_evolving_region_mask, \n",
    "#                                                     coords=[dataset2_ROI_da.y, dataset2_ROI_da.x], \n",
    "#                                                     dims=[\"y\", \"x\"]))\n",
    "\n",
    "#     dataset2_stationary_mask = np.array([[stationary_outline.contains(Point(x, y)) \n",
    "#                                         for x in dataset2_ROI_da.x.values] \n",
    "#                                         for y in dataset2_ROI_da.y.values])\n",
    "#     dataset2_stationary_masked = dataset2_ROI_da.where(xr.DataArray(dataset2_stationary_mask,\n",
    "#                                                        coords=[dataset2_ROI_da.y, dataset2_ROI_da.x],\n",
    "#                                                        dims=[\"y\", \"x\"]))\n",
    "    \n",
    "#     dataset2_stationary_region_mask = np.array([[stationary_region.contains(Point(x, y)) \n",
    "#                                                   for x in dataset2_ROI_da.x.values] \n",
    "#                                                   for y in dataset2_ROI_da.y.values])\n",
    "#     dataset2_stationary_region_masked = dataset2_ROI_da.where(xr.DataArray(dataset2_stationary_region_mask, \n",
    "#                                                     coords=[dataset2_ROI_da.y, dataset2_ROI_da.x], \n",
    "#                                                     dims=[\"y\", \"x\"]))\n",
    "\n",
    "#     # Calculate time differences using xarray's diff\n",
    "#     if dataset1 is not None:\n",
    "#         dataset1_ROI_dh = dataset1_ROI_da.diff('time')\n",
    "#         dataset1_evolving_region_dh = dataset1_evolving_region_masked.diff('time')\n",
    "#         dataset1_stationary_dh = dataset1_stationary_masked.diff('time')\n",
    "#         dataset1_stationary_region_dh = dataset1_stationary_region_masked.diff('time')\n",
    "#         dataset1_times = dataset1_ROI_da['time'].values\n",
    "#         dataset1_midcyc_datetimes = dataset1_times[:-1] + np.diff(dataset1_times) / 2\n",
    "\n",
    "#     dataset2_ROI_dh = dataset2_ROI_da.diff('time')\n",
    "#     dataset2_evolving_region_dh = dataset2_evolving_region_masked.diff('time')\n",
    "#     dataset2_stationary_dh = dataset2_stationary_masked.diff('time')\n",
    "#     dataset2_stationary_region_dh = dataset2_stationary_region_masked.diff('time')\n",
    "#     dataset2_times = dataset2_ROI_da['time'].values\n",
    "#     dataset2_midcyc_datetimes = dataset2_times[:-1] + np.diff(dataset2_times) / 2\n",
    "\n",
    "#     # Create empty lists to store metrics\n",
    "#     midcyc_datetimes = []\n",
    "#     stationary_outline_dhs = []\n",
    "#     stationary_region_dhs = []\n",
    "#     stationary_outline_dhs_corr = []\n",
    "#     stationary_dVs_corr = []\n",
    "#     evolving_outlines_areas = []\n",
    "#     evolving_outlines_dhs = []\n",
    "#     evolving_region_dhs = []\n",
    "#     evolving_outlines_dhs_corr = []\n",
    "#     evolving_outlines_dVs_corr = []\n",
    "\n",
    "#     # Suppress the specific RuntimeWarning about mean of empty slice\n",
    "#         # warnings.filterwarnings('ignore', category=RuntimeWarning, message='Mean of empty slice')\n",
    "#     with warnings.catch_warnings():\n",
    "#         warnings.simplefilter('ignore', category=RuntimeWarning)\n",
    "        \n",
    "#         # Process dataset1 if available\n",
    "#         if dataset1 is not None:\n",
    "#             for i, midcyc_datetime in enumerate(dataset1_midcyc_datetimes):\n",
    "#                 process_timestep(evolving_region_dh_slice=dataset1_evolving_region_dh.isel(time=i),\n",
    "#                     timestep_subset_evolving_outlines_gdf=evolving_outlines_gdf[\n",
    "#                         evolving_outlines_gdf['midcyc_datetime'] == midcyc_datetime],\n",
    "#                     ROI_dh_slice=dataset1_ROI_dh.isel(time=i),\n",
    "#                     stationary_dh_slice=dataset1_stationary_dh.isel(time=i),\n",
    "#                     stationary_region_dh_slice=dataset1_stationary_region_dh.isel(time=i),\n",
    "#                     timestep=midcyc_datetime,\n",
    "#                     # stationary_outline_gdf=stationary_outline_gdf,\n",
    "#                     stationary_outline_dhs=stationary_outline_dhs,\n",
    "#                     stationary_region_dhs=stationary_region_dhs,\n",
    "#                     stationary_outline_dhs_corr=stationary_outline_dhs_corr,\n",
    "#                     stationary_dVs_corr=stationary_dVs_corr,\n",
    "#                     evolving_outlines_areas=evolving_outlines_areas,\n",
    "#                     evolving_outlines_dhs=evolving_outlines_dhs,\n",
    "#                     evolving_region_dhs=evolving_region_dhs,\n",
    "#                     evolving_outlines_dhs_corr=evolving_outlines_dhs_corr,\n",
    "#                     evolving_outlines_dVs_corr=evolving_outlines_dVs_corr)\n",
    "    \n",
    "#         # Process dataset2\n",
    "#         for i, midcyc_datetime in enumerate(dataset2_midcyc_datetimes):\n",
    "#             process_timestep(evolving_region_dh_slice=dataset2_evolving_region_dh.isel(time=i),\n",
    "#                 timestep_subset_evolving_outlines_gdf=evolving_outlines_gdf[\n",
    "#                     evolving_outlines_gdf['midcyc_datetime'] == midcyc_datetime],\n",
    "#                 ROI_dh_slice=dataset2_ROI_dh.isel(time=i),\n",
    "#                 stationary_dh_slice=dataset2_stationary_dh.isel(time=i),\n",
    "#                 stationary_region_dh_slice=dataset2_stationary_region_dh.isel(time=i),\n",
    "#                 timestep=midcyc_datetime,\n",
    "#                 # stationary_outline_gdf=stationary_outline_gdf,\n",
    "#                 stationary_outline_dhs=stationary_outline_dhs,\n",
    "#                 stationary_region_dhs=stationary_region_dhs,\n",
    "#                 stationary_outline_dhs_corr=stationary_outline_dhs_corr,\n",
    "#                 stationary_dVs_corr=stationary_dVs_corr,\n",
    "#                 evolving_outlines_areas=evolving_outlines_areas,\n",
    "#                 evolving_outlines_dhs=evolving_outlines_dhs,\n",
    "#                 evolving_region_dhs=evolving_region_dhs,\n",
    "#                 evolving_outlines_dhs_corr=evolving_outlines_dhs_corr,\n",
    "#                 evolving_outlines_dVs_corr=evolving_outlines_dVs_corr)\n",
    "\n",
    "#     # Re-enable warnings after the function completes (optional)\n",
    "#     warnings.resetwarnings()\n",
    "\n",
    "#     # Create dataframe if we have data\n",
    "#     if len(midcyc_datetimes) > 0:\n",
    "#         d = {\n",
    "#             'midcyc_datetime': midcyc_datetimes,\n",
    "#             'evolving_outlines_area (m^2)': evolving_outlines_areas,\n",
    "#             'evolving_outlines_dh (m)': evolving_outlines_dhs,\n",
    "#             'evolving_outlines_region_dh (m)': evolving_region_dhs,\n",
    "#             'evolving_outlines_dh_corr (m)': evolving_outlines_dhs_corr,\n",
    "#             'evolving_outlines_dV_corr (m^3)': evolving_outlines_dVs_corr,\n",
    "#             'stationary_outline_area (m^2)': [stationary_outline_gdf['area (m^2)'].iloc[0]] * len(midcyc_datetimes),\n",
    "#             'stationary_outline_dh (m)': stationary_outline_dhs,\n",
    "#             'stationary_outline_region_dh (m)': stationary_region_dhs,\n",
    "#             'stationary_outline_dh_corr (m)': stationary_outline_dhs_corr,\n",
    "#             'stationary_outline_dV_corr (m^3)': stationary_dVs_corr\n",
    "#         }\n",
    "        \n",
    "#         df = pd.DataFrame(d)\n",
    "\n",
    "#         # Fill any NaN values with 0.0\n",
    "#         df = df.fillna(0.0)\n",
    "\n",
    "#         # Calculate bias columns\n",
    "#         df['bias_area (m^2)'] = df['evolving_outlines_area (m^2)'] - df['stationary_outline_area (m^2)']\n",
    "#         df['bias_region_dh (m)'] = df['evolving_outlines_region_dh (m)'] - df['stationary_outline_region_dh (m)']\n",
    "#         df['bias_outlines_dh_corr (m)'] = df['evolving_outlines_dh_corr (m)'] - df['stationary_outline_dh_corr (m)']\n",
    "#         df['bias_dV_corr (m^3)'] = df['evolving_outlines_dV_corr (m^3)'] - df['stationary_outline_dV_corr (m^3)']\n",
    "\n",
    "#         # Export dataframe to csv\n",
    "#         output_path = f'output/geometric_calcs/compare_evolving_and_stationary_outlines/'\n",
    "#         output_file = os.path.join(output_path, f'{lake_name}.csv')\n",
    "#         df.to_csv(output_file, index=False)\n",
    "#         print(f\"Results saved to: {output_file}\")\n",
    "        \n",
    "#         return df\n",
    "#     else:\n",
    "#         print(f\"No data processed for {lake_name}\")\n",
    "#         return None\n",
    "\n",
    "# def process_timestep(evolving_region_dh_slice,\n",
    "#                      timestep_subset_evolving_outlines_gdf,\n",
    "#                      ROI_dh_slice,\n",
    "#                      stationary_dh_slice, \n",
    "#                      stationary_region_dh_slice, \n",
    "#                      stationary_outline_gdf,\n",
    "#                      timestep,\n",
    "#                      midcyc_datetimes,\n",
    "#                      stationary_outline_dhs,\n",
    "#                      stationary_region_dhs,\n",
    "#                      stationary_outline_dhs_corr,\n",
    "#                      stationary_dVs_corr,\n",
    "#                      evolving_outlines_areas,\n",
    "#                      evolving_outlines_dhs,\n",
    "#                      evolving_region_dhs,\n",
    "#                      evolving_outlines_dhs_corr,\n",
    "#                      evolving_outlines_dVs_corr):\n",
    "#     \"\"\"\n",
    "#     Process a single timestep of lake height data and calculate various metrics.\n",
    "#     \"\"\"    \n",
    "#     # Initialize evolving outlines variables with default values of 0\n",
    "#     evolving_outlines_area = 0.0\n",
    "#     evolving_outlines_dh = 0.0\n",
    "#     evolving_outlines_dh_corr = 0.0\n",
    "#     evolving_outlines_dV_corr = 0.0\n",
    "\n",
    "#     # Calculate metrics for evolving union region\n",
    "#     evolving_region_dh = float(np.nanmean(evolving_region_dh_slice))\n",
    "\n",
    "#     if not timestep_subset_evolving_outlines_gdf.empty:\n",
    "#         # Calculate metrics for evolving outlines\n",
    "#         evolving_outlines_area = float(timestep_subset_evolving_outlines_gdf['area (m^2)'].sum())\n",
    "#         union_timestep_subset_evolving_outlines = timestep_subset_evolving_outlines_gdf['geometry'].union_all()\n",
    "#         union_timestep_subset_evolving_outlines_mask = np.array([[union_timestep_subset_evolving_outlines.contains(Point(x, y)) \n",
    "#                                                                 for x in ROI_dh_slice['x'].values] \n",
    "#                                                                 for y in ROI_dh_slice['y'].values])\n",
    "#         union_timestep_subset_evolving_outlines_masked = ROI_dh_slice.where(xr.DataArray(union_timestep_subset_evolving_outlines_mask, \n",
    "#                                                                  coords=[ROI_dh_slice.y, ROI_dh_slice.x], \n",
    "#                                                                  dims=[\"y\", \"x\"]))\n",
    "\n",
    "#         evolving_outlines_dh = float(np.nanmean(union_timestep_subset_evolving_outlines_masked))\n",
    "#         evolving_outlines_dh_corr = evolving_outlines_dh - evolving_region_dh\n",
    "#         evolving_outlines_dV_corr = evolving_outlines_dh_corr * evolving_outlines_area\n",
    "\n",
    "#     # Calculate metrics for stationary outline\n",
    "#     stationary_outline_dh = float(np.nanmean(stationary_dh_slice))\n",
    "#     stationary_region_dh = float(np.nanmean(stationary_region_dh_slice))\n",
    "#     stationary_outline_dh_corr = stationary_outline_dh - stationary_region_dh\n",
    "#     stationary_dV_corr = stationary_outline_dh_corr * stationary_outline_gdf['area (m^2)'].iloc[0]\n",
    "    \n",
    "#     # Append all metrics to lists\n",
    "#     midcyc_datetimes.append(timestep)\n",
    "#     stationary_outline_dhs.append(stationary_outline_dh)\n",
    "#     stationary_region_dhs.append(stationary_region_dh)\n",
    "#     stationary_outline_dhs_corr.append(stationary_outline_dh_corr)\n",
    "#     stationary_dVs_corr.append(stationary_dV_corr)\n",
    "#     evolving_outlines_areas.append(evolving_outlines_area)\n",
    "#     evolving_outlines_dhs.append(evolving_outlines_dh)\n",
    "#     evolving_region_dhs.append(evolving_region_dh)\n",
    "#     evolving_outlines_dhs_corr.append(evolving_outlines_dh_corr)\n",
    "#     evolving_outlines_dVs_corr.append(evolving_outlines_dV_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ba3bf-960c-4e30-8030-0995167ce62d",
   "metadata": {},
   "source": [
    "## OUTDATED METHOD: compare_union_and_stationary_outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39b5afb0-3fda-4a79-85b2-38fafb8566f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare_union_and_stationary_outlines(stationary_outline_gdf, evolving_union_gdf, dataset1, dataset2): \n",
    "#     '''\n",
    "#     Create dataframe of active area, dh, dv calculations comparing evolving union outline (or evolving+stationary union)\n",
    "#     to stationary outline. The specific comparison depends on which union GDF is passed in:\n",
    "#     - evolving_outlines_union_gdf: compares evolving outline union only\n",
    "#     - evolving_stationary_outlines_union_evolving_lakes_gdf: compares evolving+stationary union for lakes with activity\n",
    "#     - evolving_stationary_outlines_union_all_lakes_gdf: compares evolving+stationary union for all lakes\n",
    "#     '''\n",
    "#     # Define lake name\n",
    "#     lake_name = str(stationary_outline_gdf['name'].iloc[0])\n",
    "\n",
    "#     # Find appropriate evolving outlines union geodataframe row\n",
    "#     evolving_union_lake_gdf = evolving_union_gdf[evolving_union_gdf['name'] == lake_name]\n",
    "        \n",
    "#     if evolving_union_lake_gdf.empty:\n",
    "#         print(f\"No union outline found for {lake_name}, so skipping.\")\n",
    "#         return None\n",
    "    \n",
    "#     # Define region of interest for slicing from evolving and stationary outlines\n",
    "#     stationary_outline = stationary_outline_gdf['geometry'].iloc[0]\n",
    "#     stationary_outline_buffered = area_multiple_buffer(stationary_outline, 2)   \n",
    "#     evolving_union = evolving_union_lake_gdf['geometry'].iloc[0]\n",
    "#     evolving_union_buffered = area_multiple_buffer(evolving_union, 2)\n",
    "\n",
    "#     # Get masks for datasets\n",
    "#     if dataset1 is not None:\n",
    "#         dataset1_masks = create_masked_dataset(dataset1, stationary_outline, \n",
    "#                                             stationary_outline_buffered,\n",
    "#                                             evolving_union, evolving_union_buffered)\n",
    "#     else:\n",
    "#         dataset1_masks = None\n",
    "        \n",
    "#     dataset2_masks = create_masked_dataset(dataset2, stationary_outline,\n",
    "#                                          stationary_outline_buffered,\n",
    "#                                          evolving_union, evolving_union_buffered)\n",
    "\n",
    "#     # Create empty lists to store metrics\n",
    "#     stationary_dhs = []\n",
    "#     stationary_region_dhs = []\n",
    "#     stationary_dhs_corr = []\n",
    "#     stationary_dVs_corr = []\n",
    "#     evolving_union_areas = []\n",
    "#     evolving_union_dhs = []\n",
    "#     evolving_union_region_dhs = []\n",
    "#     evolving_union_dhs_corr = []\n",
    "#     evolving_union_dVs_corr = []\n",
    "#     midcyc_datetimes = []\n",
    "\n",
    "#     # Process dataset1 if available\n",
    "#     if dataset1_masks is not None and not pd.isna(stationary_outline_gdf['CS2_SARIn_start'].iloc[0]):\n",
    "#         time_period = stationary_outline_gdf['CS2_SARIn_start'].iloc[0]\n",
    "        \n",
    "#         # Calculate differences and mid-cycle times\n",
    "#         dataset1_dh = dataset1_masks['stationary'].diff('time')\n",
    "#         dataset1_region_dh = dataset1_masks['stationary_region'].diff('time')\n",
    "#         dataset1_evolving_dh = dataset1_masks['evolving_union'].diff('time')\n",
    "#         dataset1_evolving_region_dh = dataset1_masks['evolving_union_region'].diff('time')\n",
    "        \n",
    "#         # Calculate mid-cycle times\n",
    "#         dataset1_times = dataset1_masks['stationary'].time.values\n",
    "#         dataset1_midcyc_times = dataset1_times[:-1] + np.diff(dataset1_times) / 2\n",
    "        \n",
    "#         # Get valid times based on time period\n",
    "#         if time_period == '2013.75':\n",
    "#             valid_times = dataset1_midcyc_times[dataset1_midcyc_times >= np.datetime64('2013-10-01')]\n",
    "#         else:\n",
    "#             valid_times = dataset1_midcyc_times\n",
    "        \n",
    "#         for i, mid_cyc_time in enumerate(valid_times):\n",
    "#             process_union_timestep(\n",
    "#                 dh_slice=dataset1_dh.isel(time=i),\n",
    "#                 region_dh_slice=dataset1_region_dh.isel(time=i),\n",
    "#                 evolving_dh_slice=dataset1_evolving_dh.isel(time=i),\n",
    "#                 evolving_region_dh_slice=dataset1_evolving_region_dh.isel(time=i),\n",
    "#                 time=mid_cyc_time,\n",
    "#                 stationary_outline_gdf=stationary_outline_gdf,\n",
    "#                 evolving_union_lake_gdf=evolving_union_lake_gdf,\n",
    "#                 stationary_dhs=stationary_dhs,\n",
    "#                 stationary_region_dhs=stationary_region_dhs,\n",
    "#                 stationary_dhs_corr=stationary_dhs_corr,\n",
    "#                 stationary_dVs_corr=stationary_dVs_corr,\n",
    "#                 evolving_union_areas=evolving_union_areas,\n",
    "#                 evolving_union_dhs=evolving_union_dhs,\n",
    "#                 evolving_union_region_dhs=evolving_union_region_dhs,\n",
    "#                 evolving_union_dhs_corr=evolving_union_dhs_corr,\n",
    "#                 evolving_union_dVs_corr=evolving_union_dVs_corr,\n",
    "#                 midcyc_datetimes=midcyc_datetimes\n",
    "#             )\n",
    "\n",
    "#     # Process dataset2\n",
    "#     dataset2_dh = dataset2_masks['stationary'].diff('time')\n",
    "#     dataset2_region_dh = dataset2_masks['stationary_region'].diff('time')\n",
    "#     dataset2_evolving_dh = dataset2_masks['evolving_union'].diff('time')\n",
    "#     dataset2_evolving_region_dh = dataset2_masks['evolving_union_region'].diff('time')\n",
    "    \n",
    "#     # Calculate mid-cycle times for dataset2\n",
    "#     dataset2_times = dataset2_masks['stationary'].time.values\n",
    "#     dataset2_midcyc_times = dataset2_times[:-1] + np.diff(dataset2_times) / 2\n",
    "        \n",
    "#     for i, mid_cyc_time in enumerate(dataset2_midcyc_times):\n",
    "#         process_union_timestep(\n",
    "#             dh_slice=dataset2_dh.isel(time=i),\n",
    "#             region_dh_slice=dataset2_region_dh.isel(time=i),\n",
    "#             evolving_dh_slice=dataset2_evolving_dh.isel(time=i),\n",
    "#             evolving_region_dh_slice=dataset2_evolving_region_dh.isel(time=i),\n",
    "#             time=mid_cyc_time,\n",
    "#             stationary_outline_gdf=stationary_outline_gdf,\n",
    "#             evolving_union_lake_gdf=evolving_union_lake_gdf,\n",
    "#             stationary_dhs=stationary_dhs,\n",
    "#             stationary_region_dhs=stationary_region_dhs,\n",
    "#             stationary_dhs_corr=stationary_dhs_corr,\n",
    "#             stationary_dVs_corr=stationary_dVs_corr,\n",
    "#             evolving_union_areas=evolving_union_areas,\n",
    "#             evolving_union_dhs=evolving_union_dhs,\n",
    "#             evolving_union_region_dhs=evolving_union_region_dhs,\n",
    "#             evolving_union_dhs_corr=evolving_union_dhs_corr,\n",
    "#             evolving_union_dVs_corr=evolving_union_dVs_corr,\n",
    "#             midcyc_datetimes=midcyc_datetimes\n",
    "#         )\n",
    "\n",
    "#     # Create dataframe if we have data\n",
    "#     if len(midcyc_datetimes) > 0:\n",
    "#         d = {\n",
    "#             'midcyc_datetime': midcyc_datetimes,\n",
    "#             'evolving_union_area (m^2)': evolving_union_areas,\n",
    "#             'evolving_union_dh (m)': evolving_union_dhs,\n",
    "#             'evolving_union_region_dh (m)': evolving_union_region_dhs,\n",
    "#             'evolving_union_dh_corr (m)': evolving_union_dhs_corr,\n",
    "#             'evolving_union_dV_corr (m^3)': evolving_union_dVs_corr,\n",
    "#             'stationary_outline_area (m^2)': [stationary_outline_gdf['area (m^2)'].iloc[0]] * len(midcyc_datetimes),\n",
    "#             'stationary_outline_dh (m)': stationary_dhs,\n",
    "#             'stationary_outline_region_dh (m)': stationary_region_dhs,\n",
    "#             'stationary_outline_dh_corr (m)': stationary_dhs_corr,\n",
    "#             'stationary_outline_dV_corr (m^3)': stationary_dVs_corr\n",
    "#         }\n",
    "        \n",
    "#         df = pd.DataFrame(d)\n",
    "        \n",
    "#         # Calculate bias columns\n",
    "#         df['bias_area (m^2)'] = df['evolving_union_area (m^2)'] - df['stationary_outline_area (m^2)']\n",
    "#         df['bias_region_dh (m)'] = df['evolving_union_region_dh (m)'] - df['stationary_outline_region_dh (m)']\n",
    "#         df['bias_outlines_dh_corr (m)'] = df['evolving_union_dh_corr (m)'] - df['stationary_outline_dh_corr (m)']\n",
    "#         df['bias_dV_corr (m^3)'] = df['evolving_union_dV_corr (m^3)'] - df['stationary_outline_dV_corr (m^3)']\n",
    "\n",
    "#         # Determine output path based on which GDF was passed in\n",
    "#         if evolving_union_gdf is evolving_outlines_union_gdf:\n",
    "#             output_path = 'output/geometric_calcs/compare_evolving_union_and_stationary_outlines'\n",
    "#         elif evolving_union_gdf is evolving_stationary_outlines_union_evolving_lakes_gdf:\n",
    "#             output_path = 'output/geometric_calcs/compare_evolving_stationary_union_and_stationary_outlines_evolving_lakes'\n",
    "#         elif evolving_union_gdf is evolving_stationary_outlines_union_all_lakes_gdf:\n",
    "#             output_path = 'output/geometric_calcs/compare_evolving_stationary_union_and_stationary_outlines_all_lakes'\n",
    "#         else:\n",
    "#             print('Unrecognized evolving_union_gdf')\n",
    "\n",
    "#         # Export dataframe to csv\n",
    "#         output_file = os.path.join(output_path, f'{lake_name}.csv')\n",
    "#         df.to_csv(output_file, index=False)\n",
    "#         print(f\"Results saved to: {output_file}\")\n",
    "        \n",
    "#         return df\n",
    "#     else:\n",
    "#         print(f\"No data processed for {lake_name}\")\n",
    "#         return None\n",
    "\n",
    "# def process_union_timestep(dh_slice, region_dh_slice, evolving_dh_slice, evolving_region_dh_slice,\n",
    "#                          time, stationary_outline_gdf, evolving_union_lake_gdf,\n",
    "#                          stationary_dhs, stationary_region_dhs,\n",
    "#                          stationary_dhs_corr, stationary_dVs_corr,\n",
    "#                          evolving_union_areas, evolving_union_dhs,\n",
    "#                          evolving_union_region_dhs, evolving_union_dhs_corr,\n",
    "#                          evolving_union_dVs_corr, midcyc_datetimes):\n",
    "#     \"\"\"Process a single timestep and update the metric lists for union comparison\"\"\"\n",
    "    \n",
    "#     # Initialize all variables with default values of 0.0\n",
    "#     stationary_dh = 0.0\n",
    "#     stationary_region_dh = 0.0\n",
    "#     stationary_dh_corr = 0.0\n",
    "#     stationary_dV_corr = 0.0\n",
    "#     evolving_dh = 0.0\n",
    "#     evolving_region_dh = 0.0\n",
    "#     evolving_dh_corr = 0.0\n",
    "#     evolving_dV_corr = 0.0\n",
    "    \n",
    "#     if np.any(~np.isnan(dh_slice)) and np.any(~np.isnan(evolving_dh_slice)):\n",
    "#         # Calculate stationary outline metrics\n",
    "#         stationary_dh = float(np.nanmean(dh_slice))\n",
    "#         stationary_region_dh = float(np.nanmean(region_dh_slice))\n",
    "#         stationary_dh_corr = stationary_dh - stationary_region_dh\n",
    "#         stationary_dV_corr = stationary_dh_corr * stationary_outline_gdf['area (m^2)'].iloc[0]\n",
    "\n",
    "#         # Calculate evolving union metrics\n",
    "#         evolving_dh = float(np.nanmean(evolving_dh_slice))\n",
    "#         evolving_region_dh = float(np.nanmean(evolving_region_dh_slice))\n",
    "#         evolving_dh_corr = evolving_dh - evolving_region_dh\n",
    "#         evolving_dV_corr = evolving_dh_corr * evolving_union_lake_gdf['area (m^2)'].iloc[0]\n",
    "\n",
    "#     # Append metrics to their respective lists\n",
    "#     stationary_dhs.append(stationary_dh)\n",
    "#     stationary_region_dhs.append(stationary_region_dh)\n",
    "#     stationary_dhs_corr.append(stationary_dh_corr)\n",
    "#     stationary_dVs_corr.append(stationary_dV_corr)\n",
    "    \n",
    "#     evolving_union_areas.append(evolving_union_lake_gdf['area (m^2)'].iloc[0])\n",
    "#     evolving_union_dhs.append(evolving_dh)\n",
    "#     evolving_union_region_dhs.append(evolving_region_dh)\n",
    "#     evolving_union_dhs_corr.append(evolving_dh_corr)\n",
    "#     evolving_union_dVs_corr.append(evolving_dV_corr)\n",
    "    \n",
    "#     midcyc_datetimes.append(time)\n",
    "\n",
    "# def create_masked_dataset(dataset, stationary_outline, stationary_outline_buffered, \n",
    "#                          evolving_union, evolving_union_buffered):\n",
    "#     \"\"\"\n",
    "#     Create all required masks for a single dataset\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     dataset : xarray.Dataset\n",
    "#         Dataset to mask\n",
    "#     stationary_outline : shapely.geometry\n",
    "#         Original lake outline\n",
    "#     stationary_outline_buffered : shapely.geometry\n",
    "#         Buffered stationary outline\n",
    "#     evolving_union : shapely.geometry\n",
    "#         Union of evolving outlines\n",
    "#     evolving_union_buffered : shapely.geometry\n",
    "#         Buffered union of evolving outlines\n",
    "        \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     dict\n",
    "#         Dictionary containing all masked versions of the dataset\n",
    "#     \"\"\"\n",
    "#     # Create base masks\n",
    "#     stationary_mask = np.array([[stationary_outline.contains(Point(x, y)) \n",
    "#                                for x in dataset['x'].values] \n",
    "#                                for y in dataset['y'].values])\n",
    "#     stationary_mask_da = xr.DataArray(stationary_mask,\n",
    "#                                     coords=[dataset.y, dataset.x],\n",
    "#                                     dims=[\"y\", \"x\"])\n",
    "\n",
    "#     stationary_buffered_mask = np.array([[stationary_outline_buffered.contains(Point(x, y)) \n",
    "#                                         for x in dataset['x'].values] \n",
    "#                                         for y in dataset['y'].values])\n",
    "    \n",
    "#     evolving_union_mask = np.array([[evolving_union.contains(Point(x, y)) \n",
    "#                                    for x in dataset['x'].values] \n",
    "#                                    for y in dataset['y'].values])\n",
    "#     evolving_union_mask_da = xr.DataArray(evolving_union_mask,\n",
    "#                                         coords=[dataset.y, dataset.x],\n",
    "#                                         dims=[\"y\", \"x\"])\n",
    "\n",
    "#     evolving_union_buffered_mask = np.array([[evolving_union_buffered.contains(Point(x, y)) \n",
    "#                                             for x in dataset['x'].values] \n",
    "#                                             for y in dataset['y'].values])\n",
    "\n",
    "#     # Create region masks (buffered area minus main area)\n",
    "#     stationary_region_mask = stationary_buffered_mask & ~stationary_mask\n",
    "#     stationary_region_mask_da = xr.DataArray(stationary_region_mask,\n",
    "#                                            coords=[dataset.y, dataset.x],\n",
    "#                                            dims=[\"y\", \"x\"])\n",
    "\n",
    "#     evolving_union_region_mask = evolving_union_buffered_mask & ~evolving_union_mask\n",
    "#     evolving_union_region_mask_da = xr.DataArray(evolving_union_region_mask,\n",
    "#                                                coords=[dataset.y, dataset.x],\n",
    "#                                                dims=[\"y\", \"x\"])\n",
    "\n",
    "#     # Apply masks\n",
    "#     return {\n",
    "#         'stationary': dataset['delta_h'].where(stationary_mask_da),\n",
    "#         'stationary_region': dataset['delta_h'].where(stationary_region_mask_da),\n",
    "#         'evolving_union': dataset['delta_h'].where(evolving_union_mask_da),\n",
    "#         'evolving_union_region': dataset['delta_h'].where(evolving_union_region_mask_da)\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01edcb8e-2120-4629-8926-6b48ae8076ee",
   "metadata": {},
   "source": [
    "## evolving_outlines_geom_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c25ac08e-db7c-4273-bc64-a5bd89167348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolving_outlines_geom_calc(stationary_outline_gdf, dataset1, dataset2): \n",
    "    '''\n",
    "    Create dataframe of active area, dh, dV calculations for evolving compared to stationary outlines\n",
    "    Area measurements rounded to nearest whole number due to 1-km grid resolution\n",
    "    Height measurements rounded to 2 decimal places to match 9 cm measurement precision\n",
    "    Volume measurements rounded to whole numbers due to combined uncertainties\n",
    "    '''\n",
    "    # Define lake name and polygon and buffered polygon to use as off-lake region\n",
    "    lake_name = stationary_outline_gdf['name'].iloc[0]\n",
    "    print(f\"Processing lake: {lake_name}\")\n",
    "\n",
    "    # Open evolving outlines geodataframe\n",
    "    try:\n",
    "        evolving_outlines_gdf = gpd.read_file(os.path.join(\n",
    "            os.getcwd(), 'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)))\n",
    "    except fiona.errors.DriverError:\n",
    "        print(f\"File for {lake_name} not found. Skipping...\")\n",
    "        return  \n",
    "\n",
    "    # Ensure there are outlines in outines_gdf\n",
    "    if evolving_outlines_gdf.empty:\n",
    "        print('There are no evolving outlines in geodataframe.')\n",
    "        return  \n",
    "\n",
    "    # Define region of interest for slicing from evolving and stationary outlines\n",
    "    evolving_union = evolving_outlines_gdf.union_all()\n",
    "    evolving_union_region = area_multiple_buffer(evolving_union, 2, exclude_inner=True)\n",
    "    x_min, y_min, x_max, y_max = evolving_union_region.bounds\n",
    "\n",
    "    # Create empty lists to store metrics\n",
    "    midcyc_datetimes = []\n",
    "    evolving_outlines_areas = []\n",
    "    evolving_outlines_dhs = []\n",
    "    evolving_region_dhs = []\n",
    "    evolving_outlines_dhs_corr = []\n",
    "    evolving_outlines_dVs_corr = []\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore', category=RuntimeWarning)\n",
    "\n",
    "        # Process dataset1 if available\n",
    "        if dataset1 is not None:\n",
    "            # Create masks and calculations for dataset1\n",
    "            dataset1_ROI_subset = dataset1.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "            dataset1_ROI_da = dataset1_ROI_subset['delta_h']\n",
    "            \n",
    "            dataset1_evolving_region_mask = np.array([[evolving_union_region.contains(Point(x, y)) \n",
    "                                                      for x in dataset1_ROI_da.x.values] \n",
    "                                                      for y in dataset1_ROI_da.y.values])\n",
    "            dataset1_evolving_region_masked = dataset1_ROI_da.where(xr.DataArray(dataset1_evolving_region_mask, \n",
    "                                                            coords=[dataset1_ROI_da.y, dataset1_ROI_da.x], \n",
    "                                                            dims=[\"y\", \"x\"]))\n",
    "            \n",
    "            dataset1_ROI_dh = dataset1_ROI_da.diff('time')\n",
    "            dataset1_evolving_region_dh = dataset1_evolving_region_masked.diff('time')\n",
    "\n",
    "            # Calculate mid-cycle datetimes\n",
    "            dataset1_midcyc_datetimes = []\n",
    "            dataset1_datetimes = dataset1_ROI_da['time'].values\n",
    "            for i in range(1, len(dataset1_datetimes)):\n",
    "                midcyc_days = dataset1_datetimes[i] - dataset1_datetimes[i-1]\n",
    "                midcyc_datetime = dataset1_datetimes[i-1] + midcyc_days/2\n",
    "                dataset1_midcyc_datetimes.append(midcyc_datetime)\n",
    "            dataset1_midcyc_datetimes = np.array(dataset1_midcyc_datetimes)\n",
    "\n",
    "            for i, midcyc_datetime in enumerate(dataset1_midcyc_datetimes):\n",
    "                print(midcyc_datetime)\n",
    "                evolving_outlines_geom_calc_process_timestep(\n",
    "                    evolving_region_dh_slice=dataset1_evolving_region_dh.isel(time=i),\n",
    "                    timestep_subset_evolving_outlines_gdf=evolving_outlines_gdf[\n",
    "                        evolving_outlines_gdf['midcyc_datetime'] == midcyc_datetime],\n",
    "                    ROI_dh_slice=dataset1_ROI_dh.isel(time=i),\n",
    "                    timestep=midcyc_datetime,\n",
    "                    midcyc_datetimes=midcyc_datetimes,\n",
    "                    evolving_outlines_areas=evolving_outlines_areas,\n",
    "                    evolving_outlines_dhs=evolving_outlines_dhs,\n",
    "                    evolving_region_dhs=evolving_region_dhs,\n",
    "                    evolving_outlines_dhs_corr=evolving_outlines_dhs_corr,\n",
    "                    evolving_outlines_dVs_corr=evolving_outlines_dVs_corr\n",
    "                )\n",
    "\n",
    "        # Process dataset2\n",
    "        dataset2_ROI_subset = dataset2.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "        dataset2_ROI_da = dataset2_ROI_subset['delta_h']\n",
    "        \n",
    "        dataset2_evolving_region_mask = np.array([[evolving_union_region.contains(Point(x, y)) \n",
    "                                                  for x in dataset2_ROI_da.x.values] \n",
    "                                                  for y in dataset2_ROI_da.y.values])\n",
    "        dataset2_evolving_region_masked = dataset2_ROI_da.where(xr.DataArray(dataset2_evolving_region_mask, \n",
    "                                                        coords=[dataset2_ROI_da.y, dataset2_ROI_da.x], \n",
    "                                                        dims=[\"y\", \"x\"]))\n",
    "\n",
    "        dataset2_ROI_dh = dataset2_ROI_da.diff('time')\n",
    "        dataset2_evolving_region_dh = dataset2_evolving_region_masked.diff('time')\n",
    "\n",
    "        # Calculate mid-cycle datetimes\n",
    "        dataset2_midcyc_datetimes = []\n",
    "        dataset2_datetimes = dataset2_ROI_da['time'].values\n",
    "        for i in range(1, len(dataset2_datetimes)):\n",
    "            midcyc_days = dataset2_datetimes[i] - dataset2_datetimes[i-1]\n",
    "            midcyc_datetime = dataset2_datetimes[i-1] + midcyc_days/2\n",
    "            dataset2_midcyc_datetimes.append(midcyc_datetime)\n",
    "        dataset2_midcyc_datetimes = np.array(dataset2_midcyc_datetimes)\n",
    "\n",
    "        for i, midcyc_datetime in enumerate(dataset2_midcyc_datetimes):\n",
    "            evolving_outlines_geom_calc_process_timestep(\n",
    "                evolving_region_dh_slice=dataset2_evolving_region_dh.isel(time=i),\n",
    "                timestep_subset_evolving_outlines_gdf=evolving_outlines_gdf[\n",
    "                    evolving_outlines_gdf['midcyc_datetime'] == midcyc_datetime],\n",
    "                ROI_dh_slice=dataset2_ROI_dh.isel(time=i),\n",
    "                timestep=midcyc_datetime,\n",
    "                midcyc_datetimes=midcyc_datetimes,\n",
    "                evolving_outlines_areas=evolving_outlines_areas,\n",
    "                evolving_outlines_dhs=evolving_outlines_dhs,\n",
    "                evolving_region_dhs=evolving_region_dhs,\n",
    "                evolving_outlines_dhs_corr=evolving_outlines_dhs_corr,\n",
    "                evolving_outlines_dVs_corr=evolving_outlines_dVs_corr\n",
    "            )\n",
    "\n",
    "    # Create dataframe if we have data\n",
    "    if len(midcyc_datetimes) > 0:\n",
    "        d = {\n",
    "            'midcyc_datetime': midcyc_datetimes,\n",
    "            'evolving_outlines_area (m^2)': [round(x, 0) for x in evolving_outlines_areas],  # Round to whole numbers\n",
    "            'evolving_outlines_dh (m)': [round(x, 2) for x in evolving_outlines_dhs],  # 2 decimals for height\n",
    "            'evolving_outlines_region_dh (m)': [round(x, 2) for x in evolving_region_dhs],  # 2 decimals for height\n",
    "            'evolving_outlines_dh_corr (m)': [round(x, 2) for x in evolving_outlines_dhs_corr],  # 2 decimals for height\n",
    "            'evolving_outlines_dV_corr (m^3)': [round(x, 0) for x in evolving_outlines_dVs_corr],  # Round to whole numbers\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(d)\n",
    "        df = df.fillna(0.0)\n",
    "        \n",
    "        # Convert area and volume columns to integer type since we're using whole numbers\n",
    "        df['evolving_outlines_area (m^2)'] = df['evolving_outlines_area (m^2)'].astype(int)\n",
    "        df['evolving_outlines_dV_corr (m^3)'] = df['evolving_outlines_dV_corr (m^3)'].astype(int)\n",
    "        \n",
    "        output_path = f'output/geometric_calcs/evolving_outlines_geom_calc/'\n",
    "        output_file = os.path.join(output_path, f'{lake_name}.csv')\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Results saved to: {output_file}\")\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(f\"No data processed for {lake_name}\")\n",
    "        return None\n",
    "\n",
    "def evolving_outlines_geom_calc_process_timestep(evolving_region_dh_slice,\n",
    "                     timestep_subset_evolving_outlines_gdf,\n",
    "                     ROI_dh_slice,\n",
    "                     timestep,\n",
    "                     midcyc_datetimes,\n",
    "                     evolving_outlines_areas,\n",
    "                     evolving_outlines_dhs,\n",
    "                     evolving_region_dhs,\n",
    "                     evolving_outlines_dhs_corr,\n",
    "                     evolving_outlines_dVs_corr):\n",
    "    \"\"\"\n",
    "    Process a single timestep of lake height data and calculate various metrics.\n",
    "    Height measurements rounded to 2 decimal places (9cm precision)\n",
    "    Area measurements rounded to whole numbers (1-km grid resolution)\n",
    "    Volume measurements rounded to whole numbers (combined uncertainty)\n",
    "    \"\"\"    \n",
    "    # Initialize evolving outlines variables with default values of 0\n",
    "    evolving_outlines_area = 0\n",
    "    evolving_outlines_dh = 0.0\n",
    "    evolving_outlines_dh_corr = 0.0\n",
    "    evolving_outlines_dV_corr = 0\n",
    "\n",
    "    # Calculate metrics for evolving union region\n",
    "    evolving_region_dh = round(float(np.nanmean(evolving_region_dh_slice)), 2)  # 2 decimals for height\n",
    "\n",
    "    if not timestep_subset_evolving_outlines_gdf.empty:\n",
    "        # Calculate metrics for evolving outlines\n",
    "        evolving_outlines_area = round(float(timestep_subset_evolving_outlines_gdf['area (m^2)'].sum()), 0)\n",
    "        union_timestep_subset_evolving_outlines = timestep_subset_evolving_outlines_gdf['geometry'].union_all()\n",
    "        union_timestep_subset_evolving_outlines_mask = np.array([[union_timestep_subset_evolving_outlines.contains(Point(x, y)) \n",
    "                                                                for x in ROI_dh_slice['x'].values] \n",
    "                                                                for y in ROI_dh_slice['y'].values])\n",
    "        union_timestep_subset_evolving_outlines_masked = ROI_dh_slice.where(xr.DataArray(union_timestep_subset_evolving_outlines_mask, \n",
    "                                                                 coords=[ROI_dh_slice.y, ROI_dh_slice.x], \n",
    "                                                                 dims=[\"y\", \"x\"]))\n",
    "\n",
    "        evolving_outlines_dh = round(float(np.nanmean(union_timestep_subset_evolving_outlines_masked)), 2)  # 2 decimals\n",
    "        evolving_outlines_dh_corr = round(evolving_outlines_dh - evolving_region_dh, 2)  # 2 decimals\n",
    "        evolving_outlines_dV_corr = round(evolving_outlines_dh_corr * evolving_outlines_area, 0)  # Whole numbers\n",
    "    \n",
    "    # Append all metrics to lists\n",
    "    midcyc_datetimes.append(timestep)\n",
    "    evolving_outlines_areas.append(evolving_outlines_area)\n",
    "    evolving_outlines_dhs.append(evolving_outlines_dh)\n",
    "    evolving_region_dhs.append(evolving_region_dh)\n",
    "    evolving_outlines_dhs_corr.append(evolving_outlines_dh_corr)\n",
    "    evolving_outlines_dVs_corr.append(evolving_outlines_dV_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb0d7f3-3375-4f45-8349-2db26da2f4a6",
   "metadata": {},
   "source": [
    "## stationary_outline_geom_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "862cd63c-acff-4cf7-b1d6-91786a521ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationary_outline_geom_calc(stationary_outline_gdf, dataset1, dataset2, sub_dir): \n",
    "    '''\n",
    "    Create dataframe of active area, dh, dV calculations for evolving compared to stationary outlines\n",
    "    Area measurements rounded to nearest whole number due to 1-km grid resolution\n",
    "    Height measurements rounded to 2 decimal places to match 9cm measurement precision\n",
    "    Volume measurements rounded to whole numbers due to combined uncertainties\n",
    "    '''\n",
    "    # Define lake name and polygon and buffered polygon to use as off-lake region\n",
    "    lake_name = stationary_outline_gdf['name'].iloc[0]\n",
    "    print(f\"Processing lake: {lake_name}\")\n",
    "    stationary_outline = stationary_outline_gdf['geometry'].iloc[0]\n",
    "    stationary_region = area_multiple_buffer(stationary_outline, 2, exclude_inner=True)\n",
    "    \n",
    "    # Create empty lists to store metrics\n",
    "    midcyc_datetimes = []\n",
    "    stationary_outline_dhs = []\n",
    "    stationary_region_dhs = []\n",
    "    stationary_outline_dhs_corr = []\n",
    "    stationary_dVs_corr = []\n",
    "\n",
    "    # Get bounds for data slicing\n",
    "    x_min, y_min, x_max, y_max = stationary_region.bounds\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore', category=RuntimeWarning)\n",
    "        \n",
    "        # Process dataset1 if available\n",
    "        if dataset1 is not None:\n",
    "            # Prepare dataset1 masks and calculations\n",
    "            dataset1_ROI_subset = dataset1.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "            dataset1_ROI_da = dataset1_ROI_subset['delta_h']\n",
    "            \n",
    "            # Create masks\n",
    "            dataset1_stationary_mask = np.array([[stationary_outline.contains(Point(x, y)) \n",
    "                                                for x in dataset1_ROI_da.x.values] \n",
    "                                                for y in dataset1_ROI_da.y.values])\n",
    "            dataset1_stationary_masked = dataset1_ROI_da.where(xr.DataArray(dataset1_stationary_mask, \n",
    "                                                            coords=[dataset1_ROI_da.y, dataset1_ROI_da.x],\n",
    "                                                            dims=[\"y\", \"x\"]))\n",
    "            \n",
    "            dataset1_stationary_region_mask = np.array([[stationary_region.contains(Point(x, y)) \n",
    "                                                        for x in dataset1_ROI_da.x.values] \n",
    "                                                        for y in dataset1_ROI_da.y.values])\n",
    "            dataset1_stationary_region_masked = dataset1_ROI_da.where(xr.DataArray(dataset1_stationary_region_mask, \n",
    "                                                            coords=[dataset1_ROI_da.y, dataset1_ROI_da.x], \n",
    "                                                            dims=[\"y\", \"x\"]))\n",
    "\n",
    "            # Calculate dh differences\n",
    "            dataset1_ROI_dh = dataset1_ROI_da.diff('time')\n",
    "            dataset1_stationary_dh = dataset1_stationary_masked.diff('time')\n",
    "            dataset1_stationary_region_dh = dataset1_stationary_region_masked.diff('time')\n",
    "\n",
    "            # Calculate mid-cycle datetimes\n",
    "            dataset1_midcyc_datetimes = []\n",
    "            dataset1_datetimes = dataset1_ROI_da['time'].values\n",
    "            for i in range(1, len(dataset1_datetimes)):\n",
    "                midcyc_days = dataset1_datetimes[i] - dataset1_datetimes[i-1]\n",
    "                midcyc_datetime = dataset1_datetimes[i-1] + midcyc_days/2\n",
    "                dataset1_midcyc_datetimes.append(midcyc_datetime)\n",
    "            dataset1_midcyc_datetimes = np.array(dataset1_midcyc_datetimes)\n",
    "            \n",
    "            # Process timesteps for dataset1\n",
    "            for i, midcyc_datetime in enumerate(dataset1_midcyc_datetimes):\n",
    "                stationary_outline_geom_calc_process_timestep(\n",
    "                    ROI_dh_slice=dataset1_ROI_dh.isel(time=i),\n",
    "                    stationary_dh_slice=dataset1_stationary_dh.isel(time=i),\n",
    "                    stationary_region_dh_slice=dataset1_stationary_region_dh.isel(time=i),\n",
    "                    stationary_outline_gdf=stationary_outline_gdf,\n",
    "                    timestep=midcyc_datetime,\n",
    "                    midcyc_datetimes=midcyc_datetimes,\n",
    "                    stationary_outline_dhs=stationary_outline_dhs,\n",
    "                    stationary_region_dhs=stationary_region_dhs,\n",
    "                    stationary_outline_dhs_corr=stationary_outline_dhs_corr,\n",
    "                    stationary_dVs_corr=stationary_dVs_corr\n",
    "                )\n",
    "        \n",
    "        # Process dataset2\n",
    "        # Prepare dataset2 masks and calculations\n",
    "        dataset2_ROI_subset = dataset2.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "        dataset2_ROI_da = dataset2_ROI_subset['delta_h']\n",
    "        \n",
    "        # Create masks\n",
    "        dataset2_stationary_mask = np.array([[stationary_outline.contains(Point(x, y)) \n",
    "                                            for x in dataset2_ROI_da.x.values] \n",
    "                                            for y in dataset2_ROI_da.y.values])\n",
    "        dataset2_stationary_masked = dataset2_ROI_da.where(xr.DataArray(dataset2_stationary_mask,\n",
    "                                                        coords=[dataset2_ROI_da.y, dataset2_ROI_da.x],\n",
    "                                                        dims=[\"y\", \"x\"]))\n",
    "        \n",
    "        dataset2_stationary_region_mask = np.array([[stationary_region.contains(Point(x, y)) \n",
    "                                                    for x in dataset2_ROI_da.x.values] \n",
    "                                                    for y in dataset2_ROI_da.y.values])\n",
    "        dataset2_stationary_region_masked = dataset2_ROI_da.where(xr.DataArray(dataset2_stationary_region_mask, \n",
    "                                                        coords=[dataset2_ROI_da.y, dataset2_ROI_da.x], \n",
    "                                                        dims=[\"y\", \"x\"]))\n",
    "\n",
    "        # Calculate dh differences\n",
    "        dataset2_ROI_dh = dataset2_ROI_da.diff('time')\n",
    "        dataset2_stationary_dh = dataset2_stationary_masked.diff('time')\n",
    "        dataset2_stationary_region_dh = dataset2_stationary_region_masked.diff('time')\n",
    "\n",
    "        # Calculate mid-cycle datetimes\n",
    "        dataset2_midcyc_datetimes = []\n",
    "        dataset2_datetimes = dataset2_ROI_da['time'].values\n",
    "        for i in range(1, len(dataset2_datetimes)):\n",
    "            midcyc_days = dataset2_datetimes[i] - dataset2_datetimes[i-1]\n",
    "            midcyc_datetime = dataset2_datetimes[i-1] + midcyc_days/2\n",
    "            dataset2_midcyc_datetimes.append(midcyc_datetime)\n",
    "        dataset2_midcyc_datetimes = np.array(dataset2_midcyc_datetimes)\n",
    "\n",
    "        # Process timesteps for dataset2\n",
    "        for i, midcyc_datetime in enumerate(dataset2_midcyc_datetimes):\n",
    "            stationary_outline_geom_calc_process_timestep(\n",
    "                ROI_dh_slice=dataset2_ROI_dh.isel(time=i),\n",
    "                stationary_dh_slice=dataset2_stationary_dh.isel(time=i),\n",
    "                stationary_region_dh_slice=dataset2_stationary_region_dh.isel(time=i),\n",
    "                stationary_outline_gdf=stationary_outline_gdf,\n",
    "                timestep=midcyc_datetime,\n",
    "                midcyc_datetimes=midcyc_datetimes,\n",
    "                stationary_outline_dhs=stationary_outline_dhs,\n",
    "                stationary_region_dhs=stationary_region_dhs,\n",
    "                stationary_outline_dhs_corr=stationary_outline_dhs_corr,\n",
    "                stationary_dVs_corr=stationary_dVs_corr\n",
    "            )\n",
    "\n",
    "    # Create dataframe if we have data\n",
    "    if len(midcyc_datetimes) > 0:\n",
    "  \n",
    "        d = {\n",
    "            'midcyc_datetime': midcyc_datetimes,\n",
    "            'stationary_outline_area (m^2)': [stationary_outline_gdf['area (m^2)'].iloc[0]] * len(midcyc_datetimes),\n",
    "            'stationary_outline_dh (m)': [round(x, 2) for x in stationary_outline_dhs],\n",
    "            'stationary_outline_region_dh (m)': [round(x, 2) for x in stationary_region_dhs],\n",
    "            'stationary_outline_dh_corr (m)': [round(x, 2) for x in stationary_outline_dhs_corr],\n",
    "            'stationary_outline_dV_corr (m^3)': [round(x, 0) for x in stationary_dVs_corr]\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(d)\n",
    "        df = df.fillna(0.0)\n",
    "        \n",
    "        # Convert area and volume columns to integer type\n",
    "        df['stationary_outline_area (m^2)'] = df['stationary_outline_area (m^2)'].astype(int)\n",
    "        df['stationary_outline_dV_corr (m^3)'] = df['stationary_outline_dV_corr (m^3)'].astype(int)\n",
    "        \n",
    "        output_path = f'output/geometric_calcs/stationary_outline_geom_calc/'\n",
    "        output_file = os.path.join(output_path, sub_dir, f'{lake_name}.csv')\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Results saved to: {output_file}\")\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(f\"No data processed for {lake_name}\")\n",
    "        return None\n",
    "\n",
    "def stationary_outline_geom_calc_process_timestep(*,  # Force kwarg use\n",
    "                    ROI_dh_slice,\n",
    "                    stationary_dh_slice, \n",
    "                    stationary_region_dh_slice, \n",
    "                    stationary_outline_gdf,\n",
    "                    timestep,\n",
    "                    midcyc_datetimes,\n",
    "                    stationary_outline_dhs,\n",
    "                    stationary_region_dhs,\n",
    "                    stationary_outline_dhs_corr,\n",
    "                    stationary_dVs_corr):\n",
    "    \"\"\"\n",
    "    Process a single timestep of lake height data and calculate various metrics.\n",
    "    Height measurements rounded to 2 decimal places (9cm precision)\n",
    "    Area measurements rounded to nearest 1,000,000 m (1 km grid resolution)\n",
    "    Volume measurements rounded to whole numbers (combined uncertainty)\n",
    "    \"\"\"    \n",
    "    # Calculate metrics for stationary outline\n",
    "    stationary_outline_dh = round(float(np.nanmean(stationary_dh_slice)), 2)  # 2 decimals for height\n",
    "    stationary_region_dh = round(float(np.nanmean(stationary_region_dh_slice)), 2)  # 2 decimals for height\n",
    "    stationary_outline_dh_corr = round(stationary_outline_dh - stationary_region_dh, 2)  # 2 decimals for height\n",
    "    \n",
    "    # Round area to nearest 1,000,000 m before volume calculation\n",
    "    stationary_dV_corr = round(stationary_outline_dh_corr * stationary_outline_gdf['area (m^2)'].iloc[0], 0)  # Whole numbers for volume\n",
    "    \n",
    "    # Append all metrics to lists\n",
    "    midcyc_datetimes.append(timestep)\n",
    "    stationary_outline_dhs.append(stationary_outline_dh)\n",
    "    stationary_region_dhs.append(stationary_region_dh)\n",
    "    stationary_outline_dhs_corr.append(stationary_outline_dh_corr)\n",
    "    stationary_dVs_corr.append(stationary_dV_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e184bf11-21e2-4afe-9a6b-1a5db1df1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_geodesic_area(poly):\n",
    "    '''\n",
    "    Calculate geodesic area of polygon or multipolygon. Polygon or multipolygon must extracted from geodataframe\n",
    "    that has CRS EPSG:4326.\n",
    "    '''\n",
    "    # Ensure geom exists and geom is valid\n",
    "    if poly is None or not poly.is_valid:\n",
    "        return None\n",
    "\n",
    "    # Calculate geodesic area and return it\n",
    "    if isinstance(poly, Polygon):\n",
    "        return abs(geod.polygon_area_perimeter(poly.exterior.coords.xy[0], poly.exterior.coords.xy[1])[0])\n",
    "    elif isinstance(poly, MultiPolygon):\n",
    "        total_area = 0\n",
    "        for part in poly.geoms:\n",
    "            total_area += abs(geod.polygon_area_perimeter(part.exterior.coords.xy[0], part.exterior.coords.xy[1])[0])\n",
    "        return total_area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b4427d-c7a1-4d32-8d9d-c6217114d617",
   "metadata": {},
   "source": [
    "## plot_evolving_and_stationary_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd060e2f-4b17-41d3-be9c-92a1a8e106ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evolving_and_stationary_comparison(lake_gdf):\n",
    "    '''\n",
    "    Plot and compare the evolving outlines and stationary outline of a lake along with active area, dh, and dV.\n",
    "\n",
    "    This function generates plot for a given lake, showing the differences between the evolving \n",
    "    and stationary outlines over time. It includes visualizations of the outlines on a map, as well as plots for \n",
    "    active area, cumulative height change, and cumulative volume displacement. The results are saved as a PNG file.\n",
    "\n",
    "    Parameters:\n",
    "    lake_gdf (GeoDataFrame): A GeoDataFrame containing a single lake's data with attributes such as 'name' and 'geometry'.\n",
    "                             The GeoDataFrame should have a single row corresponding to the lake.\n",
    "\n",
    "    Returns:\n",
    "    None: The results are saved as PNG files in the OUTPUT_DIR + '/plot_evolving_and_stationary_comparison/' directory \n",
    "    with filenames corresponding to the lake names.\n",
    "\n",
    "    Example:\n",
    "    >>> lake_gdf = gpd.read_file('path_to_lake.geojson')\n",
    "    >>> plot_evolving_and_stationary_comparison(lake_gdf)\n",
    "    '''\n",
    "    # First check if lake_gdf is valid and has data\n",
    "    if lake_gdf is None or lake_gdf.empty:\n",
    "        print(\"Empty lake_gdf provided. Skipping...\")\n",
    "        return\n",
    "    \n",
    "    # Define lake name and polygon and buffered polygon to use as off-lake region\n",
    "    lake_name = lake_gdf['name'].values[0]\n",
    "    lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "    print(f'Processing lake: {lake_name}')\n",
    "    \n",
    "    # Open evolving outline(s) and evolving outlines and geometric calculation comparison geodataframes for plotting\n",
    "    # Attempt to open the evolving outlines GeoJSON file\n",
    "    try:\n",
    "        evolving_outlines_gdf = gpd.read_file(os.path.join(\n",
    "            'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)))\n",
    "        offlake_outlines_gdf = gpd.read_file(os.path.join(\n",
    "            OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines/{}.geojson'.format(lake_name)))\n",
    "        evolving_geom_calcs_df = pd.read_csv(f\"output/geometric_calcs/evolving_outlines_geom_calc/{lake_name}.csv\")\n",
    "        evolving_union_geom_calcs_df = pd.read_csv(f\"output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/{lake_name}.csv\")\n",
    "        stationary_geom_calcs_df = pd.read_csv(f\"output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes/{lake_name}.csv\")\n",
    "\n",
    "    except (fiona.errors.DriverError, pyogrio.errors.DataSourceError, FileNotFoundError) as e:\n",
    "        print(f\"Error loading files for {lake_name}: {str(e)}. Skipping...\")\n",
    "        return\n",
    "    \n",
    "    # Convert of strings to datetime\n",
    "    evolving_geom_calcs_df['midcyc_datetime'] = pd.to_datetime(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "\n",
    "    # Fig setup\n",
    "    nows, ncols = 1, 4\n",
    "    fig = plt.figure(figsize=(16, 5))\n",
    "    \n",
    "    # Create GridSpec to control subplot sizes\n",
    "    gs = fig.add_gridspec(nows, ncols, width_ratios=[1, 1, 1, 1])\n",
    "    ax = [fig.add_subplot(gs[0, i]) for i in range(ncols)]\n",
    "    \n",
    "    # Define colors and linestyles that will be reused and create lines for legend\n",
    "    stationary_color  = 'darkturquoise'\n",
    "    stationary_line = plt.Line2D([], [], color=stationary_color, linestyle='solid', linewidth=1)\n",
    "\n",
    "    # Set up colormap\n",
    "    cmap = plt.get_cmap('plasma', len(mid_cyc_dates)-1)\n",
    "    \n",
    "    # Norm to time variable\n",
    "    norm = plt.Normalize(mdates.date2num(cyc_dates['mid_cyc_dates'].iloc[0]), \n",
    "                         mdates.date2num(cyc_dates['mid_cyc_dates'].iloc[-1]))\n",
    "\n",
    "    \n",
    "    # Panel - evolving outlines ------------------------------------------------------\n",
    "\n",
    "    # Create buffered polygon for the area multiple within evaluation boundary\n",
    "    within_eval_poly = area_multiple_buffer(lake_gdf['geometry'].iloc[0], evolving_outlines_gdf['within_area_multiple'][0])    \n",
    "\n",
    "    # Combine stationary outline(s) with evolving outlines in unary union to plot all within bounds of plot\n",
    "    evolving_stationary_outlines_union = evolving_stationary_outlines_union_evolving_lakes_gdf[evolving_stationary_outlines_union_evolving_lakes_gdf['name'] == lake_name]['geometry'].iloc[0]\n",
    "    all_poly_union = unary_union([within_eval_poly, evolving_stationary_outlines_union])\n",
    "    x_min, y_min, x_max, y_max = all_poly_union.bounds\n",
    "    del all_poly_union\n",
    "\n",
    "    # # Make plots a uniform size\n",
    "    # # Make x_min, y_min, x_max, and y_max define a square area centered at the original midpoints\n",
    "    # # Calculate the midpoints of the current bounds\n",
    "    # x_mid = (x_min + x_max) / 2\n",
    "    # y_mid = (y_min + y_max) / 2\n",
    "    \n",
    "    # # Calculate the current spans of the x and y dimensions\n",
    "    # x_span = x_max - x_min\n",
    "    # y_span = y_max - y_min\n",
    "    \n",
    "    # # Determine the maximum span to ensure square dimensions\n",
    "    # max_span = max(x_span, y_span)\n",
    "    \n",
    "    # # Update the min and max values to match the new span, keeping the midpoint the same\n",
    "    # x_min = x_mid - max_span / 2\n",
    "    # x_max = x_mid + max_span / 2\n",
    "    # y_min = y_mid - max_span / 2\n",
    "    # y_max = y_mid + max_span / 2\n",
    "\n",
    "    buffer_frac = 0.05\n",
    "    x_buffer = abs(x_max-x_min)*buffer_frac\n",
    "    y_buffer = abs(y_max-y_min)*buffer_frac\n",
    "    mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "    mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "    \n",
    "    # Plot stationary and evolving outlines onto MOA surface imagery\n",
    "    moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "    ax[0].imshow(moa_highres_da_subset[0,:,:], cmap=\"gray\", clim=[14000, 17000], extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "    \n",
    "    # Plot evolving outlines\n",
    "    onlake_lines, offlake_lines = [], []\n",
    "    for idx, dt in enumerate(mid_cyc_dates):\n",
    "        x, y = 1, 1\n",
    "        onlake_line, = ax[0].plot(x, y, color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))), linewidth=2)\n",
    "        onlake_lines.append(onlake_line)\n",
    "        offlake_line, = ax[0].plot(x, y, color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))), linewidth=2, alpha=0.2)\n",
    "        offlake_lines.append(offlake_line)\n",
    "        \n",
    "        evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "        offlake_outlines_dt = offlake_outlines_gdf[offlake_outlines_gdf['midcyc_datetime'] == dt]\n",
    "        if not evolving_outlines_dt.empty:\n",
    "            evolving_outlines_dt.boundary.plot(ax=ax[0], \n",
    "                # color=cmap(norm(date_to_quarter_year(mid_cyc_dates[idx]))), \n",
    "                color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))),\n",
    "                linewidth=1)\n",
    "        if not offlake_outlines_dt.empty:\n",
    "            offlake_outlines_dt.boundary.plot(ax=ax[0], \n",
    "                # color=cmap(norm(date_to_quarter_year(mid_cyc_dates[idx]))), \n",
    "                color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))),\n",
    "                linewidth=1, alpha=0.25)\n",
    "\n",
    "    # Plot within evaluation polygon\n",
    "    gpd.GeoDataFrame(geometry=[within_eval_poly]).boundary.plot(ax=ax[0], edgecolor='darkgray', facecolor='none', linewidth=1)\n",
    "\n",
    "    # Create evolving outlines unary union and plot\n",
    "    evolving_union_gdf = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]\n",
    "    evolving_union_gdf.boundary.plot(ax=ax[0], color='k', linestyle='dotted', linewidth=1)\n",
    "\n",
    "    # Plot stationary outline\n",
    "    stationary_outlines_gdf.boundary.plot(ax=ax[0], color=stationary_color, linewidth=1, zorder=0)\n",
    "        \n",
    "    # Plot inset map\n",
    "    axIns = ax[0].inset_axes([0.05, 0, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "    axIns.set_aspect('equal')\n",
    "    moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    axIns.axis('off')\n",
    "    # Plot star to indicate location\n",
    "    axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "        linewidth=0.1, color='k', s=30, zorder=3)\n",
    "\n",
    "    # Create stationary region and evolving outlines region and plot\n",
    "    stationary_region = area_multiple_buffer(lake_poly, 2)\n",
    "    stationary_region = stationary_region.difference(lake_poly)\n",
    "    evolving_union_region = area_multiple_buffer(evolving_union_gdf['geometry'], 2)\n",
    "    evolving_union_region = evolving_union_region.difference(evolving_union_gdf['geometry'].iloc[0])\n",
    "    gpd.GeoDataFrame(geometry=[stationary_region], crs='3031').plot(ax=ax[0], color=stationary_color, linewidth=2, alpha=0.2)\n",
    "    gpd.GeoDataFrame(geometry=[evolving_union_region], crs='3031').plot(ax=ax[0], color='k', linewidth=2, alpha=0.2)\n",
    "\n",
    "    # Set up colormap\n",
    "    min_date = pd.to_datetime(cyc_start_dates[1])\n",
    "    max_date = pd.to_datetime(cyc_end_dates[-1])\n",
    "    date_range = pd.date_range(min_date, max_date, periods=len(mid_cyc_dates[1:]))\n",
    "    years = date_range.year.unique()\n",
    "    years = pd.to_datetime(years, format='%Y')\n",
    "    cmap = plt.get_cmap('plasma', len(mid_cyc_dates[1:]))\n",
    "    norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "    m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    m.set_array(np.array([mdates.date2num(date) for date in mid_cyc_dates[1:]]))\n",
    "\n",
    "    # Add colorbar\n",
    "    divider = make_axes_locatable(ax[0])\n",
    "    cax = divider.append_axes('bottom', size='2.5%', pad=0.5)\n",
    "    cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "\n",
    "    # Set ticks for all years but labels only for odd years\n",
    "    tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "    tick_labels = [date.strftime('%Y') if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "    cbar.set_ticks(tick_locations)\n",
    "    cbar.set_ticklabels(tick_labels)\n",
    "    \n",
    "    # Add minor ticks for quarters\n",
    "    cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Quarter intervals only\n",
    "    cbar.set_label('year', size=10, labelpad=10)\n",
    "\n",
    "    # Change polar stereographic m to km\n",
    "    km_scale = 1e3\n",
    "    ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax[0].xaxis.set_major_formatter(ticks_x)\n",
    "    ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax[0].yaxis.set_major_formatter(ticks_y)\n",
    "    \n",
    "    # Set axes limit, title, and axis label\n",
    "    ax[0].set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "    ax[0].set_xlabel('x [km]')\n",
    "    ax[0].set_ylabel('y [km]')\n",
    "\n",
    "    \n",
    "    # Panel - Active area ---------------------------------------------\n",
    "\n",
    "    # Plot horizontal zero line for reference\n",
    "    ax[1].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "    \n",
    "    # Plot stationary outline and evolving outlines unary union areas\n",
    "    ax[1].axhline(np.divide(lake_gdf['area (m^2)'], 1e6).values, \n",
    "        color=stationary_color, linestyle='solid', linewidth=2)\n",
    "    ax[1].axhline(np.divide(evolving_union_gdf['area (m^2)'], 1e6).values, \n",
    "        color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "    # Plot multi-colored line and scatter for data points\n",
    "    x = mdates.date2num(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "    y = np.divide(evolving_geom_calcs_df['evolving_outlines_area (m^2)'], 1e6)\n",
    "    \n",
    "    # # Plot evolving outlines time series using LineCollection from points/segments to plot multi-colored line\n",
    "    # points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    # segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    # lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "    # lc.set_array(x)  # Set the values used for colormapping, using matplotlib dates for colors\n",
    "    # lc.set_linewidth(2)\n",
    "    # line = ax[1].add_collection(lc)\n",
    "    # scatter = ax[1].scatter(x, y, c=x, cmap=cmap, s=9, norm=norm, zorder=2)\n",
    "\n",
    "    # Filter out points where y is zero\n",
    "    non_zero_mask = y != 0\n",
    "    x_filtered = x[non_zero_mask]\n",
    "    y_filtered = y[non_zero_mask]\n",
    "    \n",
    "    # Create points and segments for LineCollection (only using non-zero points)\n",
    "    if len(x_filtered) > 1:  # Need at least 2 points to create segments\n",
    "        points = np.array([x_filtered, y_filtered]).T.reshape(-1, 1, 2)\n",
    "        segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "        \n",
    "        # Create a LineCollection, using the discrete colormap and norm\n",
    "        lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "        lc.set_array(x_filtered)\n",
    "        lc.set_linewidth(2)\n",
    "        # Plot multi-colored line and scatter for data points\n",
    "        line = ax[1].add_collection(lc)\n",
    "    \n",
    "    # Plot scatter points (only non-zero values)\n",
    "    scatter = ax[1].scatter(x_filtered, y_filtered, c=x_filtered, cmap=cmap, s=9, norm=norm, zorder=2)\n",
    "\n",
    "    # Panel C - Cumulative dh/dt -------------------------------------------------------\n",
    "    # Plot horizontal zero line for reference\n",
    "    ax[2].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "\n",
    "    # Plot stationary outlines off-lake region dh\n",
    "    ax[2].plot(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "        np.cumsum(stationary_geom_calcs_df['stationary_outline_region_dh (m)']).astype(float), \n",
    "        color='lightgray', linestyle='solid', linewidth=1)\n",
    "\n",
    "    # Plot evolving outlines off-lake region dh\n",
    "    ax[2].plot(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']), \n",
    "        np.cumsum(evolving_geom_calcs_df['evolving_outlines_region_dh (m)']), color='dimgray', linestyle='solid', linewidth=1)\n",
    "\n",
    "    # Plot stationary outline time series\n",
    "    ax[2].plot(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "        np.cumsum(stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), color=stationary_color, linestyle='solid', linewidth=1)\n",
    "\n",
    "    # Plot multi-colored line and scatter for data points\n",
    "    y = np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'])\n",
    "    \n",
    "    # Plot evolving outlines time series using LineCollection from points/segments to plot multi-colored line\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "    lc.set_array(x)\n",
    "    lc.set_linewidth(2)\n",
    "    line = ax[2].add_collection(lc)\n",
    "    scatter = ax[2].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9, zorder=2)\n",
    "\n",
    "    # Plot evolving outlines union outline time series\n",
    "    ax[2].plot(mdates.date2num(evolving_union_geom_calcs_df['midcyc_datetime']), \n",
    "        np.cumsum(evolving_union_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "        color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "    # Plot bias\n",
    "    ax[2].plot(x, np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)']-\n",
    "                            stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "                            color='red', linestyle='solid', linewidth=1)\n",
    "    \n",
    "    \n",
    "    # Panel D - Cumulative dV/dt --------------------------------------------------\n",
    "    # Plot horizontal line at zero for reference\n",
    "    ax[3].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "    \n",
    "    # Plot stationary outline time series\n",
    "    ax[3].plot(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "        np.divide(np.cumsum(stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "        color=stationary_color, linestyle='solid', linewidth=1)\n",
    "    \n",
    "    # Plot multi-colored line and scatter for data points\n",
    "    y = np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "    \n",
    "    # Plot evolving outlines time series using LineCollection from points/segments to plot multi-colored line\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "    lc.set_array(x)\n",
    "    lc.set_linewidth(2)\n",
    "    line = ax[3].add_collection(lc)\n",
    "    scatter = ax[3].scatter(x, y, c=x, cmap=cmap, norm=norm, s=10, zorder=2)\n",
    "\n",
    "    # Plot evolving outlines union outline time series\n",
    "    ax[3].plot(mdates.date2num(evolving_union_geom_calcs_df['midcyc_datetime']), \n",
    "        np.divide(np.cumsum(evolving_union_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "        color='k', linestyle='dashed', linewidth=1)\n",
    "    \n",
    "    # Plot bias\n",
    "    ax[3].plot(x, np.divide(np.cumsum(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)']-\n",
    "                            stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "                            color='red', linestyle='solid', linewidth=1)\n",
    "\n",
    "    for i in range(1, ncols):\n",
    "        # Set x-axis limits\n",
    "        ax[i].set_xlim(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "        \n",
    "        # Set ticks for all years but labels only for even years (to match colorbar)\n",
    "        tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "        tick_labels = [date.strftime('%Y') if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "        ax[i].set_xticks(tick_locations)\n",
    "        ax[i].set_xticklabels(tick_labels)\n",
    "        \n",
    "        # Add minor ticks for quarters\n",
    "        ax[i].xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))\n",
    "        \n",
    "        ax[i].set_xlabel('year')\n",
    "    \n",
    "    # Add legends\n",
    "    evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=1)\n",
    "    within_eval_line = plt.Line2D([], [], color='darkgray', linestyle='solid', linewidth=1)\n",
    "    stationary_region_patch = mpatches.Patch(color=stationary_color, alpha=0.2)\n",
    "    evolving_union_region_patch = mpatches.Patch(color='k', alpha=0.2)\n",
    "    \n",
    "    legend = ax[0].legend([tuple(onlake_lines),\n",
    "                           tuple(offlake_lines),\n",
    "                           evolving_union_line, \n",
    "                           stationary_line, \n",
    "                           within_eval_line,\n",
    "                           stationary_region_patch,\n",
    "                           evolving_union_region_patch], \n",
    "        [f'evolving outlines ({evolving_outlines_gdf.level[0]} m)',\n",
    "         'off-lake evolving outlines', \n",
    "         'evolving outlines union',\n",
    "         'stationary outline',\n",
    "         f'within evaluation line ({int(evolving_outlines_gdf.within_area_multiple[0])}x)',\n",
    "         'stationary region',\n",
    "         'evolving union region'],\n",
    "        handlelength=3,\n",
    "        handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "        loc='upper left',\n",
    "        bbox_to_anchor=(0, 1.47))\n",
    "\n",
    "    legend = ax[1].legend([tuple(onlake_lines), \n",
    "                           evolving_union_line, \n",
    "                           stationary_line],\n",
    "        ['evolving outlines', \n",
    "         'evolving outlines union', \n",
    "         'stationary outline'], \n",
    "        handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "        fontsize='small', loc='upper left',\n",
    "        bbox_to_anchor=(0, 1.15))\n",
    "\n",
    "    evolving_region = plt.Line2D([], [], color='dimgray', linestyle='solid', linewidth=1)\n",
    "    stationary_region = plt.Line2D([], [], color='lightgray', linestyle='solid', linewidth=1)\n",
    "    bias = plt.Line2D([], [], color='red', linestyle='solid', linewidth=1)\n",
    "\n",
    "    legend = ax[2].legend(\n",
    "        [evolving_region,\n",
    "         stationary_region,\n",
    "         tuple(onlake_lines),\n",
    "         evolving_union_line,\n",
    "         stationary_line,  \n",
    "         bias],\n",
    "        ['evolving outlines region',\n",
    "         'stationary outline region',\n",
    "         'evolving outlines',\n",
    "         'evolving outlines union',\n",
    "         'stationary outline', \n",
    "         'bias (evolving - stationary)'],\n",
    "         handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "         fontsize='small', loc='upper left',\n",
    "         bbox_to_anchor=(0, 1.25))\n",
    "\n",
    "    legend = ax[3].legend([tuple(onlake_lines), \n",
    "                           stationary_line,\n",
    "                           evolving_union_line,\n",
    "                           bias],\n",
    "        ['evolving outlines', \n",
    "         'stationary outline',\n",
    "         'evolving outlines union',\n",
    "         'bias (evolving - stationary)'], \n",
    "        handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "        fontsize='small', loc='upper left',\n",
    "        bbox_to_anchor=(0, 1.15))\n",
    "\n",
    "    # Set titles\n",
    "    ax[0].set_title(f'{lake_name}', size=12, y=1.47)\n",
    "    ax[1].set_title('active area [km$^2$]', size=12, y=1.15)\n",
    "    ax[2].set_title('cumulative dh [m]', size=12, y=1.25)\n",
    "    ax[3].set_title('cumulative dV [km$^3$]', size=12, y=1.15)\n",
    "\n",
    "    # Save and close plot\n",
    "    plt.savefig(OUTPUT_DIR + '/plot_evolving_and_stationary_comparison/{}.png'.format(lake_name), dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4654b17-a0d5-449f-862c-b1457a32466f",
   "metadata": {},
   "source": [
    "## plot_evolving_and_stationary_comparison_sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "648edc4c-ce9c-41c9-ab2d-57af95af924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evolving_and_stationary_comparison_sequential(lake_gdf):\n",
    "    '''\n",
    "    Plot and compare the evolving outlines and stationary outline of a lake along with active area, dh, and dV,\n",
    "    creating separate plots for each time step showing the progression of changes.\n",
    "\n",
    "    Parameters:\n",
    "    lake_gdf (GeoDataFrame): A GeoDataFrame containing a single lake's data with attributes.\n",
    "\n",
    "    Returns:\n",
    "    None: Results saved as PNG files in OUTPUT_DIR + '/plot_evolving_and_stationary_comparison/sequential/'\n",
    "    '''\n",
    "    # Define lake name and polygon\n",
    "    lake_name = lake_gdf['name'].values[0]\n",
    "    lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "    print(f'Processing lake: {lake_name}')\n",
    "    \n",
    "    # Make output directory if it doesn't yet exist\n",
    "    sequential_dir = os.path.join(OUTPUT_DIR, f'plot_evolving_and_stationary_comparison_sequential/{lake_name}')\n",
    "    os.makedirs(sequential_dir, exist_ok=True)\n",
    "\n",
    "    # First check if lake_gdf is valid and has data\n",
    "    if lake_gdf is None or lake_gdf.empty:\n",
    "        print(\"Empty lake_gdf provided. Skipping...\")\n",
    "        return\n",
    "    \n",
    "    # Open required files\n",
    "    try:\n",
    "        evolving_outlines_gdf = gpd.read_file(os.path.join(\n",
    "            'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)))\n",
    "        offlake_outlines_gdf = gpd.read_file(os.path.join(\n",
    "            OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines/{}.geojson'.format(lake_name)))\n",
    "        evolving_geom_calcs_df = pd.read_csv(f\"output/geometric_calcs/evolving_outlines_geom_calc/{lake_name}.csv\")\n",
    "        evolving_union_geom_calcs_df = pd.read_csv(f\"output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/{lake_name}.csv\")\n",
    "        stationary_geom_calcs_df = pd.read_csv(f\"output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes/{lake_name}.csv\")\n",
    "    except (fiona.errors.DriverError, pyogrio.errors.DataSourceError, FileNotFoundError) as e:\n",
    "        print(f\"Error loading files for {lake_name}: {str(e)}. Skipping...\")\n",
    "        return\n",
    "\n",
    "    # Convert strings to datetime\n",
    "    evolving_geom_calcs_df['midcyc_datetime'] = pd.to_datetime(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "\n",
    "    # Get evolving outlines union\n",
    "    evolving_union_gdf = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]\n",
    "\n",
    "    # Define colors and setup\n",
    "    stationary_color = 'darkturquoise'\n",
    "    cmap = plt.get_cmap('plasma', len(mid_cyc_dates)-1)\n",
    "    norm = plt.Normalize(mdates.date2num(mid_cyc_dates[0]), \n",
    "                        mdates.date2num(mid_cyc_dates[-1]))\n",
    "\n",
    "    # Prepare datasets\n",
    "    dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(lake_gdf, 30)\n",
    "\n",
    "    # Get plot bounds\n",
    "    within_eval_poly = area_multiple_buffer(lake_gdf['geometry'].iloc[0], evolving_outlines_gdf['within_area_multiple'][0])\n",
    "    evolving_stationary_outlines_union = evolving_stationary_outlines_union_evolving_lakes_gdf[evolving_stationary_outlines_union_evolving_lakes_gdf['name'] == lake_name]['geometry'].iloc[0]\n",
    "    all_poly_union = unary_union([search_extent_poly, within_eval_poly, evolving_stationary_outlines_union])\n",
    "    x_min, y_min, x_max, y_max = all_poly_union.bounds\n",
    "\n",
    "    # Calculate time steps and diffs for each dataset\n",
    "    dataset1_dh = None\n",
    "    if dataset1_masked is not None:\n",
    "        dataset1_dh = dataset1_masked['delta_h'].diff('time')\n",
    "        dataset1_dh.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "        dataset1_midcyc_datetimes = []\n",
    "        dataset1_datetimes = dataset1_masked['time'].values\n",
    "        for i in range(1, len(dataset1_datetimes)):\n",
    "            midcyc_days = dataset1_datetimes[i] - dataset1_datetimes[i-1]\n",
    "            midcyc_date = dataset1_datetimes[i-1] + midcyc_days/2\n",
    "            dataset1_midcyc_datetimes.append(midcyc_date)\n",
    "        dataset1_midcyc_times = np.array(dataset1_midcyc_datetimes)        \n",
    "\n",
    "    # Get time differences for dataset2\n",
    "    dataset2_dh = dataset2_masked['delta_h'].diff('time')\n",
    "    dataset2_dh.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "    # dataset2_datetimes = dataset2_masked['time'].values\n",
    "    # dataset2_midcyc_times = dataset2_datetimes[1:] + np.diff(dataset2_datetimes) / 2\n",
    "    dataset2_midcyc_datetimes = []\n",
    "    dataset2_datetimes = dataset2_masked['time'].values\n",
    "    for i in range(1, len(dataset2_datetimes)):\n",
    "        midcyc_days = dataset2_datetimes[i] - dataset2_datetimes[i-1]\n",
    "        midcyc_date = dataset2_datetimes[i-1] + midcyc_days/2\n",
    "        dataset2_midcyc_datetimes.append(midcyc_date)\n",
    "    dataset2_midcyc_times = np.array(dataset2_midcyc_datetimes)     \n",
    "\n",
    "    # Find magnitude of dh for colorbar mapping\n",
    "    height_anom_pos = []\n",
    "    height_anom_neg = []\n",
    "\n",
    "    # Process both datasets for height anomalies\n",
    "    if dataset1_masked is not None:\n",
    "        for dh_slice in dataset1_dh:\n",
    "            if np.any(~np.isnan(dh_slice)):\n",
    "                height_anom_pos.append(np.nanmax(dh_slice))\n",
    "                height_anom_neg.append(np.nanmin(dh_slice))\n",
    "\n",
    "    for dh_slice in dataset2_dh:\n",
    "        if np.any(~np.isnan(dh_slice)):\n",
    "            height_anom_pos.append(np.nanmax(dh_slice))\n",
    "            height_anom_neg.append(np.nanmin(dh_slice))\n",
    "\n",
    "    if not height_anom_pos:\n",
    "        print(\"No valid height anomalies found for plotting\")\n",
    "        return None\n",
    "\n",
    "    # Create color normalization for height changes\n",
    "    divnorm = colors.TwoSlopeNorm(vmin=min(height_anom_neg), \n",
    "                                vcenter=0., \n",
    "                                vmax=max(height_anom_pos))\n",
    "\n",
    "    # Create on- and off-lake line segments and solid lines that will be used in legends \n",
    "    fig, ax = plt.subplots()\n",
    "    onlake_lines, offlake_lines = [], []\n",
    "    for idx, dt in enumerate(mid_cyc_dates):\n",
    "        x, y = 1, 1\n",
    "        onlake_line, = ax.plot(x, y, color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))), linewidth=2)\n",
    "        onlake_lines.append(onlake_line)\n",
    "        offlake_line, = ax.plot(x, y, color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))), linewidth=2, alpha=0.2)\n",
    "        offlake_lines.append(offlake_line)\n",
    "\n",
    "    stationary_color  = 'darkturquoise'\n",
    "    stationary_line = plt.Line2D([], [], color=stationary_color, linestyle='solid', linewidth=1)\n",
    "    within_eval_line = plt.Line2D([], [], color='darkgray', linestyle='solid', linewidth=1)\n",
    "    stationary_region_patch = mpatches.Patch(color=stationary_color, alpha=0.2)\n",
    "    evolving_union_region_patch = mpatches.Patch(color='k', alpha=0.2)\n",
    "\n",
    "    # Calculate y-axis limits for cumulative dh plot (Panel C)\n",
    "    dh_data = pd.concat([\n",
    "        pd.Series(np.cumsum(stationary_geom_calcs_df['stationary_outline_region_dh (m)'])),\n",
    "        pd.Series(np.cumsum(evolving_geom_calcs_df['evolving_outlines_region_dh (m)'])),\n",
    "        pd.Series(np.cumsum(stationary_geom_calcs_df['stationary_outline_dh_corr (m)'])),\n",
    "        pd.Series(np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)']))\n",
    "    ])\n",
    "    dh_min, dh_max = dh_data.min(), dh_data.max()\n",
    "    dh_range = dh_max - dh_min\n",
    "\n",
    "    # Calculate y-axis limits for cumulative dV plot (Panel D)\n",
    "    dv_data = pd.concat([\n",
    "        pd.Series(np.cumsum(np.divide(stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)'], 1e9))),\n",
    "        pd.Series(np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'], 1e9)))\n",
    "    ])\n",
    "    dv_min, dv_max = dv_data.min(), dv_data.max()\n",
    "    dv_range = dv_max - dv_min\n",
    "\n",
    "    # Iterate through each date to create sequential plots\n",
    "    for date_idx, current_date in enumerate(evolving_geom_calcs_df['midcyc_datetime']):\n",
    "        gc.collect()  # Garbage collection\n",
    "        \n",
    "        current_date_pd = pd.Timestamp(current_date)\n",
    "        print(f\"Creating plot for date: {current_date_pd}\")\n",
    "        \n",
    "        # Create figure\n",
    "        fig = plt.figure(figsize=(16, 5))\n",
    "        nows, ncols = 1, 4\n",
    "        gs = fig.add_gridspec(nows, ncols, \n",
    "                              width_ratios=[1.2, 0.8, 0.8, 0.8],  # Make first panel wider\n",
    "                              wspace=0.4)  # Add horizontal spacing between subplots\n",
    "        ax = [fig.add_subplot(gs[0, i]) for i in range(ncols)]\n",
    "        \n",
    "        # Filter data up to current date\n",
    "        current_mask = evolving_geom_calcs_df['midcyc_datetime'] <= current_date\n",
    "        current_evolving_geom_calcs = evolving_geom_calcs_df[current_mask]\n",
    "        current_evolving_union_geom_calcs = evolving_union_geom_calcs_df[current_mask]\n",
    "        current_stationary_geom_calcs = stationary_geom_calcs_df[current_mask]\n",
    "\n",
    "        # Panel A - dh and evolving outlines\n",
    "        # Find corresponding dh slice for the current date\n",
    "        current_dh = None\n",
    "        if dataset1_masked is not None and current_date in dataset1_midcyc_times:\n",
    "            idx = np.where(dataset1_midcyc_times == current_date)[0][0]\n",
    "            current_dh = dataset1_dh[idx]\n",
    "        elif current_date in dataset2_midcyc_times:\n",
    "            idx = np.where(dataset2_midcyc_times == current_date)[0][0]\n",
    "            current_dh = dataset2_dh[idx]\n",
    "\n",
    "        if current_dh is not None:\n",
    "            # Plot height change\n",
    "            img = ax[0].imshow(current_dh, extent=[x_min, x_max, y_min, y_max],\n",
    "                origin='lower', cmap='coolwarm_r', norm=divnorm)\n",
    "\n",
    "        # Create stationary region and evolving outlines region and plot\n",
    "        stationary_region = area_multiple_buffer(lake_poly, 2)\n",
    "        stationary_region = stationary_region.difference(lake_poly)\n",
    "        evolving_union_region = area_multiple_buffer(evolving_union_gdf['geometry'], 2)\n",
    "        evolving_union_region = evolving_union_region.difference(evolving_union_gdf['geometry'].iloc[0])\n",
    "        gpd.GeoDataFrame(geometry=[stationary_region], crs='3031').plot(ax=ax[0], color=stationary_color, linewidth=2, alpha=0.2)\n",
    "        gpd.GeoDataFrame(geometry=[evolving_union_region], crs='3031').plot(ax=ax[0], color='k', linewidth=2, alpha=0.2)\n",
    "\n",
    "        # Add colorbars\n",
    "        # First create the divider\n",
    "        divider = make_axes_locatable(ax[0])\n",
    "        \n",
    "        # Create the vertical colorbar axes (for dh)\n",
    "        cax_vertical = divider.append_axes('right', size='5%', pad=0.05)\n",
    "\n",
    "        # Add dh colorbar\n",
    "        plt.colorbar(img, cax=cax_vertical, label='height change (dh) [m]')\n",
    "        \n",
    "        # Set up colormap\n",
    "        min_date = pd.to_datetime(cyc_start_dates[1])\n",
    "        max_date = pd.to_datetime(cyc_end_dates[-1])\n",
    "        date_range = pd.date_range(min_date, max_date, periods=len(mid_cyc_dates[1:]))\n",
    "        years = date_range.year.unique()\n",
    "        years = pd.to_datetime(years, format='%Y')\n",
    "        cmap = plt.get_cmap('plasma', len(mid_cyc_dates[1:]))\n",
    "        norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "        m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        m.set_array(np.array([mdates.date2num(date) for date in mid_cyc_dates[1:]]))\n",
    "    \n",
    "        # Add colorbar\n",
    "        divider = make_axes_locatable(ax[0])\n",
    "        cax = divider.append_axes('bottom', size='2.5%', pad=0.5)\n",
    "        cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "    \n",
    "        # Set ticks for all years but labels only for odd years\n",
    "        tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "        tick_labels = [date.strftime('%Y') if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "        cbar.set_ticks(tick_locations)\n",
    "        cbar.set_ticklabels(tick_labels)\n",
    "        \n",
    "        # Add minor ticks for quarters\n",
    "        cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Quarter intervals only\n",
    "        cbar.set_label('year', size=10, labelpad=10)\n",
    "        \n",
    "        # Add inset map\n",
    "        axIns = ax[0].inset_axes([0.05, 0, 0.3, 0.3])\n",
    "        axIns.set_aspect('equal')\n",
    "        moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        axIns.axis('off')\n",
    "        axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', linewidth=0.1, color='k', s=30, zorder=3)\n",
    "        \n",
    "        # Change polar stereographic m to km\n",
    "        km_scale = 1e3\n",
    "        ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "        ax[0].xaxis.set_major_formatter(ticks_x)\n",
    "        ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "        ax[0].yaxis.set_major_formatter(ticks_y)\n",
    "\n",
    "        # Set axes limit, title, and axis label\n",
    "        ax[0].set(xlim=(x_min, x_max), ylim=(y_min, y_max))\n",
    "        ax[0].set_xlabel('x [km]')\n",
    "        ax[0].set_ylabel('y [km]')\n",
    "\n",
    "        # Store line segments for multi-colored line in legend\n",
    "        onlake_lines, offlake_lines = [], []\n",
    "        for idx, dt in enumerate(mid_cyc_dates):\n",
    "            x, y = 1, 1\n",
    "            onlake_line, = ax[0].plot(x, y, color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))), linewidth=2)\n",
    "            onlake_lines.append(onlake_line)\n",
    "            offlake_line, = ax[0].plot(x, y, color=cmap(norm(mdates.date2num(mid_cyc_dates[idx]))), linewidth=2, alpha=0.2)\n",
    "            offlake_lines.append(offlake_line)\n",
    "\n",
    "        # Plot evolving outlines up to current date\n",
    "        for idx, dt in enumerate(evolving_geom_calcs_df['midcyc_datetime'][:date_idx + 1]):\n",
    "\n",
    "            evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "            offlake_outlines_dt = offlake_outlines_gdf[offlake_outlines_gdf['midcyc_datetime'] == dt]\n",
    "            \n",
    "            if not evolving_outlines_dt.empty:\n",
    "                evolving_outlines_dt.boundary.plot(ax=ax[0], \n",
    "                    color=cmap(norm(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime'].iloc[[idx]]))),\n",
    "                    linewidth=1)\n",
    "            if not offlake_outlines_dt.empty:\n",
    "                offlake_outlines_dt.boundary.plot(ax=ax[0], \n",
    "                    color=cmap(norm(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime'].iloc[[idx]]))),\n",
    "                    linewidth=1, alpha=0.25)\n",
    "\n",
    "        # Add other map elements (evaluation boundary, stationary outline, etc.)\n",
    "        gpd.GeoDataFrame(geometry=[within_eval_poly]).boundary.plot(ax=ax[0], edgecolor='darkgray', facecolor='none', linewidth=1)\n",
    "        evolving_union_gdf.boundary.plot(ax=ax[0], color='k', linestyle='dotted', linewidth=1)\n",
    "        stationary_outlines_gdf.boundary.plot(ax=ax[0], color=stationary_color, linewidth=1, zorder=0)\n",
    "\n",
    "\n",
    "        # Panel B - Active area\n",
    "\n",
    "        ax[1].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "        ax[1].axhline(np.divide(lake_gdf['area (m^2)'], 1e6).values, \n",
    "                      color=stationary_color, linestyle='solid', linewidth=2)\n",
    "        ax[1].axhline(np.divide(evolving_union_gdf['area (m^2)'], 1e6).values, \n",
    "                      color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "        # Plot multi-colored evolving outlines time series\n",
    "        x = mdates.date2num(current_evolving_geom_calcs['midcyc_datetime'])\n",
    "        y = np.divide(current_evolving_geom_calcs['evolving_outlines_area (m^2)'], 1e6)\n",
    "        \n",
    "        # # Plot multi-colored evolving outlines time series using LineCollection from points/segments\n",
    "        # points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "        # segments = np.concatenate([points[:-1], points[1:]], axis=1) if len(points) > 1 else []\n",
    "        # if len(segments) > 0:\n",
    "        #     lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "        #     lc.set_array(x)\n",
    "        #     lc.set_linewidth(2)\n",
    "        #     ax[1].add_collection(lc)\n",
    "        # ax[1].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9, zorder=2)\n",
    "\n",
    "        # Filter out points where y is zero\n",
    "        non_zero_mask = y != 0\n",
    "        x_filtered = x[non_zero_mask]\n",
    "        y_filtered = y[non_zero_mask]\n",
    "        \n",
    "        # Create points and segments for LineCollection (only using non-zero points)\n",
    "        if len(x_filtered) > 1:  # Need at least 2 points to create segments\n",
    "            points = np.array([x_filtered, y_filtered]).T.reshape(-1, 1, 2)\n",
    "            segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "            \n",
    "            # Create a LineCollection, using the discrete colormap and norm\n",
    "            lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "            lc.set_array(x_filtered)\n",
    "            lc.set_linewidth(2)\n",
    "            # Plot multi-colored line and scatter for data points\n",
    "            line = ax[1].add_collection(lc)\n",
    "        \n",
    "        # Plot scatter points (only non-zero values)\n",
    "        scatter = ax[1].scatter(x_filtered, y_filtered, c=x_filtered, cmap=cmap, s=9, norm=norm, zorder=2)\n",
    "\n",
    "\n",
    "        # Panel C - Cumulative dh/dt\n",
    "        ax[2].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "        \n",
    "        # Plot cumulative values up to current date\n",
    "        ax[2].plot(x, np.cumsum(current_stationary_geom_calcs['stationary_outline_region_dh (m)']).astype(float), \n",
    "            color='lightgray', linestyle='solid', linewidth=1)\n",
    "        ax[2].plot(x, np.cumsum(current_evolving_geom_calcs['evolving_outlines_region_dh (m)']), \n",
    "            color='dimgray', linestyle='solid', linewidth=1)\n",
    "        ax[2].plot(x, np.cumsum(current_stationary_geom_calcs['stationary_outline_dh_corr (m)']), \n",
    "            color=stationary_color, linestyle='solid', linewidth=1)\n",
    "        ax[2].plot(x, np.cumsum(current_evolving_union_geom_calcs['stationary_outline_dh_corr (m)']), \n",
    "            color='k', linestyle='dashed', linewidth=1)\n",
    "        ax[2].plot(x, np.cumsum(current_evolving_geom_calcs['evolving_outlines_dh_corr (m)']-\n",
    "            current_stationary_geom_calcs['stationary_outline_dh_corr (m)']), \n",
    "            color='red', linestyle='solid', linewidth=1)\n",
    "        \n",
    "        # Plot multi-colored evolving outlines time series using LineCollection from points/segments\n",
    "        y = np.cumsum(current_evolving_geom_calcs['evolving_outlines_dh_corr (m)'])\n",
    "        points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "        segments = np.concatenate([points[:-1], points[1:]], axis=1) if len(points) > 1 else []\n",
    "        if len(segments) > 0:\n",
    "            lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "            lc.set_array(x)\n",
    "            lc.set_linewidth(2)\n",
    "            ax[2].add_collection(lc)\n",
    "        ax[2].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9, zorder=2)\n",
    "\n",
    "        # Set y-axis limits\n",
    "        ax[2].set_ylim(dh_min - 0.1 * dh_range, dh_max + 0.1 * dh_range)\n",
    "\n",
    "\n",
    "        # Panel D - Cumulative dV/dt\n",
    "        ax[3].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "        \n",
    "        ax[3].plot(x, np.divide(np.cumsum(current_stationary_geom_calcs['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "            color=stationary_color, linestyle='solid', linewidth=1)\n",
    "        ax[3].plot(x, np.divide(np.cumsum(current_evolving_union_geom_calcs['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "            color='k', linestyle='dashed', linewidth=1)\n",
    "        ax[3].plot(x, np.divide(np.cumsum(current_evolving_geom_calcs['evolving_outlines_dV_corr (m^3)']-\n",
    "            current_stationary_geom_calcs['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "            color='red', linestyle='solid', linewidth=1)\n",
    "\n",
    "        # Plot multi-colored evolving outlines time series using LineCollection from points/segments\n",
    "        y = np.cumsum(np.divide(current_evolving_geom_calcs['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "        points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "        segments = np.concatenate([points[:-1], points[1:]], axis=1) if len(points) > 1 else []\n",
    "        if len(segments) > 0:\n",
    "            lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "            lc.set_array(x)\n",
    "            lc.set_linewidth(2)\n",
    "            ax[3].add_collection(lc)\n",
    "        ax[3].scatter(x, y, c=x, cmap=cmap, norm=norm, s=10, zorder=2)\n",
    "\n",
    "        # Add legends\n",
    "        evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=1)\n",
    "        within_eval_line = plt.Line2D([], [], color='darkgray', linestyle='solid', linewidth=1)\n",
    "        stationary_region_patch = mpatches.Patch(color=stationary_color, alpha=0.2)\n",
    "        evolving_union_region_patch = mpatches.Patch(color='k', alpha=0.2)\n",
    "      \n",
    "        legend = ax[0].legend([tuple(onlake_lines),\n",
    "                               tuple(offlake_lines),\n",
    "                               evolving_union_line, \n",
    "                               stationary_line, \n",
    "                               within_eval_line,\n",
    "                               stationary_region_patch,\n",
    "                               evolving_union_region_patch], \n",
    "            [f'evolving outlines ({evolving_outlines_gdf.level[0]} m)',\n",
    "             'off-lake evolving outlines', \n",
    "             'evolving outlines union',\n",
    "             'stationary outline',\n",
    "             f'within evaluation line ({int(evolving_outlines_gdf.within_area_multiple[0])}x)',\n",
    "             'stationary region',\n",
    "             'evolving union region'],\n",
    "            handlelength=3,\n",
    "            handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "            loc='upper left',\n",
    "            bbox_to_anchor=(0, 1.5))\n",
    "        \n",
    "        legend = ax[1].legend([tuple(onlake_lines), \n",
    "                               evolving_union_line, \n",
    "                               stationary_line],\n",
    "            ['evolving outlines', \n",
    "             'evolving outlines union', \n",
    "             'stationary outline'], \n",
    "            handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "            fontsize='small', loc='upper left',\n",
    "            bbox_to_anchor=(0, 1.15))\n",
    "\n",
    "        evolving_region = plt.Line2D([], [], color='dimgray', linestyle='solid', linewidth=1)\n",
    "        stationary_region = plt.Line2D([], [], color='lightgray', linestyle='solid', linewidth=1)\n",
    "        evolving_union_line = plt.Line2D([], [], color='k', linestyle='dashed', linewidth=1)\n",
    "        bias = plt.Line2D([], [], color='red', linestyle='solid', linewidth=1)\n",
    "        legend = ax[2].legend(\n",
    "            [evolving_region,\n",
    "             stationary_region,\n",
    "             tuple(onlake_lines),\n",
    "             evolving_union_line,\n",
    "             stationary_line,  \n",
    "             bias],\n",
    "            ['evolving outlines region',\n",
    "             'stationary outline region',\n",
    "             'evolving outlines',\n",
    "             'evolving outlines union',\n",
    "             'stationary outline', \n",
    "             'bias (evolving - stationary)'],\n",
    "             handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "             fontsize='small', loc='upper left',\n",
    "             bbox_to_anchor=(0, 1.22))\n",
    "\n",
    "        legend = ax[3].legend([tuple(onlake_lines), \n",
    "                               evolving_union_line, \n",
    "                               stationary_line, \n",
    "                               bias],\n",
    "            ['evolving outlines',\n",
    "             'evolving outlines union',\n",
    "             'stationary outline',\n",
    "             'bias (evolving - stationary)'], \n",
    "            handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "            fontsize='small', loc='upper left',\n",
    "            bbox_to_anchor=(0, 1.15))\n",
    "\n",
    "        # Set y-axis limits\n",
    "        ax[3].set_ylim(dv_min - 0.1 * dv_range, dv_max + 0.1 * dv_range)\n",
    "\n",
    "        # # Set common attributes for time series panels\n",
    "        # for i in range(1, 4):\n",
    "        #     ax[i].set_xlim(mdates.date2num(np.min(cyc_start_dates)), mdates.date2num(np.max(cyc_end_dates)))\n",
    "        #     ax[i].xaxis.set_major_locator(mdates.YearLocator(base=2))\n",
    "        #     ax[i].xaxis.set_minor_locator(mdates.YearLocator(base=1))\n",
    "        #     ax[i].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "        #     ax[i].set_xlabel('year')\n",
    "        for i in range(1, ncols):\n",
    "            # Set x-axis limits\n",
    "            ax[i].set_xlim(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "            \n",
    "            # Set ticks for all years but labels only for even years (to match colorbar)\n",
    "            tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "            tick_labels = [date.strftime('%Y') if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "            ax[i].set_xticks(tick_locations)\n",
    "            ax[i].set_xticklabels(tick_labels)\n",
    "            \n",
    "            # Add minor ticks for quarters\n",
    "            ax[i].xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))\n",
    "            \n",
    "            ax[i].set_xlabel('year')\n",
    "\n",
    "        # Set titles\n",
    "        ax[0].set_title(f'{lake_name}\\nmid-cyc date: {current_date_pd.strftime(\"%Y-%m-%d\")}', size=12, y=1.5)\n",
    "        ax[1].set_title('active area [km$^2$]', size=12, y=1.15)\n",
    "        ax[2].set_title('cumulative dh [m]', size=12, y=1.22)\n",
    "        ax[3].set_title('cumulative dV [km$^3$]', size=12, y=1.15)\n",
    "\n",
    "        # Save and close\n",
    "        plt.savefig(os.path.join(sequential_dir, f'{lake_name}_{current_date_pd.strftime(\"%Y%m%d\")}.png'), \n",
    "                    dpi=300, bbox_inches='tight')\n",
    "\n",
    "        # Clean up to conserve memory\n",
    "        current_evolving_geom_calcs = None\n",
    "        current_stationary_geom_calcs = None\n",
    "        evolving_outlines_dt = None\n",
    "        offlake_outlines_dt = None\n",
    "        del current_dh, fig\n",
    "        if 'img' in locals():  # Only delete img if it exists\n",
    "            del img\n",
    "\n",
    "        # Explicitly clear the figure, close, and clear any reference to it\n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "\n",
    "        # Force garbage collection\n",
    "        gc.collect() \n",
    "        \n",
    "    # Convert images to video\n",
    "    try:\n",
    "        video_from_images_2(lake_gdf, output_dir=OUTPUT_DIR, \n",
    "            fps=1, img_extension='png')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating video for {lake_name}: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # Clear output\n",
    "    clear_output(wait=True)\n",
    "\n",
    "def video_from_images_2(lake_gdf, output_dir=OUTPUT_DIR, fps=1, img_extension='png'):\n",
    "    \"\"\"\n",
    "    Creates a video from still images stored in a folder based on the lake_gdf input, then deletes the images.\n",
    "    \n",
    "    Parameters:\n",
    "    - lake_gdf: GeoDataFrame containing lake information (expects a 'name' column).\n",
    "    - output_dir: Base directory where the images and video are stored/created.\n",
    "    - fps: Frames per second for the output video.\n",
    "    - img_extension: Extension of the images to look for in the folder.\n",
    "\n",
    "    # Example usage\n",
    "    video_from_images(lake_gdf, OUTPUT_DIR, fps=0.5, img_extension='png')\n",
    "    \"\"\"\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    print('Making video for', lake_name)\n",
    "\n",
    "    # Derive paths based on lake_gdf\n",
    "    images_folder = os.path.join(OUTPUT_DIR, f\"plot_evolving_and_stationary_comparison_sequential/{lake_name}\")\n",
    "    output_video_file = os.path.join(OUTPUT_DIR, \n",
    "        f\"plot_evolving_and_stationary_comparison_sequential/{lake_name}.mp4\")\n",
    "    \n",
    "    # Get all images in the folder with the specified extension\n",
    "    image_files = glob.glob(os.path.join(images_folder, f\"*.{img_extension}\"))\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_folder} with extension {img_extension}\")\n",
    "        return\n",
    "    \n",
    "    # Read the first image to determine the video size\n",
    "    frame = cv2.imread(image_files[0])\n",
    "    if frame is None:\n",
    "        print(f\"Could not read the image {image_files[0]}\")\n",
    "        return\n",
    "    height, width, layers = frame.shape\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(output_video_file, fourcc, fps, (width, height))\n",
    "\n",
    "    for image_file in sorted(image_files):\n",
    "        frame = cv2.imread(image_file)\n",
    "        if frame is not None:\n",
    "            video.write(frame)\n",
    "            # Clear the frame from memory\n",
    "            frame = None\n",
    "            gc.collect()\n",
    "\n",
    "    # Release the VideoWriter object\n",
    "    video.release()\n",
    "    video = None\n",
    "    gc.collect()\n",
    "    print(f\"Video file {output_video_file} created successfully.\")\n",
    "\n",
    "    # Delete the images in the directory\n",
    "    for image_file in image_files:\n",
    "        os.remove(image_file)\n",
    "    print(f\"Deleted {len(image_files)} image(s) from {images_folder}\")\n",
    "\n",
    "    # Force delete the folder and its contents\n",
    "    try:\n",
    "        shutil.rmtree(images_folder)\n",
    "        print(f\"Deleted folder and all contents: {images_folder}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not delete folder {images_folder}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f27227c0-08f3-49ac-b03a-ea27195fd984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_evolving_union(lake_ps, evolving_outlines_gdf, incl_stationary=True):\n",
    "    '''\n",
    "    Find the union of evolving outlines and optionally the corresponding stationary\n",
    "    outline and return as a GeoSeries.\n",
    "    \n",
    "    Args:\n",
    "    lake_ps: Pandas series of lake row from stationary lakes geodataframe\n",
    "    evolving_outlines_gdf: GeoDataFrame containing evolving outlines\n",
    "    incl_stationary: Boolean indicating whether to include stationary outline\n",
    "    Returns:\n",
    "    GeoSeries containing the union of the outlines, or None if an error occurs.\n",
    "    '''\n",
    "    lake_name = lake_ps['name']\n",
    "        \n",
    "    try:\n",
    "        if incl_stationary:\n",
    "            # Create a temporary GeoDataFrame for union\n",
    "            temp_gdf = gpd.GeoDataFrame(\n",
    "                geometry=[lake_ps['geometry']] + evolving_outlines_gdf['geometry'].tolist(),\n",
    "                crs=evolving_outlines_gdf.crs\n",
    "            )\n",
    "        else: \n",
    "            temp_gdf = evolving_outlines_gdf[['geometry']].copy()\n",
    "            \n",
    "        # Use union_all() method to find the union of outlines\n",
    "        outlines_union = temp_gdf.geometry.union_all()\n",
    "        \n",
    "        # Create a new GeoSeries with the lake name as index\n",
    "        result = gpd.GeoSeries([outlines_union], index=[lake_name], crs=evolving_outlines_gdf.crs)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating union for {lake_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ec15b-a338-457d-863f-233b4a703bb1",
   "metadata": {},
   "source": [
    "## process_continental_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b602a77-1795-4485-b4d4-23f0ceb38735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_continental_sums(geom_calc_type):\n",
    "    \"\"\"\n",
    "    Process lake data for a specific comparison type directory.\n",
    "    \n",
    "    Args:\n",
    "        geom_calc_type (str): Type of comparison being performed\n",
    "    \"\"\"\n",
    "    # Define directory\n",
    "    directory = os.path.join('output/geometric_calcs', geom_calc_type)\n",
    "    \n",
    "    # Initialize lists for different lake categories\n",
    "    dfs_superset_IS2_lakes = []\n",
    "    dfs_subset_noCS2_IS2_lakes = []\n",
    "    dfs_subset_CS2_IS2_lakes_SARInPreExpansion = []\n",
    "    dfs_subset_CS2_IS2_lakes_SARInPostExpansion = []\n",
    "    \n",
    "    # Process each CSV file\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            lake_name = os.path.splitext(file)[0]\n",
    "            lake_row = revised_stationary_outlines_gdf[revised_stationary_outlines_gdf['name'] == lake_name]\n",
    "            \n",
    "            if not lake_row.empty:\n",
    "                dfs_superset_IS2_lakes.append(df)\n",
    "                \n",
    "                SARIn_date = lake_row['CS2_SARIn_start'].values[0]\n",
    "                if SARIn_date == '<NA>':\n",
    "                    dfs_subset_noCS2_IS2_lakes.append(df)\n",
    "                if SARIn_date in ['2010.5']:\n",
    "                    dfs_subset_CS2_IS2_lakes_SARInPreExpansion.append(df)\n",
    "                if SARIn_date in ['2010.5', '2013.75']:\n",
    "                    dfs_subset_CS2_IS2_lakes_SARInPostExpansion.append(df)\n",
    "    \n",
    "    # Process and save each subset\n",
    "    for subset_name, dfs_list in [\n",
    "        ('subset_noCS2_IS2_lakes', dfs_subset_noCS2_IS2_lakes),\n",
    "        ('subset_CS2_IS2_lakes_SARInPreExpansion', dfs_subset_CS2_IS2_lakes_SARInPreExpansion),\n",
    "        ('subset_CS2_IS2_lakes_SARInPostExpansion', dfs_subset_CS2_IS2_lakes_SARInPostExpansion)\n",
    "    ]:\n",
    "        if dfs_list:\n",
    "            df_concat = pd.concat(dfs_list, ignore_index=True)\n",
    "            df_sum = df_concat.groupby('midcyc_datetime').sum().reset_index()\n",
    "            output_path = os.path.join(directory, f'{subset_name}_sum.csv')\n",
    "            df_sum.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Process superset data\n",
    "    if dfs_superset_IS2_lakes:\n",
    "        superset_IS2_lakes = pd.concat(dfs_superset_IS2_lakes, ignore_index=True)\n",
    "        superset_IS2_lakes_sum = superset_IS2_lakes.groupby('midcyc_datetime').sum().reset_index()\n",
    "        superset_IS2_lakes_sum['midcyc_datetime'] = pd.to_datetime(superset_IS2_lakes_sum['midcyc_datetime'])\n",
    "        threshold = pd.Timestamp('2019-01-01 06:00:00')\n",
    "        superset_IS2_lakes_sum = superset_IS2_lakes_sum[\n",
    "            superset_IS2_lakes_sum['midcyc_datetime'] >= threshold\n",
    "        ].reset_index(drop=True)\n",
    "        output_path = os.path.join(directory, 'superset_IS2_lakes_sum.csv')\n",
    "        superset_IS2_lakes_sum.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8476ed5e-f7d9-43a2-8e21-e11ffbf0bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def even_year_formatter(x, pos):\n",
    "    '''\n",
    "    Create custom formatter that only labels even years\n",
    "    '''\n",
    "    date = mdates.num2date(x)\n",
    "    if date.year % 2 == 0:\n",
    "        return date.strftime('%Y')\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5e0e1c-b26e-4ee3-87eb-ceea82bc5d73",
   "metadata": {},
   "source": [
    "# Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48d1542f-fb79-4965-a13f-3c84a3fa69fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import subglacial lake outlines \n",
    "stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/stationary_outlines_gdf.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1720b14b-ed5a-4870-b3f9-e9429a07e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CryoSat-2 SARIn mode mask\n",
    "# See 0_preprocess_data.ipynb for data source and pre-processing steps\n",
    "gdf_SARIn_3_1 = gpd.read_file('output/CS2_SARIn_mode_masks/gdf_SARIn_3_1.geojson')\n",
    "gdf_SARIn_3_1_3_6_diff= gpd.read_file('output/CS2_SARIn_mode_masks/gdf_SARIn_3_1_3_6_diff.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3c97b1e-bbd7-4f54-a26d-21d2aa3c9c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODIS MOA 2014 coastline and grounding line\n",
    "# https://nsidc.org/data/nsidc-0730/versions/1\n",
    "shp = DATA_DIR + '/boundaries/MODIS_MOA/2014/moa2014_coastline_v01.shp' \n",
    "moa_2014_coastline = gpd.read_file(shp)\n",
    "shp = DATA_DIR + '/boundaries/MODIS_MOA/2014/moa2014_grounding_line_v01.shp' \n",
    "moa_2014_groundingline = gpd.read_file(shp)\n",
    "# moa_2014_groundingline['geometry'] = moa_2014_groundingline.buffer(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2dc7100d-ceb8-441c-9fe2-5988420e3d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Authenticate with Earthdata Login\n",
    "# auth = earthaccess.login()\n",
    "\n",
    "# # Find MEaSUREs MODIS Mosaic of Antarctica 2013-2014 (MOA2014) Image Map, Version 1\n",
    "# results = earthaccess.search_data(\n",
    "#     doi='10.5067/RNF17BP824UM',\n",
    "#     version=1,\n",
    "#     # bounding_box=(180, -90, -180, -60),  # (lower_left_lon, lower_left_lat , upper_right_lon, upper_right_lat))\n",
    "#     cloud_hosted=True,\n",
    "# )\n",
    "\n",
    "# # Open data granules as s3 files to stream\n",
    "# files = earthaccess.open(results)\n",
    "\n",
    "# # Struggling to find stream; opting for local copy for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e44c162d-b4d2-419f-81cd-945373956006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MODIS Mosaic of Antarctica (MOA) surface imagery\n",
    "# https://nsidc.org/data/nsidc-0730/versions/1\n",
    "# Relocate to data_dir\n",
    "\n",
    "moa_highres = DATA_DIR + '/surface_imagery/MODIS_MOA/2014/moa125_2014_hp1_v01.tif'\n",
    "moa_highres_da = rioxarray.open_rasterio(moa_highres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7fd6faa-738f-4689-abe0-38324f66d6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "html[data-theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 21GB\n",
       "Dimensions:      (y: 4451, x: 5451, time: 35)\n",
       "Coordinates:\n",
       "  * y            (y) float64 36kB -2.185e+06 -2.184e+06 ... 2.264e+06 2.265e+06\n",
       "  * x            (x) float64 44kB -2.665e+06 -2.664e+06 ... 2.784e+06 2.785e+06\n",
       "    spatial_ref  int64 8B 0\n",
       "  * time         (time) datetime64[ns] 280B 2010-07-02T15:00:00 ... 2019-01-01\n",
       "Data variables:\n",
       "    mask         (y, x) float64 194MB ...\n",
       "    data_count   (time, y, x) float64 7GB ...\n",
       "    h            (time, y, x) float64 7GB ...\n",
       "    delta_h      (time, y, x) float64 7GB ...\n",
       "Attributes:\n",
       "    fileName:                mos_2010.5_2021.5.h5\n",
       "    shortName:               CS2-Smith-2017\n",
       "    identifier_product_DOI:  doi:10.5194/tc-11-451-2017</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-6a35be14-4a7b-4f63-8664-a0a72ae6a1d5' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-6a35be14-4a7b-4f63-8664-a0a72ae6a1d5' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>y</span>: 4451</li><li><span class='xr-has-index'>x</span>: 5451</li><li><span class='xr-has-index'>time</span>: 35</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-eb5a3eaa-b916-4d75-b45d-46da733c7a55' class='xr-section-summary-in' type='checkbox'  checked><label for='section-eb5a3eaa-b916-4d75-b45d-46da733c7a55' class='xr-section-summary' >Coordinates: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>y</span></div><div class='xr-var-dims'>(y)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-2.185e+06 -2.184e+06 ... 2.265e+06</div><input id='attrs-c59c8158-0df6-49d8-94c5-e3cef5348a54' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-c59c8158-0df6-49d8-94c5-e3cef5348a54' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-3137b2b7-1468-47c5-b3ba-c3ec26804100' class='xr-var-data-in' type='checkbox'><label for='data-3137b2b7-1468-47c5-b3ba-c3ec26804100' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([-2185000., -2184000., -2183000., ...,  2263000.,  2264000.,  2265000.])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>x</span></div><div class='xr-var-dims'>(x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-2.665e+06 -2.664e+06 ... 2.785e+06</div><input id='attrs-c6d95424-a216-4d84-ac69-fccd0fb54cb4' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-c6d95424-a216-4d84-ac69-fccd0fb54cb4' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-ccb65a7e-af3f-4906-950a-b7d8dc62058b' class='xr-var-data-in' type='checkbox'><label for='data-ccb65a7e-af3f-4906-950a-b7d8dc62058b' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([-2665000., -2664000., -2663000., ...,  2783000.,  2784000.,  2785000.])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>spatial_ref</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0</div><input id='attrs-ef123c49-b1ed-4e6a-8532-4a18e5bb67b3' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-ef123c49-b1ed-4e6a-8532-4a18e5bb67b3' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-135e0a7f-084b-4bbf-adb8-5554619469f2' class='xr-var-data-in' type='checkbox'><label for='data-135e0a7f-084b-4bbf-adb8-5554619469f2' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>crs_wkt :</span></dt><dd>PROJCS[&quot;WGS 84 / Antarctic Polar Stereographic&quot;,GEOGCS[&quot;WGS 84&quot;,DATUM[&quot;WGS_1984&quot;,SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563,AUTHORITY[&quot;EPSG&quot;,&quot;7030&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;6326&quot;]],PRIMEM[&quot;Greenwich&quot;,0,AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]],UNIT[&quot;degree&quot;,0.0174532925199433,AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]],PROJECTION[&quot;Polar_Stereographic&quot;],PARAMETER[&quot;latitude_of_origin&quot;,-71],PARAMETER[&quot;central_meridian&quot;,0],PARAMETER[&quot;false_easting&quot;,0],PARAMETER[&quot;false_northing&quot;,0],UNIT[&quot;metre&quot;,1,AUTHORITY[&quot;EPSG&quot;,&quot;9001&quot;]],AXIS[&quot;Easting&quot;,NORTH],AXIS[&quot;Northing&quot;,NORTH],AUTHORITY[&quot;EPSG&quot;,&quot;3031&quot;]]</dd><dt><span>semi_major_axis :</span></dt><dd>6378137.0</dd><dt><span>semi_minor_axis :</span></dt><dd>6356752.314245179</dd><dt><span>inverse_flattening :</span></dt><dd>298.257223563</dd><dt><span>reference_ellipsoid_name :</span></dt><dd>WGS 84</dd><dt><span>longitude_of_prime_meridian :</span></dt><dd>0.0</dd><dt><span>prime_meridian_name :</span></dt><dd>Greenwich</dd><dt><span>geographic_crs_name :</span></dt><dd>WGS 84</dd><dt><span>horizontal_datum_name :</span></dt><dd>World Geodetic System 1984</dd><dt><span>projected_crs_name :</span></dt><dd>WGS 84 / Antarctic Polar Stereographic</dd><dt><span>grid_mapping_name :</span></dt><dd>polar_stereographic</dd><dt><span>standard_parallel :</span></dt><dd>-71.0</dd><dt><span>straight_vertical_longitude_from_pole :</span></dt><dd>0.0</dd><dt><span>false_easting :</span></dt><dd>0.0</dd><dt><span>false_northing :</span></dt><dd>0.0</dd><dt><span>spatial_ref :</span></dt><dd>PROJCS[&quot;WGS 84 / Antarctic Polar Stereographic&quot;,GEOGCS[&quot;WGS 84&quot;,DATUM[&quot;WGS_1984&quot;,SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563,AUTHORITY[&quot;EPSG&quot;,&quot;7030&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;6326&quot;]],PRIMEM[&quot;Greenwich&quot;,0,AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]],UNIT[&quot;degree&quot;,0.0174532925199433,AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]],PROJECTION[&quot;Polar_Stereographic&quot;],PARAMETER[&quot;latitude_of_origin&quot;,-71],PARAMETER[&quot;central_meridian&quot;,0],PARAMETER[&quot;false_easting&quot;,0],PARAMETER[&quot;false_northing&quot;,0],UNIT[&quot;metre&quot;,1,AUTHORITY[&quot;EPSG&quot;,&quot;9001&quot;]],AXIS[&quot;Easting&quot;,NORTH],AXIS[&quot;Northing&quot;,NORTH],AUTHORITY[&quot;EPSG&quot;,&quot;3031&quot;]]</dd></dl></div><div class='xr-var-data'><pre>array(0)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2010-07-02T15:00:00 ... 2019-01-01</div><input id='attrs-fb266c58-22b6-4c5b-82dd-e89c96074a57' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-fb266c58-22b6-4c5b-82dd-e89c96074a57' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-cda9bf8e-b6e2-4d88-9a1b-fb1e52909883' class='xr-var-data-in' type='checkbox'><label for='data-cda9bf8e-b6e2-4d88-9a1b-fb1e52909883' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>description :</span></dt><dd>Time for each node</dd></dl></div><div class='xr-var-data'><pre>array([&#x27;2010-07-02T15:00:00.000000000&#x27;, &#x27;2010-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2011-01-01T00:00:00.000000000&#x27;, &#x27;2011-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2011-07-02T15:00:00.000000000&#x27;, &#x27;2011-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2012-01-01T00:00:00.000000000&#x27;, &#x27;2012-04-01T07:30:00.000000000&#x27;,\n",
       "       &#x27;2012-07-01T15:00:00.000000000&#x27;, &#x27;2012-09-30T22:30:00.000000000&#x27;,\n",
       "       &#x27;2013-01-01T00:00:00.000000000&#x27;, &#x27;2013-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2013-07-02T15:00:00.000000000&#x27;, &#x27;2013-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2014-01-01T00:00:00.000000000&#x27;, &#x27;2014-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2014-07-02T15:00:00.000000000&#x27;, &#x27;2014-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2015-01-01T00:00:00.000000000&#x27;, &#x27;2015-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2015-07-02T15:00:00.000000000&#x27;, &#x27;2015-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2016-01-01T00:00:00.000000000&#x27;, &#x27;2016-04-01T07:30:00.000000000&#x27;,\n",
       "       &#x27;2016-07-01T15:00:00.000000000&#x27;, &#x27;2016-09-30T22:30:00.000000000&#x27;,\n",
       "       &#x27;2017-01-01T00:00:00.000000000&#x27;, &#x27;2017-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2017-07-02T15:00:00.000000000&#x27;, &#x27;2017-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2018-01-01T00:00:00.000000000&#x27;, &#x27;2018-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2018-07-02T15:00:00.000000000&#x27;, &#x27;2018-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2019-01-01T00:00:00.000000000&#x27;], dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-afa93ea6-011d-46ca-a345-3ebdecae88e1' class='xr-section-summary-in' type='checkbox'  checked><label for='section-afa93ea6-011d-46ca-a345-3ebdecae88e1' class='xr-section-summary' >Data variables: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>mask</span></div><div class='xr-var-dims'>(y, x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-8a50ba3b-ccb2-4d85-a9dd-aaa429eead10' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-8a50ba3b-ccb2-4d85-a9dd-aaa429eead10' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-f73c7f82-6c0a-44ca-b459-541c31d86786' class='xr-var-data-in' type='checkbox'><label for='data-f73c7f82-6c0a-44ca-b459-541c31d86786' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>description: :</span></dt><dd>Data mask. 0: unknown, 1: unknown, nan: nan</dd></dl></div><div class='xr-var-data'><pre>[24262401 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>data_count</span></div><div class='xr-var-dims'>(time, y, x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-0812afea-215e-4a64-a4ec-bfa90bac025c' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-0812afea-215e-4a64-a4ec-bfa90bac025c' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-2716c4e3-343c-4205-9dcc-c1af4fe15b8e' class='xr-var-data-in' type='checkbox'><label for='data-2716c4e3-343c-4205-9dcc-c1af4fe15b8e' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>description: :</span></dt><dd>Data mask. 0: bare ground or ocean?, 1: ice?, nan: nan</dd></dl></div><div class='xr-var-data'><pre>[849184035 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>h</span></div><div class='xr-var-dims'>(time, y, x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-16d51848-0b78-402f-8f30-0ea59d3ca843' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-16d51848-0b78-402f-8f30-0ea59d3ca843' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-6ea4b000-d7ed-448a-90b9-53b7f8d186bb' class='xr-var-data-in' type='checkbox'><label for='data-6ea4b000-d7ed-448a-90b9-53b7f8d186bb' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[849184035 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>delta_h</span></div><div class='xr-var-dims'>(time, y, x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-092e7d6d-d8cf-4965-8a01-155ecde9c393' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-092e7d6d-d8cf-4965-8a01-155ecde9c393' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-8b72f2ef-ba15-4b97-b246-5dc6f97d7d34' class='xr-var-data-in' type='checkbox'><label for='data-8b72f2ef-ba15-4b97-b246-5dc6f97d7d34' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>description :</span></dt><dd>Height change relative to the ATL14 datum (Jan 1, 2020) surface</dd></dl></div><div class='xr-var-data'><pre>[849184035 values with dtype=float64]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-f90dd960-590d-4751-a01d-9231a00f3c02' class='xr-section-summary-in' type='checkbox'  ><label for='section-f90dd960-590d-4751-a01d-9231a00f3c02' class='xr-section-summary' >Indexes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>y</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-094fdf85-97c0-45ac-a231-3ec140fa4d23' class='xr-index-data-in' type='checkbox'/><label for='index-094fdf85-97c0-45ac-a231-3ec140fa4d23' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([-2185000.0, -2184000.0, -2183000.0, -2182000.0, -2181000.0, -2180000.0,\n",
       "       -2179000.0, -2178000.0, -2177000.0, -2176000.0,\n",
       "       ...\n",
       "        2256000.0,  2257000.0,  2258000.0,  2259000.0,  2260000.0,  2261000.0,\n",
       "        2262000.0,  2263000.0,  2264000.0,  2265000.0],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;y&#x27;, length=4451))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>x</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-c34989a5-02e5-48b6-bc11-9c07af54acd3' class='xr-index-data-in' type='checkbox'/><label for='index-c34989a5-02e5-48b6-bc11-9c07af54acd3' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([-2665000.0, -2664000.0, -2663000.0, -2662000.0, -2661000.0, -2660000.0,\n",
       "       -2659000.0, -2658000.0, -2657000.0, -2656000.0,\n",
       "       ...\n",
       "        2776000.0,  2777000.0,  2778000.0,  2779000.0,  2780000.0,  2781000.0,\n",
       "        2782000.0,  2783000.0,  2784000.0,  2785000.0],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;x&#x27;, length=5451))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>time</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-912e637a-b5ad-454c-a657-8091999a0702' class='xr-index-data-in' type='checkbox'/><label for='index-912e637a-b5ad-454c-a657-8091999a0702' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(DatetimeIndex([&#x27;2010-07-02 15:00:00&#x27;, &#x27;2010-10-01 22:30:00&#x27;,\n",
       "               &#x27;2011-01-01 00:00:00&#x27;, &#x27;2011-04-02 07:30:00&#x27;,\n",
       "               &#x27;2011-07-02 15:00:00&#x27;, &#x27;2011-10-01 22:30:00&#x27;,\n",
       "               &#x27;2012-01-01 00:00:00&#x27;, &#x27;2012-04-01 07:30:00&#x27;,\n",
       "               &#x27;2012-07-01 15:00:00&#x27;, &#x27;2012-09-30 22:30:00&#x27;,\n",
       "               &#x27;2013-01-01 00:00:00&#x27;, &#x27;2013-04-02 07:30:00&#x27;,\n",
       "               &#x27;2013-07-02 15:00:00&#x27;, &#x27;2013-10-01 22:30:00&#x27;,\n",
       "               &#x27;2014-01-01 00:00:00&#x27;, &#x27;2014-04-02 07:30:00&#x27;,\n",
       "               &#x27;2014-07-02 15:00:00&#x27;, &#x27;2014-10-01 22:30:00&#x27;,\n",
       "               &#x27;2015-01-01 00:00:00&#x27;, &#x27;2015-04-02 07:30:00&#x27;,\n",
       "               &#x27;2015-07-02 15:00:00&#x27;, &#x27;2015-10-01 22:30:00&#x27;,\n",
       "               &#x27;2016-01-01 00:00:00&#x27;, &#x27;2016-04-01 07:30:00&#x27;,\n",
       "               &#x27;2016-07-01 15:00:00&#x27;, &#x27;2016-09-30 22:30:00&#x27;,\n",
       "               &#x27;2017-01-01 00:00:00&#x27;, &#x27;2017-04-02 07:30:00&#x27;,\n",
       "               &#x27;2017-07-02 15:00:00&#x27;, &#x27;2017-10-01 22:30:00&#x27;,\n",
       "               &#x27;2018-01-01 00:00:00&#x27;, &#x27;2018-04-02 07:30:00&#x27;,\n",
       "               &#x27;2018-07-02 15:00:00&#x27;, &#x27;2018-10-01 22:30:00&#x27;,\n",
       "               &#x27;2019-01-01 00:00:00&#x27;],\n",
       "              dtype=&#x27;datetime64[ns]&#x27;, name=&#x27;time&#x27;, freq=None))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-9cf0d5e0-c105-498f-b5e5-126f618591b9' class='xr-section-summary-in' type='checkbox'  checked><label for='section-9cf0d5e0-c105-498f-b5e5-126f618591b9' class='xr-section-summary' >Attributes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>fileName :</span></dt><dd>mos_2010.5_2021.5.h5</dd><dt><span>shortName :</span></dt><dd>CS2-Smith-2017</dd><dt><span>identifier_product_DOI :</span></dt><dd>doi:10.5194/tc-11-451-2017</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset> Size: 21GB\n",
       "Dimensions:      (y: 4451, x: 5451, time: 35)\n",
       "Coordinates:\n",
       "  * y            (y) float64 36kB -2.185e+06 -2.184e+06 ... 2.264e+06 2.265e+06\n",
       "  * x            (x) float64 44kB -2.665e+06 -2.664e+06 ... 2.784e+06 2.785e+06\n",
       "    spatial_ref  int64 8B 0\n",
       "  * time         (time) datetime64[ns] 280B 2010-07-02T15:00:00 ... 2019-01-01\n",
       "Data variables:\n",
       "    mask         (y, x) float64 194MB ...\n",
       "    data_count   (time, y, x) float64 7GB ...\n",
       "    h            (time, y, x) float64 7GB ...\n",
       "    delta_h      (time, y, x) float64 7GB ...\n",
       "Attributes:\n",
       "    fileName:                mos_2010.5_2021.5.h5\n",
       "    shortName:               CS2-Smith-2017\n",
       "    identifier_product_DOI:  doi:10.5194/tc-11-451-2017"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Smith and others, 2017, TC method CryoSat-2 SARIn height and dheight data (closed source aquired from Ben Smith)\n",
    "CS2_Smith2017 = xr.open_dataset(DATA_DIR + '/altimetry/CryoSat2/CS2_SARIn_Smith2017method/mos_2010.5_2019.0_relative_to_ATL14_v4.nc')\n",
    "\n",
    "# Assign CRS\n",
    "CS2_Smith2017.rio.write_crs(\"EPSG:3031\", inplace=True)\n",
    "CS2_Smith2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d5dd4c7-cb4d-4e98-a6e5-2396278c2647",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9ef561241844eebb45ddef34e86aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13150f5dd47649d3a1a9f8568bab6551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6b8d2810284eb7b192b7ffddfbb843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Authenticate with Earthdata Login\n",
    "# auth = earthaccess.login()\n",
    "earthaccess.login()\n",
    "\n",
    "# Find ICESat-2 ATL15 v004 data granules\n",
    "results = earthaccess.search_data(\n",
    "    doi='10.5067/ATLAS/ATL15.004',\n",
    "    bounding_box=(180, -90, -180, -60),  # (lower_left_lon, lower_left_lat , upper_right_lon, upper_right_lat))\n",
    "    cloud_hosted=True,\n",
    ")\n",
    "\n",
    "# Open data granules as s3 files to stream\n",
    "files = earthaccess.open(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21ffb0a5-9651-474e-b47b-02340c93c87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A1_0324_01km_004_04.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A2_0324_01km_004_04.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A3_0324_01km_004_04.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A4_0324_01km_004_04.nc>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter to 1-km resolution data sets\n",
    "filtered_files = [f for f in files if '01km' in str(f)]\n",
    "\n",
    "# Filter to the specific revision number\n",
    "filtered_files = [f for f in filtered_files if '_04' in str(f)]\n",
    "\n",
    "# Delete intermediary objects for memory conservation\n",
    "del results, files\n",
    "\n",
    "# Sort alphabetically by the data set file name\n",
    "filtered_files.sort(key=lambda x: str(x).split('/')[-1])\n",
    "\n",
    "# Display filtered list\n",
    "filtered_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69c4b4f9-6096-40de-92bb-12a1d9688890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Specify the variables to keep\n",
    "# variables_to_keep = ['time', 'y', 'x', 'delta_h', 'data_count']\n",
    "\n",
    "# # List of xarray datasets\n",
    "# datasets = [ATL15_A1, ATL15_A2, ATL15_A3, ATL15_A4]\n",
    "\n",
    "# # Function to drop variables not in variables_to_keep from a dataset\n",
    "# def drop_unwanted_variables(dataset):\n",
    "#     variables_to_drop = [var for var in dataset.variables if var not in variables_to_keep]\n",
    "#     return dataset.drop_vars(variables_to_drop)\n",
    "\n",
    "# # Apply the function to each dataset\n",
    "# ATL15_A1, ATL15_A2, ATL15_A3, ATL15_A4 = [drop_unwanted_variables(ds) for ds in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dae55ad9-3130-429d-af7c-bab068eaa9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset 1/4: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A1_0324_01km_004_04.nc>\n",
      "Opening file: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A1_0324_01km_004_04.nc> (Attempt 1)\n",
      "Successfully opened file: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A1_0324_01km_004_04.nc>\n",
      "Dropping unwanted variables from: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A1_0324_01km_004_04.nc>\n",
      "Finished processing dataset 1/4\n",
      "Processing dataset 2/4: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A2_0324_01km_004_04.nc>\n",
      "Opening file: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A2_0324_01km_004_04.nc> (Attempt 1)\n",
      "Successfully opened file: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A2_0324_01km_004_04.nc>\n",
      "Dropping unwanted variables from: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A2_0324_01km_004_04.nc>\n",
      "Finished processing dataset 2/4\n",
      "Processing dataset 3/4: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A3_0324_01km_004_04.nc>\n",
      "Opening file: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A3_0324_01km_004_04.nc> (Attempt 1)\n",
      "Successfully opened file: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A3_0324_01km_004_04.nc>\n",
      "Dropping unwanted variables from: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A3_0324_01km_004_04.nc>\n",
      "Finished processing dataset 3/4\n",
      "Processing dataset 4/4: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A4_0324_01km_004_04.nc>\n",
      "Opening file: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A4_0324_01km_004_04.nc> (Attempt 1)\n",
      "Successfully opened file: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A4_0324_01km_004_04.nc>\n",
      "Dropping unwanted variables from: <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A4_0324_01km_004_04.nc>\n",
      "Finished processing dataset 4/4\n",
      "Dataset assigned to variable: ATL15_A1\n",
      "Dataset assigned to variable: ATL15_A2\n",
      "Dataset assigned to variable: ATL15_A3\n",
      "Dataset assigned to variable: ATL15_A4\n",
      "All datasets processed and assigned.\n"
     ]
    }
   ],
   "source": [
    "# Specify the variables to keep\n",
    "variables_to_keep = ['time', 'y', 'x', 'delta_h', 'data_count']\n",
    "\n",
    "# List of filtered file paths (output of the filtered_files)\n",
    "datasets_files = filtered_files\n",
    "\n",
    "# Function to drop variables not in variables_to_keep from a dataset\n",
    "def drop_unwanted_variables(dataset):\n",
    "    variables_to_drop = [var for var in dataset.variables if var not in variables_to_keep]\n",
    "    return dataset.drop_vars(variables_to_drop)\n",
    "\n",
    "# Function to open dataset safely with retries\n",
    "def safe_open_and_filter(file, group='delta_h', retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            print(f\"Opening file: {file} (Attempt {attempt + 1})\")\n",
    "            ds = xr.open_dataset(file, group=group)\n",
    "            print(f\"Successfully opened file: {file}\")\n",
    "            # Drop unwanted variables\n",
    "            print(f\"Dropping unwanted variables from: {file}\")\n",
    "            return drop_unwanted_variables(ds)\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt+1} for file {file} failed: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                print(\"Retrying...\")\n",
    "            else:\n",
    "                print(f\"Failed to open file: {file} after {retries} attempts.\")\n",
    "                raise e\n",
    "\n",
    "# Dynamically open, retry, and filter datasets\n",
    "datasets = []\n",
    "for i, file in enumerate(datasets_files):\n",
    "    print(f\"Processing dataset {i+1}/{len(datasets_files)}: {file}\")\n",
    "    datasets.append(safe_open_and_filter(file))\n",
    "    print(f\"Finished processing dataset {i+1}/{len(datasets_files)}\")\n",
    "\n",
    "# Assign filtered datasets to variables dynamically\n",
    "for i, ds in enumerate(datasets):\n",
    "    dataset_name = f\"ATL15_A{i+1}\"\n",
    "    globals()[dataset_name] = ds\n",
    "    print(f\"Dataset assigned to variable: {dataset_name}\")\n",
    "\n",
    "# Check datasets (optional)\n",
    "print(\"All datasets processed and assigned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da23c068-f494-45bc-814e-e76e22b4729b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Open locally stored files when NSIDC cloud access isn't working\n",
    "# ATL15_A1 = xr.open_dataset(DATA_DIR + '/altimetry/ICESat2/ATL15.004-Ant/ATL15_A1_0322_01km_004_02.nc', group='delta_h')\n",
    "# ATL15_A2 = xr.open_dataset(DATA_DIR + '/altimetry/ICESat2/ATL15.004-Ant/ATL15_A2_0322_01km_004_02.nc', group='delta_h')\n",
    "# ATL15_A3 = xr.open_dataset(DATA_DIR + '/altimetry/ICESat2/ATL15.004-Ant/ATL15_A3_0322_01km_004_02.nc', group='delta_h')\n",
    "# ATL15_A4 = xr.open_dataset(DATA_DIR + '/altimetry/ICESat2/ATL15.004-Ant/ATL15_A4_0322_01km_004_02.nc', group='delta_h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f875fb41-ec6f-4e03-a659-e7e7273b98df",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use xarray concatenation to stitch two quadrants togethers\n",
    "# Use xarray index selecting to occlude the duplicated x=0 vector of data\n",
    "ATL15_A12 = xr.concat([ATL15_A2.isel(x=slice(0,-1)), ATL15_A1], dim=\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "372eec1e-3eb5-42d8-94ee-a2de74316803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use xarray concatenation to stitch two quadrants togethers\n",
    "# Use xarray index selecting to occlude the duplicated x=0 vector of data\n",
    "ATL15_A34 = xr.concat([ATL15_A3.isel(x=slice(0,-1)), ATL15_A4], dim='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d14e491-fd5e-424b-ae91-75ba59381ec2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use xarray concatenation to stitch two quadrants togethers\n",
    "# Use xarray index selecting to occlude the duplicated x=0 vector of data\n",
    "ATL15_dh = xr.concat([ATL15_A34.isel(y=slice(0,-1)), ATL15_A12], dim='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d62934f-4e13-404d-b2b3-4338f498cfd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete variables to reduce memory consumption\n",
    "del ATL15_A1, ATL15_A12, ATL15_A2, ATL15_A3, ATL15_A34, ATL15_A4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e80b88e5-6957-4f63-b9d9-70591d18af2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "html[data-theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 5GB\n",
       "Dimensions:     (time: 24, y: 4461, x: 5461)\n",
       "Coordinates:\n",
       "  * time        (time) datetime64[ns] 192B 2019-01-01T06:00:00 ... 2024-10-01...\n",
       "  * x           (x) float64 44kB -2.67e+06 -2.669e+06 ... 2.789e+06 2.79e+06\n",
       "  * y           (y) float64 36kB -2.19e+06 -2.189e+06 ... 2.269e+06 2.27e+06\n",
       "Data variables:\n",
       "    delta_h     (time, y, x) float32 2GB nan nan nan nan nan ... nan nan nan nan\n",
       "    data_count  (time, y, x) float32 2GB nan nan nan nan nan ... nan nan nan nan\n",
       "Attributes:\n",
       "    description:             delta_h group includes variables describing heig...\n",
       "    identifier_product_DOI:  doi:10.5067/ATLAS/ATL15.004\n",
       "    shortName:               ATL15</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-440a36e6-7d5a-4460-9c36-651559caca71' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-440a36e6-7d5a-4460-9c36-651559caca71' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>time</span>: 24</li><li><span class='xr-has-index'>y</span>: 4461</li><li><span class='xr-has-index'>x</span>: 5461</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-0e55362b-6478-48cb-8389-fb7f4bcb6830' class='xr-section-summary-in' type='checkbox'  checked><label for='section-0e55362b-6478-48cb-8389-fb7f4bcb6830' class='xr-section-summary' >Coordinates: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2019-01-01T06:00:00 ... 2024-10-...</div><input id='attrs-61ad8e2f-e02b-44a8-87f6-7e204aa5f3ff' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-61ad8e2f-e02b-44a8-87f6-7e204aa5f3ff' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-df8bcb89-8112-4cb6-afce-9da653cf52e3' class='xr-var-data-in' type='checkbox'><label for='data-df8bcb89-8112-4cb6-afce-9da653cf52e3' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>dimensions :</span></dt><dd>time</dd><dt><span>datatype :</span></dt><dd>float64</dd><dt><span>least_significant_digit :</span></dt><dd>None</dd><dt><span>description :</span></dt><dd>Time for each node, in days since 2018-01-01:T00.00.00 UTC</dd><dt><span>long_name :</span></dt><dd>quarterly h(t) time</dd><dt><span>source :</span></dt><dd>ATBD section 4.2</dd></dl></div><div class='xr-var-data'><pre>array([&#x27;2019-01-01T06:00:00.000000000&#x27;, &#x27;2019-04-02T13:30:00.000000000&#x27;,\n",
       "       &#x27;2019-07-02T21:00:00.000000000&#x27;, &#x27;2019-10-02T04:30:00.000000000&#x27;,\n",
       "       &#x27;2020-01-01T12:00:00.000000000&#x27;, &#x27;2020-04-01T19:30:00.000000000&#x27;,\n",
       "       &#x27;2020-07-02T03:00:00.000000000&#x27;, &#x27;2020-10-01T10:30:00.000000000&#x27;,\n",
       "       &#x27;2020-12-31T18:00:00.000000000&#x27;, &#x27;2021-04-02T01:30:00.000000000&#x27;,\n",
       "       &#x27;2021-07-02T09:00:00.000000000&#x27;, &#x27;2021-10-01T16:30:00.000000000&#x27;,\n",
       "       &#x27;2022-01-01T00:00:00.000000000&#x27;, &#x27;2022-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2022-07-02T15:00:00.000000000&#x27;, &#x27;2022-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2023-01-01T06:00:00.000000000&#x27;, &#x27;2023-04-02T13:30:00.000000000&#x27;,\n",
       "       &#x27;2023-07-02T21:00:00.000000000&#x27;, &#x27;2023-10-02T04:30:00.000000000&#x27;,\n",
       "       &#x27;2024-01-01T12:00:00.000000000&#x27;, &#x27;2024-04-01T19:30:00.000000000&#x27;,\n",
       "       &#x27;2024-07-02T03:00:00.000000000&#x27;, &#x27;2024-10-01T10:30:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>x</span></div><div class='xr-var-dims'>(x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-2.67e+06 -2.669e+06 ... 2.79e+06</div><input id='attrs-26e96edd-56bf-477a-a3a1-07660f7da3b4' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-26e96edd-56bf-477a-a3a1-07660f7da3b4' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-34b6c67b-b1d1-4518-a5c1-3d4b07ebc054' class='xr-var-data-in' type='checkbox'><label for='data-34b6c67b-b1d1-4518-a5c1-3d4b07ebc054' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>meters</dd><dt><span>dimensions :</span></dt><dd>x</dd><dt><span>datatype :</span></dt><dd>float64</dd><dt><span>least_significant_digit :</span></dt><dd>None</dd><dt><span>description :</span></dt><dd>x coordinate of the 1-km cell centers, in projected coordinates</dd><dt><span>long_name :</span></dt><dd>polar stereographic x at 1km</dd><dt><span>source :</span></dt><dd>ATBD section 3.2</dd><dt><span>grid_mapping :</span></dt><dd>Polar_Stereographic</dd><dt><span>standard_name :</span></dt><dd>projection_x_coordinate</dd></dl></div><div class='xr-var-data'><pre>array([-2670000., -2669000., -2668000., ...,  2788000.,  2789000.,  2790000.])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>y</span></div><div class='xr-var-dims'>(y)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-2.19e+06 -2.189e+06 ... 2.27e+06</div><input id='attrs-9fb5698b-6546-4b0b-a5de-7e78863f1a4c' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-9fb5698b-6546-4b0b-a5de-7e78863f1a4c' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-b48cc947-b482-4ce0-8934-beec1810f2f8' class='xr-var-data-in' type='checkbox'><label for='data-b48cc947-b482-4ce0-8934-beec1810f2f8' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>meters</dd><dt><span>dimensions :</span></dt><dd>y</dd><dt><span>datatype :</span></dt><dd>float64</dd><dt><span>least_significant_digit :</span></dt><dd>None</dd><dt><span>description :</span></dt><dd>y coordinate of the 1-km cell centers, in projected coordinates</dd><dt><span>long_name :</span></dt><dd>polar stereographic y at 1km</dd><dt><span>source :</span></dt><dd>ATBD section 3.2</dd><dt><span>grid_mapping :</span></dt><dd>Polar_Stereographic</dd><dt><span>standard_name :</span></dt><dd>projection_y_coordinate</dd></dl></div><div class='xr-var-data'><pre>array([-2190000., -2189000., -2188000., ...,  2268000.,  2269000.,  2270000.])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-2ebf49ce-c3cc-4dca-81d6-924b9f42912d' class='xr-section-summary-in' type='checkbox'  checked><label for='section-2ebf49ce-c3cc-4dca-81d6-924b9f42912d' class='xr-section-summary' >Data variables: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>delta_h</span></div><div class='xr-var-dims'>(time, y, x)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>nan nan nan nan ... nan nan nan nan</div><input id='attrs-b84e488f-7186-46e0-92f1-f07a8f5a49b6' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-b84e488f-7186-46e0-92f1-f07a8f5a49b6' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-b82ea3cb-4a4f-46dc-a97e-507c8b41321f' class='xr-var-data-in' type='checkbox'><label for='data-b82ea3cb-4a4f-46dc-a97e-507c8b41321f' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>least_significant_digit :</span></dt><dd>4</dd><dt><span>units :</span></dt><dd>meters</dd><dt><span>dimensions :</span></dt><dd>time,y,x</dd><dt><span>datatype :</span></dt><dd>float32</dd><dt><span>description :</span></dt><dd>Height change relative to the datum (Jan 1, 2020) surface</dd><dt><span>long_name :</span></dt><dd>height change  at 1 km</dd><dt><span>source :</span></dt><dd>ATBD section 3.4</dd><dt><span>grid_mapping :</span></dt><dd>Polar_Stereographic</dd></dl></div><div class='xr-var-data'><pre>array([[[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "...\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>data_count</span></div><div class='xr-var-dims'>(time, y, x)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>nan nan nan nan ... nan nan nan nan</div><input id='attrs-bbd2366d-49b6-45f0-b9fc-cd9918857565' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-bbd2366d-49b6-45f0-b9fc-cd9918857565' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-214b8342-753c-40d2-967f-a2e09725de79' class='xr-var-data-in' type='checkbox'><label for='data-214b8342-753c-40d2-967f-a2e09725de79' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>least_significant_digit :</span></dt><dd>4</dd><dt><span>units :</span></dt><dd>counts</dd><dt><span>dimensions :</span></dt><dd>time,y,x</dd><dt><span>datatype :</span></dt><dd>float32</dd><dt><span>description :</span></dt><dd>Weighted number of data contributing to each node in the 1-km height-change grid</dd><dt><span>long_name :</span></dt><dd>data count </dd><dt><span>source :</span></dt><dd>ATBD section 5.2.4.4</dd><dt><span>grid_mapping :</span></dt><dd>Polar_Stereographic</dd></dl></div><div class='xr-var-data'><pre>array([[[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "...\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-8ed6df85-e701-46f6-b7e0-a2731f15fc7f' class='xr-section-summary-in' type='checkbox'  ><label for='section-8ed6df85-e701-46f6-b7e0-a2731f15fc7f' class='xr-section-summary' >Indexes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>time</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-3c879d1d-0553-4fb7-91c1-8304eb54a61a' class='xr-index-data-in' type='checkbox'/><label for='index-3c879d1d-0553-4fb7-91c1-8304eb54a61a' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(DatetimeIndex([&#x27;2019-01-01 06:00:00&#x27;, &#x27;2019-04-02 13:30:00&#x27;,\n",
       "               &#x27;2019-07-02 21:00:00&#x27;, &#x27;2019-10-02 04:30:00&#x27;,\n",
       "               &#x27;2020-01-01 12:00:00&#x27;, &#x27;2020-04-01 19:30:00&#x27;,\n",
       "               &#x27;2020-07-02 03:00:00&#x27;, &#x27;2020-10-01 10:30:00&#x27;,\n",
       "               &#x27;2020-12-31 18:00:00&#x27;, &#x27;2021-04-02 01:30:00&#x27;,\n",
       "               &#x27;2021-07-02 09:00:00&#x27;, &#x27;2021-10-01 16:30:00&#x27;,\n",
       "               &#x27;2022-01-01 00:00:00&#x27;, &#x27;2022-04-02 07:30:00&#x27;,\n",
       "               &#x27;2022-07-02 15:00:00&#x27;, &#x27;2022-10-01 22:30:00&#x27;,\n",
       "               &#x27;2023-01-01 06:00:00&#x27;, &#x27;2023-04-02 13:30:00&#x27;,\n",
       "               &#x27;2023-07-02 21:00:00&#x27;, &#x27;2023-10-02 04:30:00&#x27;,\n",
       "               &#x27;2024-01-01 12:00:00&#x27;, &#x27;2024-04-01 19:30:00&#x27;,\n",
       "               &#x27;2024-07-02 03:00:00&#x27;, &#x27;2024-10-01 10:30:00&#x27;],\n",
       "              dtype=&#x27;datetime64[ns]&#x27;, name=&#x27;time&#x27;, freq=None))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>x</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-512c774b-7375-4d97-b286-224db5ce8ae9' class='xr-index-data-in' type='checkbox'/><label for='index-512c774b-7375-4d97-b286-224db5ce8ae9' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([-2670000.0, -2669000.0, -2668000.0, -2667000.0, -2666000.0, -2665000.0,\n",
       "       -2664000.0, -2663000.0, -2662000.0, -2661000.0,\n",
       "       ...\n",
       "        2781000.0,  2782000.0,  2783000.0,  2784000.0,  2785000.0,  2786000.0,\n",
       "        2787000.0,  2788000.0,  2789000.0,  2790000.0],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;x&#x27;, length=5461))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>y</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-20ec86dd-4cdc-4633-94f2-7812eee7e14a' class='xr-index-data-in' type='checkbox'/><label for='index-20ec86dd-4cdc-4633-94f2-7812eee7e14a' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([-2190000.0, -2189000.0, -2188000.0, -2187000.0, -2186000.0, -2185000.0,\n",
       "       -2184000.0, -2183000.0, -2182000.0, -2181000.0,\n",
       "       ...\n",
       "        2261000.0,  2262000.0,  2263000.0,  2264000.0,  2265000.0,  2266000.0,\n",
       "        2267000.0,  2268000.0,  2269000.0,  2270000.0],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;y&#x27;, length=4461))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-84dfa999-5876-4ca7-970a-f5b1f83bf152' class='xr-section-summary-in' type='checkbox'  checked><label for='section-84dfa999-5876-4ca7-970a-f5b1f83bf152' class='xr-section-summary' >Attributes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>description :</span></dt><dd>delta_h group includes variables describing height differences between the model surface at any time and the DEM surface at a resolution of 1 km.</dd><dt><span>identifier_product_DOI :</span></dt><dd>doi:10.5067/ATLAS/ATL15.004</dd><dt><span>shortName :</span></dt><dd>ATL15</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset> Size: 5GB\n",
       "Dimensions:     (time: 24, y: 4461, x: 5461)\n",
       "Coordinates:\n",
       "  * time        (time) datetime64[ns] 192B 2019-01-01T06:00:00 ... 2024-10-01...\n",
       "  * x           (x) float64 44kB -2.67e+06 -2.669e+06 ... 2.789e+06 2.79e+06\n",
       "  * y           (y) float64 36kB -2.19e+06 -2.189e+06 ... 2.269e+06 2.27e+06\n",
       "Data variables:\n",
       "    delta_h     (time, y, x) float32 2GB nan nan nan nan nan ... nan nan nan nan\n",
       "    data_count  (time, y, x) float32 2GB nan nan nan nan nan ... nan nan nan nan\n",
       "Attributes:\n",
       "    description:             delta_h group includes variables describing heig...\n",
       "    identifier_product_DOI:  doi:10.5067/ATLAS/ATL15.004\n",
       "    shortName:               ATL15"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add datasets attributes\n",
    "ATL15_dh.attrs['identifier_product_DOI'] = 'doi:10.5067/ATLAS/ATL15.004'\n",
    "ATL15_dh.attrs['shortName'] = 'ATL15'\n",
    "\n",
    "# View data set\n",
    "ATL15_dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b0bae96-66b2-4c2e-8c49-5623c3fde835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scripps Grounding Line\n",
    "# https://doi.pangaea.de/10.1594/PANGAEA.819147\n",
    "Scripps_gl = gpd.read_file(DATA_DIR + \n",
    "    '/boundaries/Depoorter2013/Antarctica_masks/scripps_antarctica_polygons_v1.shp')\n",
    "\n",
    "# Isolate only land ice\n",
    "Scripps_landice = Scripps_gl[Scripps_gl['Id_text'] == 'Grounded ice or land']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df19fcb6-5ea0-4d70-946d-53e960a9bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip altimetry datasets to grounding line to limit analysis to below grounded ice\n",
    "CS2_Smith2017.rio.write_crs(3031, inplace=True)\n",
    "CS2_Smith2017 = CS2_Smith2017.rio.clip(Scripps_landice.geometry.values, Scripps_landice.crs, drop=False)\n",
    "ATL15_dh.rio.write_crs(3031, inplace=True)\n",
    "ATL15_dh = ATL15_dh.rio.clip(Scripps_landice.geometry.values, Scripps_landice.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "955c5399-d3c1-47b0-90df-ba9ecaa637a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cyc_start_dates</th>\n",
       "      <th>mid_cyc_dates</th>\n",
       "      <th>cyc_end_dates</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-07-02 15:00:00</td>\n",
       "      <td>2010-08-17 06:45:00</td>\n",
       "      <td>2010-10-01 22:30:00</td>\n",
       "      <td>CS2_Smith2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-10-01 22:30:00</td>\n",
       "      <td>2010-11-16 11:15:00</td>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>CS2_Smith2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>2011-02-15 15:45:00</td>\n",
       "      <td>2011-04-02 07:30:00</td>\n",
       "      <td>CS2_Smith2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-04-02 07:30:00</td>\n",
       "      <td>2011-05-17 23:15:00</td>\n",
       "      <td>2011-07-02 15:00:00</td>\n",
       "      <td>CS2_Smith2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-07-02 15:00:00</td>\n",
       "      <td>2011-08-17 06:45:00</td>\n",
       "      <td>2011-10-01 22:30:00</td>\n",
       "      <td>CS2_Smith2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cyc_start_dates       mid_cyc_dates       cyc_end_dates        dataset\n",
       "0 2010-07-02 15:00:00 2010-08-17 06:45:00 2010-10-01 22:30:00  CS2_Smith2017\n",
       "1 2010-10-01 22:30:00 2010-11-16 11:15:00 2011-01-01 00:00:00  CS2_Smith2017\n",
       "2 2011-01-01 00:00:00 2011-02-15 15:45:00 2011-04-02 07:30:00  CS2_Smith2017\n",
       "3 2011-04-02 07:30:00 2011-05-17 23:15:00 2011-07-02 15:00:00  CS2_Smith2017\n",
       "4 2011-07-02 15:00:00 2011-08-17 06:45:00 2011-10-01 22:30:00  CS2_Smith2017"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import mid_cyc_dates\n",
    "cyc_dates = pd.read_csv('output/cycle_dates.csv', parse_dates=['cyc_start_dates', 'mid_cyc_dates', 'cyc_end_dates'])\n",
    "\n",
    "# View dates\n",
    "cyc_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0ad65559-91e2-4419-86ca-66b37ac09980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store the cyc_dates columns as a np array with datetime64[ns] data type\n",
    "cyc_start_dates = [np.datetime64(ts) for ts in cyc_dates['cyc_start_dates']]\n",
    "mid_cyc_dates = [np.datetime64(ts) for ts in cyc_dates['mid_cyc_dates']]\n",
    "cyc_end_dates = [np.datetime64(ts) for ts in cyc_dates['cyc_end_dates']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596962e3-e1e3-47c9-b19e-62d368182982",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analysis of previously identified lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ebdf9a-2a12-4a78-aa9f-c72ced985c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find evolving outlines at optimal levels at various within_evaluation boundaries for each lake\n",
    "\n",
    "# Get list of remaining lakes left to process that don't have an output file in the folder_path\n",
    "folder_path = OUTPUT_DIR + '/levels'\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "remaining_lakes = filter_gdf_by_folder_contents(stationary_outlines_gdf, folder_path)\n",
    "\n",
    "if remaining_lakes.empty:\n",
    "    print(\"All lakes processed.\")\n",
    "else:\n",
    "    for i in range(len(remaining_lakes)):\n",
    "        print(len(remaining_lakes), 'lake(s) remain.')\n",
    "        # Process the lake\n",
    "        lake_gdf = remaining_lakes.iloc[0:1]\n",
    "        find_and_save_optimal_parameters(lake_gdf)\n",
    "        visualize_and_save_evolving_outlines(lake_gdf)\n",
    "\n",
    "        # Recheck which lakes still need processing\n",
    "        remaining_lakes = filter_gdf_by_folder_contents(stationary_outlines_gdf, folder_path)\n",
    "        \n",
    "        if remaining_lakes.empty:\n",
    "            print(\"All lakes processed.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2fc45-e536-4969-8082-d579f3c21892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry lakes with no evolving outlines found at greater within_area_multiples\n",
    "\n",
    "# Find evolving outlines at various search extents and levels for each lake\n",
    "\n",
    "# Get list of remaining lakes left to process that don't have an output file in the folder_path\n",
    "folder_path = OUTPUT_DIR + '/levels'\n",
    "remaining_lakes = filter_gdf_by_folder_contents(stationary_outlines_gdf, folder_path, exclude=False, file_extension='txt')\n",
    "\n",
    "if remaining_lakes.empty:\n",
    "    print(\"All lakes processed.\")\n",
    "else:\n",
    "    for i in range(len(remaining_lakes)):\n",
    "        print(len(remaining_lakes), 'lake(s) remain.')\n",
    "        \n",
    "        # Process the lake\n",
    "        lake_gdf = remaining_lakes.iloc[i:i+1]\n",
    "        find_and_save_optimal_parameters(lake_gdf, within_area_multiples=range(16, 21))\n",
    "        visualize_and_save_evolving_outlines(lake_gdf)\n",
    "\n",
    "        if remaining_lakes.empty:\n",
    "            print(\"All lakes processed.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bb5a49-3397-4460-9a78-503fb52994a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any \"no outlines\" TXT files when there is a levels CSV file in two directories\n",
    "cleanup_duplicate_files(directory_path=OUTPUT_DIR + '/levels',\n",
    "    keep_extension='csv', delete_extension='txt')\n",
    "\n",
    "cleanup_duplicate_files(directory_path='output/lake_outlines/evolving_outlines/',\n",
    "    keep_extension='geojson', delete_extension='txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c0a19-9713-45f5-8b37-308aa2ab7eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71468bef-565c-4ee1-bd71-281c249e8b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 lakes remain.\n",
      "Visualizing outlines for Institute_E1\n",
      "Parameters: row_index=0, within_area_multiple=14, level=0.31, doi(s)=doi:10.5194/tc-11-451-2017, doi:10.5067/ATLAS/ATL15.004\n",
      "Saved outlines to: /home/jovyan/1_evolving_lakes/output/FigS1_lake_reexamination_methods/find_evolving_outlines/offlake_outlines/Institute_E1.geojson\n",
      "Making video for Institute_E1\n",
      "\n",
      "Attempt 1 of 3\n",
      "Validating 44 images...\n",
      "Video created successfully on attempt 1\n",
      "Cleaned up folder: /home/jovyan/1_evolving_lakes/output/FigS1_lake_reexamination_methods/find_evolving_outlines/Institute_E1\n",
      "Creating evolving outlines time series plot for lake: Institute_E1\n",
      "Parameters: row_index=0, within_area_multiple=14, level=0.31\n"
     ]
    }
   ],
   "source": [
    "# FIXME: Do this if it doesn't meet this criteria or the other two criteria in the cell below\n",
    "\n",
    "# Verify all lakes were processed using the visualize_and_save_evolving_outlines func\n",
    "\n",
    "# Do this only on lakes that don't have a txt or geojson file generated\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "remaining_lakes = filter_gdf_by_folder_contents(stationary_outlines_gdf, folder_path, exclude=True)\n",
    "\n",
    "# Reprocess remaining lakes\n",
    "if remaining_lakes.empty:\n",
    "    print(\"All lakes processed.\")\n",
    "    \n",
    "else:\n",
    "    total_lakes = len(remaining_lakes)\n",
    "    processed_lakes = 0\n",
    "\n",
    "    try:\n",
    "        for i in range(len(remaining_lakes)):\n",
    "            remaining_count = total_lakes - processed_lakes\n",
    "            print(f\"{remaining_count} lakes remain.\")\n",
    "        \n",
    "            # Process the lake\n",
    "            lake_gdf = remaining_lakes.iloc[i:i+1]\n",
    "            visualize_and_save_evolving_outlines(lake_gdf)\n",
    "\n",
    "            # Increment processing counter\n",
    "            processed_lakes += 1\n",
    "\n",
    "            # Clear output of each index\n",
    "            clear_output(wait=True)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing lake at index {i}: {str(e)}\")\n",
    "            \n",
    "    print(\"All lakes processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dfbf9f-4639-470f-beea-8e1ba10a4358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "71bd85b1-9dea-4126-99bd-1a3aff0c28f5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All lakes processed.\n"
     ]
    }
   ],
   "source": [
    "# Verify all lakes were processed using the visualize_and_save_evolving_outlines func\n",
    "\n",
    "# Do this only on lake with evolving outlines saved as geojson file in evolving_outlines directory\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "remaining_lakes = filter_gdf_by_folder_contents(stationary_outlines_gdf, folder_path, exclude=False, file_extension='.geojson')\n",
    "\n",
    "# Verify all lakes were processed using the visualize_and_save_evolving_outlines func\n",
    "# Visualization mp4's saved to find_evolving_outlines directory\n",
    "folder_path = OUTPUT_DIR + '/find_evolving_outlines'\n",
    "remaining_lakes = filter_gdf_by_folder_contents(\n",
    "    remaining_lakes, \n",
    "    folder_path,\n",
    "    suffix_pattern=r'_\\d+-idx_\\d+\\.\\d+m-level_\\d+x-within',\n",
    "    file_extension='mp4'\n",
    ")\n",
    "\n",
    "# Reprocess remaining lakes\n",
    "if remaining_lakes.empty:\n",
    "    print(\"All lakes processed.\")\n",
    "    \n",
    "else:\n",
    "    total_lakes = len(remaining_lakes)\n",
    "    processed_lakes = 0\n",
    "\n",
    "    try:\n",
    "        for i in range(len(remaining_lakes)):\n",
    "            remaining_count = total_lakes - processed_lakes\n",
    "            print(f\"{remaining_count} lakes remain.\")\n",
    "        \n",
    "            # Process the lake\n",
    "            lake_gdf = remaining_lakes.iloc[i:i+1]\n",
    "            visualize_and_save_evolving_outlines(lake_gdf)\n",
    "\n",
    "            # Increment processing counter\n",
    "            processed_lakes += 1\n",
    "\n",
    "            # Clear output of each index\n",
    "            clear_output(wait=True)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing lake at index {i}: {str(e)}\")\n",
    "            \n",
    "    print(\"All lakes processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3c2b87fd-8530-4f82-a296-58c3e6ebb31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>area (m^2)</th>\n",
       "      <th>cite</th>\n",
       "      <th>CS2_SARIn_start</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty GeoDataFrame\n",
       "Columns: [name, area (m^2), cite, CS2_SARIn_start, geometry]\n",
       "Index: []"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do this only on lake with evolving outlines saved as geojson file in evolving_outlines directory\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "# remaining_lakes = filter_gdf_by_folder_contents(stationary_outlines_gdf, folder_path, exclude=False, file_extension='.geojson')\n",
    "remaining_lakes = filter_gdf_by_folder_contents(stationary_outlines_gdf, folder_path, exclude=True)\n",
    "\n",
    "# Verify all lakes were processed using the visualize_and_save_evolving_outlines func\n",
    "# Visualization mp4's saved to find_evolving_outlines directory\n",
    "folder_path = OUTPUT_DIR + '/find_evolving_outlines'\n",
    "remaining_lakes = filter_gdf_by_folder_contents(\n",
    "    remaining_lakes, \n",
    "    folder_path,\n",
    "    suffix_pattern=r'_\\d+-idx_\\d+\\.\\d+m-level_\\d+x-within',\n",
    "    file_extension='mp4'\n",
    ")\n",
    "\n",
    "remaining_lakes[remaining_lakes['name'] == 'David_s1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9a7ddd-da3f-49e8-895b-bd474c5776cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all lakes were processed using the plot_evolving_outlines_time_series func \n",
    "# (which is nested within the visualize_and_save_evolving_outlines func)\n",
    "\n",
    "# Do this only on lake with evolving outlines saved as geojson file in evolving_outlines directory\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "remaining_lakes = filter_gdf_by_folder_contents(stationary_outlines_gdf, folder_path, exclude=False, file_extension='.geojson')\n",
    "\n",
    "# Visualization png's saved to plot_evolving_outlines_time_series directory\n",
    "folder_path = OUTPUT_DIR + '/plot_evolving_outlines_time_series'\n",
    "remaining_lakes = filter_gdf_by_folder_contents(\n",
    "    remaining_lakes, \n",
    "    folder_path,\n",
    "    suffix_pattern=r'_\\d+-idx_\\d+\\.\\d+m-level_\\d+x-within',\n",
    "    file_extension='png'\n",
    ")\n",
    "\n",
    "# Reprocess remaining lakes\n",
    "if remaining_lakes.empty:\n",
    "    print(\"All lakes processed.\")\n",
    "    \n",
    "else:\n",
    "    total_lakes = len(remaining_lakes)\n",
    "    processed_lakes = 0\n",
    "\n",
    "    try:\n",
    "        for i in range(len(remaining_lakes)):\n",
    "            remaining_count = total_lakes - processed_lakes\n",
    "            print(f\"{remaining_count} lakes remain.\")\n",
    "        \n",
    "            # Process the lake\n",
    "            lake_gdf = remaining_lakes.iloc[i:i+1]\n",
    "            lake_name = lake_gdf['name'].iloc[0]\n",
    "            evolving_outlines_gdf = gpd.read_file(os.path.join(OUTPUT_DIR_GIT + f'/lake_outlines/evolving_outlines/{lake_name}.geojson'))\n",
    "            offlake_outlines_gdf = gpd.read_file(os.path.join(OUTPUT_DIR + f'/find_evolving_outlines//offlake_outlines/{lake_name}.geojson'))\n",
    "            plot_evolving_outlines_time_series(lake_gdf, evolving_outlines_gdf, offlake_outlines_gdf)\n",
    "            \n",
    "            # Increment processing counter\n",
    "            processed_lakes += 1\n",
    "\n",
    "            # Clear output of each index\n",
    "            clear_output(wait=True)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing lake at index {i}: {str(e)}\")\n",
    "            \n",
    "    print(\"All lakes processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d9f9d-2b9b-497e-bc61-6d9a13894d3b",
   "metadata": {},
   "source": [
    "# Lake groups\n",
    "\n",
    "From reviewing the evolving outlines, we observed lakes that have neighbor lake and appear to interact with that neighbor, so we analyze those lake groupings as lake systems where two or more lakes are analyzed together see if perhaps the lakes should be considered as one lake or remain as separate lakes. Additionally The upper Thwaites lakes are close neighbors we attempted to group them to see if a more optimal level could be obtained when analyzed simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2c9c41-c404-45a6-bf71-ed8ffb6303d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define lake groups\n",
    "lake_groups = [\n",
    "    ['Mac1', 'Mac2'],\n",
    "    ['Site_B', 'Site_C'],\n",
    "    ['Slessor_4', 'Slessor_5'],\n",
    "    ['Thw_70', 'Thw_124', 'Thw_142', 'Thw_170'],\n",
    "    ['Thw_124', 'Thw_142', 'Thw_170'],\n",
    "    ['Thw_124', 'Thw_142'],\n",
    "    ['Thw_142', 'Thw_170']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2978f041-1e33-44ed-b579-db0bbd74545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reverse for processing backwards using a second server\n",
    "# lake_groups.reverse()\n",
    "\n",
    "# Process each group\n",
    "for lake_group in lake_groups:\n",
    "\n",
    "    # Create combined lake group GeoDataFrame row\n",
    "    group_single_gdf = prepare_group_gdf(stationary_outlines_gdf, lake_group)\n",
    "    \n",
    "    # First find search extents and levels for the group\n",
    "    find_and_save_optimal_parameters(group_single_gdf, within_area_multiples=range(5, 16))\n",
    "        \n",
    "    # Then finalize the evolving outlines using these parameters\n",
    "    visualize_and_save_evolving_outlines(group_single_gdf)\n",
    "\n",
    "del lake_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde5fc9-103b-4a4e-886d-06a44f0e891c",
   "metadata": {},
   "source": [
    "After reviewing the results of the lake groups analysis, I found:\n",
    "* Mac1_Mac2 have distinct activity but there is an interesting dh expression between the lakes during the CryoSat-2 era that may be indicative of lake migration; however, analyzing as a group does not add new information as the intereting feature was found over Mac2 in the individual lake analysis and the level found analyzing as a lake system (0.4 m) was higher than analyzing separately: Mac1 (0.30 m) and Mac2 (0.36 m).\n",
    "* Site_B_Site_C have two dh espressions that both overlap with locations of the two lakes; it's unclear if either of the dh expressions belong to one of the lakes or the other because the two dh expressions are nearly centered between the two lakes with some lateral offset. This is an improvement from analyzing the two lakes separately where Site C had a lowest optimal level of 1.27 m and Site B had no evolving outlines found compared to analyzed as lake group had a lowest optimal level of 1.2 m.\n",
    "* Slessor_4_Slessor_5 have unconvincing evidence of being one lake system: there is one time slice where there dh expression covering both lakes, 2019-07-02 to 2019-10-02, but several other time slices have dh expressions of opposite sign (similar to observed at Slessor_23 by Siegfried and Fricker, 2018).\n",
    "* Thw_70_Thw_124_Thw_142_Thw_170 does not improve analysis from individual lakes because there is no overlapping activity and the level (1.37 m) is higher than two out of four of the lakes analyzed individually.\n",
    "* Thw_124_Thw_142_Thw_170 has some potential as the there overlapping activity over the three lakes and a lower level (0.53 m) than two of the three lakes; however, there are many off-lake outlines identified, so we will plot the second most optimal level of this lake grouping.\n",
    "* Thw_124_Thw_142 appears to have some potential in identifying overlapping outlines shared between these lakes; however, Thw_170 is included in the found outlines, so it makes sense to use that grouping instead.\n",
    "* Thw_142_Thw_170 was not useful because of the high level (1.86 m) found to be most optimal for this grouping.\n",
    "\n",
    "Based on this, we will delete the evolving outlines generated and saved to geojson files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf520436-96e4-4e51-8a2f-ae867a909e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Site_B_Site_C to Site_BC to follow naming convention used for at past lake unions\n",
    "# Lake_78 and Slessor_23\n",
    "old_name = 'output/lake_outlines/evolving_outlines/' + 'Site_B_Site_C.geojson'\n",
    "new_name = 'output/lake_outlines/evolving_outlines/' + 'Site_BC.geojson'\n",
    "\n",
    "if os.path.exists(old_name):\n",
    "    os.rename(old_name, new_name)\n",
    "    print(f'Successfully renamed {old_name} to {new_name}')\n",
    "else:\n",
    "    print(f'File {old_name} does not exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaedcc3-0194-43f9-8a13-3063da885840",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_names = ['output/lake_outlines/evolving_outlines/Site_B_Site_C.geojson', \n",
    "             OUTPUT_DIR + '/levels/Site_B_Site_C.csv',\n",
    "             OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines/Site_B_Site_C.geojson']\n",
    "new_names = ['output/lake_outlines/evolving_outlines/Site_BC.geojson',\n",
    "            OUTPUT_DIR + '/levels/Site_BC.csv',\n",
    "            OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines/Site_BC.geojson']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1cc6d-780f-432b-83a7-99e695501fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through both lists simultaneously\n",
    "for old_name, new_name in zip(old_names, new_names):\n",
    "    # Try to rename file\n",
    "    if os.path.exists(old_name):\n",
    "        os.rename(old_name, new_name)\n",
    "        print(f'Successfully renamed {old_name} to {new_name}')\n",
    "    else:\n",
    "        print(f'File not found: {old_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1def7d8-0fb3-4630-a6fc-06b075dd41a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_outputs_to_delete = [\n",
    "    'Mac1_Mac2',\n",
    "    'Site_B', 'Site_C',\n",
    "    'Slessor_4_Slessor_5',\n",
    "    'Thw_70_Thw_124_Thw_142_Thw_170',\n",
    "    'Thw_142_Thw_170'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5727487d-b1fa-4a1a-bc92-dfd4b1b4ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in lakes_outputs_to_delete:\n",
    "    dir = OUTPUT_DIR + '/levels/'\n",
    "    file_path = os.path.join(dir, f\"{filename}.csv\")\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Deleted: {file_path}\")\n",
    "    else:\n",
    "        print(f\"Not found: {file_path}\")\n",
    "\n",
    "    dir = 'output/lake_outlines/evolving_outlines/'\n",
    "    for ext in ('.geojson', '.txt'):\n",
    "        file_path = os.path.join(dir, f\"{filename}{ext}\")\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")\n",
    "        else:\n",
    "            print(f\"Not found: {file_path}\")\n",
    "\n",
    "del dir, lakes_outputs_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39f9066-a790-451e-a701-68284a467bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write no outlines txt file to two lakes that will be replaced by the combination lake\n",
    "dir = 'output/lake_outlines/evolving_outlines/'\n",
    "\n",
    "for lake in ['Site_B', 'Site_C']:\n",
    "    write_no_outlines(dir + lake)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253a34c5-5457-4a21-b6ac-179a9ccb2325",
   "metadata": {},
   "source": [
    "Next we address lakes that have evolving outlines that appear flawed because of the number of off-lake outlines that make it appear that the lowest level selected using the algorithm perhaps was too low. We address this by selecting the next most optimal level/within_area_multiple combination contained in the levels csv file for these lakes in the first row (instead of the default zeroth row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a01d67-6acc-4c00-a449-4cbdffacb083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lake groups\n",
    "lake_groups_analyze_row_1 = [\n",
    "    ['Thw_124', 'Thw_142', 'Thw_170'],\n",
    "    ['Thw_124', 'Thw_142'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c07ad-d580-4c10-a600-1ee4fe92b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lake_group in lake_groups_analyze_row_1:\n",
    "\n",
    "    # Create combined lake group GeoDataFrame row\n",
    "    group_single_gdf = prepare_group_gdf(stationary_outlines_gdf, lake_group)\n",
    "    \n",
    "    # Visualize second level stored in first row\n",
    "    visualize_and_save_evolving_outlines(group_single_gdf, row_index=1)\n",
    "\n",
    "del lake_groups_analyze_row_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4697db-30cc-4438-9237-653d15f00c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lake groups\n",
    "lake_groups_analyze_row_2 = [\n",
    "    # ['Thw_124', 'Thw_142', 'Thw_170'],\n",
    "    ['Thw_124', 'Thw_142']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd29d3d3-8648-4eb7-9936-6a1e3129ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lake_group in lake_groups_analyze_row_2:\n",
    "\n",
    "    # Create combined lake group GeoDataFrame row\n",
    "    group_single_gdf = prepare_group_gdf(stationary_outlines_gdf, lake_group)\n",
    "    \n",
    "    # Visualize second level stored in first row\n",
    "    visualize_and_save_evolving_outlines(group_single_gdf, row_index=2)\n",
    "\n",
    "del lake_groups_analyze_row_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb897137-e826-49d5-a436-7c11ef13ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lake groups\n",
    "lake_groups_analyze_row_3 = [\n",
    "    ['Thw_124', 'Thw_142', 'Thw_170'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db9a661-5115-4354-8d0c-195ca9b85dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lake_group in lake_groups_analyze_row_3:\n",
    "\n",
    "    # Create combined lake group GeoDataFrame row\n",
    "    group_single_gdf = prepare_group_gdf(stationary_outlines_gdf, lake_group)\n",
    "    \n",
    "    # Visualize second level stored in first row\n",
    "    visualize_and_save_evolving_outlines(group_single_gdf, row_index=3)\n",
    "\n",
    "del lake_groups_analyze_row_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ea689b-3ab5-4fb7-a0ea-8f125f41ac90",
   "metadata": {},
   "source": [
    "The various levels of Thw_124_Thw_142_Thw_170 did not provide better outlines than analyzing the lakes individually, but instead Thw_124_Thw_142 is able to capture the lobing activity of Thw_142 and more of Thw_124's activity at a lower level then when each lake is analyzed separately. However, their activity appears spatial distinct, so we will:\n",
    "\n",
    "1) Use evolving outlines generated in the analysis of lake group, Thw_124_Thw_142, for the individual lake evolving outlines for each respective lake by separating them spatially.\n",
    "\n",
    "2) We will delete the Thw_124_Thw_142_Thw_170 geojson file as it did not prove useful analyzing these three lakes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef041aa-ac94-4636-a8f1-cfca74ea6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "evolving_outlines_path = os.path.join(OUTPUT_DIR_GIT + '/lake_outlines/evolving_outlines/Thw_124_Thw_142.geojson')\n",
    "output_dir = os.path.join('output/lake_outlines/evolving_outlines')\n",
    "\n",
    "# Read the evolving outlines\n",
    "Thw_124_Thw_142_evolving_outlines_gdf = gpd.read_file(evolving_outlines_path)\n",
    "\n",
    "# Get the stationary outlines for each lake\n",
    "Thw_124_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == 'Thw_124']\n",
    "Thw_142_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == 'Thw_142']\n",
    "\n",
    "if Thw_124_gdf.empty or Thw_142_gdf.empty:\n",
    "    raise ValueError(f\"Could not find one or both lakes in stationary_outlines_gdf: {Thw_124_gdf['name']}, {Thw_142_gdf['name']}\")\n",
    "\n",
    "# Extract outlines that intersect with each lake\n",
    "Thw_124_outlines, Thw_124_offlake_outlines = extract_intersecting_polygons_recursive(Thw_124_Thw_142_evolving_outlines_gdf, Thw_124_gdf.geometry.iloc[0])\n",
    "Thw_142_outlines, Thw_142_offlake_outlines = extract_intersecting_polygons_recursive(Thw_124_Thw_142_evolving_outlines_gdf, Thw_142_gdf.geometry.iloc[0])\n",
    "\n",
    "if Thw_124_outlines is not None and not Thw_124_outlines.empty:\n",
    "    lake_name = 'Thw_124'\n",
    "    Thw_124_outlines.to_file(\n",
    "        os.path.join(OUTPUT_DIR, f\"{lake_name}.geojson\"),\n",
    "        driver='GeoJSON'\n",
    "    )\n",
    "    print(f\"Saved outlines for {lake_name}\")\n",
    "\n",
    "if Thw_142_outlines is not None and not Thw_142_outlines.empty:\n",
    "    lake_name = 'Thw_142'\n",
    "    Thw_142_outlines.to_file(\n",
    "        os.path.join(OUTPUT_DIR, f\"{lake_name}.geojson\"),\n",
    "        driver='GeoJSON'\n",
    "    )\n",
    "    print(f\"Saved outlines for {lake_name}\")\n",
    "\n",
    "del evolving_outlines_path, output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd132a0-c1a5-4c6a-9c30-002940f74f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick visualization to ensure outlines were split properly\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Get bounds for plot extent\n",
    "total_bounds = Thw_124_Thw_142_evolving_outlines_gdf.total_bounds\n",
    "x_min, y_min, x_max, y_max = total_bounds\n",
    "buffer_factor = 0.2\n",
    "x_buffer = (x_max - x_min) * buffer_factor\n",
    "y_buffer = (y_max - y_min) * buffer_factor\n",
    "\n",
    "# Plot MOA background\n",
    "mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "moa_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "ax.imshow(moa_subset[0,:,:], cmap=\"gray\", clim=[14000, 17000], \n",
    "         extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "\n",
    "# Plot the outlines\n",
    "Thw_124_outlines_imported = gpd.read_file('output/lake_outlines/evolving_outlines/Thw_124.geojson')\n",
    "Thw_142_outlines_imported = gpd.read_file('output/lake_outlines/evolving_outlines/Thw_142.geojson')\n",
    "Thw_124_outlines_imported.boundary.plot(ax=ax, color='red', linewidth=1, label='Thw_124 evolving')\n",
    "Thw_142_outlines_imported.boundary.plot(ax=ax, color='blue', linewidth=1, label='Thw_142 evolving')\n",
    "    \n",
    "# Plot stationary outlines\n",
    "Thw_124_gdf.boundary.plot(ax=ax, color='darkred', linestyle='--', linewidth=2, label=f\"{Thw_124_gdf['name'].iloc[0]} stationary\")\n",
    "Thw_142_gdf.boundary.plot(ax=ax, color='darkblue', linestyle='--', linewidth=2, label=f\"{Thw_142_gdf['name'].iloc[0]} stationary\")\n",
    "\n",
    "# Add inset map\n",
    "axins = ax.inset_axes([0.05, 0.05, 0.3, 0.3])\n",
    "axins.set_aspect('equal')\n",
    "moa_2014_coastline.plot(ax=axins, color='gray', edgecolor='k', linewidth=0.1)\n",
    "moa_2014_groundingline.plot(ax=axins, color='ghostwhite', edgecolor='k', linewidth=0.1)\n",
    "center_x = (x_min + x_max) / 2\n",
    "center_y = (y_min + y_max) / 2\n",
    "axins.scatter(center_x, center_y, c='red', marker='*', s=100, zorder=5)\n",
    "axins.axis('off')\n",
    "\n",
    "# Format main plot\n",
    "km_scale = 1e3\n",
    "ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "ax.set_xlabel('x [km]')\n",
    "ax.set_ylabel('y [km]')\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2)\n",
    "ax.set_title('Split Evolving Lake Outlines')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfa50b-245a-433c-bfce-ccfcc9d9296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot evolving outlines time series for Thw_124 and Thw_142\n",
    "Thw_124_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == 'Thw_124']\n",
    "Thw_142_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == 'Thw_142']\n",
    "plot_evolving_outlines_time_series(Thw_124_gdf, Thw_124_outlines, Thw_124_offlake_outlines)\n",
    "plot_evolving_outlines_time_series(Thw_142_gdf, Thw_142_outlines, Thw_142_offlake_outlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8e019-df5f-45b7-9f98-36bd7f39a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del Thw_124_gdf, Thw_124_outlines, Thw_124_offlake_outlines\n",
    "del Thw_142_gdf, Thw_142_outlines, Thw_142_offlake_outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c1b497-8580-48e0-9bc1-c9958cfc6c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_outputs_to_delete = [\n",
    "    'Thw_124_Thw_142_Thw_170',\n",
    "    'Thw_124_Thw_142'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d8f9dc-1458-41d6-8ddb-a7876954a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in lakes_outputs_to_delete:\n",
    "    dir = OUTPUT_DIR + '/levels/'\n",
    "    file_path = os.path.join(dir, f\"{filename}.csv\")\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Deleted: {file_path}\")\n",
    "    else:\n",
    "        print(f\"Not found: {file_path}\")\n",
    "\n",
    "    dir = 'output/lake_outlines/evolving_outlines/'\n",
    "    for ext in ('.geojson', '.txt'):\n",
    "        file_path = os.path.join(dir, f\"{filename}{ext}\")\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")\n",
    "        else:\n",
    "            print(f\"Not found: {file_path}\")\n",
    "\n",
    "del dir, lakes_outputs_to_delete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d89cea-4589-4838-a74f-ab12eeab3fa7",
   "metadata": {},
   "source": [
    "# Revise stationary_outlines_gdf\n",
    "\n",
    "We will revise stationary_outlines_gdf to not have Site_B and Site_C as individual lakes, but instead have Site_BC as a combined lake group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc837b4d-ea77-402a-8e68-ff7672c6ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of stationary_outlines_gdf\n",
    "revised_stationary_outlines_gdf = stationary_outlines_gdf.copy(deep=True)\n",
    "\n",
    "# Create combined Site_B_Site_C row\n",
    "site_bc_row = prepare_group_gdf(revised_stationary_outlines_gdf, ['Site_B', 'Site_C'])\n",
    "\n",
    "# Copy the citation to new Site_B_Site_C row if Sites B and C have the same citation\n",
    "if (revised_stationary_outlines_gdf[revised_stationary_outlines_gdf['name'] == 'Site_B']['cite'].iloc[0] == \n",
    "    revised_stationary_outlines_gdf[revised_stationary_outlines_gdf['name'] == 'Site_C']['cite'].iloc[0]):\n",
    "    site_bc_row['cite'] = (revised_stationary_outlines_gdf[revised_stationary_outlines_gdf['name'] == 'Site_B']\n",
    "                          ['cite'].iloc[0])\n",
    "\n",
    "# Drop individual lakes we are replacing\n",
    "revised_stationary_outlines_gdf = revised_stationary_outlines_gdf.drop(\n",
    "    revised_stationary_outlines_gdf[revised_stationary_outlines_gdf['name'].isin(['Site_B', 'Site_C'])].index)\n",
    "\n",
    "# Get evolving outlines and calculate area for Site_BC\n",
    "try:\n",
    "    lake_name = 'Site_BC'\n",
    "    \n",
    "    evolving_outlines_gdf = gpd.read_file('output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name))\n",
    "    \n",
    "    # Get evolving outlines union\n",
    "    evolving_union_gs = find_evolving_union(site_bc_row.iloc[0], evolving_outlines_gdf, incl_stationary=False)\n",
    "    \n",
    "    if evolving_union_gs is not None:\n",
    "        # Create temporary GeoDataFrame with the union\n",
    "        temp_gdf = gpd.GeoDataFrame(\n",
    "            index=[0], \n",
    "            crs='EPSG:3031', \n",
    "            geometry=[evolving_union_gs.iloc[0]])\n",
    "        \n",
    "        # Convert to 4326 for area calculation\n",
    "        temp_gdf = temp_gdf.to_crs('4326')\n",
    "        \n",
    "        # Calculate area\n",
    "        area = calculate_geodesic_area(temp_gdf['geometry'].iloc[0])\n",
    "        \n",
    "        # Update site_bc_row with calculated area and geometry\n",
    "        site_bc_row['area (m^2)'] = area if area is not None else None\n",
    "        site_bc_row['geometry'] = evolving_union_gs.iloc[0]\n",
    "\n",
    "\n",
    "    # Rename to follow combination naming convention used for at passed lakes Lake_78 and Slessor_23\n",
    "    site_bc_row['name'] = 'Site_BC'\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error processing {lake_name}: {str(e)}\")\n",
    "\n",
    "# Ensure that new entry isn't already in inventory before adding to avoid duplicate entry\n",
    "gdf_diff = site_bc_row[~site_bc_row['name'].isin(revised_stationary_outlines_gdf['name'])]\n",
    "\n",
    "# Add the new row to stationary_outlines_gdf\n",
    "revised_stationary_outlines_gdf = pd.concat([revised_stationary_outlines_gdf, gdf_diff], ignore_index=True)\n",
    "\n",
    "# Sort the GeoDataFrame alphabetically by the 'name' column; reset the index after sorting\n",
    "revised_stationary_outlines_gdf = revised_stationary_outlines_gdf.sort_values('name').reset_index(drop=True)\n",
    "\n",
    "# Print processing confirmation\n",
    "print(f\"\\nProcessed {lake_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e71d1a-29a5-4349-92a7-40ac75485980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View new row to ensure worked properly\n",
    "revised_stationary_outlines_gdf[revised_stationary_outlines_gdf['name'] == 'Site_BC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b428502e-ed81-480f-9a62-90f9b9a560ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "revised_stationary_outlines_gdf.to_file(\n",
    "    'output/lake_outlines/stationary_outlines/revised_stationary_outlines_gdf.geojson',\n",
    "    driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a146a886-92a4-4525-8e8e-6278af3caea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "revised_stationary_outlines_gdf = gpd.read_file(\n",
    "    'output/lake_outlines/stationary_outlines/revised_stationary_outlines_gdf.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d725d06-64e9-4c64-b3ee-24502c1cc4b4",
   "metadata": {},
   "source": [
    "# Review evolving outlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc765b3-b2cc-4c81-bd06-ebc0cd663c7d",
   "metadata": {},
   "source": [
    "In your OUTPUT_DIR/FigS1_lake_reexamination_methods, navigate to the `plot_evolving_outlines_time_series` folder. There you will see the time series of evolving outlines plotted in aggregate for each lake. Some lakes will have very few evolving outlines that don't appear much different from the off-lake outlines generated. We additionally looking at the data_counts, dh, and evolving outline video for each time lake in the `find_evolving_outlines` folder for each lake.\n",
    "\n",
    "In these cases we cannot be certain the evolving outlines are indicative of lake behavior or just background height anomalies. So we delete these evolving outlines geojson files and conclude there were no evolving outlines found for these lakes.\n",
    "\n",
    "Some deletions are due to a lake's evolving outlines being those of a close neighbor, (e.g., Mac5 evolving outlines were those of Mac4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db4fb7-676b-4c87-8644-0a2792ab0fe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lake_outlines_to_discard = [\n",
    "    'Bindschadler_1',\n",
    "    'Bindschadler_3',\n",
    "    'Bindschadler_4',\n",
    "    'Bindschadler_5',\n",
    "    'Bindschadler_6',\n",
    "    'Byrd_s1',\n",
    "    'Byrd_s5',\n",
    "    'Byrd_s7',\n",
    "    'Byrd_s14',\n",
    "    'Cook_E1',\n",
    "    'David_s4',\n",
    "    'David_s5',\n",
    "    'EAP_3',\n",
    "    'EAP_5',\n",
    "    'EAP_6',\n",
    "    'EAP_7',\n",
    "    'EAP_8',\n",
    "    'EAP_9',\n",
    "    'Foundation_2',\n",
    "    'Foundation_4',\n",
    "    'Foundation_9',\n",
    "    'Foundation_14',\n",
    "    'Institute_W1',\n",
    "    'JG_Combined_D2_b_E1',\n",
    "    'JG_D1_a',\n",
    "    'JG_D2_a',\n",
    "    'JG_F1',\n",
    "    'Kamb_1',\n",
    "    'Kamb_2',\n",
    "    'Kamb_3',\n",
    "    'Kamb_4',\n",
    "    'Kamb_9',\n",
    "    'Kamb_11',\n",
    "    'Lambert_1',\n",
    "    'LennoxKing_1',\n",
    "    'Mac5',\n",
    "    'Mac6',\n",
    "    'Mertz_1',\n",
    "    'Rec5',\n",
    "    'Rec10',\n",
    "    'Slessor_5',\n",
    "    'Slessor_6',\n",
    "    'Slessor_7',\n",
    "    'TL122',\n",
    "    'U1',\n",
    "    'U3',\n",
    "    'V1',\n",
    "    'Whillans_8',\n",
    "    'Wilkes_1',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811cb148-3575-4e49-a2ad-4aab0592b6d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Discard outlines by moving out of git repo and into non-git repo\n",
    "discard_lake_outlines(\n",
    "    lake_outlines_to_discard=lake_outlines_to_discard,\n",
    "    source_dir='output/lake_outlines/evolving_outlines',\n",
    "    dest_dir=OUTPUT_DIR + '/find_evolving_outlines/discarded_outlines'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e05fcb-1dcf-41ae-bc12-f65c3912f31a",
   "metadata": {},
   "source": [
    "Similar to the lake groups, we try the next highest level for evolving outlines that appear flawed because of the number of off-lake outlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639cd0a4-5fce-43e3-9d5e-060a07c51d81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakes_analyze_row_1 = [\n",
    "    'Byrd_s10',\n",
    "    'Byrd_s11',\n",
    "    'Byrd_s13',\n",
    "    'Byrd_s15',\n",
    "    'ConwaySubglacialLake',\n",
    "    'EngelhardtSubglacialLake',\n",
    "    'Foundation_N1',\n",
    "    'Foundation_N3',\n",
    "    'KT2',\n",
    "    'KT3',\n",
    "    'L1',\n",
    "    'Lake78',\n",
    "    'Mac1',\n",
    "    'Nimrod_2',\n",
    "    'R1',\n",
    "    'Rec1',\n",
    "    'Rec2',\n",
    "    'Rec6',\n",
    "    'Slessor_4',\n",
    "    'Slessor_23',\n",
    "    'Thw_170',\n",
    "    'UpperSubglacialLakeConway',\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6927714-3424-40a8-98e7-0e8681b1f085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for lake in lakes_analyze_row_1:\n",
    "    # Process the lake at the next highest level\n",
    "    lake_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == lake]\n",
    "    visualize_and_save_evolving_outlines(lake_gdf, row_index=1)\n",
    "\n",
    "del lakes_analyze_row_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a9fd6e-9147-401a-ba2b-2275872c9d1f",
   "metadata": {},
   "source": [
    "We assess these results (in `OUTPUT_DIR/FigS1_lake_reexamination_methods/plot_evolving_outlines_time_series` folder) and decide if 1) lakes analyzed at next highest dh level are satisfactory, or 2) outlines that can be deleted because they are too similar to background off-lake activity or 2) lakes that need to be visualized at the next level because they are too similar to background off-lake activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fa1921-4f69-46a8-afbd-02a9423b8107",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_analyze_row_2 = [\n",
    "    'ConwaySubglacialLake',\n",
    "    'David_1',\n",
    "    'EngelhardtSubglacialLake',\n",
    "    'Lake78',\n",
    "    'Mac1',\n",
    "    'R1',\n",
    "    'Rec1',\n",
    "    'Rec2',\n",
    "    'Slessor_4',\n",
    "    'Slessor_23',\n",
    "    'Thw_170',\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5edc11-5dae-4269-a938-c41a24fd4968",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lake in lakes_analyze_row_2:\n",
    "    # Process the lake at all other levels\n",
    "    lake_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == lake]\n",
    "    visualize_and_save_evolving_outlines(lake_gdf, row_index=2)\n",
    "    \n",
    "del lakes_analyze_row_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb22059f-5f46-4815-a04e-f24898a545bc",
   "metadata": {},
   "source": [
    "We assess these results (in `OUTPUT_DIR/FigS1_lake_reexamination_methods/plot_evolving_outlines_time_series` folder) and decide if 1) lakes analyzed at next highest dh level are satisfactory, or 2) outlines that can be deleted because they are too similar to background off-lake activity or 2) lakes that need to be visualized at the next level because they are too similar to background off-lake activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b124741c-3323-4b76-a4d4-a8b9ec34a4e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lake_outlines_to_discard = [\n",
    "    'David_1',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf38a47-b021-4719-b296-69567f2e59f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Discard outlines by moving out of git repo and into non-git repo\n",
    "discard_lake_outlines(\n",
    "    lake_outlines_to_discard=lake_outlines_to_discard,\n",
    "    source_dir='output/lake_outlines/evolving_outlines',\n",
    "    dest_dir=OUTPUT_DIR + '/find_evolving_outlines/discarded_outlines'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdbc72d-f295-44f6-9392-ac8782df1ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_analyze_row_3 = [\n",
    "    'ConwaySubglacialLake',\n",
    "    'Foundation_N1',\n",
    "    'R1',\n",
    "    'Rec2',\n",
    "    'Slessor_4',\n",
    "    'Thw_170'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50beaef3-0a7e-46a8-bca7-1eddb2623a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lake in lakes_analyze_row_3:\n",
    "    # Process the lake at all other levels\n",
    "    lake_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == lake]\n",
    "    visualize_and_save_evolving_outlines(lake_gdf, row_index=3)\n",
    "    \n",
    "del lakes_analyze_row_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f6bca-c64d-43df-91d3-51460230218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_analyze_row_4 = [\n",
    "    'ConwaySubglacialLake',\n",
    "    'Rec2',\n",
    "    'Slessor_4',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783c5c2c-bf6c-4dfa-84cd-bc9a1fb90af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lake in lakes_analyze_row_4:\n",
    "    # Process the lake at all other levels\n",
    "    lake_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == lake]\n",
    "    visualize_and_save_evolving_outlines(lake_gdf, row_index=4)\n",
    "    \n",
    "del lakes_analyze_row_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "923f9fdd-63c7-4bbd-8fe3-60fa9ffba6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_analyze_row_5 = [\n",
    "    'ConwaySubglacialLake',\n",
    "    # 'Rec2',\n",
    "    # 'Slessor_4',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "36661c41-6b4d-4aea-9864-4b8f6bf1abdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating evolving outlines time series plot for lake: ConwaySubglacialLake\n",
      "Parameters: row_index=5, within_area_multiple=2, level=0.58\n",
      "Successfully saved plot to: /home/jovyan/1_evolving_lakes/output/FigS1_lake_reexamination_methods/plot_evolving_outlines_time_series/ConwaySubglacialLake_5-idx_0.58m-level_2x-within.png\n"
     ]
    }
   ],
   "source": [
    "for lake in lakes_analyze_row_5:\n",
    "    # Process the lake at all other levels\n",
    "    lake_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == lake]\n",
    "    visualize_and_save_evolving_outlines(lake_gdf, row_index=5)\n",
    "\n",
    "del lakes_analyze_row_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c239c59-5973-4bf0-afa0-31f90f24e054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there are no lakes with both geojson and txt file for evolving outlines\n",
    "dir = OUTPUT_DIR + '/levels'\n",
    "file_dict = defaultdict(list)\n",
    "\n",
    "# Get all files in directory\n",
    "for file_path in Path(dir).glob('**/*'):\n",
    "    if file_path.is_file():\n",
    "        # Get base name without extension\n",
    "        base_name = file_path.stem\n",
    "        # Add full filename to list under base name\n",
    "        file_dict[base_name].append(file_path.name)\n",
    "\n",
    "# Filter to only files with duplicates\n",
    "duplicates = {k: v for k, v in file_dict.items() if len(v) > 1}\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1912df48-84f8-489b-bf30-e190a856d389",
   "metadata": {},
   "source": [
    "# Union outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca8bc5-fd0c-494e-aa96-68a57e6e2b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two new lakes geodataframe that are the union of \n",
    "# 1) the evolving outlines for each lake found to have evolving outlines\n",
    "# 2) the evolving outlines and the stationary outline for lakes with activity (found to have evolving outlines)\n",
    "# 3) the evolving outlines and the stationary outline for all lakes\n",
    "\n",
    "# List all folders in target directory\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "\n",
    "# Filter stationary_outlines_gdf to only include lakes that have evolving outlines found at them\n",
    "stationary_outlines_gdf_evolving_lakes = filter_gdf_by_folder_contents(revised_stationary_outlines_gdf, folder_path, file_extension='geojson', exclude=False)\n",
    "\n",
    "# Remove Site A, B, C, LSLM, and LSLC because their outlines were perfect circles created using their \n",
    "# point locations and approx. areas so should not be part of the union\n",
    "exclude_list = ['Site_A', 'LowerConwaySubglacialLake', 'LowerMercerSubglacialLake']\n",
    "stationary_outlines_gdf_evolving_lakes = stationary_outlines_gdf_evolving_lakes[~stationary_outlines_gdf_evolving_lakes['name'].isin(exclude_list)]\n",
    "stationary_outlines_gdf_all_lakes = revised_stationary_outlines_gdf[~revised_stationary_outlines_gdf['name'].isin(exclude_list)]\n",
    "\n",
    "# Create initial GDFs\n",
    "evolving_outlines_union_gdf = stationary_outlines_gdf_evolving_lakes.copy(deep=True)\n",
    "evolving_stationary_outlines_union_evolving_lakes_gdf = stationary_outlines_gdf_evolving_lakes.copy(deep=True)\n",
    "evolving_stationary_outlines_union_all_lakes_gdf = stationary_outlines_gdf_all_lakes.copy(deep=True)\n",
    "\n",
    "# First process lakes with evolving outlines\n",
    "for idx, row in stationary_outlines_gdf_evolving_lakes.iterrows():\n",
    "    try:\n",
    "        lake_ps = stationary_outlines_gdf_evolving_lakes.loc[idx]\n",
    "        lake_name = lake_ps['name']\n",
    "        \n",
    "        try:\n",
    "            evolving_outlines_gdf = gpd.read_file('output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping because evolving outlines geojson file not found for {lake_name}.\")\n",
    "            continue\n",
    "            \n",
    "        # Process evolving outlines\n",
    "        evolving_union_gs = find_evolving_union(lake_ps, evolving_outlines_gdf, incl_stationary=False)\n",
    "        evolving_stationary_outlines_union_gs = find_evolving_union(lake_ps, evolving_outlines_gdf, incl_stationary=True)\n",
    "\n",
    "        if evolving_stationary_outlines_union_gs is None or evolving_union_gs is None:\n",
    "            print(f\"Skipping {lake_name}: Could not create union of outlines\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Store polygon from geoseries in geodataframes with CRS\n",
    "            evolving_outlines_union_gdf_idx = gpd.GeoDataFrame(\n",
    "                index=[0], crs='EPSG:3031', geometry=[evolving_union_gs.iloc[0]])\n",
    "            evolving_stationary_outlines_union_evolving_lakes_gdf_idx = gpd.GeoDataFrame(\n",
    "                index=[0], crs='EPSG:3031', geometry=[evolving_stationary_outlines_union_gs.iloc[0]])\n",
    "            \n",
    "            # Convert GeoDataFrames to EPSG:4326 CRS for geodesic area calculation\n",
    "            evolving_outlines_union_gdf_idx = evolving_outlines_union_gdf_idx.to_crs('4326')\n",
    "            evolving_stationary_outlines_union_evolving_lakes_gdf_idx = evolving_stationary_outlines_union_evolving_lakes_gdf_idx.to_crs('4326')\n",
    "            \n",
    "            # Update geometries in union GDFs using the correct index\n",
    "            evolving_outlines_union_gdf.loc[idx, 'geometry'] = evolving_union_gs.iloc[0]\n",
    "            evolving_stationary_outlines_union_evolving_lakes_gdf.loc[idx, 'geometry'] = evolving_stationary_outlines_union_gs.iloc[0]\n",
    "            \n",
    "            # Calculate and store areas\n",
    "            area1 = calculate_geodesic_area(evolving_outlines_union_gdf_idx['geometry'].iloc[0])\n",
    "            area2 = calculate_geodesic_area(evolving_stationary_outlines_union_evolving_lakes_gdf_idx['geometry'].iloc[0])\n",
    "            \n",
    "            if area1 is not None:\n",
    "                evolving_outlines_union_gdf.loc[idx, 'area (m^2)'] = area1\n",
    "            if area2 is not None:\n",
    "                evolving_stationary_outlines_union_evolving_lakes_gdf.loc[idx, 'area (m^2)'] = area2\n",
    "                \n",
    "            # Update the all_lakes GDF for this lake\n",
    "            mask = evolving_stationary_outlines_union_all_lakes_gdf['name'] == lake_name\n",
    "            if mask.any():\n",
    "                evolving_stationary_outlines_union_all_lakes_gdf.loc[mask, 'geometry'] = evolving_stationary_outlines_union_gs.iloc[0]\n",
    "                evolving_stationary_outlines_union_all_lakes_gdf.loc[mask, 'area (m^2)'] = area2\n",
    "                \n",
    "        except (IndexError, AttributeError) as e:\n",
    "            print(f\"Error processing geometries for {lake_name}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error processing {lake_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Then process remaining lakes for all_lakes GDF\n",
    "remaining_lakes = set(stationary_outlines_gdf_all_lakes['name']) - set(stationary_outlines_gdf_evolving_lakes['name'])\n",
    "for lake_name in remaining_lakes:\n",
    "    try:\n",
    "        # Get the lake's data using boolean indexing\n",
    "        mask = evolving_stationary_outlines_union_all_lakes_gdf['name'] == lake_name\n",
    "        if not mask.any():\n",
    "            print(f\"Lake {lake_name} not found in all_lakes GDF\")\n",
    "            continue\n",
    "            \n",
    "        lake_geom = evolving_stationary_outlines_union_all_lakes_gdf.loc[mask, 'geometry'].iloc[0]\n",
    "        \n",
    "        # Convert to 4326 for area calculation\n",
    "        temp_gdf = gpd.GeoDataFrame(\n",
    "            index=[0], crs='EPSG:3031', geometry=[lake_geom])\n",
    "        temp_gdf = temp_gdf.to_crs('4326')\n",
    "        area = calculate_geodesic_area(temp_gdf['geometry'].iloc[0])\n",
    "        \n",
    "        if area is not None:\n",
    "            evolving_stationary_outlines_union_all_lakes_gdf.loc[mask, 'area (m^2)'] = area\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing non-evolving lake {lake_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Delete intermediary GDFs\n",
    "del stationary_outlines_gdf_evolving_lakes, stationary_outlines_gdf_all_lakes\n",
    "\n",
    "# Make additional_lakes_gdf for Site A, LSLM, and LSLC whose stationary outlines were removed \n",
    "# because they were approximations using point location and reported area instead of an actual outline\n",
    "initial_lakes = ['Site_A', 'LowerConwaySubglacialLake', 'LowerEngelhardtSubglacialLake', 'LowerMercerSubglacialLake']\n",
    "additional_lakes_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'].isin(initial_lakes)].copy()\n",
    "\n",
    "# Add these additional lakes as rows to the union gdf's\n",
    "for idx, row in additional_lakes_gdf.iterrows():\n",
    "    try:\n",
    "        lake_ps = additional_lakes_gdf.loc[idx]\n",
    "        lake_name = lake_ps['name']\n",
    "        \n",
    "        try:\n",
    "            evolving_outlines_gdf = gpd.read_file('output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping because evolving outlines geojson file not found for {lake_name}.\")\n",
    "            continue\n",
    "           \n",
    "        # Only get evolving outlines union since there's no stationary outline\n",
    "        evolving_union_gs = find_evolving_union(lake_ps, evolving_outlines_gdf, incl_stationary=False)\n",
    "        \n",
    "        if evolving_union_gs is None:\n",
    "            print(f\"Skipping {lake_name}: Could not create union of outlines\")\n",
    "            continue\n",
    "           \n",
    "        try:\n",
    "            # Create temporary GeoDataFrame with the union\n",
    "            temp_gdf = gpd.GeoDataFrame(\n",
    "                index=[0], \n",
    "                crs='EPSG:3031', \n",
    "                geometry=[evolving_union_gs.iloc[0]])\n",
    "            \n",
    "            # Convert to 4326 for area calculation\n",
    "            temp_gdf = temp_gdf.to_crs('4326')\n",
    "            \n",
    "            # Calculate area\n",
    "            area = calculate_geodesic_area(temp_gdf['geometry'].iloc[0])\n",
    "            \n",
    "            # Create new row from the current lake's data\n",
    "            new_row_gdf = gpd.GeoDataFrame([{\n",
    "                'name': lake_name,\n",
    "                'area (m^2)': area if area is not None else None,\n",
    "                'cite': lake_ps['cite'],\n",
    "                'CS2_SARIn_start': lake_ps['CS2_SARIn_start'],\n",
    "                'geometry': evolving_union_gs.iloc[0]\n",
    "            }], crs='EPSG:3031')\n",
    "\n",
    "            # For evolving_outlines_union_gdf\n",
    "            if not lake_name in evolving_outlines_union_gdf['name'].values:\n",
    "                evolving_outlines_union_gdf = pd.concat([\n",
    "                    evolving_outlines_union_gdf, \n",
    "                    new_row_gdf\n",
    "                ], ignore_index=True)\n",
    "            else:\n",
    "                print(f'Skipping {lake_name} for evolving_outlines_union_gdf - already exists')\n",
    "                \n",
    "            # Check if the lake is already in either GeoDataFrame and only append if it's new\n",
    "            # For evolving_stationary_outlines_union_evolving_lakes_gdf\n",
    "            if not lake_name in evolving_stationary_outlines_union_evolving_lakes_gdf['name'].values:\n",
    "                evolving_stationary_outlines_union_evolving_lakes_gdf = pd.concat([\n",
    "                    evolving_stationary_outlines_union_evolving_lakes_gdf, \n",
    "                    new_row_gdf], ignore_index=True)\n",
    "            else:\n",
    "                print(f'Skipping {lake_name} for evolving_stationary_outlines_union_evolving_lakes_gdf - already exists')\n",
    "\n",
    "            # Check if the lake is already in either GeoDataFrame and only append if it's new\n",
    "            # For evolving_stationary_outlines_union_evolving_lakes_gdf\n",
    "            if not lake_name in evolving_stationary_outlines_union_all_lakes_gdf['name'].values:\n",
    "                evolving_stationary_outlines_union_all_lakes_gdf = pd.concat([\n",
    "                    evolving_stationary_outlines_union_all_lakes_gdf, \n",
    "                    new_row_gdf], ignore_index=True)\n",
    "            else:\n",
    "                print(f'Skipping {lake_name} for evolving_stationary_outlines_union_evolving_lakes_gdf - already exists')\n",
    "\n",
    "           \n",
    "        except (IndexError, AttributeError) as e:\n",
    "            print(f\"Error processing geometries for {lake_name}: {str(e)}\")\n",
    "            continue\n",
    "           \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error processing {lake_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Sort the GeoDataFrame alphabetically by the 'name' column; Reset the index after sorting; Reproject GeoDataFrame to EPSG:3031\n",
    "evolving_outlines_union_gdf = evolving_outlines_union_gdf.sort_values('name').reset_index(drop=True).set_crs('3031')\n",
    "evolving_stationary_outlines_union_evolving_lakes_gdf = evolving_stationary_outlines_union_evolving_lakes_gdf.sort_values('name').reset_index(drop=True)\n",
    "evolving_stationary_outlines_union_all_lakes_gdf = evolving_stationary_outlines_union_all_lakes_gdf.sort_values('name').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c94a8-b762-437b-8e93-be5a0b098df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows where processing failed (geometry is None or invalid)\n",
    "evolving_outlines_union_gdf = evolving_outlines_union_gdf[\n",
    "    evolving_outlines_union_gdf.geometry.notna() & \n",
    "    evolving_outlines_union_gdf.geometry.is_valid]\n",
    "\n",
    "evolving_stationary_outlines_union_evolving_lakes_gdf = evolving_stationary_outlines_union_evolving_lakes_gdf[\n",
    "    evolving_stationary_outlines_union_evolving_lakes_gdf.geometry.notna() & \n",
    "    evolving_stationary_outlines_union_evolving_lakes_gdf.geometry.is_valid]\n",
    "\n",
    "evolving_stationary_outlines_union_all_lakes_gdf = evolving_stationary_outlines_union_all_lakes_gdf[\n",
    "    evolving_stationary_outlines_union_all_lakes_gdf.geometry.notna() & \n",
    "    evolving_stationary_outlines_union_all_lakes_gdf.geometry.is_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d9d33b-b0c0-4abc-90e2-0141c34cd589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print lengths of GDFs to ensure everything worked properly\n",
    "print(\"Length of GDF's for all lakes:\")\n",
    "print(len(revised_stationary_outlines_gdf))\n",
    "print(len(evolving_stationary_outlines_union_all_lakes_gdf))\n",
    "\n",
    "# List all folders in target directory\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "\n",
    "# Filter stationary_outlines_gdf to only include lakes that have evolving outlines found at them\n",
    "stationary_outlines_gdf_evolving_lakes = filter_gdf_by_folder_contents(revised_stationary_outlines_gdf, folder_path, file_extension='geojson', exclude=False)\n",
    "print(\"Length of GDF's for lakes that were found to have evolving outlines:\")\n",
    "print(len(stationary_outlines_gdf_evolving_lakes))\n",
    "print(len(evolving_outlines_union_gdf))\n",
    "print(len(evolving_stationary_outlines_union_evolving_lakes_gdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920832b9-a1f2-43a6-9948-2ece19b69777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_names(df1, df2):\n",
    "    \"\"\"\n",
    "    Compare name columns between two dataframes and find unique entries.\n",
    "    \n",
    "    Parameters:\n",
    "    df1, df2: pandas DataFrames containing a 'name' column\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (names_only_in_df1, names_only_in_df2)\n",
    "    \"\"\"\n",
    "    # Find names that are in df1 but not in df2\n",
    "    only_in_df1 = df1[~df1['name'].isin(df2['name'])]['name']\n",
    "    \n",
    "    # Find names that are in df2 but not in df1\n",
    "    only_in_df2 = df2[~df2['name'].isin(df1['name'])]['name']\n",
    "    \n",
    "    return only_in_df1, only_in_df2\n",
    "\n",
    "# Example usage:\n",
    "names_df1_only, names_df2_only = compare_names(revised_stationary_outlines_gdf, evolving_stationary_outlines_union_all_lakes_gdf)\n",
    "print(\"Names only in first dataframe:\", names_df1_only.tolist())\n",
    "print(\"Names only in second dataframe:\", names_df2_only.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f100e4c8-4e31-41d3-837a-a49b6dbd3b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "stationary_outlines_gdf.boundary.plot(ax=ax, color='blue')\n",
    "evolving_outlines_union_gdf.boundary.plot(ax=ax, color='red')\n",
    "evolving_stationary_outlines_union_evolving_lakes_gdf.boundary.plot(ax=ax, color='purple')\n",
    "evolving_stationary_outlines_union_all_lakes_gdf.boundary.plot(ax=ax, color='k', linestyle='dashed')\n",
    "Scripps_landice.boundary.plot(ax=ax, lw=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5512d2-a3ea-4bb3-80fb-3c1fe8e3aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "evolving_outlines_union_gdf.to_file('output/lake_outlines/evolving_outlines_union_gdf.geojson', driver='GeoJSON')\n",
    "evolving_stationary_outlines_union_evolving_lakes_gdf.to_file('output/lake_outlines/evolving_stationary_outlines_union_evolving_lakes_gdf.geojson', driver='GeoJSON')\n",
    "evolving_stationary_outlines_union_all_lakes_gdf.to_file('output/lake_outlines/evolving_stationary_outlines_union_all_lakes_gdf.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d35df-ec1a-4713-9a9b-9e3c3d0c32ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import to ensure saved properly\n",
    "evolving_outlines_union_gdf = gpd.read_file(\n",
    "    'output/lake_outlines/evolving_outlines_union_gdf.geojson')\n",
    "evolving_stationary_outlines_union_evolving_lakes_gdf = gpd.read_file(\n",
    "    'output/lake_outlines/evolving_stationary_outlines_union_evolving_lakes_gdf.geojson')\n",
    "evolving_stationary_outlines_union_all_lakes_gdf = gpd.read_file(\n",
    "    'output/lake_outlines/evolving_stationary_outlines_union_all_lakes_gdf.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38187942-2948-4996-a955-18782ee8a52a",
   "metadata": {},
   "source": [
    "# Geometric calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fcfcbcb3-cec0-4bb4-9e07-5e918fdcdaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load geodataframes needed for these geometric calculations\n",
    "revised_stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/revised_stationary_outlines_gdf.geojson')\n",
    "evolving_outlines_union_gdf = gpd.read_file('output/lake_outlines/evolving_outlines_union_gdf.geojson')\n",
    "evolving_stationary_outlines_union_evolving_lakes_gdf = gpd.read_file('output/lake_outlines/evolving_stationary_outlines_union_evolving_lakes_gdf.geojson')\n",
    "evolving_stationary_outlines_union_all_lakes_gdf = gpd.read_file('output/lake_outlines/evolving_stationary_outlines_union_all_lakes_gdf.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd862020-3685-4bf3-9f99-ad008c0326fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories before processing any lakes\n",
    "os.makedirs('output/geometric_calcs/evolving_outlines_geom_calc', exist_ok=True)\n",
    "os.makedirs('output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes', exist_ok=True)\n",
    "os.makedirs('output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes', exist_ok=True)\n",
    "os.makedirs('output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes', exist_ok=True)\n",
    "os.makedirs('output/geometric_calcs/stationary_outline_geom_calc/evolving_stationary_union_at_evolving_lakes', exist_ok=True)\n",
    "os.makedirs('output/geometric_calcs/stationary_outline_geom_calc/evolving_stationary_union_at_all_lakes', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c966f417-1d75-41c5-8cef-f7e50a3f49c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 lake(s) to process.\n",
      "1 lakes remain.\n",
      "Processing lake: David_s1\n",
      "2010-08-17T06:45:00.000000000\n",
      "2010-11-16T11:15:00.000000000\n",
      "2011-02-15T15:45:00.000000000\n",
      "2011-05-17T23:15:00.000000000\n",
      "2011-08-17T06:45:00.000000000\n",
      "2011-11-16T11:15:00.000000000\n",
      "2012-02-15T15:45:00.000000000\n",
      "2012-05-16T23:15:00.000000000\n",
      "2012-08-16T06:45:00.000000000\n",
      "2012-11-15T23:15:00.000000000\n",
      "2013-02-15T15:45:00.000000000\n",
      "2013-05-17T23:15:00.000000000\n",
      "2013-08-17T06:45:00.000000000\n",
      "2013-11-16T11:15:00.000000000\n",
      "2014-02-15T15:45:00.000000000\n",
      "2014-05-17T23:15:00.000000000\n",
      "2014-08-17T06:45:00.000000000\n",
      "2014-11-16T11:15:00.000000000\n",
      "2015-02-15T15:45:00.000000000\n",
      "2015-05-17T23:15:00.000000000\n",
      "2015-08-17T06:45:00.000000000\n",
      "2015-11-16T11:15:00.000000000\n",
      "2016-02-15T15:45:00.000000000\n",
      "2016-05-16T23:15:00.000000000\n",
      "2016-08-16T06:45:00.000000000\n",
      "2016-11-15T23:15:00.000000000\n",
      "2017-02-15T15:45:00.000000000\n",
      "2017-05-17T23:15:00.000000000\n",
      "2017-08-17T06:45:00.000000000\n",
      "2017-11-16T11:15:00.000000000\n",
      "2018-02-15T15:45:00.000000000\n",
      "2018-05-17T23:15:00.000000000\n",
      "2018-08-17T06:45:00.000000000\n",
      "2018-11-16T11:15:00.000000000\n",
      "Results saved to: output/geometric_calcs/evolving_outlines_geom_calc/David_s1.csv\n",
      "Processing lake: David_s1\n",
      "Results saved to: output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes/David_s1.csv\n",
      "Processing lake: David_s1\n",
      "Results saved to: output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/David_s1.csv\n",
      "Processing lake: David_s1\n",
      "Results saved to: output/geometric_calcs/stationary_outline_geom_calc/evolving_stationary_union_at_evolving_lakes/David_s1.csv\n",
      "All remaining lakes have been processed.\n"
     ]
    }
   ],
   "source": [
    "# Get list of remaining lakes left to process based on\n",
    "\n",
    "# 1) Filter out lakes that have evolving outlines\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "evolving_lakes_gdf = filter_gdf_by_folder_contents(revised_stationary_outlines_gdf, \n",
    "    folder_path, exclude=True, file_extension='txt')\n",
    "\n",
    "# 2) Filter out lakes that have already been processed in each directory\n",
    "folder_paths = [\n",
    "    OUTPUT_DIR_GIT + '/geometric_calcs/evolving_outlines_geom_calc/',\n",
    "    OUTPUT_DIR_GIT + '/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/',\n",
    "    OUTPUT_DIR_GIT + '/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes/',\n",
    "    OUTPUT_DIR_GIT + '/geometric_calcs/stationary_outline_geom_calc/evolving_stationary_union_at_evolving_lakes/',\n",
    "]\n",
    "\n",
    "# Create a list to store filtered GeoDataFrames\n",
    "filtered_gdfs = []\n",
    "\n",
    "# Filter the original GeoDataFrame for each directory and add to list\n",
    "for folder_path in folder_paths:\n",
    "    filtered_gdf = filter_gdf_by_folder_contents(evolving_lakes_gdf, folder_path)\n",
    "    if not filtered_gdf.empty:\n",
    "        filtered_gdfs.append(filtered_gdf)\n",
    "\n",
    "# Concatenate all filtered GeoDataFrames and drop duplicates\n",
    "if filtered_gdfs:\n",
    "    remaining_lakes = pd.concat(filtered_gdfs, ignore_index=True)\n",
    "    remaining_lakes = remaining_lakes.drop_duplicates(subset='name').reset_index(drop=True)\n",
    "else:\n",
    "    remaining_lakes = gpd.GeoDataFrame(columns=evolving_lakes_gdf.columns)\n",
    "\n",
    "print(f\"{len(remaining_lakes)} lake(s) to process.\")\n",
    "\n",
    "# Process geometric calculations on lakes\n",
    "total_lakes = len(remaining_lakes)\n",
    "processed_lakes = 0\n",
    "\n",
    "for i in remaining_lakes.index:\n",
    "    remaining_count = total_lakes - processed_lakes\n",
    "    print(f\"{remaining_count} lakes remain.\")\n",
    "    \n",
    "    try:\n",
    "        # Isolate lake from remaining_lakes as geodataframe\n",
    "        stationary_outline_gdf = remaining_lakes.loc[[i]]\n",
    "        lake_name = stationary_outline_gdf['name'].iloc[0]\n",
    "        \n",
    "        if stationary_outline_gdf.empty:\n",
    "            print(\"Skipping empty lake entry\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare data sets\n",
    "        dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(stationary_outline_gdf, 30)\n",
    "    \n",
    "        # Check if lake has evolving outlines\n",
    "        has_evolving_outlines = os.path.exists(os.path.join(\n",
    "            os.getcwd(), 'output/lake_outlines/evolving_outlines', f\"{stationary_outline_gdf['name'].iloc[0]}.geojson\"))\n",
    "        \n",
    "        if has_evolving_outlines:\n",
    "            # Calculate active area, dh, and dV for lakes with evolving outlines\n",
    "            evolving_outlines_geom_calc(stationary_outline_gdf=stationary_outline_gdf,\n",
    "                dataset1=dataset1_masked, dataset2=dataset2_masked)\n",
    "\n",
    "            # And for stationary outlines (at evolving lakes only)\n",
    "            stationary_outline_geom_calc(stationary_outline_gdf=stationary_outline_gdf,\n",
    "               dataset1=dataset1_masked, dataset2=dataset2_masked, sub_dir='stationary_outlines_at_evolving_lakes')\n",
    "            \n",
    "            # And for evolving outlines union (at evolving lakes only)\n",
    "            evolving_union_gdf = evolving_outlines_union_gdf[\n",
    "                evolving_outlines_union_gdf['name'] == lake_name]\n",
    "\n",
    "            if not evolving_union_gdf.empty:\n",
    "                stationary_outline_geom_calc(stationary_outline_gdf=evolving_union_gdf,\n",
    "                    dataset1=dataset1_masked, dataset2=dataset2_masked, sub_dir='evolving_union_at_evolving_lakes')\n",
    "            else:\n",
    "                print(f\"No evolving union outline found for lake: {lake_name}\")\n",
    "            \n",
    "            # And for evolving-stationary outlines union (at evolving lakes only)\n",
    "            evolving_stationary_union_gdf = evolving_stationary_outlines_union_evolving_lakes_gdf[\n",
    "                evolving_stationary_outlines_union_evolving_lakes_gdf['name'] == lake_name]\n",
    "\n",
    "            if not evolving_stationary_union_gdf.empty:\n",
    "                stationary_outline_geom_calc(stationary_outline_gdf=evolving_stationary_union_gdf,\n",
    "                    dataset1=dataset1_masked, dataset2=dataset2_masked, sub_dir='evolving_stationary_union_at_evolving_lakes')\n",
    "            else:\n",
    "                print(f\"No evolving-stationary union outline found for lake: {lake_name}\")\n",
    "            \n",
    "        # Increment processing counter\n",
    "        processed_lakes += 1\n",
    "        # Clear output of each index\n",
    "        # clear_output(wait=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing lake '{lake_name}' at index {i}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "if processed_lakes == total_lakes:\n",
    "    print(\"All remaining lakes have been processed.\")\n",
    "else:\n",
    "    print(f\"Processing complete. {total_lakes - processed_lakes} lakes were skipped due to errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "222fc432-78dc-4a64-9655-a3a67e155748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 lake(s) to process.\n",
      "All remaining lakes have been processed.\n"
     ]
    }
   ],
   "source": [
    "# Get list of remaining lakes left to process based on\n",
    "\n",
    "# Filter out lakes that have already been processed in each directory\n",
    "folder_paths = [\n",
    "    OUTPUT_DIR_GIT + '/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes/',\n",
    "    OUTPUT_DIR_GIT + '/geometric_calcs/stationary_outline_geom_calc/evolving_stationary_union_at_all_lakes/'\n",
    "]\n",
    "\n",
    "# Create a list to store filtered GeoDataFrames\n",
    "filtered_gdfs = []\n",
    "\n",
    "# Filter the original GeoDataFrame for each directory and add to list\n",
    "for folder_path in folder_paths:\n",
    "    filtered_gdf = filter_gdf_by_folder_contents(revised_stationary_outlines_gdf, folder_path)\n",
    "    if not filtered_gdf.empty:\n",
    "        filtered_gdfs.append(filtered_gdf)\n",
    "\n",
    "# Concatenate all filtered GeoDataFrames and drop duplicates\n",
    "if filtered_gdfs:\n",
    "    remaining_lakes = pd.concat(filtered_gdfs, ignore_index=True)\n",
    "    remaining_lakes = remaining_lakes.drop_duplicates(subset='name').reset_index(drop=True)\n",
    "else:\n",
    "    remaining_lakes = gpd.GeoDataFrame(columns=revised_stationary_outlines_gdf.columns)\n",
    "\n",
    "print(f\"{len(remaining_lakes)} lake(s) to process.\")\n",
    "\n",
    "# Process geometric calculations on lakes\n",
    "total_lakes = len(remaining_lakes)\n",
    "processed_lakes = 0\n",
    "\n",
    "for i in remaining_lakes.index:\n",
    "    remaining_count = total_lakes - processed_lakes\n",
    "    print(f\"{remaining_count} lakes remain.\")\n",
    "    \n",
    "    try:\n",
    "        # Isolate lake from remaining_lakes as geodataframe\n",
    "        stationary_outline_gdf = remaining_lakes.loc[[i]]\n",
    "        lake_name = stationary_outline_gdf['name'].iloc[0]\n",
    "        \n",
    "        if stationary_outline_gdf.empty:\n",
    "            print(\"Skipping empty lake entry\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare data sets\n",
    "        dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(stationary_outline_gdf, 30)\n",
    "\n",
    "        # And for stationary outlines (at all lakes)\n",
    "        stationary_outline_geom_calc(stationary_outline_gdf=stationary_outline_gdf,\n",
    "           dataset1=dataset1_masked, dataset2=dataset2_masked, sub_dir='stationary_outlines_at_all_lakes')\n",
    "        # And for evolving-stationary outlines union (at all lakes)\n",
    "        evolving_stationary_union_gdf = evolving_stationary_outlines_union_all_lakes_gdf[\n",
    "            evolving_stationary_outlines_union_all_lakes_gdf['name'] == lake_name]\n",
    "        if not evolving_stationary_union_gdf.empty:\n",
    "            stationary_outline_geom_calc(stationary_outline_gdf=evolving_stationary_union_gdf,\n",
    "                dataset1=dataset1_masked, dataset2=dataset2_masked, sub_dir='evolving_stationary_union_at_all_lakes')\n",
    "        else:\n",
    "            print(f\"No evolving-stationary union outline found for lake: {lake_name}\")\n",
    "            \n",
    "        # Increment processing counter\n",
    "        processed_lakes += 1\n",
    "        # Clear output of each index\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing lake '{lake_name}' at index {i}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "if processed_lakes == total_lakes:\n",
    "    print(\"All remaining lakes have been processed.\")\n",
    "else:\n",
    "    print(f\"Processing complete. {total_lakes - processed_lakes} lakes were skipped due to errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e438f2-91c7-42c3-9d8b-b4b967af55f0",
   "metadata": {},
   "source": [
    "# Visualizations of geometric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7dc31d79-fba8-4065-9f4b-2862db687bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load datasets necessary for plotting\n",
    "# revised_stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/revised_stationary_outlines_gdf.geojson')\n",
    "# evolving_outlines_union_gdf = gpd.read_file('output/lake_outlines/evolving_outlines_union_gdf.geojson')\n",
    "# evolving_stationary_outlines_union_evolving_lakes_gdf = gpd.read_file('output/lake_outlines/evolving_stationary_outlines_union_evolving_lakes_gdf.geojson')\n",
    "# evolving_stationary_outlines_union_all_lakes_gdf = gpd.read_file('output/lake_outlines/evolving_stationary_outlines_union_all_lakes_gdf.geojson')\n",
    "\n",
    "# # Get list of remaining lakes left to process based on\n",
    "# # 1) lakes that have evolving outlines\n",
    "# folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "# remaining_lakes = filter_gdf_by_folder_contents(revised_stationary_outlines_gdf, \n",
    "#     folder_path, exclude=True, file_extension='txt')\n",
    "\n",
    "# # 2) lakes that have already be processed using the desired func\n",
    "# folder_path = OUTPUT_DIR + '/plot_evolving_and_stationary_comparison'\n",
    "# os.makedirs(folder_path, exist_ok=True)\n",
    "# remaining_lakes = filter_gdf_by_folder_contents(remaining_lakes, folder_path, exclude=True)\n",
    "\n",
    "# # Reset index to avoid indexing issues\n",
    "# remaining_lakes = remaining_lakes.reset_index(drop=True)\n",
    "\n",
    "# if remaining_lakes.empty:\n",
    "#     print(\"All lakes processed.\")\n",
    "# else:\n",
    "#     while not remaining_lakes.empty:\n",
    "#         print(f\"{len(remaining_lakes)} lake(s) remain.\")\n",
    "        \n",
    "#         # Always get the first lake to process\n",
    "#         lake_gdf = remaining_lakes.iloc[[0]]  # Always take the first row\n",
    "#         lake_name = lake_gdf['name'].iloc[0]\n",
    "\n",
    "#         if lake_gdf.empty:\n",
    "#             print(\"Warning: Empty lake_gdf. Skipping...\")\n",
    "#             continue\n",
    "\n",
    "#         try:\n",
    "#             plot_evolving_and_stationary_comparison(lake_gdf)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing lake {lake_name}: {str(e)}\")\n",
    "#             continue\n",
    "\n",
    "#         # Recheck remaining lakes after processing\n",
    "#         remaining_lakes = filter_gdf_by_folder_contents(remaining_lakes, folder_path)\n",
    "\n",
    "#         if remaining_lakes is None or remaining_lakes.empty:\n",
    "#             print(\"No lakes remain after recheck.\")\n",
    "#             break\n",
    "\n",
    "#         # Reset index to prevent index misalignment\n",
    "#         remaining_lakes = remaining_lakes.reset_index(drop=True)\n",
    "\n",
    "#         # Clear output of each index\n",
    "#         clear_output(wait=True)\n",
    "\n",
    "#     print(\"All lakes processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1c6fceb9-a3f3-4cb0-baeb-f7557a0b6933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get list of remaining lakes left to process based on\n",
    "\n",
    "# # 1) lakes that have evolving outlines\n",
    "# folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "# remaining_lakes = filter_gdf_by_folder_contents(revised_stationary_outlines_gdf, \n",
    "#     folder_path, file_extension='txt')\n",
    "\n",
    "# # 2) lakes that have already be processed\n",
    "# folder_path = OUTPUT_DIR + '/plot_evolving_and_stationary_comparison_sequential'\n",
    "# os.makedirs(folder_path, exist_ok=True)\n",
    "# remaining_lakes = filter_gdf_by_folder_contents(remaining_lakes, folder_path, file_extension='mp4')\n",
    "\n",
    "# if remaining_lakes.empty:\n",
    "#     print(\"All lakes processed.\")\n",
    "# else:\n",
    "#     for i in range(len(remaining_lakes)):\n",
    "#         print(len(remaining_lakes), 'lake(s) remain.')\n",
    "        \n",
    "#         # Process the lake\n",
    "#         lake_gdf = remaining_lakes.iloc[0:1]\n",
    "#         plot_evolving_and_stationary_comparison_sequential(lake_gdf)\n",
    "\n",
    "#         # Recheck which lakes still need processing\n",
    "#         remaining_lakes = filter_gdf_by_folder_contents(remaining_lakes, folder_path)\n",
    "\n",
    "#         # Clear output of each index\n",
    "#         clear_output(wait=True)\n",
    "\n",
    "#         if remaining_lakes.empty:\n",
    "#             print(\"All lakes processed.\")\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78b888c-22cd-47a7-8c12-d206ac090d7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TESTING: help me to combine these two code blocks into one that will check both plot_evolving_and_stationary_comparison and plot_evolving_and_stationary_comparison_sequential folders, and if the lake has not been processed in both, then run both functions, plot_evolving_and_stationary_comparison and plot_evolving_and_stationary_comparison_sequential.\n",
    "\n",
    "# Load geodataframes needed\n",
    "revised_stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/revised_stationary_outlines_gdf.geojson')\n",
    "evolving_outlines_union_gdf = gpd.read_file('output/lake_outlines/evolving_outlines_union_gdf.geojson')\n",
    "evolving_stationary_outlines_union_evolving_lakes_gdf = gpd.read_file('output/lake_outlines/evolving_stationary_outlines_union_evolving_lakes_gdf.geojson')\n",
    "# evolving_stationary_outlines_union_all_lakes_gdf = gpd.read_file('output/lake_outlines/evolving_stationary_outlines_union_all_lakes_gdf.geojson')\n",
    "\n",
    "# Get list of remaining lakes left to process based on\n",
    "# 1) lakes that have evolving outlines\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "remaining_lakes = filter_gdf_by_folder_contents(revised_stationary_outlines_gdf, \n",
    "    folder_path, exclude=True, file_extension='txt')\n",
    "\n",
    "# 2) Check which lakes need processing in either of the two output folders\n",
    "comparison_folder = OUTPUT_DIR + '/plot_evolving_and_stationary_comparison'\n",
    "sequential_folder = OUTPUT_DIR + '/plot_evolving_and_stationary_comparison_sequential'\n",
    "\n",
    "# Make sure both output directories exist\n",
    "os.makedirs(comparison_folder, exist_ok=True)\n",
    "os.makedirs(sequential_folder, exist_ok=True)\n",
    "\n",
    "# Reset index to avoid indexing issues\n",
    "remaining_lakes = remaining_lakes.reset_index(drop=True)\n",
    "\n",
    "if remaining_lakes.empty:\n",
    "    print(\"All lakes processed.\")\n",
    "else:\n",
    "    while not remaining_lakes.empty:\n",
    "        print(f\"{len(remaining_lakes)} lake(s) remain.\")\n",
    "        \n",
    "        # Always get the first lake to process\n",
    "        lake_gdf = remaining_lakes.iloc[[0]]  # Always take the first row\n",
    "        lake_name = lake_gdf['name'].iloc[0]\n",
    "        \n",
    "        if lake_gdf.empty:\n",
    "            print(f\"Warning: Empty lake_gdf for {lake_name}. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Check if the lake needs processing in the comparison folder\n",
    "        needs_comparison = not lake_gdf['name'].iloc[0] in [\n",
    "            os.path.splitext(f)[0] for f in os.listdir(comparison_folder)\n",
    "        ]\n",
    "        \n",
    "        # Check if the lake needs processing in the sequential folder\n",
    "        needs_sequential = not lake_gdf['name'].iloc[0] in [\n",
    "            os.path.splitext(f)[0] for f in os.listdir(sequential_folder) \n",
    "            if f.endswith('.mp4')\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            # Process with the first function if needed\n",
    "            if needs_comparison:\n",
    "                print(f\"Processing {lake_name} with plot_evolving_and_stationary_comparison\")\n",
    "                plot_evolving_and_stationary_comparison(lake_gdf)\n",
    "            \n",
    "            # Process with the second function if needed\n",
    "            if needs_sequential:\n",
    "                print(f\"Processing {lake_name} with plot_evolving_and_stationary_comparison_sequential\")\n",
    "                plot_evolving_and_stationary_comparison_sequential(lake_gdf)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing lake {lake_name}: {str(e)}\")\n",
    "            continue\n",
    "        \n",
    "        # Recheck remaining lakes after processing\n",
    "        # A lake is considered processed if it exists in both folders\n",
    "        processed_in_comparison = filter_gdf_by_folder_contents(\n",
    "            remaining_lakes, comparison_folder, exclude=False)\n",
    "        processed_in_sequential = filter_gdf_by_folder_contents(\n",
    "            remaining_lakes, sequential_folder, exclude=False, file_extension='mp4')\n",
    "        \n",
    "        # Lakes that have been processed in both folders\n",
    "        processed_lakes = pd.merge(\n",
    "            processed_in_comparison, processed_in_sequential, \n",
    "            on=list(processed_in_comparison.columns)\n",
    "        )\n",
    "        \n",
    "        # Remove processed lakes from remaining_lakes\n",
    "        if not processed_lakes.empty:\n",
    "            remaining_lakes = remaining_lakes[\n",
    "                ~remaining_lakes['name'].isin(processed_lakes['name'])\n",
    "            ]\n",
    "        \n",
    "        # Reset index to prevent index misalignment\n",
    "        remaining_lakes = remaining_lakes.reset_index(drop=True)\n",
    "        \n",
    "        # Clear output of each index\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        if remaining_lakes.empty:\n",
    "            print(\"All lakes processed.\")\n",
    "            break\n",
    "            \n",
    "    print(\"All lakes processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e55cf8a-3eea-4bf5-8daa-3f199633f87b",
   "metadata": {},
   "source": [
    "# Final check that all lakes have been processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92df904-9b3d-4471-8bf0-d12206e32e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load geodataframe needed\n",
    "revised_stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/revised_stationary_outlines_gdf.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff17b42-ea5b-4e56-b44a-0daf34850c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all lakes were reanalyzed\n",
    "print('Analysis done on all previously identified lakes')\n",
    "\n",
    "print(len(stationary_outlines_gdf), 'lakes reanalyzed')\n",
    "\n",
    "dir = OUTPUT_DIR + '/levels'\n",
    "print(len([f for f in os.listdir(dir) if os.path.isfile(os.path.join(dir, f))]), \n",
    "      'lakes analyzed using find_and_save_optimal_paraters func')\n",
    "\n",
    "# Breakdown of lakes where evolving outlines were found vs. not\n",
    "print(len([f for f in os.listdir('output/lake_outlines/evolving_outlines') if f.endswith('.geojson')]),\n",
    "      'lakes found to have evolving outlines')\n",
    "print(len([f for f in os.listdir('output/lake_outlines/evolving_outlines') if f.endswith('.txt')]),\n",
    "      'lakes found to have no evolving outlines')\n",
    "print(len([f for f in os.listdir('output/lake_outlines/evolving_outlines') if f.endswith('.geojson')]) +\n",
    "      len([f for f in os.listdir('output/lake_outlines/evolving_outlines') if f.endswith('.txt')]), \n",
    "      'sum of lakes found with and without evolving outlines')\n",
    "\n",
    "# Analysis done on evolving lakes\n",
    "print('\\nAnalysis done only on previously identified lakes found to have evolving outlines')\n",
    "\n",
    "# These directories will have four more csv files because of continental sums files for four sets of lakes\n",
    "# so if this code is rerun after the 'Continental summations' section, the counts will be higher than expected\n",
    "\n",
    "print(len(revised_stationary_outlines_gdf), \n",
    "    'lakes analyzed in revised inventory due to Site_B and Site_C being combined into Site_BC')\n",
    "\n",
    "dir = 'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.csv')]),\n",
    "      'lakes analyzed using stationary_outline_geom_calc/stationary_outlines_at_all_lakes')\n",
    "\n",
    "dir = 'output/geometric_calcs/stationary_outline_geom_calc/evolving_stationary_union_at_all_lakes'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.csv')]),\n",
    "      'lakes analyzed using stationary_outline_geom_calc/evolving_stationary_union_at_all_lakes')\n",
    "\n",
    "dir = 'output/geometric_calcs/evolving_outlines_geom_calc'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.csv') and not Path(f).stem.endswith('_sum')]),\n",
    "      'lakes analyzed using evolving_outlines_geom_calc')\n",
    "\n",
    "dir = 'output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.csv') and not Path(f).stem.endswith('_sum')]),\n",
    "      'lakes analyzed using stationary_outline_geom_calc/evolving_union_at_evolving_lakes')\n",
    "\n",
    "dir = 'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.csv') and not Path(f).stem.endswith('_sum')]),\n",
    "      'lakes analyzed using stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes')\n",
    "\n",
    "dir = 'output/geometric_calcs/stationary_outline_geom_calc/evolving_stationary_union_at_evolving_lakes'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.csv') and not Path(f).stem.endswith('_sum')]),\n",
    "      'lakes analyzed using stationary_outline_geom_calc/evolving_stationary_union_at_evolving_lakes')\n",
    "\n",
    "dir = OUTPUT_DIR + '/plot_evolving_and_stationary_comparison'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.png')]),\n",
    "      'lakes analyzed using plot_evolving_and_stationary_comparison')\n",
    "\n",
    "dir = OUTPUT_DIR + '/plot_evolving_and_stationary_comparison_sequential'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.mp4')]),\n",
    "      'lakes analyzed using plot_evolving_and_stationary_comparison_sequential')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5be2863-8295-476c-8929-1f809318ca49",
   "metadata": {},
   "source": [
    "# Continental summations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e265cdf-dfb8-407b-80d0-4d9d12d94bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load geodataframe needed\n",
    "revised_stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/revised_stationary_outlines_gdf.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ba2ece-c90f-4879-9835-e7f6df453e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of comparison types\n",
    "geom_calc_types = [\n",
    "    'stationary_outline_geom_calc/stationary_outlines_at_all_lakes',\n",
    "    'stationary_outline_geom_calc/evolving_stationary_union_at_all_lakes',\n",
    "    'evolving_outlines_geom_calc',\n",
    "    'stationary_outline_geom_calc/evolving_union_at_evolving_lakes',\n",
    "    'stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes',\n",
    "    'stationary_outline_geom_calc/evolving_stationary_union_at_evolving_lakes',\n",
    "]\n",
    "\n",
    "# Process each comparison type\n",
    "for geom_calc_type in geom_calc_types:\n",
    "    process_continental_sums(geom_calc_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442a61ae-b4a1-41c2-970d-c2a3f41d926a",
   "metadata": {},
   "source": [
    "## Explaining continental sum trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4306138-4da3-4898-8a31-b1dd1d621476",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'output/geometric_calcs/evolving_outlines_geom_calc'\n",
    "\n",
    "def create_interactive_plot(directory, stationary_outlines_gdf):\n",
    "    # Load and process all lake data\n",
    "    dfs = []\n",
    "    lake_names = []\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            lake_name = os.path.splitext(file)[0]\n",
    "            \n",
    "            # Process each lake's data\n",
    "            df['lake_name'] = lake_name\n",
    "            df['datetime'] = pd.to_datetime(df['midcyc_datetime'])\n",
    "            df['cumsum_vol'] = np.cumsum(df['evolving_outlines_dV_corr (m^3)'])\n",
    "            dfs.append(df)\n",
    "            lake_names.append(lake_name)\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Create plot using hvplot\n",
    "    plot = combined_df.hvplot.line(\n",
    "        x='datetime',\n",
    "        y='cumsum_vol',\n",
    "        by='lake_name',\n",
    "        width=800,\n",
    "        height=400,\n",
    "        title='Lake Volume Changes Over Time',\n",
    "        xlabel='Date',\n",
    "        ylabel='Cumulative Volume Change (m)',\n",
    "        grid=True,\n",
    "        legend=False\n",
    "    )\n",
    "    \n",
    "    return plot\n",
    "\n",
    "# Usage:\n",
    "plot = create_interactive_plot(directory, stationary_outlines_gdf)\n",
    "plot  # Display in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03af0cc7-93fe-43ce-b036-1594040f8ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory = 'output/geometric_calcs/evolving_outlines_geom_calc'\n",
    "\n",
    "def create_interactive_plot(directory, stationary_outlines_gdf):\n",
    "    # Load and process all lake data\n",
    "    dfs_subset_CS2_IS2_lakes = []\n",
    "    lake_names = []\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            lake_name = os.path.splitext(file)[0]\n",
    "            \n",
    "            # Process each lake's data\n",
    "            df['lake_name'] = lake_name\n",
    "            df['datetime'] = pd.to_datetime(df['midcyc_datetime'])\n",
    "            df['cumsum_vol'] = np.cumsum(np.divide(df['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "            dfs_subset_CS2_IS2_lakes.append(df)\n",
    "            lake_names.append(lake_name)\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(dfs_subset_CS2_IS2_lakes, ignore_index=True)\n",
    "    \n",
    "    # Create the plot using Dataset and Curve\n",
    "    dataset = hv.Dataset(combined_df)\n",
    "    curves = dataset.to(hv.Curve, \n",
    "                       kdims=['datetime'], \n",
    "                       vdims=['cumsum_vol', 'lake_name'],\n",
    "                       groupby='lake_name')\n",
    "    \n",
    "    # Apply options to the plot\n",
    "    plot = curves.opts(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        tools=['hover'],\n",
    "        title='Lake Volume Changes Over Time',\n",
    "        xlabel='Date',\n",
    "        ylabel='Cumulative Volume Change (m)',\n",
    "        show_grid=True,\n",
    "        toolbar='above'\n",
    "    )\n",
    "    \n",
    "    return plot\n",
    "\n",
    "# Usage:\n",
    "plot = create_interactive_plot(directory, stationary_outlines_gdf)\n",
    "plot  # Display in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed060331-f121-4702-910c-e3c852aab6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e92875-6e7b-4a5d-8032-503997712fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the lakes driving the deviation of evolving and stationary \n",
    "\n",
    "evolving_directory = 'output/geometric_calcs/evolving_outlines_geom_calc'\n",
    "stationary_directory = 'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes'\n",
    "\n",
    "def process_lake_data(directory, is_evolving=True):\n",
    "    dfs = {}\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            lake_name = os.path.splitext(file)[0]\n",
    "            \n",
    "            # Process each lake's data\n",
    "            df['lake_name'] = lake_name\n",
    "            df['datetime'] = pd.to_datetime(df['midcyc_datetime'])\n",
    "            \n",
    "            # Calculate cumulative volume based on directory type\n",
    "            if is_evolving:\n",
    "                df['cumsum_vol'] = np.cumsum(df['evolving_outlines_dV_corr (m^3)'])\n",
    "            else:\n",
    "                df['cumsum_vol'] = np.cumsum(df['stationary_outline_dV_corr (m^3)'])\n",
    "            \n",
    "            dfs[lake_name] = df\n",
    "    \n",
    "    return dfs\n",
    "\n",
    "def create_interactive_plot(evolving_directory, stationary_directory):\n",
    "    # Load data from both directories\n",
    "    evolving_dfs = process_lake_data(evolving_directory, is_evolving=True)\n",
    "    stationary_dfs = process_lake_data(stationary_directory, is_evolving=False)\n",
    "    \n",
    "    # Initialize lists to store processed dataframes\n",
    "    plot_dfs = []\n",
    "    \n",
    "    # Process common lakes\n",
    "    common_lakes = set(evolving_dfs.keys()) & set(stationary_dfs.keys())\n",
    "    for lake_name in common_lakes:\n",
    "        evolving_df = evolving_dfs[lake_name].copy()\n",
    "        stationary_df = stationary_dfs[lake_name].copy()\n",
    "        \n",
    "        # Calculate difference (evolving - stationary)\n",
    "        merged_df = pd.merge(\n",
    "            evolving_df[['datetime', 'cumsum_vol']], \n",
    "            stationary_df[['datetime', 'cumsum_vol']], \n",
    "            on='datetime', \n",
    "            suffixes=('_evolving', '_stationary')\n",
    "        )\n",
    "        merged_df['cumsum_vol'] = merged_df['cumsum_vol_evolving'] - merged_df['cumsum_vol_stationary']\n",
    "        merged_df['lake_name'] = lake_name + '_difference'\n",
    "        plot_dfs.append(merged_df[['datetime', 'cumsum_vol', 'lake_name']])\n",
    "    \n",
    "    # Process lakes only in stationary directory\n",
    "    stationary_only = set(stationary_dfs.keys()) - set(evolving_dfs.keys())\n",
    "    for lake_name in stationary_only:\n",
    "        df = stationary_dfs[lake_name].copy()\n",
    "        df['lake_name'] = lake_name + '_stationary'\n",
    "        plot_dfs.append(df[['datetime', 'cumsum_vol', 'lake_name']])\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(plot_dfs, ignore_index=True)\n",
    "    \n",
    "    # Create plot using hvplot\n",
    "    plot = combined_df.hvplot.line(\n",
    "        x='datetime',\n",
    "        y='cumsum_vol',\n",
    "        by='lake_name',\n",
    "        width=800,\n",
    "        height=400,\n",
    "        title='Lake Volume Changes Over Time',\n",
    "        xlabel='Date',\n",
    "        ylabel='Cumulative Volume Change (m)',\n",
    "        grid=True,\n",
    "        legend=False\n",
    "    )\n",
    "    \n",
    "    return plot\n",
    "\n",
    "# Usage:\n",
    "plot = create_interactive_plot(evolving_directory, stationary_directory)\n",
    "plot  # Display in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0d9ed4-5dc6-45a6-8af0-e51da56cae4d",
   "metadata": {},
   "source": [
    "# Fig. S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c602343-390f-48b2-a908-b932c7a4c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add something along the lines of code below to illustrate lake area/outline for evolving outline union boundary/area:\n",
    "# # Plot polygons in the GeoDataFrame\n",
    "# gdf.plot(ax=ax, color='lightblue', edgecolor='black', linewidth=1, label='Lake area')\n",
    "# gdf.boundary.plot(ax=ax, color='red', linewidth=2, label='Lake outline')\n",
    "\n",
    "# Add off-lake, lake (no corr.) to dh plot to illustrate correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277eb44-f225-4553-ad0c-62452e3b3eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.family': 'DejaVu Sans',\n",
    "    'font.size': 9,\n",
    "    'axes.labelsize': 10,\n",
    "    'axes.titlesize': 9,\n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 9,\n",
    "    'legend.fontsize': 9,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e971a37-5db7-46e3-9b7f-a9ee01739403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets necessary for plotting\n",
    "revised_stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/revised_stationary_outlines_gdf.geojson')\n",
    "evolving_outlines_union_gdf = gpd.read_file('output/lake_outlines/evolving_outlines_union_gdf.geojson')\n",
    "evolving_stationary_outlines_union_evolving_lakes_gdf = gpd.read_file('output/lake_outlines/evolving_stationary_outlines_union_evolving_lakes_gdf.geojson')\n",
    "evolving_stationary_outlines_union_all_lakes_gdf = gpd.read_file('output/lake_outlines/evolving_stationary_outlines_union_all_lakes_gdf.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a5ff1-334d-494b-9dc2-16d32c89d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make output directory for figures\n",
    "# os.makedirs(OUTPUT_DIR + '/figures', exist_ok=True)\n",
    "\n",
    "# # Select lake\n",
    "# lake_gdf = revised_stationary_outlines_gdf[revised_stationary_outlines_gdf['name'] == 'Institute_E1']\n",
    "\n",
    "# # Define lake name and polygon\n",
    "# lake_name = lake_gdf['name'].iloc[0]\n",
    "# lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "\n",
    "# # Load evolving outlines as geodataframe\n",
    "# try:\n",
    "#     onlake_outlines_gdf = gpd.read_file(os.path.join(\n",
    "#         os.getcwd(), 'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)))\n",
    "# except fiona.errors.DriverError:\n",
    "#     print(f\"File for {lake_name} not found.\")\n",
    "\n",
    "# # Load off-lake evolving outlines as geodataframe\n",
    "# try:\n",
    "#     offlake_outlines_gdf = gpd.read_file(OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines/{}.geojson'.format(lake_name))\n",
    "# except fiona.errors.DriverError:\n",
    "#     print(f\"File for {lake_name} not found.\")\n",
    "\n",
    "# # Ensure the result is a GeoDataFrame with proper geometry\n",
    "# evolving_outlines_gdf = gpd.GeoDataFrame(\n",
    "#     pd.concat([onlake_outlines_gdf, offlake_outlines_gdf], ignore_index=True),\n",
    "#     geometry='geometry', crs=onlake_outlines_gdf.crs)\n",
    "\n",
    "# # Load evolving outlines union\n",
    "# evolving_stationary_union_gdf = evolving_stationary_outlines_union_evolving_lakes_gdf[evolving_stationary_outlines_union_evolving_lakes_gdf['name'] == lake_name]\n",
    "# evolving_union_gdf = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]\n",
    "\n",
    "# # Load evolving outlines search parameters\n",
    "# row_index=evolving_outlines_gdf['row_index'][0]\n",
    "# within_area_multiple=evolving_outlines_gdf['within_area_multiple'][0]\n",
    "# level=evolving_outlines_gdf['level'][0]\n",
    "\n",
    "# # Attempt to open the geometric calculations CSV files\n",
    "# try:\n",
    "#     evolving_geom_calcs_df = pd.read_csv(os.path.join(\n",
    "#         os.getcwd(), 'output/geometric_calcs/evolving_outlines_geom_calc/{}.csv'.format(lake_name)))\n",
    "#     stationary_geom_calcs_df = pd.read_csv(os.path.join(\n",
    "#         os.getcwd(), \n",
    "#         'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes/{}.csv'.format(lake_name)))\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"CSV files for {lake_name} not found.\")\n",
    "\n",
    "# # Convert of strings to datetime\n",
    "# evolving_geom_calcs_df['midcyc_datetime'] = pd.to_datetime(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "# stationary_geom_calcs_df['midcyc_datetime'] = pd.to_datetime(stationary_geom_calcs_df['midcyc_datetime'])\n",
    "\n",
    "# # Prepare datasets - using larger buffer for initial masking\n",
    "# dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(lake_gdf, 25)\n",
    "\n",
    "# dataset1_dh = dataset1_masked['delta_h'].diff('time')\n",
    "# dataset2_dh = dataset2_masked['delta_h'].diff('time')\n",
    "\n",
    "# # Select time step to show for CryoSat-2 (CS2) and ICESat-2 (IS2) eras\n",
    "# CS2_i = 1  \n",
    "# IS2_i = 12\n",
    "\n",
    "# # Isolate dh time steps\n",
    "# CS2_timestep = dataset1_masked['time'][CS2_i].values\n",
    "# IS2_timestep = dataset2_masked['time'][IS2_i].values\n",
    "\n",
    "# # Convert date_list to numpy array\n",
    "# CS2_cyc_start_dates_npa = np.array(cyc_start_dates, dtype='datetime64')\n",
    "# IS2_cyc_start_dates_npa = np.array(cyc_start_dates, dtype='datetime64')\n",
    "\n",
    "# # Find matching cyc_dates index\n",
    "# CS2_cyc_dates_idx = np.where(CS2_cyc_start_dates_npa == CS2_timestep)[0][0]\n",
    "# IS2_cyc_dates_idx = np.where(IS2_cyc_start_dates_npa == IS2_timestep)[0][0]\n",
    "\n",
    "# # Initialize empty lists for height anomalies\n",
    "# height_anom_pos = []\n",
    "# height_anom_neg = []\n",
    "\n",
    "# # Get height anomalies for CryoSat-2 timestep (i = 20)\n",
    "# if dataset1_masked is not None:\n",
    "#     if np.any(~np.isnan(dataset1_dh[CS2_i])):\n",
    "#         height_anom_pos.append(np.nanmax(dataset1_dh[CS2_i]))\n",
    "#         height_anom_neg.append(np.nanmin(dataset1_dh[CS2_i]))\n",
    "\n",
    "# # Get height anomalies for ICESat-2 timestep (i = 12)\n",
    "# if np.any(~np.isnan(dataset2_dh[IS2_i])):\n",
    "#     height_anom_pos.append(np.nanmax(dataset2_dh[IS2_i]))\n",
    "#     height_anom_neg.append(np.nanmin(dataset2_dh[IS2_i]))\n",
    "\n",
    "# # Find max height anomalies across both time slices\n",
    "# max_height_anom_pos = max(height_anom_pos)\n",
    "# max_height_anom_neg = min(height_anom_neg)\n",
    "# max_anom = max([max_height_anom_pos, abs(max_height_anom_neg)])\n",
    "# del height_anom_pos, height_anom_neg\n",
    "\n",
    "# # Create the diverging normalization for the colormap\n",
    "# divnorm = colors.TwoSlopeNorm(vmin=-max_anom, vcenter=0., vmax=max_anom)\n",
    "# del max_height_anom_pos, max_height_anom_neg\n",
    "\n",
    "# # Establish x_min, x_max, y_min, y_max\n",
    "# ROI_poly = area_multiple_buffer(lake_poly, 25)\n",
    "# x_min, y_min, x_max, y_max = ROI_poly.bounds\n",
    "# x_buffer, y_buffer = abs(x_max-x_min)*0.02, abs(y_max-y_min)*0.02\n",
    "\n",
    "# # Subsetting dataset\n",
    "# dataset1 = CS2_Smith2017\n",
    "# dataset2 = ATL15_dh\n",
    "\n",
    "# # Prepare datasets\n",
    "# dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(lake_gdf, 25)\n",
    "\n",
    "# # Create figure with GridSpec - modified to include timeline panel\n",
    "# fig = plt.figure(figsize=(10,16))  # Increased height to accommodate new panel\n",
    "# gs = fig.add_gridspec(4, 2, height_ratios=[0.24, 1, 1, 1], hspace=0.3, wspace=0.3)\n",
    "\n",
    "# # Create remaining axes array excluding timeline row\n",
    "# axs = np.array([[fig.add_subplot(gs[i,j]) for j in range(2)] for i in range(1,4)])\n",
    "\n",
    "\n",
    "# # Panel - Satellite era timeline\n",
    "\n",
    "# # Add timeline panel spanning both columns\n",
    "# timeline_ax = fig.add_subplot(gs[0, :])\n",
    "\n",
    "# # Create timeline visualization\n",
    "# timeline_ax.set_xlim(cyc_start_dates[0], cyc_end_dates[-1])\n",
    "# timeline_ax.xaxis.set_major_locator(mdates.YearLocator(base=1))\n",
    "# timeline_ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "# # Plot colored bars for different satellite eras\n",
    "# for i, date in enumerate(cyc_dates['mid_cyc_dates']):\n",
    "#     dataset = cyc_dates['dataset'][i]\n",
    "#     color = 'lightblue' if dataset == 'CS2_Smith2017' else 'lightgreen'\n",
    "#     label = 'CryoSat-2' if dataset == 'CS2_Smith2017' else 'ICESat-2'\n",
    "    \n",
    "#     if i == 0:  # Only add label for first occurrence of each dataset\n",
    "#         timeline_ax.axvspan(cyc_dates['cyc_start_dates'][i], cyc_dates['cyc_end_dates'][i], \n",
    "#                           color=color, alpha=0.3, label=label)\n",
    "#     else:\n",
    "#         # Check if dataset changed from previous\n",
    "#         if cyc_dates['dataset'][i] != cyc_dates['dataset'][i-1]:\n",
    "#             timeline_ax.axvspan(cyc_dates['cyc_start_dates'][i], cyc_dates['cyc_end_dates'][i], \n",
    "#                               color=color, alpha=0.3, label=label)\n",
    "#         else:\n",
    "#             timeline_ax.axvspan(cyc_dates['cyc_start_dates'][i], cyc_dates['cyc_end_dates'][i], \n",
    "#                               color=color, alpha=0.3)\n",
    "\n",
    "# # Get the dates for our timesteps shown in top panels\n",
    "# # For CryoSat-2\n",
    "# cs2_start_date = cyc_start_dates[CS2_cyc_dates_idx]\n",
    "# cs2_end_date = cyc_end_dates[CS2_cyc_dates_idx]\n",
    "\n",
    "# # For ICESat-2\n",
    "# is2_start_date = cyc_start_dates[IS2_cyc_dates_idx]\n",
    "# is2_end_date = cyc_end_dates[IS2_cyc_dates_idx]\n",
    "\n",
    "# # Add vertical spans for the timesteps shown in top panels\n",
    "# # For CryoSat-2\n",
    "# timeline_ax.axvspan(mdates.date2num(cs2_start_date), \n",
    "#                               mdates.date2num(cs2_end_date),\n",
    "#                               color='gray', alpha=0.3)\n",
    "\n",
    "# # For ICESat-2\n",
    "# timeline_ax.axvspan(mdates.date2num(is2_start_date),\n",
    "#                               mdates.date2num(is2_end_date),\n",
    "#                               color='gray', alpha=0.3)\n",
    "# # Create patches for legend\n",
    "# cs2_patch = mpatches.Patch(color='lightblue', alpha=0.3, label='CryoSat-2')\n",
    "# is2_patch = mpatches.Patch(color='lightgreen', alpha=0.3, label='ICESat-2')\n",
    "# timestep_patch = mpatches.Patch(color='gray', alpha=0.3, label='displayed time step')\n",
    "\n",
    "# # Add legend with all patches\n",
    "# timeline_ax.legend(handles=[cs2_patch, is2_patch, timestep_patch], \n",
    "#                   loc='upper right')\n",
    "\n",
    "# # Customize timeline appearance\n",
    "# # timeline_ax.legend(loc='upper right')\n",
    "# timeline_ax.set_title('Multi-mission satellite timeline')\n",
    "\n",
    "# # Remove y-axis ticks and labels\n",
    "# timeline_ax.set_yticks([])\n",
    "\n",
    "\n",
    "# # Panel - CryoSat-2 dh time step with evolving outlines and area multiple within evaluation lines\n",
    "\n",
    "# # Plot dh time step\n",
    "# img = axs[0,0].imshow(dataset1_dh[CS2_i], cmap='RdBu', norm=divnorm, \n",
    "#                       origin='lower', extent=[x_min, x_max, y_min, y_max])\n",
    "\n",
    "# dt = mid_cyc_dates[CS2_cyc_dates_idx]\n",
    "# evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "\n",
    "# if not evolving_outlines_gdf.empty:\n",
    "#     # Split into positive and negative dh values\n",
    "#     positive_outlines = evolving_outlines_dt[evolving_outlines_dt['dh (m)'] > 0]\n",
    "#     negative_outlines = evolving_outlines_dt[evolving_outlines_dt['dh (m)'] < 0]\n",
    "    \n",
    "#     # Plot positive dh in blue\n",
    "#     if not positive_outlines.empty:\n",
    "#         positive_outlines.boundary.plot(ax=axs[0,0], color='blue', linewidth=1)\n",
    "    \n",
    "#     # Plot negative dh in red\n",
    "#     if not negative_outlines.empty:\n",
    "#         negative_outlines.boundary.plot(ax=axs[0,0], color='red', linewidth=1)\n",
    "\n",
    "# # Plot inset map\n",
    "# axIns = axs[0,0].inset_axes([0.02, 0, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "# axIns.set_aspect('equal')\n",
    "# moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1)\n",
    "# moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1)\n",
    "# axIns.axis('off')\n",
    "# # Plot red star to indicate location\n",
    "# axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "#     linewidth=1, color='k', s=75)\n",
    "\n",
    "# # Set a title for the axes\n",
    "# title_text = f'Height change from \\n{date_to_quarter_year(mid_cyc_dates[CS2_cyc_dates_idx])} to {date_to_quarter_year(mid_cyc_dates[CS2_cyc_dates_idx+1])}'\n",
    "# axs[0,0].set_title(title_text, y=1)\n",
    "\n",
    "# # Create lines for legend\n",
    "# stationary_color = 'darkturquoise'\n",
    "# stationary_line = plt.Line2D([], [], color=stationary_color, linestyle='solid', linewidth=2)\n",
    "# within_eval_lines = plt.Line2D([], [], color='gray', linestyle='dotted', linewidth=2)\n",
    "# optimal_within_eval_line = plt.Line2D([], [], color='gray', linestyle='solid', linewidth=2)\n",
    "\n",
    "# # Plot legend\n",
    "# legend = axs[0,0].legend([stationary_line, \n",
    "#                           within_eval_lines,\n",
    "#                           optimal_within_eval_line], \n",
    "#     ['stationary outline', \n",
    "#      'within evaluation boundaries',\n",
    "#      'optimal within boundary'],\n",
    "#     handlelength=3,\n",
    "#     handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#     loc='upper right')\n",
    "\n",
    "\n",
    "# # Panel - ICESat-2 dh time step with evolving outlines and area multiple within evaluation lines  \n",
    "\n",
    "# # Plot dh time step\n",
    "# img = axs[0,1].imshow(dataset2_dh[IS2_i], cmap='RdBu', norm=divnorm, \n",
    "#                       origin='lower', extent=[x_min, x_max, y_min, y_max])\n",
    "\n",
    "# dt = mid_cyc_dates[IS2_cyc_dates_idx]\n",
    "# evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "\n",
    "# if not evolving_outlines_gdf.empty:\n",
    "#     # Split into positive and negative dh values\n",
    "#     positive_outlines = evolving_outlines_dt[evolving_outlines_dt['dh (m)'] > 0]\n",
    "#     negative_outlines = evolving_outlines_dt[evolving_outlines_dt['dh (m)'] < 0]\n",
    "    \n",
    "#     # Plot positive dh in blue\n",
    "#     if not positive_outlines.empty:\n",
    "#         positive_outlines.boundary.plot(ax=axs[0,1], color='blue', linewidth=1)\n",
    "    \n",
    "#     # Plot negative dh in red\n",
    "#     if not negative_outlines.empty:\n",
    "#         negative_outlines.boundary.plot(ax=axs[0,1], color='red', linewidth=1)\n",
    "\n",
    "# # Add colorbar space to both axes for consistent sizing\n",
    "# for ax in [axs[0,0], axs[0,1]]:\n",
    "#     divider = make_axes_locatable(ax)\n",
    "#     cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "#     if ax == axs[0,1]:  # Only add the actual colorbar to the second plot\n",
    "#         cb = fig.colorbar(img, cax=cax)\n",
    "#         cb.set_label('dh [m quarter$^{-1}$]')\n",
    "#     else:\n",
    "#         # Hide the empty axis for the first plot\n",
    "#         cax.set_visible(False)\n",
    "\n",
    "# # Set a title for the axes\n",
    "# title_text = f'Height change from \\n{date_to_quarter_year(mid_cyc_dates[IS2_cyc_dates_idx])} to {date_to_quarter_year(mid_cyc_dates[IS2_cyc_dates_idx+1])}'\n",
    "# axs[0,1].set_title(title_text, y=1)\n",
    "\n",
    "# for ax in [axs[0,0], axs[0,1]]:\n",
    "#     ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "# axs[0,1].sharey(axs[0,0])\n",
    "\n",
    "# # Create lines for legend\n",
    "# pos_dh_anom = plt.Line2D([], [], color='blue', linestyle='solid', linewidth=2)\n",
    "# neg_dh_anom = plt.Line2D([], [], color='red', linestyle='solid', linewidth=2)\n",
    "\n",
    "# # Plot legend\n",
    "# legend = axs[0,1].legend([pos_dh_anom, \n",
    "#                           neg_dh_anom], \n",
    "#     [f'pos. dh anomaly (>+{level} m)', \n",
    "#      f'neg. dh anomaly (<{level} m)'],\n",
    "#     handlelength=3,\n",
    "#     handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#     loc='upper right')\n",
    "\n",
    "\n",
    "# # Panel - Evolving outlines time series plot ---------------------------------------------\n",
    "\n",
    "# # Set up colormap\n",
    "# cmap = plt.get_cmap('plasma', len(mid_cyc_dates)-1)\n",
    "\n",
    "# # Norm to time variable\n",
    "# norm = plt.Normalize(mdates.date2num(cyc_dates['mid_cyc_dates'].iloc[0]), \n",
    "#                      mdates.date2num(cyc_dates['mid_cyc_dates'].iloc[-1]))\n",
    "\n",
    "# # Zoom in slightly to bounds of optimal within evaluation boundary\n",
    "# # Establish x_min, x_max, y_min, y_max\n",
    "# optimal_within_eval_poly = area_multiple_buffer(lake_poly, within_area_multiple)\n",
    "# x_min, y_min, x_max, y_max = optimal_within_eval_poly.bounds\n",
    "# x_buffer, y_buffer = abs(x_max-x_min)*0.01, abs(y_max-y_min)*0.01\n",
    "\n",
    "# # Plot MOA surface imagery\n",
    "# mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "# mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "# moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "# axs[1,0].imshow(moa_highres_da_subset[0,:,:], cmap='gray', clim=[14000, 17000], \n",
    "#           extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "\n",
    "# # Plot evolving outlines\n",
    "# onlake_lines, offlake_lines = [], []\n",
    "# for i, dt in enumerate(mid_cyc_dates):\n",
    "#     x, y = 1, 1\n",
    "#     onlake_line, = axs[1,0].plot(x, y, color=cmap(norm(mdates.date2num(mid_cyc_dates[i]))), linewidth=2)\n",
    "#     onlake_lines.append(onlake_line)\n",
    "#     offlake_line, = axs[1,0].plot(x, y, color=cmap(norm(date_to_quarter_year(mid_cyc_dates[i]))), linewidth=2, alpha=0.2)\n",
    "#     offlake_lines.append(offlake_line)\n",
    "    \n",
    "#     onlake_outlines_dt = onlake_outlines_gdf[onlake_outlines_gdf['midcyc_datetime'] == dt]\n",
    "#     offlake_outlines_dt = offlake_outlines_gdf[offlake_outlines_gdf['midcyc_datetime'] == dt]\n",
    "    \n",
    "#     if not onlake_outlines_dt.empty:\n",
    "#         onlake_outlines_dt.boundary.plot(ax=axs[1,0], \n",
    "#             color=cmap(norm(mdates.date2num(mid_cyc_dates[i]))), \n",
    "#             linewidth=1)\n",
    "#     if not offlake_outlines_dt.empty:\n",
    "#         offlake_outlines_dt.boundary.plot(ax=axs[1,0], \n",
    "#             color=cmap(norm(mdates.date2num(mid_cyc_dates[i]))), \n",
    "#             linewidth=1, alpha=0.5)\n",
    "\n",
    "# # Plot evolving outlines unary union\n",
    "# evolving_union_gdf.boundary.plot(ax=axs[1,0], color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "# # Create stationary region and evolving outlines region and plot\n",
    "# stationary_region = area_multiple_buffer(lake_poly, 2)\n",
    "# stationary_region = stationary_region.difference(lake_poly)\n",
    "# evolving_union_region = area_multiple_buffer(evolving_union_gdf['geometry'], 2)\n",
    "# evolving_union_region = evolving_union_region.difference(evolving_union_gdf['geometry'].iloc[0])\n",
    "# gpd.GeoDataFrame(geometry=[stationary_region], crs='3031').plot(ax=axs[1,0], color=stationary_color, linewidth=2, alpha=0.2)\n",
    "# gpd.GeoDataFrame(geometry=[evolving_union_region], crs='3031').plot(ax=axs[1,0], color='k', linewidth=2, alpha=0.2)\n",
    "\n",
    "# # Create colorbar\n",
    "# m = plt.cm.ScalarMappable(cmap=cmap)\n",
    "# m.set_array(np.array([mdates.date2num(date) for date in mid_cyc_dates]))\n",
    "# divider = make_axes_locatable(axs[1,0])\n",
    "# cax = divider.append_axes('bottom', size='3%', pad=0.6)\n",
    "\n",
    "# # Define major and minor years\n",
    "# # major_years = [2012, 2014, 2016, 2018, 2020, 2022, 2024]\n",
    "# major_years = [2012, 2016, 2020, 2024]\n",
    "# minor_years = list(range(2011, 2025))\n",
    "# major_dates = [mdates.date2num(datetime.datetime(year, 1, 1)) for year in major_years]\n",
    "# minor_dates = [mdates.date2num(datetime.datetime(year, 1, 1)) for year in minor_years]\n",
    "\n",
    "# cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "# cbar.set_ticks(major_dates)\n",
    "# cbar.set_ticklabels(major_years)\n",
    "# cbar.ax.xaxis.set_minor_locator(ticker.FixedLocator(minor_dates))\n",
    "# cbar.set_label('evolving outline year', labelpad=5)\n",
    "\n",
    "# axs[1,0].set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "    \n",
    "# # Emphasize zeroth row_index within evaluation line selected for particular lake\n",
    "# gpd.GeoSeries(area_multiple_buffer(lake_poly, within_area_multiple)).boundary.plot(ax=ax, linestyle='solid', color='gray')\n",
    "\n",
    "# # Plot legend\n",
    "# evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=2)\n",
    "# stationary_region_patch = mpatches.Patch(color=stationary_color, alpha=0.2)\n",
    "# evolving_union_region_patch = mpatches.Patch(color='k', alpha=0.2)\n",
    "\n",
    "# legend = axs[1,0].legend([stationary_region_patch,\n",
    "#                        evolving_union_region_patch,\n",
    "#                        tuple(onlake_lines),\n",
    "#                        # tuple(offlake_lines),\n",
    "#                        evolving_union_line], \n",
    "#     ['stationary region',\n",
    "#      'evolving union region',\n",
    "#      'evolving outlines',\n",
    "#      # f'evolving outlines ({evolving_outlines_gdf.level[0]} m)',\n",
    "#      # 'off-lake evolving outlines', \n",
    "#      'evolving outlines union'],\n",
    "#     handlelength=3,\n",
    "#     handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#     loc='upper right')\n",
    "\n",
    "\n",
    "# # Panel - active area ---------------------------------------------\n",
    "    \n",
    "# # Plot horizontal zero line for reference\n",
    "# axs[1,1].axhline(0, color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "# # Plot stationary outline and evolving outlines unary union areas\n",
    "# axs[1,1].axhline(np.divide(lake_gdf['area (m^2)'], 1e6).values, \n",
    "#                  color=stationary_color, linestyle='solid', linewidth=2)\n",
    "# axs[1,1].axhline(np.divide(evolving_union_gdf['area (m^2)'], 1e6).values, \n",
    "#                  color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "# # Plot multi-colored line and scatter for data points\n",
    "# x = mdates.date2num(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "# y = np.divide(evolving_geom_calcs_df['evolving_outlines_area (m^2)'], 1e6)\n",
    "\n",
    "# # Plot evolving outlines time series using LineCollection from points/segments to plot multi-colored line\n",
    "# points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "# segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "# lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "# lc.set_array(x)  # Set the values used for colormapping, using matplotlib dates for colors\n",
    "# lc.set_linewidth(2)\n",
    "# line = axs[1,1].add_collection(lc)\n",
    "# scatter = axs[1,1].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "# # Add legend\n",
    "# legend = axs[1,1].legend([stationary_line, \n",
    "#                           evolving_union_line, \n",
    "#                           tuple(onlake_lines)], \n",
    "#     ['stationary outline',\n",
    "#      'evolving outlines union',\n",
    "#      'evolving outlines',],\n",
    "#      handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#      loc='upper right')\n",
    "\n",
    "\n",
    "# # Panel - dh/dt -------------------------------------------------------\n",
    "\n",
    "# # Plot horizontal zero line for reference\n",
    "# axs[2,0].axhline(0, color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "# # Plot stationary outlines off-lake region dh\n",
    "# axs[2,0].plot(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "#     np.cumsum(stationary_geom_calcs_df['stationary_outline_region_dh (m)']),\n",
    "#     color='lightgray', linestyle='solid', linewidth=2)\n",
    "# axs[2,0].scatter(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "#     np.cumsum(stationary_geom_calcs_df['stationary_outline_region_dh (m)']),\n",
    "#     color='lightgray', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# # Plot evolving outlines off-lake region dh\n",
    "# axs[2,0].plot(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']), \n",
    "#     np.cumsum(evolving_geom_calcs_df['evolving_outlines_region_dh (m)']), \n",
    "#     color='dimgray', linestyle='solid', linewidth=2)\n",
    "# axs[2,0].scatter(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']), \n",
    "#     np.cumsum(evolving_geom_calcs_df['evolving_outlines_region_dh (m)']),\n",
    "#     color='dimgray', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# # Plot stationary outline time series\n",
    "# axs[2,0].plot(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "#     np.cumsum(stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "#     color=stationary_color, linestyle='solid', linewidth=2)\n",
    "# axs[2,0].scatter(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "#     np.cumsum(stationary_geom_calcs_df['stationary_outline_dh_corr (m)']),\n",
    "#     color=stationary_color, linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# # Plot multi-colored line and scatter for data points\n",
    "# x = mdates.date2num(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "# y = np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'])\n",
    "\n",
    "# # Plot evolving outlines time series using multi-colored LineCollection from points/segments\n",
    "# points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "# segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "# lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "# lc.set_array(x)\n",
    "# lc.set_linewidth(2)\n",
    "# line = axs[2,0].add_collection(lc)\n",
    "# scatter = axs[2,0].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "# # Plot bias\n",
    "# axs[2,0].plot(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']),\n",
    "#     np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'] -\n",
    "#               stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "#     color='red', linestyle='solid', linewidth=2)\n",
    "# axs[2,0].scatter(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']),\n",
    "#     np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'] -\n",
    "#               stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "#     color='red', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# # Add legend\n",
    "# evolving_region = plt.Line2D([], [], color='dimgray', linestyle='solid', linewidth=2)\n",
    "# stationary_region = plt.Line2D([], [], color='lightgray', linestyle='solid', linewidth=2)\n",
    "# bias = plt.Line2D([], [], color='red', linestyle='solid', linewidth=2)\n",
    "# legend = axs[2,0].legend(\n",
    "#     [evolving_region,\n",
    "#      stationary_region,\n",
    "#      tuple(onlake_lines),\n",
    "#      stationary_line,  \n",
    "#      bias],\n",
    "#     ['evolving outlines regional',\n",
    "#      'stationary outline regional',\n",
    "#      'evolving outlines',\n",
    "#      'stationary outline', \n",
    "#      'bias (evolving  stationary)'],\n",
    "#      handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#      loc='upper right')\n",
    "\n",
    "\n",
    "# # Panel - dV/dt --------------------------------------------------\n",
    "\n",
    "# # Plot horizontal line at zero for reference\n",
    "# axs[2,1].axhline(0, color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "# # Plot stationary outline time series\n",
    "# axs[2,1].plot(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "#     np.divide(np.cumsum(stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "#     color=stationary_color, linestyle='solid', linewidth=2)\n",
    "# axs[2,1].scatter(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "#     np.divide(np.cumsum(stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "#     color=stationary_color, linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# # Plot multi-colored line and scatter for data points\n",
    "# x = mdates.date2num(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "# y = np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "\n",
    "# # Plot evolving outlines time series using LineCollection from points/segments to plot multi-colored line\n",
    "# points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "# segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "# lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "# lc.set_array(x)\n",
    "# lc.set_linewidth(2)\n",
    "# line = axs[2,1].add_collection(lc)\n",
    "# scatter = axs[2,1].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "# # Plot bias\n",
    "# axs[2,1].plot(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']),\n",
    "#     np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "#                         stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)'], 1e9)), \n",
    "#     color='red', linestyle='solid', linewidth=2)\n",
    "# axs[2,1].scatter(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']),\n",
    "#     np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "#                         stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)'], 1e9)), \n",
    "#     color='red', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# # Add legend\n",
    "# legend = axs[2,1].legend([tuple(onlake_lines), stationary_line, bias],\n",
    "#     ['evolving outlines', \n",
    "#      'stationary outline',\n",
    "#      'bias (evolving  stationary)'], \n",
    "#     handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#     loc='upper right')\n",
    "\n",
    "# # Adjust y-axis limits to avoid legend/data overlap\n",
    "# axs[1,1].set_ylim(-25, 625)\n",
    "# axs[2,0].set_ylim(-1, 8)\n",
    "# axs[2,1].set_ylim(-0.4, 0.4)\n",
    "\n",
    "# # Get the dates for our timesteps shown in top panels\n",
    "# # For CryoSat-2\n",
    "# cs2_start_date = cyc_start_dates[CS2_cyc_dates_idx]\n",
    "# cs2_end_date = cyc_end_dates[CS2_cyc_dates_idx]\n",
    "\n",
    "# # For ICESat-2\n",
    "# is2_start_date = cyc_start_dates[IS2_cyc_dates_idx]\n",
    "# is2_end_date = cyc_end_dates[IS2_cyc_dates_idx]\n",
    "\n",
    "# # Add vertical spans to time series plots\n",
    "# for ax in [axs[1,1], axs[2,0], axs[2,1]]:\n",
    "#     # Add vertical spans for both timesteps\n",
    "#     ax.axvspan(mdates.date2num(cs2_start_date), \n",
    "#                          mdates.date2num(cs2_end_date),\n",
    "#                          color='gray', alpha=0.1)\n",
    "    \n",
    "#     ax.axvspan(mdates.date2num(is2_start_date),\n",
    "#                          mdates.date2num(is2_end_date),\n",
    "#                          color='gray', alpha=0.1)\n",
    "\n",
    "# # Label the timeline panel\n",
    "# timeline_ax.text(0.01, 0.98, 'a', transform=timeline_ax.transAxes, \n",
    "#                 fontsize=20, va='top', ha='left')\n",
    "\n",
    "# # Plot elements common to more than one ax object\n",
    "# for i in range(axs.shape[0]):\n",
    "#     for j in range(axs.shape[1]):\n",
    "#         # Set common tick size for all axes\n",
    "#         axs[i,j].tick_params(axis='both')\n",
    "        \n",
    "#         # Add subplot labels (a, b, c, etc.)\n",
    "#         axs[i,j].text(0.02, 0.98, chr(97 + i*2 + j + 1), transform=axs[i,j].transAxes, \n",
    "#                      fontsize=16, va='top', ha='left')\n",
    "\n",
    "#         # Special formatting for map plots (top row and [1,0])\n",
    "#         if (i == 0) or (i == 1 and j == 0):\n",
    "#             # Set x and y labels for map plots\n",
    "#             axs[i,j].set_xlabel('x [km]')\n",
    "#             if j == 0:  # Only for first column\n",
    "#                 axs[i,j].set_ylabel('y [km]')\n",
    "\n",
    "#             # Add map-specific elements\n",
    "#             revised_stationary_outlines_gdf.boundary.plot(ax=axs[i,j], \n",
    "#                 edgecolor=stationary_color, linestyle='solid', linewidth=2, zorder=0)\n",
    "            \n",
    "#             # Convert meters to kilometers\n",
    "#             km_scale = 1e3\n",
    "#             ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "#             ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "#             axs[i,j].xaxis.set_major_formatter(ticks_x)\n",
    "#             axs[i,j].yaxis.set_major_formatter(ticks_y)\n",
    "\n",
    "#             # Add evaluation lines\n",
    "#             for within_area_multiple_i in range(2,16):\n",
    "#                 gpd.GeoSeries(area_multiple_buffer(lake_poly, within_area_multiple_i)).boundary.plot(\n",
    "#                     ax=axs[i,j], linestyle='dotted', color='gray')\n",
    "            \n",
    "#             # Add emphasized evaluation line\n",
    "#             gpd.GeoSeries(area_multiple_buffer(lake_poly, within_area_multiple)).boundary.plot(\n",
    "#                 ax=axs[i,j], linestyle='solid', color='gray')\n",
    "\n",
    "#         # Format time series plots\n",
    "#         else:\n",
    "#             axs[i,j].set_xlabel('year')\n",
    "            \n",
    "#             # Set y labels for time series plots\n",
    "#             if j == 0:  # First column\n",
    "#                 if i == 2:\n",
    "#                     axs[i,j].set_ylabel('cumulative dh [m]')\n",
    "#             elif j == 1:  # Second column\n",
    "#                 if i == 1:\n",
    "#                     axs[i,j].set_ylabel('active area [km$^2$]')\n",
    "#                 elif i == 2:\n",
    "#                     axs[i,j].set_ylabel('cumulative dV [km$^3$]')\n",
    "            \n",
    "#             # Add date formatting for specific time series plots\n",
    "#             if (i == 1 and j == 1) or (i == 2):  # axs[1,1], axs[2,0], and axs[2,1]\n",
    "#                 axs[i,j].xaxis.set_minor_locator(mdates.YearLocator(base=1))\n",
    "#                 axs[i,j].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "#                 axs[i,j].set_xlim(cyc_start_dates[0], cyc_end_dates[-1])\n",
    "\n",
    "# # Save and close plot\n",
    "# plt.savefig(OUTPUT_DIR + '/figures/Figure_S1.png',\n",
    "#     dpi=300, bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15678ca9-d0e5-4ae7-946f-a337e514e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make output directory for figures\n",
    "os.makedirs(OUTPUT_DIR + '/figures', exist_ok=True)\n",
    "\n",
    "# Select lake\n",
    "lake_gdf = revised_stationary_outlines_gdf[revised_stationary_outlines_gdf['name'] == 'Institute_E1']\n",
    "\n",
    "# Define lake name and polygon\n",
    "lake_name = lake_gdf['name'].iloc[0]\n",
    "lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "\n",
    "# Load evolving outlines as geodataframe\n",
    "try:\n",
    "    onlake_outlines_gdf = gpd.read_file(os.path.join(\n",
    "        os.getcwd(), 'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)))\n",
    "except fiona.errors.DriverError:\n",
    "    print(f\"File for {lake_name} not found.\")\n",
    "\n",
    "# Load off-lake evolving outlines as geodataframe\n",
    "try:\n",
    "    offlake_outlines_gdf = gpd.read_file(OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines/{}.geojson'.format(lake_name))\n",
    "except fiona.errors.DriverError:\n",
    "    print(f\"File for {lake_name} not found.\")\n",
    "\n",
    "# Ensure the result is a GeoDataFrame with proper geometry\n",
    "evolving_outlines_gdf = gpd.GeoDataFrame(\n",
    "    pd.concat([onlake_outlines_gdf, offlake_outlines_gdf], ignore_index=True),\n",
    "    geometry='geometry', crs=onlake_outlines_gdf.crs)\n",
    "\n",
    "# Load evolving outlines union\n",
    "evolving_stationary_union_gdf = evolving_stationary_outlines_union_evolving_lakes_gdf[evolving_stationary_outlines_union_evolving_lakes_gdf['name'] == lake_name]\n",
    "evolving_union_gdf = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]\n",
    "\n",
    "# Load evolving outlines search parameters\n",
    "row_index=evolving_outlines_gdf['row_index'][0]\n",
    "within_area_multiple=evolving_outlines_gdf['within_area_multiple'][0]\n",
    "level=evolving_outlines_gdf['level'][0]\n",
    "\n",
    "# Attempt to open the geometric calculations CSV files\n",
    "try:\n",
    "    evolving_geom_calcs_df = pd.read_csv(os.path.join(\n",
    "        os.getcwd(), 'output/geometric_calcs/evolving_outlines_geom_calc/{}.csv'.format(lake_name)))\n",
    "    stationary_geom_calcs_df = pd.read_csv(os.path.join(\n",
    "        os.getcwd(), \n",
    "        'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes/{}.csv'.format(lake_name)))\n",
    "except FileNotFoundError:\n",
    "    print(f\"CSV files for {lake_name} not found.\")\n",
    "\n",
    "# Convert of strings to datetime\n",
    "evolving_geom_calcs_df['midcyc_datetime'] = pd.to_datetime(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "stationary_geom_calcs_df['midcyc_datetime'] = pd.to_datetime(stationary_geom_calcs_df['midcyc_datetime'])\n",
    "\n",
    "# Prepare datasets - using larger buffer for initial masking\n",
    "dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(lake_gdf, 25)\n",
    "\n",
    "dataset1_dh = dataset1_masked['delta_h'].diff('time')\n",
    "dataset2_dh = dataset2_masked['delta_h'].diff('time')\n",
    "\n",
    "# Select time step to show for CryoSat-2 (CS2) and ICESat-2 (IS2) eras\n",
    "CS2_i = 1  \n",
    "IS2_i = 12\n",
    "\n",
    "# Isolate dh time steps\n",
    "CS2_timestep = dataset1_masked['time'][CS2_i].values\n",
    "IS2_timestep = dataset2_masked['time'][IS2_i].values\n",
    "\n",
    "# Convert date_list to numpy array\n",
    "CS2_cyc_start_dates_npa = np.array(cyc_start_dates, dtype='datetime64')\n",
    "IS2_cyc_start_dates_npa = np.array(cyc_start_dates, dtype='datetime64')\n",
    "\n",
    "# Find matching cyc_dates index\n",
    "CS2_cyc_dates_idx = np.where(CS2_cyc_start_dates_npa == CS2_timestep)[0][0]\n",
    "IS2_cyc_dates_idx = np.where(IS2_cyc_start_dates_npa == IS2_timestep)[0][0]\n",
    "\n",
    "# Initialize empty lists for height anomalies\n",
    "height_anom_pos = []\n",
    "height_anom_neg = []\n",
    "\n",
    "# Get height anomalies for CryoSat-2 timestep (i = 20)\n",
    "if dataset1_masked is not None:\n",
    "    if np.any(~np.isnan(dataset1_dh[CS2_i])):\n",
    "        height_anom_pos.append(np.nanmax(dataset1_dh[CS2_i]))\n",
    "        height_anom_neg.append(np.nanmin(dataset1_dh[CS2_i]))\n",
    "\n",
    "# Get height anomalies for ICESat-2 timestep (i = 12)\n",
    "if np.any(~np.isnan(dataset2_dh[IS2_i])):\n",
    "    height_anom_pos.append(np.nanmax(dataset2_dh[IS2_i]))\n",
    "    height_anom_neg.append(np.nanmin(dataset2_dh[IS2_i]))\n",
    "\n",
    "# Find max height anomalies across both time slices\n",
    "max_height_anom_pos = max(height_anom_pos)\n",
    "max_height_anom_neg = min(height_anom_neg)\n",
    "max_anom = max([max_height_anom_pos, abs(max_height_anom_neg)])\n",
    "del height_anom_pos, height_anom_neg\n",
    "\n",
    "# Create the diverging normalization for the colormap\n",
    "divnorm = colors.TwoSlopeNorm(vmin=-max_anom, vcenter=0., vmax=max_anom)\n",
    "del max_height_anom_pos, max_height_anom_neg\n",
    "\n",
    "# Establish x_min, x_max, y_min, y_max\n",
    "ROI_poly = area_multiple_buffer(lake_poly, 25)\n",
    "x_min, y_min, x_max, y_max = ROI_poly.bounds\n",
    "x_buffer, y_buffer = abs(x_max-x_min)*0.02, abs(y_max-y_min)*0.02\n",
    "\n",
    "# Subsetting dataset\n",
    "dataset1 = CS2_Smith2017\n",
    "dataset2 = ATL15_dh\n",
    "\n",
    "# Prepare datasets\n",
    "dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(lake_gdf, 25)\n",
    "\n",
    "# Create figure with GridSpec - modified to include timeline panel\n",
    "fig = plt.figure(figsize=(10,16))  # Increased height to accommodate new panel\n",
    "gs = fig.add_gridspec(4, 2, height_ratios=[0.3, 1, 1, 1], hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Create remaining axes array excluding timeline row\n",
    "axs = np.array([[fig.add_subplot(gs[i,j]) for j in range(2)] for i in range(1,4)])\n",
    "\n",
    "\n",
    "# Panel - Satellite era timeline\n",
    "\n",
    "# Add timeline panel spanning both columns\n",
    "timeline_ax = fig.add_subplot(gs[0, :])\n",
    "\n",
    "# Create timeline visualization\n",
    "timeline_ax.set_xlim(cyc_start_dates[0], cyc_end_dates[-1])\n",
    "timeline_ax.xaxis.set_major_locator(mdates.YearLocator(base=1))\n",
    "timeline_ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "timeline_ax.xaxis.set_minor_locator(mdates.MonthLocator([1, 4, 7, 10]))\n",
    "\n",
    "# Plot colored bars for different satellite eras\n",
    "for i, date in enumerate(cyc_dates['mid_cyc_dates']):\n",
    "    dataset = cyc_dates['dataset'][i]   \n",
    "    color = 'lightblue' if dataset == 'CS2_Smith2017' else 'lightgreen'\n",
    "    label = 'CryoSat-2' if dataset == 'CS2_Smith2017' else 'ICESat-2'\n",
    "    \n",
    "    if i == 0:  # Only add label for first occurrence of each dataset\n",
    "        timeline_ax.axvspan(cyc_dates['cyc_start_dates'][i], cyc_dates['cyc_end_dates'][i], \n",
    "                          color='k', alpha=0.5, label='first cycle - no dh avail.')\n",
    "\n",
    "    elif i == 1:  # Only add label for first occurrence of each dataset\n",
    "        timeline_ax.axvspan(cyc_dates['cyc_start_dates'][i], cyc_dates['cyc_end_dates'][i], \n",
    "                          color=color, alpha=0.3, label=label)\n",
    "    else:\n",
    "        # Check if dataset changed from previous\n",
    "        if cyc_dates['dataset'][i] != cyc_dates['dataset'][i-1]:\n",
    "            timeline_ax.axvspan(cyc_dates['cyc_start_dates'][i], cyc_dates['cyc_end_dates'][i], \n",
    "                              color=color, alpha=0.3, label=label)\n",
    "        else:\n",
    "            timeline_ax.axvspan(cyc_dates['cyc_start_dates'][i], cyc_dates['cyc_end_dates'][i], \n",
    "                              color=color, alpha=0.3)\n",
    "\n",
    "# Get the dates for our timesteps shown in top panels\n",
    "# For CryoSat-2\n",
    "cs2_start_date = cyc_start_dates[CS2_cyc_dates_idx]\n",
    "cs2_end_date = cyc_end_dates[CS2_cyc_dates_idx]\n",
    "\n",
    "# For ICESat-2\n",
    "is2_start_date = cyc_start_dates[IS2_cyc_dates_idx]\n",
    "is2_end_date = cyc_end_dates[IS2_cyc_dates_idx]\n",
    "\n",
    "# Add vertical spans for the timesteps shown in top panels\n",
    "# For CryoSat-2\n",
    "timeline_ax.axvspan(mdates.date2num(cs2_start_date), \n",
    "                              mdates.date2num(cs2_end_date),\n",
    "                              color='gray', alpha=0.3)\n",
    "\n",
    "# For ICESat-2\n",
    "timeline_ax.axvspan(mdates.date2num(is2_start_date),\n",
    "                              mdates.date2num(is2_end_date),\n",
    "                              color='gray', alpha=0.3)\n",
    "# Create patches for legend\n",
    "cs2_patch = mpatches.Patch(color='lightblue', alpha=0.3, label='CryoSat-2')\n",
    "is2_patch = mpatches.Patch(color='lightgreen', alpha=0.3, label='ICESat-2')\n",
    "no_dh_patch = mpatches.Patch(color='darkgray', alpha=0.9, label='first cycle - no dh avail.')\n",
    "timestep_patch = mpatches.Patch(color='gray', alpha=0.3, label='displayed time step')\n",
    "\n",
    "# Add legend with all patches\n",
    "timeline_ax.legend(handles=[is2_patch, cs2_patch, no_dh_patch, timestep_patch], \n",
    "                  loc='upper right')\n",
    "\n",
    "# Customize timeline appearance\n",
    "# timeline_ax.legend(loc='upper right')\n",
    "timeline_ax.set_title('Multi-mission satellite timeline')\n",
    "\n",
    "# Remove y-axis ticks and labels\n",
    "timeline_ax.set_yticks([])\n",
    "\n",
    "\n",
    "# Panel - CryoSat-2 dh time step with evolving outlines and area multiple within evaluation lines\n",
    "\n",
    "# Plot dh time step\n",
    "img = axs[0,0].imshow(dataset1_dh[CS2_i], cmap='RdBu', norm=divnorm, \n",
    "                      origin='lower', extent=[x_min, x_max, y_min, y_max])\n",
    "\n",
    "dt = mid_cyc_dates[CS2_cyc_dates_idx]\n",
    "evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "\n",
    "if not evolving_outlines_gdf.empty:\n",
    "    # Split into positive and negative dh values\n",
    "    positive_outlines = evolving_outlines_dt[evolving_outlines_dt['dh (m)'] > 0]\n",
    "    negative_outlines = evolving_outlines_dt[evolving_outlines_dt['dh (m)'] < 0]\n",
    "    \n",
    "    # Plot positive dh in blue\n",
    "    if not positive_outlines.empty:\n",
    "        positive_outlines.boundary.plot(ax=axs[0,0], color='blue', linewidth=1)\n",
    "    \n",
    "    # Plot negative dh in red\n",
    "    if not negative_outlines.empty:\n",
    "        negative_outlines.boundary.plot(ax=axs[0,0], color='red', linewidth=1)\n",
    "\n",
    "# Plot inset map\n",
    "axIns = axs[0,0].inset_axes([0.02, 0, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "axIns.set_aspect('equal')\n",
    "moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1)\n",
    "moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1)\n",
    "axIns.axis('off')\n",
    "# Plot red star to indicate location\n",
    "axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "    linewidth=1, color='k', s=75)\n",
    "\n",
    "# Set a title for the axes\n",
    "title_text = f'Height change from \\n{date_to_quarter_year(mid_cyc_dates[CS2_cyc_dates_idx])} to {date_to_quarter_year(mid_cyc_dates[CS2_cyc_dates_idx+1])}'\n",
    "axs[0,0].set_title(title_text, y=1)\n",
    "\n",
    "# Create lines for legend\n",
    "stationary_color = 'darkturquoise'\n",
    "stationary_line = plt.Line2D([], [], color=stationary_color, linestyle='solid', linewidth=2)\n",
    "within_eval_lines = plt.Line2D([], [], color='gray', linestyle='dotted', linewidth=2)\n",
    "optimal_within_eval_line = plt.Line2D([], [], color='gray', linestyle='solid', linewidth=2)\n",
    "\n",
    "# Plot legend\n",
    "legend = axs[0,0].legend([stationary_line, \n",
    "                          within_eval_lines,\n",
    "                          optimal_within_eval_line], \n",
    "    ['stationary outline', \n",
    "     'within evaluation boundaries',\n",
    "     'optimal within boundary'],\n",
    "    handlelength=3,\n",
    "    handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper right')\n",
    "\n",
    "\n",
    "# Panel - ICESat-2 dh time step with evolving outlines and area multiple within evaluation lines  \n",
    "\n",
    "# Plot dh time step\n",
    "img = axs[0,1].imshow(dataset2_dh[IS2_i], cmap='RdBu', norm=divnorm, \n",
    "                      origin='lower', extent=[x_min, x_max, y_min, y_max])\n",
    "\n",
    "dt = mid_cyc_dates[IS2_cyc_dates_idx]\n",
    "evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "\n",
    "if not evolving_outlines_gdf.empty:\n",
    "    # Split into positive and negative dh values\n",
    "    positive_outlines = evolving_outlines_dt[evolving_outlines_dt['dh (m)'] > 0]\n",
    "    negative_outlines = evolving_outlines_dt[evolving_outlines_dt['dh (m)'] < 0]\n",
    "    \n",
    "    # Plot positive dh in blue\n",
    "    if not positive_outlines.empty:\n",
    "        positive_outlines.boundary.plot(ax=axs[0,1], color='blue', linewidth=1)\n",
    "    \n",
    "    # Plot negative dh in red\n",
    "    if not negative_outlines.empty:\n",
    "        negative_outlines.boundary.plot(ax=axs[0,1], color='red', linewidth=1)\n",
    "\n",
    "# Add colorbar space to both axes for consistent sizing\n",
    "for ax in [axs[0,0], axs[0,1]]:\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "    if ax == axs[0,1]:  # Only add the actual colorbar to the second plot\n",
    "        cb = fig.colorbar(img, cax=cax)\n",
    "        cb.set_label('dh [m quarter$^{-1}$]')\n",
    "    else:\n",
    "        # Hide the empty axis for the first plot\n",
    "        cax.set_visible(False)\n",
    "\n",
    "# Set a title for the axes\n",
    "title_text = f'Height change from \\n{date_to_quarter_year(mid_cyc_dates[IS2_cyc_dates_idx])} to {date_to_quarter_year(mid_cyc_dates[IS2_cyc_dates_idx+1])}'\n",
    "axs[0,1].set_title(title_text, y=1)\n",
    "\n",
    "for ax in [axs[0,0], axs[0,1]]:\n",
    "    ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "axs[0,1].sharey(axs[0,0])\n",
    "\n",
    "# Create lines for legend\n",
    "pos_dh_anom = plt.Line2D([], [], color='blue', linestyle='solid', linewidth=2)\n",
    "neg_dh_anom = plt.Line2D([], [], color='red', linestyle='solid', linewidth=2)\n",
    "\n",
    "# Plot legend\n",
    "legend = axs[0,1].legend([pos_dh_anom, \n",
    "                          neg_dh_anom], \n",
    "    [f'pos. dh anomaly (>+{level} m)', \n",
    "     f'neg. dh anomaly (<{level} m)'],\n",
    "    handlelength=3,\n",
    "    handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper right')\n",
    "\n",
    "\n",
    "# Panel - Evolving outlines time series plot ---------------------------------------------\n",
    "\n",
    "# Set up colormap\n",
    "cmap = plt.get_cmap('plasma', len(mid_cyc_dates)-1)\n",
    "\n",
    "# Norm to time variable\n",
    "norm = plt.Normalize(mdates.date2num(cyc_dates['mid_cyc_dates'].iloc[0]), \n",
    "                     mdates.date2num(cyc_dates['mid_cyc_dates'].iloc[-1]))\n",
    "\n",
    "# Zoom in slightly to bounds of optimal within evaluation boundary\n",
    "# Establish x_min, x_max, y_min, y_max\n",
    "optimal_within_eval_poly = area_multiple_buffer(lake_poly, within_area_multiple)\n",
    "x_min, y_min, x_max, y_max = optimal_within_eval_poly.bounds\n",
    "x_buffer, y_buffer = abs(x_max-x_min)*0.01, abs(y_max-y_min)*0.01\n",
    "\n",
    "# Plot MOA surface imagery\n",
    "mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "axs[1,0].imshow(moa_highres_da_subset[0,:,:], cmap='gray', clim=[14000, 17000], \n",
    "          extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "\n",
    "# Plot evolving outlines\n",
    "onlake_lines, offlake_lines = [], []\n",
    "for i, dt in enumerate(mid_cyc_dates):\n",
    "    x, y = 1, 1\n",
    "    onlake_line, = axs[1,0].plot(x, y, color=cmap(norm(mdates.date2num(mid_cyc_dates[i]))), linewidth=2)\n",
    "    onlake_lines.append(onlake_line)\n",
    "    offlake_line, = axs[1,0].plot(x, y, color=cmap(norm(date_to_quarter_year(mid_cyc_dates[i]))), linewidth=2, alpha=0.2)\n",
    "    offlake_lines.append(offlake_line)\n",
    "    \n",
    "    onlake_outlines_dt = onlake_outlines_gdf[onlake_outlines_gdf['midcyc_datetime'] == dt]\n",
    "    offlake_outlines_dt = offlake_outlines_gdf[offlake_outlines_gdf['midcyc_datetime'] == dt]\n",
    "    \n",
    "    if not onlake_outlines_dt.empty:\n",
    "        onlake_outlines_dt.boundary.plot(ax=axs[1,0], \n",
    "            color=cmap(norm(mdates.date2num(mid_cyc_dates[i]))), \n",
    "            linewidth=1)\n",
    "    if not offlake_outlines_dt.empty:\n",
    "        offlake_outlines_dt.boundary.plot(ax=axs[1,0], \n",
    "            color=cmap(norm(mdates.date2num(mid_cyc_dates[i]))), \n",
    "            linewidth=1, alpha=0.5)\n",
    "\n",
    "# Plot evolving outlines unary union\n",
    "evolving_union_gdf.boundary.plot(ax=axs[1,0], color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "# Create stationary region and evolving outlines region and plot\n",
    "stationary_region = area_multiple_buffer(lake_poly, 2)\n",
    "stationary_region = stationary_region.difference(lake_poly)\n",
    "evolving_union_region = area_multiple_buffer(evolving_union_gdf['geometry'], 2)\n",
    "evolving_union_region = evolving_union_region.difference(evolving_union_gdf['geometry'].iloc[0])\n",
    "gpd.GeoDataFrame(geometry=[stationary_region], crs='3031').plot(ax=axs[1,0], color=stationary_color, linewidth=2, alpha=0.2)\n",
    "gpd.GeoDataFrame(geometry=[evolving_union_region], crs='3031').plot(ax=axs[1,0], color='k', linewidth=2, alpha=0.2)\n",
    "\n",
    "# Set up colormap\n",
    "min_date = pd.to_datetime(cyc_start_dates[1])\n",
    "max_date = pd.to_datetime(cyc_end_dates[-1])\n",
    "date_range = pd.date_range(min_date, max_date, periods=len(mid_cyc_dates[1:]))\n",
    "years = date_range.year.unique()\n",
    "years = pd.to_datetime(years, format='%Y')\n",
    "n_dates = len(mid_cyc_dates[1:])\n",
    "cmap = plt.get_cmap('plasma', n_dates)\n",
    "norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "m.set_array(np.linspace(mdates.date2num(min_date), mdates.date2num(max_date), n_dates))\n",
    "\n",
    "# Add colorbar\n",
    "divider = make_axes_locatable(axs[1,0])\n",
    "cax = divider.append_axes('bottom', size='3%', pad=0.6)\n",
    "cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "\n",
    "# Set ticks for all years but labels only for odd years\n",
    "tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "tick_labels = [date.strftime('%Y') if date.year % 4 == 0 else '' for date in years[1:]]\n",
    "cbar.set_ticks(tick_locations)\n",
    "cbar.set_ticklabels(tick_labels)\n",
    "\n",
    "# Add minor ticks for quarters\n",
    "cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Quarter intervals only\n",
    "cbar.set_label('year', size=12, labelpad=10)\n",
    "\n",
    "axs[1,0].set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "    \n",
    "# Emphasize zeroth row_index within evaluation line selected for particular lake\n",
    "gpd.GeoSeries(area_multiple_buffer(lake_poly, within_area_multiple)).boundary.plot(ax=ax, linestyle='solid', color='gray')\n",
    "\n",
    "# Plot legend\n",
    "evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=2)\n",
    "stationary_region_patch = mpatches.Patch(color=stationary_color, alpha=0.2)\n",
    "evolving_union_region_patch = mpatches.Patch(color='k', alpha=0.2)\n",
    "\n",
    "legend = axs[1,0].legend([stationary_region_patch,\n",
    "                       evolving_union_region_patch,\n",
    "                       tuple(onlake_lines),\n",
    "                       # tuple(offlake_lines),\n",
    "                       evolving_union_line], \n",
    "    ['stationary region',\n",
    "     'evolving union region',\n",
    "     'evolving outlines',\n",
    "     # f'evolving outlines ({evolving_outlines_gdf.level[0]} m)',\n",
    "     # 'off-lake evolving outlines', \n",
    "     'evolving outlines union'],\n",
    "    handlelength=3,\n",
    "    handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper right')\n",
    "\n",
    "\n",
    "# Panel - active area ---------------------------------------------\n",
    "    \n",
    "# Plot horizontal zero line for reference\n",
    "axs[1,1].axhline(0, color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "# Plot stationary outline and evolving outlines unary union areas\n",
    "axs[1,1].axhline(np.divide(lake_gdf['area (m^2)'], 1e6).values, \n",
    "                 color=stationary_color, linestyle='solid', linewidth=2)\n",
    "axs[1,1].axhline(np.divide(evolving_union_gdf['area (m^2)'], 1e6).values, \n",
    "                 color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "# Plot multi-colored line and scatter for data points\n",
    "x = mdates.date2num(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "y = np.divide(evolving_geom_calcs_df['evolving_outlines_area (m^2)'], 1e6)\n",
    "\n",
    "# Plot evolving outlines time series using LineCollection from points/segments to plot multi-colored line\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(x)  # Set the values used for colormapping, using matplotlib dates for colors\n",
    "lc.set_linewidth(2)\n",
    "line = axs[1,1].add_collection(lc)\n",
    "scatter = axs[1,1].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "# Add legend\n",
    "legend = axs[1,1].legend([stationary_line, \n",
    "                          evolving_union_line, \n",
    "                          tuple(onlake_lines)], \n",
    "    ['stationary outline',\n",
    "     'evolving outlines union',\n",
    "     'evolving outlines',],\n",
    "     handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "     loc='upper right')\n",
    "\n",
    "\n",
    "# Panel - dh/dt -------------------------------------------------------\n",
    "\n",
    "# Plot horizontal zero line for reference\n",
    "axs[2,0].axhline(0, color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "# Plot stationary outlines off-lake region dh\n",
    "axs[2,0].plot(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "    np.cumsum(stationary_geom_calcs_df['stationary_outline_region_dh (m)']),\n",
    "    color='lightgray', linestyle='solid', linewidth=2)\n",
    "axs[2,0].scatter(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "    np.cumsum(stationary_geom_calcs_df['stationary_outline_region_dh (m)']),\n",
    "    color='lightgray', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# Plot evolving outlines off-lake region dh\n",
    "axs[2,0].plot(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']), \n",
    "    np.cumsum(evolving_geom_calcs_df['evolving_outlines_region_dh (m)']), \n",
    "    color='dimgray', linestyle='solid', linewidth=2)\n",
    "axs[2,0].scatter(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']), \n",
    "    np.cumsum(evolving_geom_calcs_df['evolving_outlines_region_dh (m)']),\n",
    "    color='dimgray', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# Plot stationary outline time series\n",
    "axs[2,0].plot(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "    np.cumsum(stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "    color=stationary_color, linestyle='solid', linewidth=2)\n",
    "axs[2,0].scatter(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "    np.cumsum(stationary_geom_calcs_df['stationary_outline_dh_corr (m)']),\n",
    "    color=stationary_color, linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# Plot multi-colored line and scatter for data points\n",
    "x = mdates.date2num(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "y = np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'])\n",
    "\n",
    "# Plot evolving outlines time series using multi-colored LineCollection from points/segments\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(2)\n",
    "line = axs[2,0].add_collection(lc)\n",
    "scatter = axs[2,0].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "# Plot bias\n",
    "axs[2,0].plot(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']),\n",
    "    np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'] -\n",
    "              stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "    color='red', linestyle='solid', linewidth=2)\n",
    "axs[2,0].scatter(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']),\n",
    "    np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'] -\n",
    "              stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "    color='red', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# Add legend\n",
    "evolving_region = plt.Line2D([], [], color='dimgray', linestyle='solid', linewidth=2)\n",
    "stationary_region = plt.Line2D([], [], color='lightgray', linestyle='solid', linewidth=2)\n",
    "bias = plt.Line2D([], [], color='red', linestyle='solid', linewidth=2)\n",
    "legend = axs[2,0].legend(\n",
    "    [evolving_region,\n",
    "     stationary_region,\n",
    "     tuple(onlake_lines),\n",
    "     stationary_line,  \n",
    "     bias],\n",
    "    ['evolving outlines regional',\n",
    "     'stationary outline regional',\n",
    "     'evolving outlines',\n",
    "     'stationary outline', \n",
    "     'bias (evolving  stationary)'],\n",
    "     handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "     loc='upper right')\n",
    "\n",
    "\n",
    "# Panel - dV/dt --------------------------------------------------\n",
    "\n",
    "# Plot horizontal line at zero for reference\n",
    "axs[2,1].axhline(0, color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "# Plot stationary outline time series\n",
    "axs[2,1].plot(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "    np.divide(np.cumsum(stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "    color=stationary_color, linestyle='solid', linewidth=2)\n",
    "axs[2,1].scatter(mdates.date2num(stationary_geom_calcs_df['midcyc_datetime']), \n",
    "    np.divide(np.cumsum(stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "    color=stationary_color, linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# Plot multi-colored line and scatter for data points\n",
    "x = mdates.date2num(evolving_geom_calcs_df['midcyc_datetime'])\n",
    "y = np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "\n",
    "# Plot evolving outlines time series using LineCollection from points/segments to plot multi-colored line\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(2)\n",
    "line = axs[2,1].add_collection(lc)\n",
    "scatter = axs[2,1].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "# Plot bias\n",
    "axs[2,1].plot(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']),\n",
    "    np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "                        stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)'], 1e9)), \n",
    "    color='red', linestyle='solid', linewidth=2)\n",
    "axs[2,1].scatter(mdates.date2num(evolving_geom_calcs_df['midcyc_datetime']),\n",
    "    np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "                        stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)'], 1e9)), \n",
    "    color='red', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# Add legend\n",
    "legend = axs[2,1].legend([tuple(onlake_lines), stationary_line, bias],\n",
    "    ['evolving outlines', \n",
    "     'stationary outline',\n",
    "     'bias (evolving  stationary)'], \n",
    "    handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper right')\n",
    "\n",
    "# Adjust y-axis limits to avoid legend/data overlap\n",
    "axs[1,1].set_ylim(-25, 625)\n",
    "axs[2,0].set_ylim(-1, 8)\n",
    "axs[2,1].set_ylim(-0.4, 0.4)\n",
    "\n",
    "# Add vertical spans to time series plots\n",
    "for ax in [axs[1,1], axs[2,0], axs[2,1]]:\n",
    "    # Add vertical spans for both timesteps\n",
    "    ax.axvspan(mdates.date2num(cs2_start_date), \n",
    "                         mdates.date2num(cs2_end_date),\n",
    "                         color='gray', alpha=0.1)\n",
    "    \n",
    "    ax.axvspan(mdates.date2num(is2_start_date),\n",
    "                         mdates.date2num(is2_end_date),\n",
    "                         color='gray', alpha=0.1)\n",
    "\n",
    "# Label the timeline panel\n",
    "timeline_ax.text(0.01, 0.98, 'a', transform=timeline_ax.transAxes, \n",
    "                fontsize=20, va='top', ha='left')\n",
    "\n",
    "# Plot elements common to more than one ax object\n",
    "for i in range(axs.shape[0]):\n",
    "    for j in range(axs.shape[1]):\n",
    "        # Set common tick size for all axes\n",
    "        axs[i,j].tick_params(axis='both')\n",
    "        \n",
    "        # Add subplot labels (a, b, c, etc.)\n",
    "        axs[i,j].text(0.02, 0.98, chr(97 + i*2 + j + 1), transform=axs[i,j].transAxes, \n",
    "                     fontsize=16, va='top', ha='left')\n",
    "\n",
    "        # Special formatting for map plots (top row and [1,0])\n",
    "        if (i == 0) or (i == 1 and j == 0):\n",
    "            # Set x and y labels for map plots\n",
    "            axs[i,j].set_xlabel('x [km]')\n",
    "            if j == 0:  # Only for first column\n",
    "                axs[i,j].set_ylabel('y [km]')\n",
    "\n",
    "            # Add map-specific elements\n",
    "            revised_stationary_outlines_gdf.boundary.plot(ax=axs[i,j], \n",
    "                edgecolor=stationary_color, linestyle='solid', linewidth=2, zorder=0)\n",
    "            \n",
    "            # Convert meters to kilometers\n",
    "            km_scale = 1e3\n",
    "            ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "            ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "            axs[i,j].xaxis.set_major_formatter(ticks_x)\n",
    "            axs[i,j].yaxis.set_major_formatter(ticks_y)\n",
    "\n",
    "            # Add evaluation lines\n",
    "            for within_area_multiple_i in range(2,16):\n",
    "                gpd.GeoSeries(area_multiple_buffer(lake_poly, within_area_multiple_i)).boundary.plot(\n",
    "                    ax=axs[i,j], linestyle='dotted', color='gray')\n",
    "            \n",
    "            # Add emphasized evaluation line\n",
    "            gpd.GeoSeries(area_multiple_buffer(lake_poly, within_area_multiple)).boundary.plot(\n",
    "                ax=axs[i,j], linestyle='solid', color='gray')\n",
    "\n",
    "        # Format time series plots\n",
    "        else:\n",
    "            axs[i,j].set_xlabel('year')\n",
    "            \n",
    "            # Set y labels for time series plots\n",
    "            if j == 0:  # First column\n",
    "                if i == 2:\n",
    "                    axs[i,j].set_ylabel('cumulative dh [m]')\n",
    "            elif j == 1:  # Second column\n",
    "                if i == 1:\n",
    "                    axs[i,j].set_ylabel('active area [km$^2$]')\n",
    "                elif i == 2:\n",
    "                    axs[i,j].set_ylabel('cumulative dV [km$^3$]')\n",
    "            \n",
    "            # Add date formatting for specific time series plots\n",
    "            if (i == 1 and j == 1) or (i == 2):  # axs[1,1], axs[2,0], and axs[2,1]\n",
    "                axs[i,j].xaxis.set_major_formatter(ticker.FuncFormatter(even_year_formatter))\n",
    "                axs[i,j].xaxis.set_major_locator(mdates.YearLocator())\n",
    "                axs[i,j].xaxis.set_minor_locator(mdates.MonthLocator([1, 4, 7, 10]))  # Quarter year ticks (Jan, Apr, Jul, Oct)\n",
    "                axs[i,j].set_xlim(cyc_start_dates[1], cyc_end_dates[-1])\n",
    "\n",
    "# Save and preview plot\n",
    "plt.savefig(OUTPUT_DIR + '/figures/Figure_S1.png',\n",
    "    dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c404262a-4294-4c03-a7c3-b71bcc8f6edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0df42f8-a25a-4f86-b4ad-3f0c60e82611",
   "metadata": {},
   "source": [
    "# within_area_multiple, level distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6683e271-99bd-4d5c-8420-f1da2c55ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_first_rows(folder_path):\n",
    "#     \"\"\"\n",
    "#     Reads all CSV files in the specified folder and combines their first rows into a single DataFrame.\n",
    "    \n",
    "#     Parameters:\n",
    "#     folder_path (str): Path to the folder containing CSV files\n",
    "    \n",
    "#     Returns:\n",
    "#     pandas.DataFrame: DataFrame containing the first row from each CSV file\n",
    "#     \"\"\"\n",
    "#     # List to store first rows\n",
    "#     first_rows = []\n",
    "    \n",
    "#     # Iterate through all files in the folder\n",
    "#     for filename in os.listdir(folder_path):\n",
    "#         if filename.endswith('.csv'):\n",
    "#             file_path = os.path.join(folder_path, filename)\n",
    "#             try:\n",
    "#                 # Read the CSV file\n",
    "#                 df = pd.read_csv(file_path)\n",
    "                \n",
    "#                 # Get the first row and add filename as a column\n",
    "#                 if not df.empty:\n",
    "#                     first_row = df.iloc[0:1].copy()\n",
    "#                     first_row['source_file'] = filename\n",
    "#                     first_rows.append(first_row)\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing {filename}: {str(e)}\")\n",
    "    \n",
    "#     # Combine all first rows into a single DataFrame\n",
    "#     if first_rows:\n",
    "#         result = pd.concat(first_rows, ignore_index=True)\n",
    "#         return result\n",
    "#     else:\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "# # # Example usage:\n",
    "# # folder_path = \"/home/jovyan/1_evolving_lakes/output/FigS1_lake_reexamination_methods/levels\"\n",
    "# # result_df = combine_first_rows(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e45624d-3170-4b09-afcf-8554199dc56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_first_rows(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all CSV files in the specified folder and combines their first rows into a single DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    folder_path (str): Path to the folder containing CSV files\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame containing the first row from each CSV file\n",
    "    \"\"\"\n",
    "    # List to store first rows\n",
    "    first_rows = []\n",
    "    \n",
    "    # Iterate through all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Get the first row and add filename as a column\n",
    "                if not df.empty:\n",
    "                    first_row = df.iloc[0:1].copy()\n",
    "                    \n",
    "                    # Add full filename and filename without extension as columns\n",
    "                    first_row['name'] = os.path.splitext(filename)[0]\n",
    "                    \n",
    "                    first_rows.append(first_row)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {str(e)}\")\n",
    "    \n",
    "    # Combine all first rows into a single DataFrame\n",
    "    if first_rows:\n",
    "        result = pd.concat(first_rows, ignore_index=True)\n",
    "        return result\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b283216-6cb4-4077-a7ac-6073746f8af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/home/jovyan/1_evolving_lakes/output/FigS1_lake_reexamination_methods/levels\"\n",
    "result_df = combine_first_rows(folder_path)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e2a964-b4de-4716-a327-96042e68f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_level_histogram(df, column='level', bins=10):\n",
    "    \"\"\"\n",
    "    Creates a histogram of a specified column with enhanced styling\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the data\n",
    "    column (str): Name of the column to create histogram for\n",
    "    bins (int): Number of bins for the histogram\n",
    "    \"\"\"\n",
    "    # Create figure and axis objects with larger size\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create histogram\n",
    "    plt.hist(df[column], bins=bins, color='skyblue', edgecolor='black')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title(f'Distribution of {column}', pad=15, fontsize=14)\n",
    "    plt.xlabel(column, fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Rotate x-axis labels if needed\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_level_histogram(result_df, column='level', bins=35)\n",
    "# plot_level_histogram(result_df, column='within_area_multiple', bins=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29ab093-3ac1-4384-bbac-4afa727776d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_level_histogram(result_df, column='level', bins=35)\n",
    "plot_level_histogram(result_df, column='within_area_multiple', bins=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e555de-b55b-47b7-9d7f-aa535429e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[result_df['level'] > 0.3].sort_values('level', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad66e48-0282-4f66-8f5a-c279cef80145",
   "metadata": {},
   "source": [
    "# TODO: Export a final inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef7c08f-714d-4baf-9434-e476b210ca01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886a85e5-6c0d-4c11-90bf-da5e806cee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /home/jovyan/1_evolving_lakes/Sauthoff-202X-evolving-lakes/output/lake_outlines/evolving_outlines_union_gdf.geojson /home/jovyan/data/boundaries/evolving_outlines_union_gdf.geojson"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
