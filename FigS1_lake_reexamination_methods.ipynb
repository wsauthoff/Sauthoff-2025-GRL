{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a180750-d435-44ea-94f0-c96b9dbea099",
   "metadata": {
    "user_expressions": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# change plot_height_changes to have 15x search extent or none at all?\n",
    "\n",
    "# Import/read ATL11 (try h5coro)\n",
    "# Add ATL11 overlay\n",
    "\n",
    "# Add something along the lines of code below to illustrate lake area/outline:\n",
    "# # Plot polygons in the GeoDataFrame\n",
    "# gdf.plot(ax=ax, color='lightblue', edgecolor='black', linewidth=1, label='Polygon Area')\n",
    "# # Optionally, if you want to plot just the boundaries with a different style\n",
    "# gdf.boundary.plot(ax=ax, color='red', linewidth=2, label='Polygon Boundary')\n",
    "\n",
    "# Simplify subsetting code:\n",
    "# CS2_dh_sub = CS2_dh.sel(y=slice(y_max, y_min), x=slice(x_min, x_max))\n",
    "# ATL15_dh_sub = ATL15_dh.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "\n",
    "# Simplify codes to makes lines:\n",
    "# Creating custom legend entries as lines\n",
    "    # line_3_1 = mlines.Line2D([], [], color='red', linewidth=2, label='v3.1')\n",
    "    # line_3_6 = mlines.Line2D([], [], color='blue', linewidth=2, label='v3.6')\n",
    "\n",
    "# # Saved geojson's behave differently: evolving_outlines_gdfs have midcyc_datetime open as timestamp whereas \n",
    "# # compare_evolving_static_outlines_gdfs have midcyc_datetime open as string\n",
    "# evolving_outlines_gdf = gpd.read_file(os.path.join(\n",
    "#     os.getcwd(), 'output/lake_outlines/evolving_outlines/{}.geojson'.format(SF18_lake_gdf['name'].values[0])))\n",
    "# print(type(evolving_outlines_gdf['midcyc_datetime'][0]))\n",
    "\n",
    "# geom_calcs_df = pd.read_csv(os.path.join(\n",
    "#     os.getcwd(), 'output/lake_outlines/compare_evolving_static_outlines/{}.csv'.format(SF18_lake_gdf['name'].values[0])))\n",
    "# print(type(geom_calcs_df['midcyc_datetime'][0]))\n",
    "\n",
    "# Within plot_data_counts and plot_height_changes func's add handing of lakes with shorted CS2 time period\n",
    "\n",
    "# Add Hodgson doline lake and AP lakes\n",
    "\n",
    "# To extract_intersecting_polygons, add while loop to do all polygons within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46ca6a6e-114b-437a-babe-8dfa175f4299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to Fig. S1 of Sauthoff and others, 202X\n",
    "# This code runs continental-scale operations on multiple datasets and\n",
    "# requires a 32 GB server or local memory\n",
    "#\n",
    "# Written 2023-07-11 by W. Sauthoff (wsauthoff.github.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff75638-a801-40a3-bd59-7d6536dbc054",
   "metadata": {},
   "source": [
    "# Set up computing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67b2a28f-a575-4f1e-b1c9-f601327149aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /srv/conda/envs/notebook/lib/python3.11/site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in /srv/conda/envs/notebook/lib/python3.11/site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad47cdb0-00aa-4268-8691-9e44b3bfda2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import datetime\n",
    "import earthaccess\n",
    "import fiona\n",
    "import gc\n",
    "import geopandas as gpd\n",
    "from IPython.display import clear_output\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from pyproj import CRS, Geod, Transformer\n",
    "import rioxarray\n",
    "from rioxarray.exceptions import NoDataInBounds\n",
    "from shapely.geometry import box, Point, Polygon\n",
    "from shapely.ops import unary_union\n",
    "from skimage import measure\n",
    "import time\n",
    "import xarray as xr\n",
    "\n",
    "# Magic functions\n",
    "%matplotlib widget\n",
    "# %matplotlib inline\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "def play_sound():\n",
    "    display(Audio(url=\"http://codeskulptor-demos.commondatastorage.googleapis.com/pang/pop.mp3\", autoplay=True))\n",
    "\n",
    "# Define data directories dependent on home environment\n",
    "# Replace with your directory file paths\n",
    "if os.getenv('HOME') == '/home/jovyan':\n",
    "    DATA_DIR = '/home/jovyan/data'\n",
    "    SCRIPT_DIR = '/home/jovyan/repos_my/script_dir'\n",
    "    OUTPUT_DIR = '/home/jovyan/1_outlines_candidates/output/FigS1_lake_reexamination_methods.ipynb'\n",
    "\n",
    "# Define constants and coordinate transforms for the geodesic area calculation\n",
    "CRS_LL = \"EPSG:4326\" # wgs84 in lon,lat\n",
    "GEOD = CRS(CRS_LL).get_geod() # geod object for calculating geodesic area on defined ellipsoid\n",
    "CRS_XY = \"EPSG:3031\" # Antarctic Polar Stereographic in x, y\n",
    "XY_TO_LL = Transformer.from_crs(CRS_XY, CRS_LL, always_xy = True) # make coord transformer\n",
    "\n",
    "# Change default font to increase font size\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "\n",
    "# Create a Geod object for calculating area on the WGS84 ellipsoid\n",
    "geod = Geod(ellps=\"WGS84\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4adef9-57d4-4ab4-b066-ba4a4ebb38a5",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e074c049-cbfc-41a5-8d42-35f57f5cdffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def read_field(group, field):\n",
    "#     \"\"\"\n",
    "#     generic field-reading function\n",
    "#     \"\"\"\n",
    "#     data=np.array(group[field])\n",
    "#     bad=(data==group[field].attrs['_FillValue'])\n",
    "#     data[bad]=np.NaN\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "649ccc68-98f0-4c40-969e-d20c4d37de06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def read_ATL11(filename, pair):\n",
    "#     \"\"\"\n",
    "#     ATL11 reader\n",
    "#     \"\"\"\n",
    "#     with h5py.File(filename,'r') as h5f:\n",
    "#         longitude=read_field(h5f[pair],'longitude')\n",
    "#         latitude=read_field(h5f[pair],'latitude')\n",
    "#         h_corr=read_field(h5f[pair],'h_corr')\n",
    "#         h_corr_sigma=read_field(h5f[pair],'h_corr_sigma')\n",
    "#         h_corr_sigma_s=read_field(h5f[pair],'h_corr_sigma_systematic')\n",
    "#         quality=np.array(h5f[pair]['quality_summary'])\n",
    "#     for col in range(h_corr.shape[1]):\n",
    "#         h_corr[quality==1]=np.NaN\n",
    "#     # return the values\n",
    "#     return longitude, latitude, h_corr, np.sqrt(h_corr_sigma**2+h_corr_sigma_s**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4f894df-e7b5-42e7-9469-523ab310517b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def datetime64_to_fractional_year(date):\n",
    "    \"\"\"\n",
    "    Convert a numpy.datetime64 object into a fractional year, rounded to 0, .25, .5, or .75.\n",
    "    \"\"\"\n",
    "    year = date.astype('datetime64[Y]').astype(int) + 1970\n",
    "    start_of_year = np.datetime64(f'{year}-01-01')\n",
    "    start_of_next_year = np.datetime64(f'{year + 1}-01-01')\n",
    "    year_length = (start_of_next_year - start_of_year).astype('timedelta64[D]').astype(int)\n",
    "    day_of_year = (date - start_of_year).astype('timedelta64[D]').astype(int)\n",
    "    fractional_year = year + day_of_year / year_length\n",
    "\n",
    "    # Round to nearest quarter\n",
    "    rounded_fractional_year = round(fractional_year * 4) / 4\n",
    "    return rounded_fractional_year\n",
    "\n",
    "# # Example usage\n",
    "# date = np.datetime64('2024-01-05')\n",
    "# fractional_year = datetime64_to_fractional_year(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "323680f2-75c7-4398-93c9-7c4ecbf76d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def muliple_area_buffer(polygon, area_multiple, precision=100):\n",
    "    \"\"\"\n",
    "    This function takes a polygon and returns a polygon with a buffer such that the area of the buffered polygon\n",
    "    is approximately the specified multiple the area of the original polygon.\n",
    "\n",
    "    :param polygon: Shapely Polygon object\n",
    "    :param area_multiple: the multiple of the original polygon area you wish the buffered polygon to be\n",
    "    :param precision: Precision for the iterative process to find the buffer distance\n",
    "    :return: Buffered Polygon\n",
    "    \"\"\"\n",
    "    original_area = polygon.area\n",
    "    target_area = area_multiple * original_area\n",
    "    buffer_distance = 0\n",
    "    buffered_polygon = polygon\n",
    "\n",
    "    while True:\n",
    "        buffered_polygon = polygon.buffer(buffer_distance)\n",
    "        if buffered_polygon.area >= target_area:\n",
    "            break\n",
    "        buffer_distance += precision\n",
    "    \n",
    "    # Convert to geodataframe\n",
    "    buffered_polygon_gdf = gpd.GeoDataFrame({'geometry': [buffered_polygon]})\n",
    "    \n",
    "#     # Set CRS\n",
    "#     buffered_polygon_gdf.geometry.crs = 'EPSG:3031'\n",
    "\n",
    "#     return buffered_polygon_gdf\n",
    "    return buffered_polygon\n",
    "\n",
    "# # Example usage\n",
    "# # Define a simple square polygon\n",
    "# square = Polygon([(0, 0), (1, 0), (1, 1), (0, 1)])\n",
    "\n",
    "# # Apply the function to find the buffered polygon area and bounds\n",
    "# buffered_poly = double_area_buffer(square)\n",
    "# buffered_poly.area, buffered_poly.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c6ac957-3f76-4167-b794-ef6704a7d63b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_folder(directory):\n",
    "    # Check if the directory already exists\n",
    "    if not os.path.exists(directory):\n",
    "        # If it doesn't exist, create a new directory\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Folder '{directory}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Folder '{directory}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "333fd087-aed3-42ee-ba7a-1f7fc88c43a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_data_counts(lake_gdf, dataset1, dataset2): \n",
    "    '''\n",
    "    Create planview plots of counts going into gridded ice-surface height change (dh/dt) products\n",
    "    \n",
    "    Inputs:\n",
    "    * lake_gdf: geodataframe of lake to be analyzed\n",
    "    * dataset1: dataset1 to be analyzed\n",
    "    * dataset2: dataset2 to be analyzed in conjunction with dataset1; currently configured to \n",
    "    splice CryoSat-2 and ICESat-2 eras\n",
    "    \n",
    "    Outputs:\n",
    "    * Sequence of planview data count visuals of lake around surrounding area\n",
    "    '''\n",
    "    # Define lake name and polygon\n",
    "    lake_name = lake_gdf.name.values[0]\n",
    "    lake_poly = lake_gdf.iloc[0].geometry\n",
    "    \n",
    "    # # Create buffered polygons for various multiples of lake area to find which\n",
    "    # # best emcompasses the height change signals at previously identified lakes\n",
    "    search_extent_poly = muliple_area_buffer(lake_poly, 10)\n",
    "    \n",
    "    # Establish x_min, x_max, y_min, y_max\n",
    "    # x_min, y_min, x_max, y_max = search_extent_poly.iloc[0].geometry.bounds\n",
    "    x_min, y_min, x_max, y_max = search_extent_poly.bounds\n",
    "    x_buffer = abs(x_max-x_min)*0.5\n",
    "    y_buffer = abs(y_max-y_min)*0.5\n",
    "\n",
    "    # Subsetting datasets\n",
    "    mask_x = (dataset1.x >= x_min - x_buffer) & (dataset1.x <= x_max + x_buffer)\n",
    "    mask_y = (dataset1.y >= y_min - y_buffer) & (dataset1.y <= y_max + y_buffer)\n",
    "    dataset1_subset = dataset1.where(mask_x & mask_y, drop=True)\n",
    "    mask_x = (dataset2.x >= x_min - x_buffer) & (dataset2.x <= x_max + x_buffer)\n",
    "    mask_y = (dataset2.y >= y_min - y_buffer) & (dataset2.y <= y_max + y_buffer)\n",
    "    dataset2_subset = dataset2.where(mask_x & mask_y, drop=True)\n",
    "    \n",
    "    # Make output folders\n",
    "    create_folder(OUTPUT_DIR + '/{}'.format('plot_data_counts'))\n",
    "    create_folder(OUTPUT_DIR + '/plot_data_counts/{}'.format(lake_name))\n",
    "    print('Now creating/saving plots...')\n",
    "\n",
    "    # Create lines for legend\n",
    "    S09_color = 'paleturquoise'\n",
    "    SF18_color  = 'turquoise'\n",
    "    lakes_gdf_postSF18_color = 'darkturquoise'\n",
    "    Smith2009 = plt.Line2D((0, 1), (0, 0), color=S09_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "    SiegfriedFricker2018 = plt.Line2D((0, 1), (0, 0), color=SF18_color, linestyle=(0, (1, 2)), linewidth=2)\n",
    "    lakes_gdf_postSF18_line = plt.Line2D((0, 1), (0, 0), color=lakes_gdf_postSF18_color, linestyle=(0, (1, 1)), linewidth=2)\n",
    "    search_extent_line = plt.Line2D((0, 1), (0, 0), color='red', linestyle='solid', linewidth=2)\n",
    "\n",
    "    # Calculate cycle-to-cycle dHeight at each cycle of the spliced data sets\n",
    "    for idx in range(len(midcyc_dates)):\n",
    "        # For midcyc_dates indexes <= 32, use CryoSat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the CryoSat-2 era before ICESat-2 launch (2010-08-17 to 2018-08-17)\n",
    "        if idx <= 32:\n",
    "            count_subset = dataset1_subset['data_count'][idx,:,:]\n",
    "            count_subset.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "        # For midcyc_dates indexes > 32, use ICESat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the ICESat-2 era (2018-11-16 to most recently available data)\n",
    "        elif idx > 32:\n",
    "            # Subtract 33 from idx to start over at index zero with new dataset\n",
    "            count_subset = dataset2_subset['data_count'][(idx-33),:,:]\n",
    "            count_subset.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "            \n",
    "        # Only plot arrays that have data in them; some time slices have no data because no CryoSat-2 SARIn coverage\n",
    "        # Returns True if there is at least one non-NaN value in data_stacked, and False if all values are NaN\n",
    "        if np.any(~np.isnan(count_subset)):\n",
    "            # Create fig, ax\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "            # Plot figure\n",
    "            img = ax.imshow(count_subset, extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer], \n",
    "                origin='lower', cmap='viridis')\n",
    "\n",
    "            # Plot buffered polygons\n",
    "            gpd.GeoDataFrame(geometry=[search_extent_poly]).boundary.plot(ax=ax, edgecolor='r', facecolor='none', linewidth=2)\n",
    "            \n",
    "            # Overlay published active lake outlines for visual comparison and grounding line\n",
    "            S09_outlines.boundary.plot(ax=ax, edgecolor=S09_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "            SF18_outlines.boundary.plot(ax=ax, edgecolor=SF18_color, linestyle=(0, (1, 2)), linewidth=2)\n",
    "            lakes_gdf_postSF18.boundary.plot(ax=ax, edgecolor=lakes_gdf_postSF18_color, linestyle=(0, (1, 1)), linewidth=2)          \n",
    "            \n",
    "            # Change polar stereographic m to km\n",
    "            km_scale = 1e3\n",
    "            ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "            ax.xaxis.set_major_formatter(ticks_x)\n",
    "            ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "            ax.yaxis.set_major_formatter(ticks_y)  \n",
    "\n",
    "            # Label axes, set limits, and set title\n",
    "            ax.set_xlabel('x [km]', size=15)\n",
    "            ax.set_ylabel('y [km]', size=15) \n",
    "            ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "            # Plot inset map\n",
    "            axIns = ax.inset_axes([0.01, 0.01, 0.2, 0.2]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "            axIns.set_aspect('equal')\n",
    "            moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "            moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "            axIns.axis('off')\n",
    "\n",
    "            # Plot red star to indicate location\n",
    "            axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "                linewidth=1, edgecolor='k', facecolor='r', s=100, zorder=3)\n",
    "\n",
    "            # Add colorbar \n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "            fig.colorbar(img, cax=cax).set_label('data counts', size=15)\n",
    "\n",
    "            # Add legend\n",
    "            ax.legend([Smith2009, \n",
    "                       SiegfriedFricker2018, \n",
    "                       lakes_gdf_postSF18_line,\n",
    "                       search_extent_line                       \n",
    "                      ],\n",
    "                ['static outline (S09)', \n",
    "                 'static outline (SF18)', \n",
    "                 'other',\n",
    "                 'search extent'\n",
    "                ], \n",
    "                loc='upper center')\n",
    "            \n",
    "            # Set a title for the axes\n",
    "            ax.set_title('Data counts from from {} to {}'.format(cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)))\n",
    "            \n",
    "            # Save and close fig\n",
    "            plt.savefig(OUTPUT_DIR + \n",
    "                '/plot_data_counts/{}/plot_data_counts_{}_{}-{}.png'\n",
    "                .format(lake_name, lake_name, cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "    print('Complete')\n",
    "        \n",
    "# # Example usage\n",
    "# plot_data_counts(lake_gdf=lake_gdf, dataset1=CS2_Smith2017, dataset2=ATL15_dh): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9970b6cb-b3b0-494c-b6c6-290679b5194d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_height_changes(lake_gdf, dataset1, dataset2): \n",
    "    '''\n",
    "    Create planview plots of ice surface height changes (dh/dt)\n",
    "    \n",
    "    Inputs:\n",
    "    * lake_gdf: geodataframe of lake to be analyzed\n",
    "    * dataset1: dataset1 to be analyzed\n",
    "    * dataset2: dataset2 to be analyzed in conjunction with dataset1; currently configured to \n",
    "    splice CryoSat-2 and ICESat-2 eras\n",
    "    \n",
    "    Outputs: \n",
    "    * Sequence of planview delta height visuals of lake around surrounding area\n",
    "    '''\n",
    "    # Define lake name and polygon\n",
    "    lake_name = lake_gdf.name.values[0]\n",
    "    lake_poly = lake_gdf.iloc[0].geometry\n",
    "    \n",
    "    # Create buffered polygons for various multiples of lake area to find which\n",
    "    # best emcompasses the height change signals at previously identified lakes\n",
    "    search_extent_poly = muliple_area_buffer(lake_poly, 10)\n",
    "\n",
    "    # Establish x_min, x_max, y_min, y_max\n",
    "    # x_min, y_min, x_max, y_max = search_extent_poly.iloc[0].geometry.bounds\n",
    "    x_min, y_min, x_max, y_max = search_extent_poly.bounds\n",
    "    x_buffer = abs(x_max-x_min)*0.2\n",
    "    y_buffer = abs(y_max-y_min)*0.2\n",
    "    \n",
    "    # Subsetting datasets\n",
    "    mask_x = (dataset1.x >= x_min - x_buffer) & (dataset1.x <= x_max + x_buffer)\n",
    "    mask_y = (dataset1.y >= y_min - y_buffer) & (dataset1.y <= y_max + y_buffer)\n",
    "    dataset1_subset = dataset1.where(mask_x & mask_y, drop=True)\n",
    "    mask_x = (dataset2.x >= x_min - x_buffer) & (dataset2.x <= x_max + x_buffer)\n",
    "    mask_y = (dataset2.y >= y_min - y_buffer) & (dataset2.y <= y_max + y_buffer)\n",
    "    dataset2_subset = dataset2.where(mask_x & mask_y, drop=True)\n",
    "    \n",
    "    # Find magnitude of ice surface deformation in bounding box to create appropriate color map scale\n",
    "    # Create empty lists to store data\n",
    "    height_anom_pos = []\n",
    "    height_anom_neg = []\n",
    "    for idx in range(len(midcyc_dates)):\n",
    "        if idx <= 32:\n",
    "            dhdt_subset = dataset1_subset.delta_h[idx+1,:,:]-dataset1_subset.delta_h[idx,:,:]\n",
    "        elif idx > 32:\n",
    "            # Subtract 33 from idx to start over at index zero with new dataset\n",
    "            dhdt_subset = dataset2_subset.delta_h[(idx-33)+1,:,:]-dataset2_subset.delta_h[(idx-33),:,:]\n",
    "        if np.any(~np.isnan(dhdt_subset)):       \n",
    "            pos = np.nanmax(dhdt_subset)\n",
    "            neg = np.nanmin(dhdt_subset)\n",
    "            height_anom_pos += [pos]\n",
    "            height_anom_neg += [neg]\n",
    "    \n",
    "    # Store max pos/neg height anomalies from all cycles to create colorbar bounds later\n",
    "    max_height_anom_pos = max(height_anom_pos)\n",
    "    max_height_anom_neg = min(height_anom_neg)\n",
    "    \n",
    "    # Establish diverging colorbar\n",
    "    divnorm=colors.TwoSlopeNorm(vmin=max_height_anom_neg, vcenter=0., vmax=max_height_anom_pos)\n",
    "   \n",
    "    # Make output folders\n",
    "    create_folder(OUTPUT_DIR + '/{}'.format('plot_height_changes'))\n",
    "    create_folder(OUTPUT_DIR + '/plot_height_changes/{}'.format(lake_name))\n",
    "    print('Now creating/saving plots...')\n",
    "\n",
    "    # Create lines for legend\n",
    "    S09_color = 'paleturquoise'\n",
    "    SF18_color  = 'turquoise'\n",
    "    lakes_gdf_postSF18_color = 'darkturquoise'\n",
    "    # S09_line = plt.Line2D((0, 1), (0, 0), color=S09_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "    S09_line = plt.Line2D([], [], color=S09_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "    SF18_line = plt.Line2D((0, 1), (0, 0), color=SF18_color, linestyle=(0, (1, 2)), linewidth=2)\n",
    "    lakes_gdf_postSF18_line = plt.Line2D((0, 1), (0, 0), color=lakes_gdf_postSF18_color, linestyle=(0, (1, 1)), linewidth=2)\n",
    "    search_extent_line = plt.Line2D((0, 1), (0, 0), color='red', linestyle='solid', linewidth=2)\n",
    "\n",
    "    # Calculate cycle-to-cycle dHeight at each cycle of the spliced data sets\n",
    "    for idx in range(len(midcyc_dates)):\n",
    "        # For midcyc_dates indexes <= 32, use CryoSat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the CryoSat-2 era before ICESat-2 launch (2010-08-17 to 2018-08-17)\n",
    "        if idx <= 32:\n",
    "            dhdt_subset = dataset1_subset.delta_h[idx+1,:,:]-dataset1_subset.delta_h[idx,:,:]\n",
    "            # dhdt_subset.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "        # For midcyc_dates indexes > 32, use ICESat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the ICESat-2 era (2018-11-16 to most recently available data)\n",
    "        elif idx > 32:\n",
    "            # Subtract 33 from idx to reset idx to zero for new dataset\n",
    "            dhdt_subset = dataset2_subset.delta_h[(idx-33)+1,:,:]-dataset2_subset.delta_h[(idx-33),:,:]\n",
    "            # dhdt_subset.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "        # Only plot arrays that have data in them; some time slices have no data because no CryoSat-2 SARIn coverage\n",
    "        # Returns True if there is at least one non-NaN value in data_stacked, and False if all values are NaN\n",
    "        if np.any(~np.isnan(dhdt_subset)):\n",
    "            # Create fig, ax\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "            # Plot figure\n",
    "            img = ax.imshow(dhdt_subset, extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer],\n",
    "                origin='lower', cmap='coolwarm_r', \n",
    "                norm=divnorm)\n",
    "\n",
    "            # Plot buffered polygons\n",
    "            gpd.GeoDataFrame(geometry=[search_extent_poly]).boundary.plot(ax=ax, edgecolor='red', facecolor='none', linewidth=2)\n",
    "            \n",
    "            # Overlay published active lake outlines for visual comparison and grounding line\n",
    "            S09_outlines.boundary.plot(ax=ax, edgecolor=S09_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "            SF18_outlines.boundary.plot(ax=ax, edgecolor=SF18_color, linestyle=(0, (1, 2)), linewidth=2)\n",
    "            lakes_gdf_postSF18.boundary.plot(ax=ax, edgecolor=lakes_gdf_postSF18_color, linestyle=(0, (1, 1)), linewidth=2)          \n",
    "            \n",
    "            # Change polar stereographic m to km\n",
    "            km_scale = 1e3\n",
    "            ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "            ax.xaxis.set_major_formatter(ticks_x)\n",
    "            ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "            ax.yaxis.set_major_formatter(ticks_y)  \n",
    "\n",
    "            # Label axes, set limits, and set title\n",
    "            ax.set_xlabel('x [km]', size=15)\n",
    "            ax.set_ylabel('y [km]', size=15) \n",
    "            ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "            # Plot inset map\n",
    "            axIns = ax.inset_axes([0.01, 0.01, 0.2, 0.2]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "            axIns.set_aspect('equal')\n",
    "            moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "            moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "            axIns.axis('off')\n",
    "\n",
    "            # Plot red star to indicate location\n",
    "            axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "                linewidth=1, edgecolor='k', facecolor='r', s=100, zorder=3)\n",
    "\n",
    "            # Add colorbar \n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "            fig.colorbar(img, cax=cax).set_label('height change (dh) [m]', size=15)\n",
    "\n",
    "            # Add legend\n",
    "            ax.legend([S09_line, \n",
    "                       SF18_line, \n",
    "                       lakes_gdf_postSF18_line,\n",
    "                       search_extent_line\n",
    "                      ],\n",
    "                ['static outline (S09)', \n",
    "                 'static outline (SF18)', \n",
    "                 'other',\n",
    "                 'search extent'\n",
    "                ], \n",
    "                loc='upper center')\n",
    "            \n",
    "            # Set a title for the axes\n",
    "            ax.set_title('Height change from from {} to {}'.format(cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)))\n",
    "            \n",
    "            # Save and close fig\n",
    "            plt.savefig(OUTPUT_DIR + \n",
    "                '/plot_height_changes/{}/plot_height_changes_{}_{}-{}.png'\n",
    "                .format(lake_name, lake_name, cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "    print('Complete')\n",
    "        \n",
    "# # Example usage\n",
    "# plot_height_changes(lake_gdf=lake_gdf, dataset1=CS2_Smith2017, dataset2=ATL15_dh): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43829be4-1170-408c-b125-18d6087638d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_evolving_outlines(lake_gdf, area_multiple_search_extent, level, dataset1, dataset2, plot=False): \n",
    "    '''\n",
    "    Create planview dh/dt plots of ice surface height changes \n",
    "    Create time-variable outlines using skimage contour to plot evolving outlines as polygons.\n",
    "    \n",
    "    Inputs:\n",
    "    * ROI_name: str of the region of interest for using in file name saving\n",
    "    * ROI_poly: list of polygon(s) that define evolving outline search extent\n",
    "    * level: vertical distance in meters to delineate ice surface deformation contour\n",
    "    * dataset1: dataset1 to be analyzed\n",
    "    * dataset2: dataset2 to be analyzed in conjunction with dataset1; currently configured to \n",
    "    splice CryoSat-2 and ICESat-2 eras\n",
    "    \n",
    "    Outputs: \n",
    "    * Sequence of planview delta height visuals of ICESat-2 ATL15 with variable ice surface \n",
    "    deformation contours plotted to delineate evolving lake boundaries.\n",
    "    * geopandas geodataframe of polygons created at each step (would need to modify to collect all polygons \n",
    "    at all time steps)\n",
    "    '''\n",
    "    # Define lake name and polygon\n",
    "    lake_name = lake_gdf.name.values[0]\n",
    "    lake_poly = lake_gdf.iloc[0].geometry\n",
    "    buffered_poly = muliple_area_buffer(lake_poly, area_multiple_search_extent+1)\n",
    "\n",
    "    # Establish x_min, x_max, y_min, y_max\n",
    "    x_min, y_min, x_max, y_max = buffered_poly.bounds\n",
    "    x_buffer = abs(x_max-x_min)*0.2\n",
    "    y_buffer = abs(y_max-y_min)*0.2\n",
    "    \n",
    "    # Masking datasets\n",
    "    if dataset1 != 'none':\n",
    "        dataset1_buffered_poly_sub = dataset1.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "        dataset1_mask = np.array([[buffered_poly.contains(Point(x, y)) for x in dataset1_buffered_poly_sub['x'].values] for y in dataset1_buffered_poly_sub['y'].values])\n",
    "        dataset1_mask_da = xr.DataArray(dataset1_mask, coords=[dataset1_buffered_poly_sub.y, dataset1_buffered_poly_sub.x], dims=[\"y\", \"x\"])\n",
    "        dataset1_dh_masked = dataset1['delta_h'].where(dataset1_mask_da, drop=True)\n",
    "    dataset2_buffered_poly_sub = dataset2.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "    dataset2_mask = np.array([[buffered_poly.contains(Point(x, y)) for x in dataset2_buffered_poly_sub['x'].values] for y in dataset2_buffered_poly_sub['y'].values])\n",
    "    dataset2_mask_da = xr.DataArray(dataset2_mask, coords=[dataset2_buffered_poly_sub.y, dataset2_buffered_poly_sub.x], dims=[\"y\", \"x\"])\n",
    "    dataset2_dh_masked = dataset2['delta_h'].where(dataset2_mask_da, drop=True)\n",
    "    \n",
    "    # Only proceed with plotting if plot=True\n",
    "    if plot:\n",
    "        # Find magnitude of ice surface deformation in bounding box to create appropriate color map scale\n",
    "        # Create empty lists to store data\n",
    "        height_anom_pos = []\n",
    "        height_anom_neg = []\n",
    "        \n",
    "        for idx in range(len(midcyc_dates)-1):  # Less one because cycle-to-cycle differencing reduces length by one\n",
    "            if lake_gdf['CS2_SARIn_time_period'].iloc[0] == '2013.75-2018.75' and 20 <= idx <= 32:\n",
    "                continue  # Skip the rest of the loop for indexes 21-32 because this dataset has reduced number of time slices\n",
    "\n",
    "            if idx <= 32:\n",
    "                if dataset1 != 'none':\n",
    "                    dhdt_masked = dataset1_dh_masked[idx+1,:,:]-dataset1_dh_masked[idx,:,:]\n",
    "                else:\n",
    "                    continue\n",
    "            elif idx > 32:\n",
    "                # Subtract 33 from idx to start over at index zero with new dataset\n",
    "                dhdt_masked = dataset2_dh_masked[(idx-33)+1,:,:]-dataset2_dh_masked[(idx-33),:,:]\n",
    "            if np.any(~np.isnan(dhdt_masked)):       \n",
    "                pos = np.nanmax(dhdt_masked)\n",
    "                neg = np.nanmin(dhdt_masked)\n",
    "                height_anom_pos += [pos]\n",
    "                height_anom_neg += [neg]\n",
    "\n",
    "        # Store max pos/neg height anomalies from all cycles to create colorbar bounds later\n",
    "        max_height_anom_pos = max(height_anom_pos)\n",
    "        max_height_anom_neg = min(height_anom_neg)\n",
    "\n",
    "        # Establish diverging colorbar\n",
    "        divnorm=colors.TwoSlopeNorm(vmin=max_height_anom_neg, vcenter=0., vmax=max_height_anom_pos)\n",
    "\n",
    "        # Create lines for legend\n",
    "        S09_color = 'paleturquoise'\n",
    "        SF18_color  = 'turquoise'\n",
    "        lakes_gdf_postSF18_color = 'darkturquoise'\n",
    "        S09_line = plt.Line2D((0, 1), (0, 0), color=S09_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "        SF18_line = plt.Line2D((0, 1), (0, 0), color=SF18_color, linestyle=(0, (1, 2)), linewidth=2)\n",
    "        lakes_gdf_postSF18_line = plt.Line2D((0, 1), (0, 0), color=lakes_gdf_postSF18_color, linestyle=(0, (1, 1)), linewidth=2)\n",
    "        uplift = plt.Line2D((0, 1), (0, 0), color='mediumblue', linestyle=(0, (3, 5, 1, 5)), linewidth=2)\n",
    "        subsidence = plt.Line2D((0, 1), (0, 0), color='maroon', linestyle=(0, (3, 5, 1, 5, 1, 5)), linewidth=2)\n",
    "        search_extent_line = plt.Line2D((0, 1), (0, 0), color='red', linestyle='solid', linewidth=2)\n",
    "    \n",
    "    # Create empty list to store polygons, areas, dh's, dvol's and dates\n",
    "    polys = []\n",
    "    areas = []\n",
    "    dhs = []\n",
    "    dvols =[]\n",
    "    midcyc_datetimes = []\n",
    "\n",
    "    # Calculate cycle-to-cycle dHeight at each cycle of the spliced data sets\n",
    "    for idx in range(len(midcyc_dates)-1):  # Less one because cycle-to-cycle differencing reduces length by one\n",
    "        if lake_gdf['CS2_SARIn_time_period'].iloc[0] == '2013.75-2018.75' and 20 <= idx <= 32:\n",
    "            continue  # Skip the rest of the loop for indexes 21-32 because this dataset has reduced number of time slices\n",
    "        \n",
    "        # For midcyc_dates indexes <= 32, use CryoSat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the CryoSat-2 era before ICESat-2 launch (2010-08-17 to 2018-08-17)\n",
    "        if idx <= 32:\n",
    "            if dataset1 != 'none':\n",
    "                dhdt_masked = dataset1_dh_masked[idx+1,:,:]-dataset1_dh_masked[idx,:,:]\n",
    "                dhdt_masked.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # For midcyc_dates indexes > 32, use ICESat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the ICESat-2 era (2018-11-16 to most recently available data)\n",
    "        elif idx > 32:\n",
    "            # Subtract 33 from idx to start over at index zero with new dataset\n",
    "            dhdt_masked = dataset2_dh_masked[(idx-33)+1,:,:]-dataset2_dh_masked[(idx-33),:,:]\n",
    "            dhdt_masked.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "        # Create mapping conversion factor to map array location to polar stereographic x,y\n",
    "        x_conv = (x_max-x_min)/dhdt_masked.shape[1]\n",
    "        y_conv = (y_max-y_min)/dhdt_masked.shape[0]\n",
    "\n",
    "        # Only plot arrays that have data in them; some time slices have no data because no CryoSat-2 SARIn coverage\n",
    "        # Returns True if there is at least one non-NaN value in data_stacked, and False if all values are NaN\n",
    "        if np.any(~np.isnan(dhdt_masked)):\n",
    "            if plot:\n",
    "                # Create fig, ax\n",
    "                fig, ax = plt.subplots()\n",
    "\n",
    "                # Plot figure\n",
    "                img = ax.imshow(dhdt_masked, extent=[x_min, x_max, y_min, y_max], \n",
    "                    origin='lower', cmap='coolwarm_r', \n",
    "                    norm=divnorm)\n",
    "\n",
    "                # Plot the boundary of the evolving outline search extent\n",
    "                gpd.GeoSeries(buffered_poly).boundary.plot(ax=ax, color='red')\n",
    "\n",
    "            # Create empty lists to store contours \n",
    "            contours_pos = []\n",
    "            contours_neg = []\n",
    "\n",
    "            # Create contours at the positive and negative threshold \n",
    "            contour_pos = measure.find_contours(dhdt_masked.values, level)\n",
    "            # If at least one contour, add to list of contours\n",
    "            if len(contour_pos) > 0: \n",
    "                contours_pos += [contour_pos]\n",
    "            contour_neg = measure.find_contours(dhdt_masked.values, -level)\n",
    "            if len(contour_neg) > 0: \n",
    "                contours_neg += [contour_neg]\n",
    "\n",
    "            # Plot contours and make into polygons\n",
    "            for i in range(len(contours_pos)): \n",
    "                for j in range(len(contours_pos[i])):\n",
    "                    x = x_min+contours_pos[i][j][:,1]*x_conv\n",
    "                    y = y_min+contours_pos[i][j][:,0]*y_conv\n",
    "                    if plot:\n",
    "                        ax.plot(x, y, color='mediumblue', linestyle='dashdot', linewidth=1, label=level)\n",
    "\n",
    "                    # Make polygons from evolving outlines and store to list\n",
    "                    if len(contours_pos[i][j][:,1]) > 2: \n",
    "                        poly = Polygon(list(zip(x, y)))\n",
    "                        try:\n",
    "                            dhdt_poly = dhdt_masked.rio.clip([poly])   \n",
    "                            lon, lat = XY_TO_LL.transform(x,y)\n",
    "                            poly_area = abs(GEOD.polygon_area_perimeter(lon, lat)[0])\n",
    "                            if np.any(~np.isnan(dhdt_poly)):\n",
    "                                poly_dh = np.nanmean(dhdt_poly)\n",
    "                                poly_dvol = poly_dh*poly_area\n",
    "                                polys += [poly]\n",
    "                                areas += [poly_area]\n",
    "                                dhs += [poly_dh]\n",
    "                                dvols += [poly_dvol]\n",
    "                                midcyc_datetimes += [midcyc_dates[idx]]\n",
    "                        except NoDataInBounds:\n",
    "                            pass  # Skip to the next iteration\n",
    "                        except Exception as e:\n",
    "                            # Handle any other exceptions\n",
    "                            raise\n",
    "\n",
    "            for i in range(len(contours_neg)): \n",
    "                for j in range(len(contours_neg[i])):\n",
    "                    x = x_min+contours_neg[i][j][:,1]*x_conv\n",
    "                    y = y_min+contours_neg[i][j][:,0]*y_conv\n",
    "                    if plot:\n",
    "                        ax.plot(x, y, color='maroon', linestyle=(0, (3, 1, 1, 1)), linewidth=1, label=-level)\n",
    "\n",
    "                    if len(contours_neg[i][j][:,1]) > 2: \n",
    "                        poly = Polygon(list(zip(x, y)))\n",
    "                        try:\n",
    "                            dhdt_poly = dhdt_masked.rio.clip([poly]) \n",
    "                            lon, lat = XY_TO_LL.transform(x,y)\n",
    "                            poly_area = abs(GEOD.polygon_area_perimeter(lon, lat)[0])\n",
    "                            if np.any(~np.isnan(dhdt_poly)):\n",
    "                                poly_dh = np.nanmean(dhdt_poly)\n",
    "                                poly_dvol = poly_dh*poly_area\n",
    "                                polys += [poly]\n",
    "                                areas += [poly_area]\n",
    "                                dhs += [poly_dh]\n",
    "                                dvols += [poly_dvol]\n",
    "                                midcyc_datetimes += [midcyc_dates[idx]]\n",
    "                        except NoDataInBounds:\n",
    "                            pass  # Skip to the next iteration\n",
    "                        except Exception as e:\n",
    "                            # Handle any other exceptions\n",
    "                            raise\n",
    "\n",
    "            if plot:\n",
    "                # Overlay published active lake outlines for visual comparison and grounding line\n",
    "                S09_color = 'paleturquoise'\n",
    "                SF18_color  = 'turquoise'\n",
    "                lakes_gdf_postSF18_color = 'darkturquoise'\n",
    "                S09_outlines.boundary.plot(ax=ax, edgecolor=S09_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "                SF18_outlines.boundary.plot(ax=ax, edgecolor=SF18_color, linestyle=(0, (1, 2)), linewidth=2)\n",
    "                lakes_gdf_postSF18.boundary.plot(ax=ax, edgecolor=lakes_gdf_postSF18_color, linestyle=(0, (1, 1)), linewidth=2)          \n",
    "\n",
    "                # Change polar stereographic m to km\n",
    "                km_scale = 1e3\n",
    "                ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "                ax.xaxis.set_major_formatter(ticks_x)\n",
    "                ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "                ax.yaxis.set_major_formatter(ticks_y)  \n",
    "\n",
    "                # Label axes, set limits, and set title\n",
    "                ax.set_xlabel('x [km]', size=15)\n",
    "                ax.set_ylabel('y [km]', size=15) \n",
    "                ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "                # Plot inset map\n",
    "                axIns = ax.inset_axes([-0.01, 0.01, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "                axIns.set_aspect('equal')\n",
    "                moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "                moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "                axIns.axis('off')\n",
    "\n",
    "                # # Plot black rectangle to indicate location\n",
    "                # rect = Rectangle((x_min, y_min), (x_max-x_min), (y_max-y_min), fill=False, linewidth=2, color='k', zorder=3)\n",
    "                # axIns.add_artist(rect)\n",
    "\n",
    "                # Plot red star to indicate location\n",
    "                axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "                    linewidth=1, edgecolor='k', facecolor='r', s=100, zorder=3)\n",
    "\n",
    "                # Add colorbar \n",
    "                divider = make_axes_locatable(ax)\n",
    "                cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "                fig.colorbar(img, cax=cax).set_label('height change (dh) [m]', size=15)\n",
    "\n",
    "                # Add legend\n",
    "                ax.legend([S09_line, \n",
    "                           SF18_line, \n",
    "                           lakes_gdf_postSF18_line,\n",
    "                           uplift, \n",
    "                           subsidence,\n",
    "                           search_extent_line],\n",
    "                    ['static outline (S09)', \n",
    "                     'static outline (SF18)', \n",
    "                     'other',\n",
    "                     ('+ '+str(level)+' m uplift evolving outline'), \n",
    "                     ('â€“ '+str(level)+' m subsidence evolving outline'),\n",
    "                     'search extent'], \n",
    "                    loc='upper left')\n",
    "\n",
    "                # Set a title for the axes\n",
    "                ax.set_title('Evolving outlines and height change \\nfrom from {} to {}'.format(cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)))\n",
    "\n",
    "                # Save and close fig\n",
    "                plt.savefig(OUTPUT_DIR + \n",
    "                    '/find_evolving_outlines/{}/find_evolving_outlines_{}_{}x-search-extent_{}m-level_{}-{}.png'\n",
    "                    .format(lake_name, lake_name, area_multiple_search_extent, level, cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)), dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "\n",
    "    # Store optimal search extent and level information\n",
    "    area_multiple_search_extents = [area_multiple_search_extent for _ in range(len(polys))]\n",
    "    levels = [level for _ in range(len(polys))]\n",
    "\n",
    "    # Store polygons in geopandas geodataframe for further analysis\n",
    "    d = {'area_multiple_search_extent': area_multiple_search_extents,\n",
    "         'level': levels,\n",
    "         'geometry': polys, \n",
    "         'area (m^2)': areas, \n",
    "         'dh (m)': dhs, \n",
    "         'vol (m^3)': dvols,\n",
    "         'midcyc_datetime': midcyc_datetimes}\n",
    "    gdf = gpd.GeoDataFrame(d, crs=\"EPSG:3031\")\n",
    "\n",
    "    return gdf\n",
    "\n",
    "# # Example usage\n",
    "# outlines_gdf = find_evolving_outlines(lake_gdf=lake_gdf, area_multiple_search_extent=2, level=0.1, dataset1=dataset1, dataset2=dataset2, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff943efc-e148-4c68-b7d4-61a4542cffbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_intersecting_polygons(outlines_gdf, polygon):\n",
    "    \"\"\"\n",
    "    Extract polygons from a GeoDataFrame that intersect with a specified polygon,\n",
    "    as well as polygons that intersect with those intersected polygons.\n",
    "\n",
    "    Parameters:\n",
    "    outlines_gdf (geopandas.GeoDataFrame): A GeoDataFrame containing polygon geometries (here evolving outlines).\n",
    "    polygon (shapely.Polygon): A shapely polygon\n",
    "\n",
    "    Returns:\n",
    "    geopandas.GeoDataFrame: A GeoDataFrame containing the polygons that either\n",
    "                            intersect directly with the input polygon or intersect with\n",
    "                            polygons that intersect with the initial polygon.\n",
    "    \"\"\"\n",
    "    # Step 0: Ensure there are outlines in outines_gdf and deal with invalid polygon\n",
    "    if outlines_gdf.empty:\n",
    "        print('There are no evolving outlines in geodataframe.')\n",
    "        pass\n",
    "        \n",
    "    # Attempt to make polygon valid if it's invalid\n",
    "    if not polygon.is_valid:\n",
    "        polygon = polygon.buffer(0)\n",
    "        # raise ValueError(\"Input polygon is invalid and will be excluded.\")\n",
    "\n",
    "    # Exclude invalid geometries in the GeoDataFrame before processing\n",
    "    valid_gdf = outlines_gdf[outlines_gdf['geometry'].is_valid]\n",
    "    \n",
    "    # Step 1: Find polygons that intersect with the given polygon\n",
    "    directly_intersecting = valid_gdf[valid_gdf.intersects(polygon)]\n",
    "    \n",
    "    if directly_intersecting.empty:\n",
    "        # If no directly intersecting polygons, return an empty GeoDataFrame\n",
    "        return gpd.GeoDataFrame(columns=valid_gdf.columns, geometry='geometry')\n",
    "\n",
    "    # Step 2: Find polygons that intersect with the polygons found in Step 1\n",
    "    # We use 'unary_union' to merge all polygons found in Step 1 into a single geometry\n",
    "    # Then we select polygons from the original GeoDataFrame that intersect with this merged geometry\n",
    "    indirectly_intersecting = valid_gdf[valid_gdf.intersects(directly_intersecting.unary_union)]\n",
    "\n",
    "    # Step 3: Find polygons that intersect with the polygons found in Step 1\n",
    "    # We use 'unary_union' to merge all polygons found in Step 1 into a single geometry\n",
    "    # Then we select polygons from the original GeoDataFrame that intersect with this merged geometry\n",
    "    second_indirectly_intersecting = valid_gdf[valid_gdf.intersects(indirectly_intersecting.unary_union)]\n",
    "\n",
    "    # Combine both directly and indirectly intersecting polygons to ensure no duplicates\n",
    "    result_gdf = gpd.GeoDataFrame(pd.concat([directly_intersecting, indirectly_intersecting, second_indirectly_intersecting], ignore_index=True).drop_duplicates(subset='geometry'))\n",
    "\n",
    "    return result_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63031188-089e-4870-897e-d13483ecbf2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_intersecting_polygons(outlines_gdf, polygon):\n",
    "    \"\"\"\n",
    "    Extract polygons from a GeoDataFrame that intersect with a specified polygon,\n",
    "    as well as polygons that intersect with those intersected polygons.\n",
    "\n",
    "    Parameters:\n",
    "    outlines_gdf (geopandas.GeoDataFrame): A GeoDataFrame containing polygon geometries (here evolving outlines).\n",
    "    polygon (shapely.Polygon): A shapely polygon\n",
    "\n",
    "    Returns:\n",
    "    geopandas.GeoDataFrame: A GeoDataFrame containing the polygons that either\n",
    "                            intersect directly with the input polygon or intersect with\n",
    "                            polygons that intersect with the initial polygon.\n",
    "    \"\"\"\n",
    "    # Step 0: Ensure there are outlines in outines_gdf and deal with invalid polygon\n",
    "    if outlines_gdf.empty:\n",
    "        print('There are no evolving outlines in geodataframe.')\n",
    "        pass\n",
    "        \n",
    "    # Attempt to make polygon valid if it's invalid\n",
    "    if not polygon.is_valid:\n",
    "        polygon = polygon.buffer(0)\n",
    "        # raise ValueError(\"Input polygon is invalid and will be excluded.\")\n",
    "\n",
    "    # Exclude invalid geometries in the GeoDataFrame before processing\n",
    "    valid_gdf = outlines_gdf[outlines_gdf['geometry'].is_valid]\n",
    "    \n",
    "    # Step 1: Find polygons that intersect with the given polygon\n",
    "    directly_intersecting = valid_gdf[valid_gdf.intersects(polygon)]\n",
    "    \n",
    "    if directly_intersecting.empty:\n",
    "        # If no directly intersecting polygons, return an empty GeoDataFrame\n",
    "        return gpd.GeoDataFrame(columns=valid_gdf.columns, geometry='geometry')\n",
    "\n",
    "    # Step 2: Find polygons that intersect with the polygons found in Step 1\n",
    "    # We use 'unary_union' to merge all polygons found in Step 1 into a single geometry\n",
    "    # Then we select polygons from the original GeoDataFrame that intersect with this merged geometry\n",
    "    indirectly_intersecting = valid_gdf[valid_gdf.intersects(directly_intersecting.unary_union)]\n",
    "\n",
    "    # Step 3: Find polygons that intersect with the polygons found in Step 1\n",
    "    # We use 'unary_union' to merge all polygons found in Step 1 into a single geometry\n",
    "    # Then we select polygons from the original GeoDataFrame that intersect with this merged geometry\n",
    "    second_indirectly_intersecting = valid_gdf[valid_gdf.intersects(indirectly_intersecting.unary_union)]\n",
    "\n",
    "    # Combine both directly and indirectly intersecting polygons to ensure no duplicates\n",
    "    result_gdf = gpd.GeoDataFrame(pd.concat([directly_intersecting, indirectly_intersecting, second_indirectly_intersecting], ignore_index=True).drop_duplicates(subset='geometry'))\n",
    "\n",
    "    return result_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "289e6700-cfc7-49c9-9643-925794b574d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change to while loop to get all polygons\n",
    "def plot_evolving_outlines(lake_gdf, outlines_gdf):\n",
    "    '''\n",
    "    Func to plot evolving outlines in aggregate\n",
    "    '''\n",
    "    # Step 0: Ensure there are outlines in outines_gdf and deal with invalid polygon\n",
    "    if outlines_gdf.empty:\n",
    "        print('There are no evolving outlines in geodataframe.')\n",
    "        return  # Exit the function immediately if outlines_gdf is empty\n",
    "    \n",
    "    # Create fig, ax\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    \n",
    "    # Define lake name and polygon\n",
    "    lake_name = lake_gdf.name.values[0]\n",
    "    lake_poly = lake_gdf.iloc[0].geometry\n",
    "    \n",
    "    # Create buffered polygons for various multiples of lake area to find which\n",
    "    # best emcompasses the height change signals at previously identified lakes\n",
    "    search_extent_buffered_lake_poly = muliple_area_buffer(lake_poly, int(outlines_gdf['area_multiple_search_extent'][0]))\n",
    "    \n",
    "    # Plot buffered polygons\n",
    "    gpd.GeoDataFrame(geometry=[search_extent_buffered_lake_poly]).boundary.plot(ax=ax, edgecolor='red', facecolor='none', linewidth=1)\n",
    "\n",
    "    # Set colormap and normalize to date values\n",
    "    cmap = plt.get_cmap('plasma', len(midcyc_dates)-1)\n",
    "    norm = plt.Normalize(datetime64_to_fractional_year(midcyc_dates[0]), datetime64_to_fractional_year(midcyc_dates[-1]))\n",
    "    m = plt.cm.ScalarMappable(cmap=cmap)\n",
    "    m.set_array(np.array([datetime64_to_fractional_year(date) for date in midcyc_dates[0:-1]]))\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"bottom\", size=\"2.5%\", pad=0.5)\n",
    "    fig.colorbar(m, cax=cax, orientation='horizontal').set_label('evolving outline year', size=10)\n",
    "\n",
    "    # Overlay published active lake outlines for visual comparison and grounding line\n",
    "    S09_color = 'paleturquoise'\n",
    "    SF18_color  = 'turquoise'\n",
    "    lakes_gdf_postSF18_color = 'darkturquoise'\n",
    "    S09_outlines.boundary.plot(ax=ax, facecolor=S09_color, linestyle=(0, (1, 3)), linewidth=2, alpha=0.25)\n",
    "    S09_outlines.boundary.plot(ax=ax, edgecolor=S09_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "    SF18_outlines.boundary.plot(ax=ax, facecolor=SF18_color, linestyle=(0, (1, 2)), linewidth=2, alpha=0.25)\n",
    "    SF18_outlines.boundary.plot(ax=ax, edgecolor=SF18_color, linestyle=(0, (1, 2)), linewidth=2)\n",
    "    lakes_gdf_postSF18.boundary.plot(ax=ax, facecolor=lakes_gdf_postSF18_color, linestyle=(0, (1, 1)), linewidth=2, alpha=0.25)\n",
    "    lakes_gdf_postSF18.boundary.plot(ax=ax, edgecolor=lakes_gdf_postSF18_color, linestyle=(0, (1, 1)), linewidth=2)\n",
    "\n",
    "    # Use for loop to plot each outline in the geopandas dataframe and color by date\n",
    "    lines = []  # list of lines to be used for the legend\n",
    "    for idx, dt in enumerate(midcyc_dates):\n",
    "        x = 1; y = 1\n",
    "        line, = ax.plot(x, y, color=cmap(norm(datetime64_to_fractional_year(midcyc_dates[idx]))), linewidth=2)\n",
    "        lines.append(line)\n",
    "        # Filter GeoDataFrame for the current date\n",
    "        current_outlines = outlines_gdf[outlines_gdf['midcyc_datetime'] == dt]\n",
    "        # Check if the filtered GeoSeries is empty\n",
    "        if not current_outlines.empty:\n",
    "            current_outlines.boundary.plot(ax=ax, \n",
    "                color=cmap(norm(datetime64_to_fractional_year(midcyc_dates[idx]))), linewidth=1)\n",
    "\n",
    "    # Change polar stereographic m to km for cleaner-looking axes labels\n",
    "    km_scale = 1e3\n",
    "    ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax.xaxis.set_major_formatter(ticks_x)\n",
    "    ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax.yaxis.set_major_formatter(ticks_y)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Label axes and set limits\n",
    "    ax.set_xlabel('x [km]', size=10)\n",
    "    ax.set_ylabel('y [km]', size=10)\n",
    "    # Extract x_min, y_min, x_max, y_max from the total_bounds attribute\n",
    "    x_min, y_min, x_max, y_max = search_extent_buffered_lake_poly.bounds\n",
    "    x_buffer = abs(x_max-x_min)*0.2\n",
    "    y_buffer = abs(y_max-y_min)*0.2\n",
    "    ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "    # Create lines for legend\n",
    "    S09_color = 'paleturquoise'\n",
    "    SF18_color  = 'turquoise'\n",
    "    lakes_gdf_postSF18_color = 'darkturquoise'\n",
    "    S09_line = plt.Line2D((0, 1), (0, 0), color=S09_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "    SF18_line = plt.Line2D((0, 1), (0, 0), color=SF18_color, linestyle=(0, (1, 2)), linewidth=2)\n",
    "    lakes_gdf_postSF18_line = plt.Line2D((0, 1), (0, 0), color=lakes_gdf_postSF18_color, linestyle=(0, (1, 1)), linewidth=2)\n",
    "    search_extent = plt.Line2D((0, 1), (0, 0), color='red', linestyle='solid', linewidth=2)\n",
    "    \n",
    "    # Create legend\n",
    "    ax.legend(handles=[S09_line, \n",
    "        SF18_line,\n",
    "        lakes_gdf_postSF18_line,\n",
    "        tuple(lines),\n",
    "        search_extent], \n",
    "        labels=['static outline (Smith and others, 2009)',\n",
    "            'static outline (Siegfried & Fricker, 2018)', \n",
    "            'other',\n",
    "            'evolving outline (this study)',\n",
    "            'search extent'], \n",
    "              handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "             loc='upper left', bbox_to_anchor=(0, 1.35))\n",
    "\n",
    "    # Plot inset map to show location \n",
    "    axIns = ax.inset_axes([0.02, 0.01, 0.25, 0.25]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "    axIns.set_aspect('equal')\n",
    "    moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', linewidth=1, edgecolor='k', facecolor='r', s=50, zorder=3)\n",
    "    # rect = Rectangle((x_min, y_min), (x_max-x_min), (y_max-y_min), fill=False, linewidth=2, color='k', zorder=3)\n",
    "    # axIns.add_artist(rect) \n",
    "    axIns.axis('off')\n",
    "\n",
    "    plt.savefig(OUTPUT_DIR + '/plot_evolving_outlines/plot_evolving_outlines_{}_{}x-search-extent_{}m-level.png'\n",
    "        .format(lake_name, outlines_gdf['area_multiple_search_extent'][0], outlines_gdf['level'][0]),\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09a64b8c-09ac-424f-a6ed-225e5ff60791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def remove_outlier_polygon(gdf, axis='x', max_or_min='max'):\n",
    "#     '''\n",
    "#     Func to remove outline outliers by removing a polygon\n",
    "#     with the most extreme x or y coordinates\n",
    "    \n",
    "#     Inputs\n",
    "#     * geopandas geodataframe\n",
    "#     * axis indicates whether the x or y axis will \n",
    "#     have its extreme polygon removed\n",
    "#     * max_or_min indicates whether the max or min centroid axis\n",
    "#     value will be removed\n",
    "    \n",
    "#     Ouputs\n",
    "#     * geopandas geodataframe with one outline removed\n",
    "#     '''\n",
    "    \n",
    "#     # Check if the GeoDataFrame is empty\n",
    "#     if gdf.empty:\n",
    "#         print(\"GeoDataFrame is empty. Nothing to remove.\")\n",
    "#         return gdf\n",
    "    \n",
    "#     # Choose the axis and max or min for extreme value\n",
    "#     if axis not in ['x', 'y']:\n",
    "#         raise ValueError(\"Invalid axis. Use 'x' or 'y'.\")\n",
    "#     if max_or_min not in ['max', 'min']:\n",
    "#         raise ValueError(\"Invalid max_or_min. Use 'max' or 'min'.\")\n",
    "\n",
    "#     # Calculate the centroid and extreme value\n",
    "#     if axis == 'x':\n",
    "#         if max_or_min == 'max':\n",
    "#             idx_to_remove = gdf.geometry.bounds['maxx'].idxmax()\n",
    "#         else:\n",
    "#             idx_to_remove = gdf.geometry.bounds['minx'].idxmin()\n",
    "#     else:\n",
    "#         if max_or_min == 'max':\n",
    "#             idx_to_remove = gdf.geometry.bounds['maxy'].idxmax()\n",
    "#         else:\n",
    "#             idx_to_remove = gdf.geometry.bounds['miny'].idxmin()\n",
    "\n",
    "#     # Remove the polygon with the specified polygon index to remove\n",
    "#     gdf_filtered = gdf.drop(idx_to_remove)\n",
    "\n",
    "#     print(f\"Removed polygon with extreme {max_or_min} {axis}-value at index {idx_to_remove}.\")\n",
    "\n",
    "#     return gdf_filtered\n",
    "\n",
    "# # Example usage:\n",
    "# # Assuming 'gdf' is your GeoDataFrame\n",
    "# # new_gdf = remove_extreme_polygon(gdf, axis='x', positive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e38cbfea-ab55-48ad-83fe-719a5885ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_evolving_and_static_outlines(lake_gdf, outlines_gdf, dataset1, dataset2): \n",
    "    '''\n",
    "    Create geodataframe of dArea, dHeight, dVol calculations for static compared evolving outlines\n",
    "    '''\n",
    "    # Step 0: Ensure there are outlines in outines_gdf and deal with invalid polygon\n",
    "    if outlines_gdf.empty:\n",
    "        print('There are no evolving outlines in geodataframe.')\n",
    "        return  # Exit the function immediately if outlines_gdf is empty\n",
    "    \n",
    "    # Define lake name and polygon and buffered polygon to use as off-lake region\n",
    "    lake_name = lake_gdf.name.values[0]\n",
    "    static_outline = lake_gdf.iloc[0].geometry\n",
    "    static_outline_buffered = muliple_area_buffer(static_outline, 2)\n",
    "    evolving_outlines_unary_union = outlines_gdf.unary_union\n",
    "    evolving_outlines_unary_union_buffered = muliple_area_buffer(evolving_outlines_unary_union, 2)\n",
    "\n",
    "    # Combine static outline with evolving outlines from outlines_gdf\n",
    "    combined_outline = unary_union([static_outline] + list(outlines_gdf.geometry))\n",
    "\n",
    "    # Establish x_min, x_max, y_min, y_max from the combined outline\n",
    "    x_min, y_min, x_max, y_max = combined_outline.bounds\n",
    "    del combined_outline\n",
    "\n",
    "    # Clipping datasets\n",
    "    if dataset1 != 'none':\n",
    "        # Create and apply the mask to dataset variable\n",
    "        dataset1_combined_outline_sub = dataset1.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "        dataset1_static_outline_mask = np.array([[static_outline.contains(Point(x, y)) for x in dataset1_combined_outline_sub['x'].values] for y in dataset1_combined_outline_sub['y'].values])\n",
    "        dataset1_static_outline_mask_da = xr.DataArray(dataset1_static_outline_mask, coords=[dataset1_combined_outline_sub.y, dataset1_combined_outline_sub.x], dims=[\"y\", \"x\"])\n",
    "        dataset1_static_outline_masked_data = dataset1['delta_h'].where(dataset1_static_outline_mask_da, drop=True)\n",
    "\n",
    "        dataset1_static_outline_buffered_mask = np.array([[static_outline_buffered.contains(Point(x, y)) for x in dataset1_combined_outline_sub['x'].values] for y in dataset1_combined_outline_sub['y'].values])\n",
    "        dataset1_static_region_mask = dataset1_static_outline_buffered_mask & ~dataset1_static_outline_mask\n",
    "        dataset1_static_region_mask_da = xr.DataArray(dataset1_static_region_mask, coords=[dataset1_combined_outline_sub.y, dataset1_combined_outline_sub.x], dims=[\"y\", \"x\"])\n",
    "        dataset1_static_region_masked_data = dataset1['delta_h'].where(dataset1_static_region_mask_da, drop=True)\n",
    "\n",
    "        dataset1_evolving_outlines_unary_union_buffered_mask = np.array([[evolving_outlines_unary_union_buffered.contains(Point(x, y)) for x in dataset1_combined_outline_sub['x'].values] for y in dataset1_combined_outline_sub['y'].values])\n",
    "        dataset1_evolving_outlines_unary_union_mask = np.array([[evolving_outlines_unary_union.contains(Point(x, y)) for x in dataset1_combined_outline_sub['x'].values] for y in dataset1_combined_outline_sub['y'].values])\n",
    "        dataset1_evolving_outlines_region_mask = dataset1_evolving_outlines_unary_union_buffered_mask & ~dataset1_evolving_outlines_unary_union_mask\n",
    "        dataset1_evolving_outlines_region_mask_da = xr.DataArray(dataset1_evolving_outlines_region_mask, coords=[dataset1_combined_outline_sub.y, dataset1_combined_outline_sub.x], dims=[\"y\", \"x\"])\n",
    "        dataset1_evolving_outlines_region_masked_data = dataset1_combined_outline_sub['delta_h'].where(dataset1_evolving_outlines_region_mask_da, drop=True)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Do same for dataset2\n",
    "    dataset2_combined_outline_sub = dataset2.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "    dataset2_static_outline_mask = np.array([[static_outline.contains(Point(x, y)) for x in dataset2_combined_outline_sub['x'].values] for y in dataset2_combined_outline_sub['y'].values])\n",
    "    dataset2_static_outline_mask_da = xr.DataArray(dataset2_static_outline_mask, coords=[dataset2_combined_outline_sub.y, dataset2_combined_outline_sub.x], dims=[\"y\", \"x\"])\n",
    "    dataset2_static_outline_masked_data = dataset2['delta_h'].where(dataset2_static_outline_mask_da, drop=True)\n",
    "\n",
    "    dataset2_static_outline_buffered_mask = np.array([[static_outline_buffered.contains(Point(x, y)) for x in dataset2_combined_outline_sub['x'].values] for y in dataset2_combined_outline_sub['y'].values])\n",
    "    dataset2_static_region_mask = dataset2_static_outline_buffered_mask & ~dataset2_static_outline_mask\n",
    "    dataset2_static_region_mask_da = xr.DataArray(dataset2_static_region_mask, coords=[dataset2_combined_outline_sub.y, dataset2_combined_outline_sub.x], dims=[\"y\", \"x\"])\n",
    "    dataset2_static_region_masked_data = dataset2['delta_h'].where(dataset2_static_region_mask_da, drop=True)\n",
    "\n",
    "    dataset2_evolving_outlines_unary_union_buffered_mask = np.array([[evolving_outlines_unary_union_buffered.contains(Point(x, y)) for x in dataset2_combined_outline_sub['x'].values] for y in dataset2_combined_outline_sub['y'].values])\n",
    "    dataset2_evolving_outlines_unary_union_mask = np.array([[evolving_outlines_unary_union.contains(Point(x, y)) for x in dataset2_combined_outline_sub['x'].values] for y in dataset2_combined_outline_sub['y'].values])\n",
    "    dataset2_evolving_outlines_region_mask = dataset2_evolving_outlines_unary_union_buffered_mask & ~dataset2_evolving_outlines_unary_union_mask\n",
    "    dataset2_evolving_outlines_region_mask_da = xr.DataArray(dataset2_evolving_outlines_region_mask, coords=[dataset2_combined_outline_sub.y, dataset2_combined_outline_sub.x], dims=[\"y\", \"x\"])\n",
    "    dataset2_evolving_outlines_region_masked_data = dataset2_combined_outline_sub['delta_h'].where(dataset2_evolving_outlines_region_mask_da, drop=True)\n",
    "\n",
    "    # Create empty list to store polygons, areas, perimeters and dates\n",
    "    static_outline_dhs = []\n",
    "    static_outline_region_dhs = []\n",
    "    static_outline_dhs_corr = []\n",
    "    static_outline_dvols_corr = []\n",
    "    evolving_outlines_dareas = []\n",
    "    evolving_outlines_dhs = []\n",
    "    evolving_outlines_region_dhs = []\n",
    "    evolving_outlines_dhs_corr = []\n",
    "    evolving_outlines_dvols_corr = []\n",
    "    midcyc_datetimes = []\n",
    "\n",
    "    # Calculate cycle-to-cycle dHeight at each cycle of the spliced data sets\n",
    "    for idx, dt in enumerate(cyc_dates['midcyc_dates']):\n",
    "        # For midcyc_dates indexes <= 32, use CryoSat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the CryoSat-2 era before ICESat-2 launch (2010-08-17 to 2018-08-17)\n",
    "        # Skip indices based on CS2_SARIn_time_period\n",
    "        if lake_gdf['CS2_SARIn_time_period'].iloc[0] == '2013.75-2018.75' and 20 <= idx <= 32:\n",
    "            continue  # Skip the rest of the loop for indexes 21-32 because this dataset has reduced number of time slices\n",
    "        \n",
    "        \n",
    "        print('Working on', cyc_dates['midcyc_dates'][idx])\n",
    "        if idx <= 32:\n",
    "            if dataset1 != 'none':\n",
    "                static_outline_dh = dataset1_static_outline_masked_data[idx+1,:,:]-dataset1_static_outline_masked_data[idx,:,:]\n",
    "                static_outline_region_dh = dataset1_static_region_masked_data[idx+1,:,:]-dataset1_static_region_masked_data[idx,:,:]\n",
    "    \n",
    "                # Filter rows that match the current time slice\n",
    "                outlines_gdf_dt_sub = outlines_gdf[outlines_gdf['midcyc_datetime'] == dt]\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Check if outlines_gdf_dt_sub has any rows before proceeding\n",
    "            if not outlines_gdf_dt_sub.empty:\n",
    "                outlines_gdf_dt_sub_unary_union = outlines_gdf_dt_sub.unary_union\n",
    "                dataset1_evolving_outlines_dt_sub_mask = np.array([[outlines_gdf_dt_sub_unary_union.contains(Point(x, y)) for x in dataset1_combined_outline_sub['x'].values] for y in dataset1_combined_outline_sub['y'].values])\n",
    "                dataset1_evolving_outlines_dt_sub_mask_da = xr.DataArray(dataset1_evolving_outlines_dt_sub_mask, coords=[dataset1_combined_outline_sub.y, dataset1_combined_outline_sub.x], dims=[\"y\", \"x\"])\n",
    "                dataset1_evolving_outlines_dt_sub_masked_data = dataset1_combined_outline_sub['delta_h'].where(dataset1_evolving_outlines_dt_sub_mask_da, drop=True)\n",
    "                evolving_outlines_dh = dataset1_evolving_outlines_dt_sub_masked_data[idx+1,:,:]-dataset1_evolving_outlines_dt_sub_masked_data[idx,:,:]\n",
    "                evolving_outlines_region_dh = dataset1_evolving_outlines_region_masked_data[idx+1,:,:]-dataset1_evolving_outlines_region_masked_data[idx,:,:]\n",
    "            elif outlines_gdf_dt_sub.empty:\n",
    "                evolving_outlines_dh = 0\n",
    "                evolving_outlines_region_dh = 0\n",
    "\n",
    "        # For midcyc_dates indexes > 32, use ICESat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the ICESat-2 ATL06 v003 (2018-11-16 to 2023-04-02)\n",
    "        elif idx > 32:\n",
    "            # Subtract 33 from idx to start over at index zero with new dataset\n",
    "            static_outline_dh = dataset2_static_outline_masked_data[(idx-33)+1,:,:]-dataset2_static_outline_masked_data[(idx-33),:,:]\n",
    "            static_outline_region_dh = dataset2_static_region_masked_data[(idx-33)+1,:,:]-dataset2_static_region_masked_data[(idx-33),:,:]\n",
    "\n",
    "            # Filter rows that match the current time slice\n",
    "            outlines_gdf_dt_sub = outlines_gdf[outlines_gdf['midcyc_datetime'] == dt]\n",
    "\n",
    "            if not outlines_gdf_dt_sub.empty:\n",
    "                outlines_gdf_dt_sub_unary_union = outlines_gdf_dt_sub.unary_union\n",
    "                dataset2_evolving_outlines_dt_sub_mask = np.array([[outlines_gdf_dt_sub_unary_union.contains(Point(x, y)) for x in dataset2_combined_outline_sub['x'].values] for y in dataset2_combined_outline_sub['y'].values])\n",
    "                dataset2_evolving_outlines_dt_sub_mask_da = xr.DataArray(dataset2_evolving_outlines_dt_sub_mask, coords=[dataset2_combined_outline_sub.y, dataset2_combined_outline_sub.x], dims=[\"y\", \"x\"])\n",
    "                dataset2_evolving_outlines_dt_sub_masked_data = dataset2_combined_outline_sub['delta_h'].where(dataset2_evolving_outlines_dt_sub_mask_da, drop=True)\n",
    "                evolving_outlines_dh = dataset2_evolving_outlines_dt_sub_masked_data[(idx-33)+1,:,:]-dataset2_evolving_outlines_dt_sub_masked_data[(idx-33),:,:]\n",
    "                evolving_outlines_region_dh = dataset2_evolving_outlines_region_masked_data[(idx-33)+1,:,:]-dataset2_evolving_outlines_region_masked_data[(idx-33),:,:]\n",
    "            elif outlines_gdf_dt_sub.empty:\n",
    "                evolving_outlines_dh = 0\n",
    "                evolving_outlines_region_dh = 0\n",
    "\n",
    "        # Store data into lists\n",
    "        static_outline_dh_mean = np.nanmean(static_outline_dh)\n",
    "        static_outline_region_dh_mean = np.nanmean(static_outline_region_dh)\n",
    "        static_outline_dh_corr = static_outline_dh_mean - static_outline_region_dh_mean\n",
    "        static_outline_dvol_corr = static_outline_dh_corr*lake_gdf['area (m^2)'].values[0]\n",
    "\n",
    "        static_outline_dhs += [static_outline_dh_mean]\n",
    "        static_outline_region_dhs += [static_outline_region_dh_mean]\n",
    "        static_outline_dhs_corr += [static_outline_dh_corr]\n",
    "        static_outline_dvols_corr += [static_outline_dvol_corr]\n",
    "\n",
    "        outlines_gdf_dt_sub = outlines_gdf_dt_sub.to_crs('EPSG:4326')\n",
    "        evolving_outlines_darea = sum(outlines_gdf_dt_sub['geometry'].apply(    \n",
    "            lambda poly: abs(geod.polygon_area_perimeter(\n",
    "            poly.exterior.coords.xy[0], poly.exterior.coords.xy[1])[0]) if poly is not None and poly.is_valid else None))\n",
    "\n",
    "        evolving_outlines_dh_mean = np.nanmean(evolving_outlines_dh)\n",
    "        evolving_outlines_region_dh_mean = np.nanmean(evolving_outlines_region_dh)\n",
    "        evolving_outlines_dh_corr = evolving_outlines_dh_mean - evolving_outlines_region_dh_mean\n",
    "        evolving_outlines_dvol_corr = evolving_outlines_dh_corr*evolving_outlines_darea\n",
    "\n",
    "        evolving_outlines_dareas += [evolving_outlines_darea]\n",
    "        evolving_outlines_dhs += [evolving_outlines_dh_mean]\n",
    "        evolving_outlines_region_dhs += [evolving_outlines_region_dh_mean]\n",
    "        evolving_outlines_dhs_corr += [evolving_outlines_dh_corr]\n",
    "        evolving_outlines_dvols_corr += [evolving_outlines_dvol_corr]\n",
    "\n",
    "        midcyc_datetimes += [midcyc_dates[idx]]\n",
    "\n",
    "        # Clear the output of each index\n",
    "        # clear_output(wait=True)\n",
    "\n",
    "    # Store static outline area\n",
    "    static_outline_areas = [lake_gdf['area (m^2)'].values[0] for _ in range(len(midcyc_datetimes))]\n",
    "\n",
    "    # Store polygons in dataframe for further analysis\n",
    "    d = {'midcyc_datetime': midcyc_datetimes,\n",
    "        'evolving_outlines_darea (m^2)': evolving_outlines_dareas,\n",
    "        'evolving_outlines_dh (m)': evolving_outlines_dhs,\n",
    "        'evolving_region_dh (m)': evolving_outlines_region_dhs,\n",
    "        'evolving_outlines_dh_corr (m)': evolving_outlines_dhs_corr,\n",
    "        'evolving_outlines_dvol_corr (m^3)': evolving_outlines_dvols_corr,\n",
    "        'static_outline_area': static_outline_areas,\n",
    "        'static_outline_dh (m)': static_outline_dhs, \n",
    "        'static_outline_region_dh (m)': static_outline_region_dhs,\n",
    "        'static_outline_dh_corr (m)': static_outline_dhs_corr,\n",
    "        'static_outline_dvol_corr (m^3)': static_outline_dvols_corr,\n",
    "        'bias_area (m^2)': [evolving - static for evolving, static in zip(evolving_outlines_dareas, static_outline_areas)],\n",
    "        'bias_region_dh (m)': [evolving - static for evolving, static in zip(evolving_outlines_region_dhs, static_outline_region_dhs)],\n",
    "        'bias_outlines_dh_corr (m)': [evolving - static for evolving, static in zip(evolving_outlines_dhs_corr, static_outline_dhs_corr)],\n",
    "        'bias_dvol_corr (m^3)': [evolving - static for evolving, static in zip(evolving_outlines_dvols_corr, static_outline_dvols_corr)]}\n",
    "    df = pd.DataFrame(d)\n",
    "\n",
    "    # Export dataframe to csv\n",
    "    df.to_csv('output/lake_outlines/compare_evolving_and_static_outlines/{}.csv'.format(lake_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1907dab-3325-4905-9f4c-3f25c68ae927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_to_fractional_year(timestamp):\n",
    "    # Check if the year is a leap year\n",
    "    year = timestamp.year\n",
    "    if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\n",
    "        days_in_year = 366\n",
    "    else:\n",
    "        days_in_year = 365\n",
    "\n",
    "    # Calculate the day of the year\n",
    "    day_of_year = timestamp.timetuple().tm_yday\n",
    "\n",
    "    # Calculate the fractional year\n",
    "    fractional_year = year + (day_of_year - 1) / days_in_year\n",
    "    \n",
    "    return fractional_year\n",
    "\n",
    "# # Example usage\n",
    "# timestamp = pd.Timestamp('2023-03-07 12:34:56')\n",
    "# fractional_year = timestamp_to_fractional_year(timestamp)\n",
    "# print(f\"Fractional Year: {fractional_year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e16a960-6411-4725-b1bb-ff86f4ced3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evolving_and_static_comparison(lake_name):\n",
    "    '''\n",
    "    Plot static and evolving outlines with dArea, dHeight, and dVolume\n",
    "    '''\n",
    "    print('working on {}'.format(lake_name))\n",
    "    fig, ax = plt.subplots(1,4, figsize=(12,5))\n",
    "    \n",
    "    # Define colors and linestyles that will be reused and create lines for legend\n",
    "    # S09_color = 'paleturquoise'\n",
    "    SF18_color  = 'turquoise'\n",
    "    # lake_locations_postSF18_color = 'darkturquoise'\n",
    "    # S09_linestyle=(0, (1, 2))\n",
    "    SF18_linestyle=(0, (1, 1))\n",
    "    # S09_line = plt.Line2D((0, 1), (0, 0), color=S09_color, linestyle=S09_linestyle, linewidth=2)\n",
    "    SF18_line = plt.Line2D((0, 1), (0, 0), color=SF18_color, linestyle='solid', linewidth=1)\n",
    "    \n",
    "    # Panel A - evolving outlines ------------------------------------------------------\n",
    "    # Plot static and evolving outlines onto MOA surface imagery\n",
    "    # Open static outline(s) and evolving outlines and geometric calculation comparison geodataframes for plotting\n",
    "    # S09_lake_gdf = S09_outlines[S09_outlines['Name'] == 'Whillans_4']\n",
    "    lake_gdf = lakes_gdf[lakes_gdf['name'] == lake_name]\n",
    "    # Attempt to open the evolving outlines GeoJSON file\n",
    "    try:\n",
    "        evolving_outlines_gdf = gpd.read_file(os.path.join(\n",
    "            os.getcwd(), 'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)))\n",
    "    except fiona.errors.DriverError:\n",
    "        print(f\"File for {lake_name} not found. Skipping...\")\n",
    "        return  # Skip the rest of the function if the file doesn't exist\n",
    "    \n",
    "    # Attempt to open the geometric calculations CSV file\n",
    "    try:\n",
    "        geom_calcs_df = pd.read_csv(os.path.join(\n",
    "            os.getcwd(), 'output/lake_outlines/compare_evolving_and_static_outlines/{}.csv'.format(lake_name)))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"CSV file for {lake_name} not found. Skipping...\")\n",
    "        return  # Skip the rest of the function if the file doesn't exist\n",
    "    \n",
    "    # Convert of strings to datetime\n",
    "    geom_calcs_df['midcyc_datetime'] = pd.to_datetime(geom_calcs_df['midcyc_datetime'])\n",
    "    \n",
    "    # Combine static outline(s) with evolving outlines in unary union to plot all within bounds of plot\n",
    "    # Use .buffer(0) to fix any invalid geometries\n",
    "    # all_outlines_unary_union = unary_union([S09_lake_gdf.geometry.iloc[0], SF18_lake_gdf.geometry.iloc[0]] + list(evolving_outlines_gdf.geometry))\n",
    "    all_outlines_unary_union = unary_union([lake_gdf.geometry.iloc[0].buffer(0)] + list(evolving_outlines_gdf.geometry.buffer(0)))\n",
    "    x_min, y_min, x_max, y_max = all_outlines_unary_union.bounds\n",
    "    buffer_frac = 0.2\n",
    "    x_buffer = abs(x_max-x_min)*buffer_frac\n",
    "    y_buffer = abs(y_max-y_min)*buffer_frac\n",
    "    mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "    mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "    moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "    ax[0].imshow(moa_highres_da_subset[0,:,:], cmap=\"gray\", clim=[14000, 17000], extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "    \n",
    "    # Pick colormap and make continuous cmap discrete for evolving outlines\n",
    "    colormap = 'plasma'\n",
    "    continuous_cmap = matplotlib.colormaps[colormap]\n",
    "    discrete_cmap = colors.ListedColormap(continuous_cmap(np.linspace(0, 1, len(cyc_dates['midcyc_dates'])-1)))\n",
    "    \n",
    "    # Norm to time variable\n",
    "    norm = plt.Normalize(mdates.date2num(cyc_dates['midcyc_dates'].iloc[0]), \n",
    "                         mdates.date2num(cyc_dates['midcyc_dates'].iloc[-1]))\n",
    "    \n",
    "    # Use for loop to store each time slice as line segment to use in legend\n",
    "    # And plot each evolving outline in the geodataframe color by date\n",
    "    lines = []  # list of lines to be used for the legend\n",
    "    for idx, dt in enumerate(cyc_dates['midcyc_dates']):\n",
    "        x = 1; y = 1\n",
    "        line, = ax[0].plot(x, y, color=discrete_cmap(norm(mdates.date2num(cyc_dates['midcyc_dates'][idx]))), linewidth=1)\n",
    "        lines.append(line)\n",
    "        \n",
    "        # Filter rows that match the current time slice\n",
    "        evolving_outlines_gdf_dt_sub = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "    \n",
    "        # Plotting the subset if not empty\n",
    "        if not evolving_outlines_gdf_dt_sub.empty:\n",
    "            evolving_outlines_gdf_dt_sub.boundary.plot(ax=ax[0], color=discrete_cmap(norm(mdates.date2num(cyc_dates['midcyc_dates'][idx]))), linewidth=1)\n",
    "    \n",
    "    # Plot static outline\n",
    "    lake_gdf.boundary.plot(ax=ax[0], color=SF18_color, linewidth=1)\n",
    "    \n",
    "    # Plot inset map\n",
    "    axIns = ax[0].inset_axes([0.8, 0.8, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "    axIns.set_aspect('equal')\n",
    "    moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    axIns.axis('off')\n",
    "    \n",
    "    # Plot red star to indicate location\n",
    "    axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "        linewidth=1, edgecolor='k', facecolor='r', s=50, zorder=3)\n",
    "    \n",
    "    # Plot legend\n",
    "    legend = ax[0].legend([tuple(lines), SF18_line], ['evolving outlines', 'static outline'],\n",
    "        handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "        loc='upper left')\n",
    "    # legend.get_frame().set_linewidth(0.0)\n",
    "    ax[0].patch.set_alpha(1)\n",
    "    \n",
    "    # Create colorbar \n",
    "    m = plt.cm.ScalarMappable(cmap=discrete_cmap)\n",
    "    m.set_array(np.array([timestamp_to_fractional_year(date) for date in cyc_dates['midcyc_dates']]))\n",
    "    cax = inset_axes(ax[0],\n",
    "                     width=\"100%\",\n",
    "                     height=\"3%\",\n",
    "                     loc=3,\n",
    "                     bbox_to_anchor=[0,-0.2,1,1],\n",
    "                     bbox_transform=ax[0].transAxes,\n",
    "                     borderpad=0,\n",
    "                     )\n",
    "    cbar=fig.colorbar(m, ticks=np.array([2010,2012,2014,2016,2018,2020,2022]), \n",
    "                 cax=cax, orientation='horizontal')#.set_label('evolving outline year', size=15)\n",
    "    \n",
    "    # Set the label for the colorbar and adjust its size\n",
    "    cbar.set_label('evolving outline year', size=10, labelpad=5)\n",
    "    \n",
    "    # Change polar stereographic m to km\n",
    "    km_scale = 1e3\n",
    "    ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax[0].xaxis.set_major_formatter(ticks_x)\n",
    "    ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax[0].yaxis.set_major_formatter(ticks_y)\n",
    "    \n",
    "    # Set axes limit, title, and axis label\n",
    "    ax[0].set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "    ax[0].set_title('evolving and static\\noutline comparison', size=12, pad=8)\n",
    "    ax[0].set_xlabel('X [km]')\n",
    "    ax[0].set_ylabel('Y [km]')\n",
    "    \n",
    "    # Panel - da/dt ---------------------------------------------\n",
    "    # Plot multi-colored line and scatter for data points\n",
    "    x=mdates.date2num(geom_calcs_df['midcyc_datetime'])\n",
    "    y=np.cumsum(np.divide(geom_calcs_df['evolving_outlines_darea (m^2)'], 1e6))\n",
    "    \n",
    "    # Create points and segments for LineCollection\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    \n",
    "    # Create a LineCollection, using the discrete colormap and norm\n",
    "    lc = LineCollection(segments, cmap=discrete_cmap, norm=norm, linestyle=(0, (2, 1)))\n",
    "    \n",
    "    # Set the values used for colormapping, using matplotlib dates for colors\n",
    "    lc.set_array(x)\n",
    "    lc.set_linewidth(2)\n",
    "    line = ax[1].add_collection(lc)\n",
    "    \n",
    "    # Scatter plot, using the discrete colormap and norm for coloring\n",
    "    # Convert x values (fractional years) back to matplotlib dates for consistent coloring\n",
    "    scatter = ax[1].scatter(x, y, c=x, cmap=discrete_cmap, norm=norm)\n",
    "    \n",
    "    # Plot static outline area\n",
    "    ax[1].axhline(np.divide(lake_gdf['area (m^2)'], 1e6).values, color=SF18_color, linestyle='solid', linewidth=1)\n",
    "    \n",
    "    # Plot bias\n",
    "    ax[1].plot(mdates.date2num(geom_calcs_df['midcyc_datetime']),\n",
    "        np.divide(geom_calcs_df['bias_area (m^2)'], 1e6), color='red', linestyle='solid', linewidth=1)\n",
    "    \n",
    "    # Add legend\n",
    "    bias = plt.Line2D((0, 1), (0, 0), color='red', linestyle='solid', linewidth=1)\n",
    "    legend = ax[1].legend([tuple(lines), SF18_line, bias],\n",
    "        ['evolving outlines', 'static outline', 'bias (evolving - static)'], \n",
    "        handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "        loc='upper left')\n",
    "\n",
    "    # Format the x-axis to display years only\n",
    "    ax[1].xaxis.set_major_locator(mdates.YearLocator())  # Tick every year\n",
    "    ax[1].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Display ticks as years\n",
    "    \n",
    "    # # Set x-axis limits\n",
    "    # start_date = geom_calcs_df['midcyc_datetime'].min()\n",
    "    # end_date = geom_calcs_df['midcyc_datetime'].max()\n",
    "    # ax[1].set_xlim(mdates.date2num(start_date), mdates.date2num(end_date))\n",
    "    \n",
    "    # Set title and axis label\n",
    "    ax[1].set_title('wetted area [km$^2$]', size=12, pad=8)\n",
    "    ax[1].set_xlabel('year')\n",
    "    \n",
    "    # Panel C - dh/dt -------------------------------------------------------\n",
    "    # Plot horizontal zero line for reference\n",
    "    ax[2].axhline(0, color='k', linestyle='solid', linewidth=1)\n",
    "    \n",
    "    # Plot multi-colored line and scatter for data points\n",
    "    x=mdates.date2num(geom_calcs_df['midcyc_datetime'])\n",
    "    y=np.cumsum(geom_calcs_df['evolving_outlines_dh_corr (m)'])\n",
    "    \n",
    "    # Create points and segments for LineCollection\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    \n",
    "    # Create a LineCollection, using the discrete colormap and norm\n",
    "    lc = LineCollection(segments, cmap=discrete_cmap, norm=norm, linestyle=(0, (2, 1)))\n",
    "    \n",
    "    # Set the values used for colormapping, using matplotlib dates for colors\n",
    "    lc.set_array(x)\n",
    "    lc.set_linewidth(2)\n",
    "    line = ax[2].add_collection(lc)\n",
    "    \n",
    "    # Scatter plot, using the discrete colormap and norm for coloring\n",
    "    # Convert x values (fractional years) back to matplotlib dates for consistent coloring\n",
    "    scatter = ax[2].scatter(x, y, c=x, cmap=discrete_cmap, norm=norm)\n",
    "    \n",
    "    # Plot static outline time series\n",
    "    ax[2].plot(mdates.date2num(geom_calcs_df['midcyc_datetime']), \n",
    "        np.cumsum(geom_calcs_df['static_outline_dh_corr (m)']), color=SF18_color, linestyle='solid', linewidth=1)\n",
    "    \n",
    "    # Plot bias\n",
    "    ax[2].plot(mdates.date2num(geom_calcs_df['midcyc_datetime']),\n",
    "        np.cumsum(geom_calcs_df['bias_outlines_dh_corr (m)']), color='red', linestyle='solid', linewidth=1)\n",
    "    \n",
    "    # Format the x-axis to display years only\n",
    "    ax[2].xaxis.set_major_locator(mdates.YearLocator())  # Tick every year\n",
    "    ax[2].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Display ticks as years\n",
    "    \n",
    "    # # Set x-axis limits\n",
    "    # start_date = geom_calcs_df['midcyc_datetime'].min()\n",
    "    # end_date = geom_calcs_df['midcyc_datetime'].max()\n",
    "    # ax[2].set_xlim(mdates.date2num(start_date), mdates.date2num(end_date))\n",
    "    \n",
    "    # Set title and axis label\n",
    "    ax[2].set_title('cumulative\\nheight change [m]', size=12, pad=8)\n",
    "    ax[2].set_xlabel('year')\n",
    "    \n",
    "    # Panel D - dv/dt --------------------------------------------------\n",
    "    # Plot horizontal line at zero for reference\n",
    "    ax[3].axhline(0, color='k', linestyle='solid', linewidth=1)\n",
    "    \n",
    "    # Plot static outline time series\n",
    "    ax[3].plot(mdates.date2num(geom_calcs_df['midcyc_datetime']), \n",
    "        np.divide(np.cumsum(geom_calcs_df['static_outline_dvol_corr (m^3)']), 1e9), \n",
    "        color=SF18_color, linestyle='solid', linewidth=1)\n",
    "    \n",
    "    # Plot multi-colored line and scatter for data points\n",
    "    x=mdates.date2num(geom_calcs_df['midcyc_datetime'])\n",
    "    y=np.cumsum(np.divide(geom_calcs_df['evolving_outlines_dvol_corr (m^3)'], 1e9))\n",
    "    \n",
    "    # Create points and segments for LineCollection\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    \n",
    "    # Create a LineCollection, using the discrete colormap and norm\n",
    "    lc = LineCollection(segments, cmap=discrete_cmap, norm=norm, linestyle=(0, (2, 1)))\n",
    "    \n",
    "    # Set the values used for colormapping, using matplotlib dates for colors\n",
    "    lc.set_array(x)\n",
    "    lc.set_linewidth(2)\n",
    "    line = ax[3].add_collection(lc)\n",
    "    \n",
    "    # Scatter plot, using the discrete colormap and norm for coloring\n",
    "    scatter = ax[3].scatter(x, y, c=x, cmap=discrete_cmap, norm=norm)\n",
    "    \n",
    "    # Plot bias\n",
    "    ax[3].plot(mdates.date2num(geom_calcs_df['midcyc_datetime']),\n",
    "        np.cumsum(np.divide(geom_calcs_df['bias_dvol_corr (m^3)'], 1e9)), color='red', linestyle='solid', linewidth=1)\n",
    "    \n",
    "    # Format the x-axis to display years only\n",
    "    ax[3].xaxis.set_major_locator(mdates.YearLocator())  # Tick every year\n",
    "    ax[3].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Display ticks as years\n",
    "    \n",
    "    # # Set x-axis limits\n",
    "    # start_date = geom_calcs_df['midcyc_datetime'].min()\n",
    "    # end_date = geom_calcs_df['midcyc_datetime'].max()\n",
    "    # ax[3].set_xlim(mdates.date2num(start_date), mdates.date2num(end_date))\n",
    "    \n",
    "    # Set title and axis label\n",
    "    ax[3].set_title('cumulative ice volume\\ndisplacement [km$^3$]', size=12, pad=8)\n",
    "    ax[3].set_xlabel('year')\n",
    "    \n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    \n",
    "    # Save and close plot\n",
    "    plt.savefig(OUTPUT_DIR + '/plot_evolving_and_static_comparison/{}.png'\n",
    "        .format(lake_name),\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6aaf623-3e84-469e-94e0-f780b13cc099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_folders_in_directory(directory):\n",
    "    '''\n",
    "    Counts the number of folders in the given directory.\n",
    "    '''\n",
    "    folder_count = 0\n",
    "    for item in os.listdir(directory):\n",
    "        # Construct full path to the item\n",
    "        item_path = os.path.join(directory, item)\n",
    "        # Check if the item is a directory\n",
    "        if os.path.isdir(item_path):\n",
    "            folder_count += 1\n",
    "    return folder_count\n",
    "\n",
    "# # Example usage\n",
    "# directory_path = '/home/jovyan/1_outlines_candidates/output/FigS1_lake_reexamination_methods.ipynb/plot_height_changes'  # Replace this with the path to your directory\n",
    "# print(f'There are {count_folders_in_directory(directory_path)} folders in \"{directory_path}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5e0e1c-b26e-4ee3-87eb-ceea82bc5d73",
   "metadata": {},
   "source": [
    "# Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48d1542f-fb79-4965-a13f-3c84a3fa69fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import subglacial lake outlines \n",
    "S09_outlines = gpd.read_file('/home/jovyan/1_outlines_candidates/Sauthoff-2025-J.Glaciol./output/lake_outlines/static_outlines/S09_outlines.geojson')\n",
    "SF18_outlines = gpd.read_file('/home/jovyan/1_outlines_candidates/Sauthoff-2025-J.Glaciol./output/lake_outlines/static_outlines/SF18_outlines.geojson')\n",
    "lakes_gdf = gpd.read_file('/home/jovyan/1_outlines_candidates/Sauthoff-2025-J.Glaciol./output/lake_outlines/static_outlines/lakes_gdf.geojson')\n",
    "lakes_gdf_postSF18 = gpd.read_file('/home/jovyan/1_outlines_candidates/Sauthoff-2025-J.Glaciol./output/lake_outlines/static_outlines/lakes_gdf_postSF18.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3c97b1e-bbd7-4f54-a26d-21d2aa3c9c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODIS MOA 2014 coastline and grounding line\n",
    "# https://nsidc.org/data/nsidc-0730/versions/1\n",
    "shp = DATA_DIR + '/boundaries/MODIS_MOA/2014/moa2014_coastline_v01.shp' \n",
    "moa_2014_coastline = gpd.read_file(shp)\n",
    "shp = DATA_DIR + '/boundaries/MODIS_MOA/2014/moa2014_grounding_line_v01.shp' \n",
    "moa_2014_groundingline = gpd.read_file(shp, crs=3031)\n",
    "# moa_2014_groundingline['geometry'] = moa_2014_groundingline.buffer(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e44c162d-b4d2-419f-81cd-945373956006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning 1: TIFFReadDirectory:Invalid data type for tag StripByteCounts\n",
      "Warning 1: TIFFFetchNormalTag:Incorrect value for \"GeoKeyDirectory\"; tag ignored\n",
      "Warning 1: TIFFFetchNormalTag:ASCII value for tag \"GeoASCIIParams\" contains null byte in value; value incorrectly truncated during reading due to implementation limitations\n"
     ]
    }
   ],
   "source": [
    "# Import MODIS Mosaic of Antarctica (MOA) surface imagery\n",
    "# https://nsidc.org/data/nsidc-0730/versions/1\n",
    "# Relocate to data_dir\n",
    "# Open into an xarray.DataArray\n",
    "# moa_lowres = DATA_DIR + '/surface_imagery/MODIS_MOA/2014/moa750_2014_hp1_v01.tif' \n",
    "# moa_lowres_da = rioxarray.open_rasterio(moa_lowres)\n",
    "\n",
    "moa_highres = DATA_DIR + '/surface_imagery/MODIS_MOA/2014/moa125_2014_hp1_v01.tif' \n",
    "moa_highres_da = rioxarray.open_rasterio(moa_highres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d43f854-e9c7-4251-a30d-522a248383b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: 'â–º';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: 'â–¼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 20GB\n",
       "Dimensions:      (y: 4451, x: 5451, time: 34)\n",
       "Coordinates:\n",
       "  * y            (y) float64 36kB -2.185e+06 -2.184e+06 ... 2.264e+06 2.265e+06\n",
       "  * x            (x) float64 44kB -2.665e+06 -2.664e+06 ... 2.784e+06 2.785e+06\n",
       "    spatial_ref  int64 8B 0\n",
       "  * time         (time) datetime64[ns] 272B 2010-07-02T15:00:00 ... 2018-10-0...\n",
       "Data variables:\n",
       "    mask         (y, x) float64 194MB ...\n",
       "    data_count   (time, y, x) float64 7GB ...\n",
       "    h            (time, y, x) float64 7GB ...\n",
       "    delta_h      (time, y, x) float64 7GB ...\n",
       "Attributes:\n",
       "    fileName:                mos_2010.5_2021.5.h5\n",
       "    shortName:               CS2-Smith-2017\n",
       "    identifier_product_DOI:  doi:10.5194/tc-11-451-2017</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-1ada2807-36f9-4e87-964c-95ea18566868' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-1ada2807-36f9-4e87-964c-95ea18566868' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>y</span>: 4451</li><li><span class='xr-has-index'>x</span>: 5451</li><li><span class='xr-has-index'>time</span>: 34</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-ee319188-0c98-45cc-9928-1e809920012c' class='xr-section-summary-in' type='checkbox'  checked><label for='section-ee319188-0c98-45cc-9928-1e809920012c' class='xr-section-summary' >Coordinates: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>y</span></div><div class='xr-var-dims'>(y)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-2.185e+06 -2.184e+06 ... 2.265e+06</div><input id='attrs-ee119f81-2a8a-4ff5-9520-e89facfbbd70' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-ee119f81-2a8a-4ff5-9520-e89facfbbd70' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-a128bc04-35bc-42fa-a1f2-884e999f042b' class='xr-var-data-in' type='checkbox'><label for='data-a128bc04-35bc-42fa-a1f2-884e999f042b' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([-2185000., -2184000., -2183000., ...,  2263000.,  2264000.,  2265000.])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>x</span></div><div class='xr-var-dims'>(x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-2.665e+06 -2.664e+06 ... 2.785e+06</div><input id='attrs-fbc58eb7-6a5a-47ac-87bd-18b5d069c937' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-fbc58eb7-6a5a-47ac-87bd-18b5d069c937' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-e5d38943-1f57-4cff-8fb2-063e76059a13' class='xr-var-data-in' type='checkbox'><label for='data-e5d38943-1f57-4cff-8fb2-063e76059a13' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([-2665000., -2664000., -2663000., ...,  2783000.,  2784000.,  2785000.])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>spatial_ref</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0</div><input id='attrs-3f35c3ec-e966-4c80-b742-27b47ad6d00d' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-3f35c3ec-e966-4c80-b742-27b47ad6d00d' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-79af8f5e-6f14-4b2d-ba70-64bf960cd7ce' class='xr-var-data-in' type='checkbox'><label for='data-79af8f5e-6f14-4b2d-ba70-64bf960cd7ce' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>crs_wkt :</span></dt><dd>PROJCS[&quot;WGS 84 / Antarctic Polar Stereographic&quot;,GEOGCS[&quot;WGS 84&quot;,DATUM[&quot;WGS_1984&quot;,SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563,AUTHORITY[&quot;EPSG&quot;,&quot;7030&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;6326&quot;]],PRIMEM[&quot;Greenwich&quot;,0,AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]],UNIT[&quot;degree&quot;,0.0174532925199433,AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]],PROJECTION[&quot;Polar_Stereographic&quot;],PARAMETER[&quot;latitude_of_origin&quot;,-71],PARAMETER[&quot;central_meridian&quot;,0],PARAMETER[&quot;false_easting&quot;,0],PARAMETER[&quot;false_northing&quot;,0],UNIT[&quot;metre&quot;,1,AUTHORITY[&quot;EPSG&quot;,&quot;9001&quot;]],AXIS[&quot;Easting&quot;,NORTH],AXIS[&quot;Northing&quot;,NORTH],AUTHORITY[&quot;EPSG&quot;,&quot;3031&quot;]]</dd><dt><span>semi_major_axis :</span></dt><dd>6378137.0</dd><dt><span>semi_minor_axis :</span></dt><dd>6356752.314245179</dd><dt><span>inverse_flattening :</span></dt><dd>298.257223563</dd><dt><span>reference_ellipsoid_name :</span></dt><dd>WGS 84</dd><dt><span>longitude_of_prime_meridian :</span></dt><dd>0.0</dd><dt><span>prime_meridian_name :</span></dt><dd>Greenwich</dd><dt><span>geographic_crs_name :</span></dt><dd>WGS 84</dd><dt><span>horizontal_datum_name :</span></dt><dd>World Geodetic System 1984</dd><dt><span>projected_crs_name :</span></dt><dd>WGS 84 / Antarctic Polar Stereographic</dd><dt><span>grid_mapping_name :</span></dt><dd>polar_stereographic</dd><dt><span>standard_parallel :</span></dt><dd>-71.0</dd><dt><span>straight_vertical_longitude_from_pole :</span></dt><dd>0.0</dd><dt><span>false_easting :</span></dt><dd>0.0</dd><dt><span>false_northing :</span></dt><dd>0.0</dd><dt><span>spatial_ref :</span></dt><dd>PROJCS[&quot;WGS 84 / Antarctic Polar Stereographic&quot;,GEOGCS[&quot;WGS 84&quot;,DATUM[&quot;WGS_1984&quot;,SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563,AUTHORITY[&quot;EPSG&quot;,&quot;7030&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;6326&quot;]],PRIMEM[&quot;Greenwich&quot;,0,AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]],UNIT[&quot;degree&quot;,0.0174532925199433,AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]],PROJECTION[&quot;Polar_Stereographic&quot;],PARAMETER[&quot;latitude_of_origin&quot;,-71],PARAMETER[&quot;central_meridian&quot;,0],PARAMETER[&quot;false_easting&quot;,0],PARAMETER[&quot;false_northing&quot;,0],UNIT[&quot;metre&quot;,1,AUTHORITY[&quot;EPSG&quot;,&quot;9001&quot;]],AXIS[&quot;Easting&quot;,NORTH],AXIS[&quot;Northing&quot;,NORTH],AUTHORITY[&quot;EPSG&quot;,&quot;3031&quot;]]</dd></dl></div><div class='xr-var-data'><pre>array(0)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2010-07-02T15:00:00 ... 2018-10-...</div><input id='attrs-f501787a-c2a7-4e25-9d7f-6a6862949246' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-f501787a-c2a7-4e25-9d7f-6a6862949246' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-53482369-ff8a-4667-ab00-c54690b0cf3e' class='xr-var-data-in' type='checkbox'><label for='data-53482369-ff8a-4667-ab00-c54690b0cf3e' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>description :</span></dt><dd>Time for each node</dd></dl></div><div class='xr-var-data'><pre>array([&#x27;2010-07-02T15:00:00.000000000&#x27;, &#x27;2010-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2011-01-01T00:00:00.000000000&#x27;, &#x27;2011-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2011-07-02T15:00:00.000000000&#x27;, &#x27;2011-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2012-01-01T00:00:00.000000000&#x27;, &#x27;2012-04-01T07:30:00.000000000&#x27;,\n",
       "       &#x27;2012-07-01T15:00:00.000000000&#x27;, &#x27;2012-09-30T22:30:00.000000000&#x27;,\n",
       "       &#x27;2013-01-01T00:00:00.000000000&#x27;, &#x27;2013-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2013-07-02T15:00:00.000000000&#x27;, &#x27;2013-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2014-01-01T00:00:00.000000000&#x27;, &#x27;2014-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2014-07-02T15:00:00.000000000&#x27;, &#x27;2014-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2015-01-01T00:00:00.000000000&#x27;, &#x27;2015-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2015-07-02T15:00:00.000000000&#x27;, &#x27;2015-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2016-01-01T00:00:00.000000000&#x27;, &#x27;2016-04-01T07:30:00.000000000&#x27;,\n",
       "       &#x27;2016-07-01T15:00:00.000000000&#x27;, &#x27;2016-09-30T22:30:00.000000000&#x27;,\n",
       "       &#x27;2017-01-01T00:00:00.000000000&#x27;, &#x27;2017-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2017-07-02T15:00:00.000000000&#x27;, &#x27;2017-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2018-01-01T00:00:00.000000000&#x27;, &#x27;2018-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2018-07-02T15:00:00.000000000&#x27;, &#x27;2018-10-01T22:30:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-a360f07c-4507-497d-8094-183dcb6adaf9' class='xr-section-summary-in' type='checkbox'  checked><label for='section-a360f07c-4507-497d-8094-183dcb6adaf9' class='xr-section-summary' >Data variables: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>mask</span></div><div class='xr-var-dims'>(y, x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-b88d08db-8463-4872-8cac-16fcadc1e0ad' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-b88d08db-8463-4872-8cac-16fcadc1e0ad' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-e1f043cf-cf44-46f4-9ae7-5f4754872b4b' class='xr-var-data-in' type='checkbox'><label for='data-e1f043cf-cf44-46f4-9ae7-5f4754872b4b' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>description: :</span></dt><dd>Data mask. 0: unknown, 1: unknown, nan: nan</dd><dt><span>grid_mapping :</span></dt><dd>spatial_ref</dd></dl></div><div class='xr-var-data'><pre>[24262401 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>data_count</span></div><div class='xr-var-dims'>(time, y, x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-e740063f-0911-4cd9-ab3b-efb52e2f796b' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-e740063f-0911-4cd9-ab3b-efb52e2f796b' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-68d7ce13-3619-4806-be7b-d813a2f3a8a2' class='xr-var-data-in' type='checkbox'><label for='data-68d7ce13-3619-4806-be7b-d813a2f3a8a2' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>description: :</span></dt><dd>Data mask. 0: bare ground or ocean?, 1: ice?, nan: nan</dd><dt><span>grid_mapping :</span></dt><dd>spatial_ref</dd></dl></div><div class='xr-var-data'><pre>[824921634 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>h</span></div><div class='xr-var-dims'>(time, y, x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-09af3612-558f-46b2-8207-874d9a4f3ec5' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-09af3612-558f-46b2-8207-874d9a4f3ec5' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-2e1d319c-f309-454c-809f-eeae74b74314' class='xr-var-data-in' type='checkbox'><label for='data-2e1d319c-f309-454c-809f-eeae74b74314' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[824921634 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>delta_h</span></div><div class='xr-var-dims'>(time, y, x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-bcc2a0d1-f3b3-4f1e-9f7c-eb5f25152a71' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-bcc2a0d1-f3b3-4f1e-9f7c-eb5f25152a71' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-69ef9068-2c18-440b-970c-bd8356edb55b' class='xr-var-data-in' type='checkbox'><label for='data-69ef9068-2c18-440b-970c-bd8356edb55b' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>description :</span></dt><dd>Height change relative to the ATL14 datum (Jan 1, 2020) surface</dd></dl></div><div class='xr-var-data'><pre>[824921634 values with dtype=float64]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-71319422-fa48-4592-a1be-3b01371c75b2' class='xr-section-summary-in' type='checkbox'  ><label for='section-71319422-fa48-4592-a1be-3b01371c75b2' class='xr-section-summary' >Indexes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>y</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-8f6dc9cc-b30a-4c2c-aa73-57cde3c1e3e4' class='xr-index-data-in' type='checkbox'/><label for='index-8f6dc9cc-b30a-4c2c-aa73-57cde3c1e3e4' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([-2185000.0, -2184000.0, -2183000.0, -2182000.0, -2181000.0, -2180000.0,\n",
       "       -2179000.0, -2178000.0, -2177000.0, -2176000.0,\n",
       "       ...\n",
       "        2256000.0,  2257000.0,  2258000.0,  2259000.0,  2260000.0,  2261000.0,\n",
       "        2262000.0,  2263000.0,  2264000.0,  2265000.0],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;y&#x27;, length=4451))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>x</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-2a7afe2d-9b45-4bbc-af01-29073c636475' class='xr-index-data-in' type='checkbox'/><label for='index-2a7afe2d-9b45-4bbc-af01-29073c636475' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([-2665000.0, -2664000.0, -2663000.0, -2662000.0, -2661000.0, -2660000.0,\n",
       "       -2659000.0, -2658000.0, -2657000.0, -2656000.0,\n",
       "       ...\n",
       "        2776000.0,  2777000.0,  2778000.0,  2779000.0,  2780000.0,  2781000.0,\n",
       "        2782000.0,  2783000.0,  2784000.0,  2785000.0],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;x&#x27;, length=5451))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>time</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-77cfbd13-f4d9-4773-9020-a29e936a0782' class='xr-index-data-in' type='checkbox'/><label for='index-77cfbd13-f4d9-4773-9020-a29e936a0782' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(DatetimeIndex([&#x27;2010-07-02 15:00:00&#x27;, &#x27;2010-10-01 22:30:00&#x27;,\n",
       "               &#x27;2011-01-01 00:00:00&#x27;, &#x27;2011-04-02 07:30:00&#x27;,\n",
       "               &#x27;2011-07-02 15:00:00&#x27;, &#x27;2011-10-01 22:30:00&#x27;,\n",
       "               &#x27;2012-01-01 00:00:00&#x27;, &#x27;2012-04-01 07:30:00&#x27;,\n",
       "               &#x27;2012-07-01 15:00:00&#x27;, &#x27;2012-09-30 22:30:00&#x27;,\n",
       "               &#x27;2013-01-01 00:00:00&#x27;, &#x27;2013-04-02 07:30:00&#x27;,\n",
       "               &#x27;2013-07-02 15:00:00&#x27;, &#x27;2013-10-01 22:30:00&#x27;,\n",
       "               &#x27;2014-01-01 00:00:00&#x27;, &#x27;2014-04-02 07:30:00&#x27;,\n",
       "               &#x27;2014-07-02 15:00:00&#x27;, &#x27;2014-10-01 22:30:00&#x27;,\n",
       "               &#x27;2015-01-01 00:00:00&#x27;, &#x27;2015-04-02 07:30:00&#x27;,\n",
       "               &#x27;2015-07-02 15:00:00&#x27;, &#x27;2015-10-01 22:30:00&#x27;,\n",
       "               &#x27;2016-01-01 00:00:00&#x27;, &#x27;2016-04-01 07:30:00&#x27;,\n",
       "               &#x27;2016-07-01 15:00:00&#x27;, &#x27;2016-09-30 22:30:00&#x27;,\n",
       "               &#x27;2017-01-01 00:00:00&#x27;, &#x27;2017-04-02 07:30:00&#x27;,\n",
       "               &#x27;2017-07-02 15:00:00&#x27;, &#x27;2017-10-01 22:30:00&#x27;,\n",
       "               &#x27;2018-01-01 00:00:00&#x27;, &#x27;2018-04-02 07:30:00&#x27;,\n",
       "               &#x27;2018-07-02 15:00:00&#x27;, &#x27;2018-10-01 22:30:00&#x27;],\n",
       "              dtype=&#x27;datetime64[ns]&#x27;, name=&#x27;time&#x27;, freq=None))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-358fdc3f-ab63-4928-8013-7d87867e71e1' class='xr-section-summary-in' type='checkbox'  checked><label for='section-358fdc3f-ab63-4928-8013-7d87867e71e1' class='xr-section-summary' >Attributes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>fileName :</span></dt><dd>mos_2010.5_2021.5.h5</dd><dt><span>shortName :</span></dt><dd>CS2-Smith-2017</dd><dt><span>identifier_product_DOI :</span></dt><dd>doi:10.5194/tc-11-451-2017</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset> Size: 20GB\n",
       "Dimensions:      (y: 4451, x: 5451, time: 34)\n",
       "Coordinates:\n",
       "  * y            (y) float64 36kB -2.185e+06 -2.184e+06 ... 2.264e+06 2.265e+06\n",
       "  * x            (x) float64 44kB -2.665e+06 -2.664e+06 ... 2.784e+06 2.785e+06\n",
       "    spatial_ref  int64 8B 0\n",
       "  * time         (time) datetime64[ns] 272B 2010-07-02T15:00:00 ... 2018-10-0...\n",
       "Data variables:\n",
       "    mask         (y, x) float64 194MB ...\n",
       "    data_count   (time, y, x) float64 7GB ...\n",
       "    h            (time, y, x) float64 7GB ...\n",
       "    delta_h      (time, y, x) float64 7GB ...\n",
       "Attributes:\n",
       "    fileName:                mos_2010.5_2021.5.h5\n",
       "    shortName:               CS2-Smith-2017\n",
       "    identifier_product_DOI:  doi:10.5194/tc-11-451-2017"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Smith and others, 2017, TC method CryoSat-2 SARIn height and dheight data (closed source aquired from Ben Smith)\n",
    "CS2_Smith2017 = xr.open_dataset(DATA_DIR + '/altimetry/CryoSat2/CS2_SARIn_Smith2017method/mos_2010.5_2019.0_relative_to_ATL14.nc')\n",
    "# Assign CRS\n",
    "CS2_Smith2017.rio.write_crs(\"EPSG:3031\", inplace=True)\n",
    "CS2_Smith2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3248a5a-3a1a-4855-a3a7-97e2050e1fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CS2_Smith2017_count = CS2_Smith2017.where(CS2_Smith2017['count'] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75f6efce-7341-400e-a660-2e42bbb24d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with ATL11 read in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79b17078-d831-4b2d-a16d-214e5c79b867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<earthaccess.auth.Auth at 0x7fbc7cb81790>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log into NASA Earthdata to search for datasets\n",
    "earthaccess.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f33130e-0262-4ba9-a696-7f9bda24bf11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Find ICESat-2 ATL11 r003 data granules\n",
    "# results = earthaccess.search_data(\n",
    "#     doi='10.5067/ATLAS/ATL11.006',\n",
    "#     # short_name='ATL15',\n",
    "#     # version='003',\n",
    "#     bounding_box=(180, -90, -180, -60),  # (lower_left_lon, lower_left_lat , upper_right_lon, upper_right_lat))\n",
    "#     cloud_hosted=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1fc8217a-507e-4cbe-9e62-207236db2639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Open data granules as s3 files to stream\n",
    "# files = earthaccess.open(results)\n",
    "# # files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c38e74d8-78d9-44f9-8fe1-651cae14789a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0fae3032-84d5-4123-a67c-4daf2cf4643d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import datatree as dt\n",
    "\n",
    "# # Open the HDF5 file\n",
    "# with h5py.File(files[0], 'r') as h5file:\n",
    "#     # Load the entire HDF5 file into a DataTree\n",
    "#     data_tree = dt.DataTree.from_hdf5(h5file)\n",
    "\n",
    "# # Now `data_tree` is a DataTree object containing the structure and data of the HDF5 file\n",
    "# # You can navigate and manipulate this tree structure as needed\n",
    "\n",
    "# # For example, to print the contents of the DataTree\n",
    "# print(data_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22bb7921-a63e-48c2-8215-20ceda095180",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # create empty lists\n",
    "# lon=[]\n",
    "# lat=[]\n",
    "# h_corr=[]\n",
    "# sigma_h=[]\n",
    "\n",
    "# # fill lists\n",
    "# for file in files:\n",
    "#     print(file)\n",
    "#     try:\n",
    "#         for pair in ['pt1','pt2','pt3']:\n",
    "#             lons, lats, hh, ss=read_ATL11(file, pair)\n",
    "#             lon += [lons]\n",
    "#             lat += [lats]\n",
    "#             h_corr += [hh]\n",
    "#             sigma_h += [ss]\n",
    "#     except Exception as E:\n",
    "#         pass\n",
    "\n",
    "# # concatenate lists\n",
    "# lon=np.concatenate(lon)\n",
    "# lat=np.concatenate(lat)\n",
    "# h_corr=np.concatenate(h_corr, axis=0)\n",
    "# sigma_h=np.concatenate(sigma_h, axis=0)\n",
    "# x,y=ll2ps(lon,lat) # transform geodetic lon, lat to polar stereographic x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dd3fa5-b94f-4831-82e5-b4f889ee0b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d5dd4c7-cb4d-4e98-a6e5-2396278c2647",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granules found: 16\n"
     ]
    }
   ],
   "source": [
    "# Find ICESat-2 ATL15 r003 data granules\n",
    "results = earthaccess.search_data(\n",
    "    doi='10.5067/ATLAS/ATL15.003',\n",
    "    # short_name='ATL15',\n",
    "    # version='003',\n",
    "    bounding_box=(180, -90, -180, -60),  # (lower_left_lon, lower_left_lat , upper_right_lon, upper_right_lat))\n",
    "    cloud_hosted=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c6223853-0e77-4e12-8ba1-a5dcde961801",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening 16 granules, approx size: 5.05 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5701d3c0cb4d38afca0a9ad6032931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c785a5150be94b5981fc172f0b1668e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f4eda9471340ab8a3547d549e2605e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL15.003/2019.03.29/ATL15_A4_0318_40km_003_01.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL15.003/2019.03.29/ATL15_A4_0318_10km_003_01.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL15.003/2019.03.29/ATL15_A4_0318_20km_003_01.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL15.003/2019.03.29/ATL15_A2_0318_01km_003_01.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL15.003/2019.03.29/ATL15_A3_0318_20km_003_01.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL15.003/2019.03.29/ATL15_A2_0318_20km_003_01.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL15.003/2019.03.29/ATL15_A2_0318_40km_003_01.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL15.003/2019.03.29/ATL15_A2_0318_10km_003_01.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL15.003/2019.03.29/ATL15_A3_0318_40km_003_01.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL15.003/2019.03.29/ATL15_A3_0318_01km_003_01.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL15.003/2019.03.29/ATL15_A3_0318_10km_003_01.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL15.003/2019.03.29/ATL15_A4_0318_01km_003_01.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL15.003/2019.03.29/ATL15_A1_0318_20km_003_01.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL15.003/2019.03.29/ATL15_A1_0318_40km_003_01.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL15.003/2019.03.29/ATL15_A1_0318_10km_003_01.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL15.003/2019.03.29/ATL15_A1_0318_01km_003_01.nc>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open data granules as s3 files to stream\n",
    "files = earthaccess.open(results)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a8acc8a-b616-446b-bd14-41e6c26c46f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL15.003/2019.03.29/ATL15_A1_0318_01km_003_01.nc>\n",
      "<File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL15.003/2019.03.29/ATL15_A2_0318_01km_003_01.nc>\n",
      "<File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL15.003/2019.03.29/ATL15_A3_0318_01km_003_01.nc>\n",
      "<File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL15.003/2019.03.29/ATL15_A4_0318_01km_003_01.nc>\n"
     ]
    }
   ],
   "source": [
    "# After viewing files, index the files you wish to open\n",
    "print(files[15])\n",
    "print(files[3])\n",
    "print(files[9])\n",
    "print(files[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0e8bdda-d8c0-415b-87b2-fae741baa41d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ClientResponseError",
     "evalue": "401, message='Unauthorized', url=URL('https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL15.003/2019.03.29/ATL15_A1_0318_01km_003_01.nc')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientResponseError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Open each file, which are quadrants in polar stereographic coordinations around the Geographic South Pole\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ATL15_A1 \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdelta_h\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m ATL15_A2 \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(files[\u001b[38;5;241m3\u001b[39m], group\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta_h\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m ATL15_A3 \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(files[\u001b[38;5;241m9\u001b[39m], group\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta_h\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/backends/api.py:573\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    562\u001b[0m     decode_cf,\n\u001b[1;32m    563\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    569\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    570\u001b[0m )\n\u001b[1;32m    572\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 573\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    580\u001b[0m     backend_ds,\n\u001b[1;32m    581\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    592\u001b[0m )\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/backends/h5netcdf_.py:419\u001b[0m, in \u001b[0;36mH5netcdfBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, format, group, lock, invalid_netcdf, phony_dims, decode_vlen_strings, driver, driver_kwds)\u001b[0m\n\u001b[1;32m    405\u001b[0m store \u001b[38;5;241m=\u001b[39m H5NetCDFStore\u001b[38;5;241m.\u001b[39mopen(\n\u001b[1;32m    406\u001b[0m     filename_or_obj,\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    414\u001b[0m     driver_kwds\u001b[38;5;241m=\u001b[39mdriver_kwds,\n\u001b[1;32m    415\u001b[0m )\n\u001b[1;32m    417\u001b[0m store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[0;32m--> 419\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mstore_entrypoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_and_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_and_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcat_characters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat_characters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cftime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cftime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_timedelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_timedelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/backends/store.py:43\u001b[0m, in \u001b[0;36mStoreBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]  # allow LSP violation, not supporting **kwargs\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     31\u001b[0m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[Any] \u001b[38;5;241m|\u001b[39m BufferedIOBase \u001b[38;5;241m|\u001b[39m AbstractDataStore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     decode_timedelta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename_or_obj, AbstractDataStore)\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28mvars\u001b[39m, attrs \u001b[38;5;241m=\u001b[39m \u001b[43mfilename_or_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m filename_or_obj\u001b[38;5;241m.\u001b[39mget_encoding()\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mvars\u001b[39m, attrs, coord_names \u001b[38;5;241m=\u001b[39m conventions\u001b[38;5;241m.\u001b[39mdecode_cf_variables(\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28mvars\u001b[39m,\n\u001b[1;32m     48\u001b[0m         attrs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m         decode_timedelta\u001b[38;5;241m=\u001b[39mdecode_timedelta,\n\u001b[1;32m     56\u001b[0m     )\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/backends/common.py:251\u001b[0m, in \u001b[0;36mAbstractDataStore.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    230\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m    This loads the variables and attributes simultaneously.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m    A centralized loading function makes it easier to create\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m    are requested, so care should be taken to make sure its fast.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     variables \u001b[38;5;241m=\u001b[39m FrozenDict(\n\u001b[0;32m--> 251\u001b[0m         (_decode_variable_name(k), v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    252\u001b[0m     )\n\u001b[1;32m    253\u001b[0m     attributes \u001b[38;5;241m=\u001b[39m FrozenDict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_attrs())\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m variables, attributes\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/backends/h5netcdf_.py:238\u001b[0m, in \u001b[0;36mH5NetCDFStore.get_variables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_variables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFrozenDict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_store_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/core/utils.py:443\u001b[0m, in \u001b[0;36mFrozenDict\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mFrozenDict\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Frozen:\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Frozen(\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/backends/h5netcdf_.py:239\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_variables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FrozenDict(\n\u001b[0;32m--> 239\u001b[0m         (k, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_store_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mvariables\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    240\u001b[0m     )\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/backends/h5netcdf_.py:201\u001b[0m, in \u001b[0;36mH5NetCDFStore.open_store_variable\u001b[0;34m(self, name, var)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_store_variable\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, var):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     dimensions \u001b[38;5;241m=\u001b[39m \u001b[43mvar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdimensions\u001b[49m\n\u001b[1;32m    202\u001b[0m     data \u001b[38;5;241m=\u001b[39m indexing\u001b[38;5;241m.\u001b[39mLazilyIndexedArray(H5NetCDFArrayWrapper(name, \u001b[38;5;28mself\u001b[39m))\n\u001b[1;32m    203\u001b[0m     attrs \u001b[38;5;241m=\u001b[39m _read_attributes(var)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/h5netcdf/core.py:261\u001b[0m, in \u001b[0;36mBaseVariable.dimensions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return variable dimension names.\"\"\"\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dimensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dimensions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lookup_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dimensions\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/h5netcdf/core.py:154\u001b[0m, in \u001b[0;36mBaseVariable._lookup_dimensions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# normal variable carrying DIMENSION_LIST\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# extract hdf5 file references and get objects name\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDIMENSION_LIST\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attrs:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# check if malformed variable and raise\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_unlabeled_dimension_mix\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_h5ds\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabeled\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;66;03m# If a dimension has attached more than one scale for some reason, then\u001b[39;00m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;66;03m# take the last one. This is in line with netcdf-c and netcdf4-python.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    158\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root\u001b[38;5;241m.\u001b[39m_h5file[ref[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    159\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m ref \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h5ds\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDIMENSION_LIST\u001b[39m\u001b[38;5;124m\"\u001b[39m, []))\n\u001b[1;32m    160\u001b[0m         )\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# need to use the h5ds name here to distinguish from collision dimensions\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/h5netcdf/core.py:454\u001b[0m, in \u001b[0;36m_unlabeled_dimension_mix\u001b[0;34m(h5py_dataset)\u001b[0m\n\u001b[1;32m    452\u001b[0m     status \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodim\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     dimset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdimlist\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;66;03m# either all dimensions have exactly one scale\u001b[39;00m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;66;03m# or all dimensions have no scale\u001b[39;00m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dimset \u001b[38;5;241m^\u001b[39m {\u001b[38;5;241m0\u001b[39m} \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mset\u001b[39m():\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/h5netcdf/core.py:454\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    452\u001b[0m     status \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodim\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     dimset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m dimlist])\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;66;03m# either all dimensions have exactly one scale\u001b[39;00m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;66;03m# or all dimensions have no scale\u001b[39;00m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dimset \u001b[38;5;241m^\u001b[39m {\u001b[38;5;241m0\u001b[39m} \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mset\u001b[39m():\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/h5py/_hl/dims.py:61\u001b[0m, in \u001b[0;36mDimensionProxy.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;129m@with_phil\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mh5ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_scales\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dimension\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5ds.pyx:72\u001b[0m, in \u001b[0;36mh5py.h5ds.get_num_scales\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5fd.pyx:163\u001b[0m, in \u001b[0;36mh5py.h5fd.H5FD_fileobj_read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/fsspec/spec.py:1800\u001b[0m, in \u001b[0;36mAbstractBufferedFile.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m   1795\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"mirrors builtin file's readinto method\u001b[39;00m\n\u001b[1;32m   1796\u001b[0m \n\u001b[1;32m   1797\u001b[0m \u001b[38;5;124;03mhttps://docs.python.org/3/library/io.html#io.RawIOBase.readinto\u001b[39;00m\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1799\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1800\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnbytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1801\u001b[0m out[: \u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m   1802\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/fsspec/spec.py:1790\u001b[0m, in \u001b[0;36mAbstractBufferedFile.read\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1788\u001b[0m     \u001b[38;5;66;03m# don't even bother calling fetch\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1790\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out)\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/fsspec/caching.py:383\u001b[0m, in \u001b[0;36mBytesCache._fetch\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m start \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart:\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend \u001b[38;5;241m-\u001b[39m end \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize:\n\u001b[0;32m--> 383\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m=\u001b[39m start\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/fsspec/asyn.py:121\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/fsspec/asyn.py:106\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreturn_result\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_result\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/fsspec/asyn.py:61\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     59\u001b[0m     coro \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(coro, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     63\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m ex\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/fsspec/implementations/http.py:655\u001b[0m, in \u001b[0;36mHTTPFile.async_fetch_range\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m416\u001b[39m:\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;66;03m# range request outside file\u001b[39;00m\n\u001b[1;32m    654\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 655\u001b[0m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# If the server has handled the range request, it should reply\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# with status 206 (partial content). But we'll guess that a suitable\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m# Content-Range header or a Content-Length no more than the\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# requested range also mean we have got the desired range.\u001b[39;00m\n\u001b[1;32m    661\u001b[0m response_is_range \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    662\u001b[0m     r\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m206\u001b[39m\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_content_range(r\u001b[38;5;241m.\u001b[39mheaders)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m start\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mint\u001b[39m(r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Length\u001b[39m\u001b[38;5;124m\"\u001b[39m, end \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m    665\u001b[0m )\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/aiohttp/client_reqrep.py:1060\u001b[0m, in \u001b[0;36mClientResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m-> 1060\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[1;32m   1061\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_info,\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory,\n\u001b[1;32m   1063\u001b[0m     status\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m   1064\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason,\n\u001b[1;32m   1065\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1066\u001b[0m )\n",
      "\u001b[0;31mClientResponseError\u001b[0m: 401, message='Unauthorized', url=URL('https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL15.003/2019.03.29/ATL15_A1_0318_01km_003_01.nc')"
     ]
    }
   ],
   "source": [
    "# Open each file, which are quadrants in polar stereographic coordinations around the Geographic South Pole\n",
    "ATL15_A1 = xr.open_dataset(files[15], group='delta_h')\n",
    "ATL15_A2 = xr.open_dataset(files[3], group='delta_h')\n",
    "ATL15_A3 = xr.open_dataset(files[9], group='delta_h')\n",
    "ATL15_A4 = xr.open_dataset(files[11], group='delta_h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da23c068-f494-45bc-814e-e76e22b4729b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Open locally stored files when NSIDC cloud access isn't working\n",
    "# ATL15_A1 = xr.open_dataset(DATA_DIR + '/altimetry/ICESat2/ATL15.003-Ant/ATL15_A1_0318_01km_003_01.nc', group='delta_h')\n",
    "# ATL15_A2 = xr.open_dataset(DATA_DIR + '/altimetry/ICESat2/ATL15.003-Ant/ATL15_A2_0318_01km_003_01.nc', group='delta_h')\n",
    "# ATL15_A3 = xr.open_dataset(DATA_DIR + '/altimetry/ICESat2/ATL15.003-Ant/ATL15_A3_0318_01km_003_01.nc', group='delta_h')\n",
    "# ATL15_A4 = xr.open_dataset(DATA_DIR + '/altimetry/ICESat2/ATL15.003-Ant/ATL15_A4_0318_01km_003_01.nc', group='delta_h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c4b4f9-6096-40de-92bb-12a1d9688890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the variables to keep\n",
    "variables_to_keep = ['time', 'y', 'x', 'delta_h', 'data_count']\n",
    "\n",
    "# List of xarray datasets\n",
    "datasets = [ATL15_A1, ATL15_A2, ATL15_A3, ATL15_A4]\n",
    "\n",
    "# Function to drop variables not in variables_to_keep from a dataset\n",
    "def drop_unwanted_variables(dataset):\n",
    "    variables_to_drop = [var for var in dataset.variables if var not in variables_to_keep]\n",
    "    return dataset.drop_vars(variables_to_drop)\n",
    "\n",
    "# Apply the function to each dataset\n",
    "ATL15_A1, ATL15_A2, ATL15_A3, ATL15_A4 = [drop_unwanted_variables(ds) for ds in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f875fb41-ec6f-4e03-a659-e7e7273b98df",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use xarray concatenation to stitch two quadrants togethers\n",
    "# Use xarray index selecting to occlude the duplicated x=0 vector of data\n",
    "ATL15_A12 = xr.concat([ATL15_A2.isel(x=slice(0,-1)), ATL15_A1], dim=\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372eec1e-3eb5-42d8-94ee-a2de74316803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use xarray concatenation to stitch two quadrants togethers\n",
    "# Use xarray index selecting to occlude the duplicated x=0 vector of data\n",
    "ATL15_A34 = xr.concat([ATL15_A3.isel(x=slice(0,-1)), ATL15_A4], dim='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d14e491-fd5e-424b-ae91-75ba59381ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use xarray concatenation to stitch two quadrants togethers\n",
    "# Use xarray index selecting to occlude the duplicated x=0 vector of data\n",
    "ATL15_dh = xr.concat([ATL15_A34.isel(y=slice(0,-1)), ATL15_A12], dim='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d62934f-4e13-404d-b2b3-4338f498cfd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete variables to reduce memory consumption\n",
    "del ATL15_A1, ATL15_A12, ATL15_A2, ATL15_A3, ATL15_A34, ATL15_A4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b88e5-6957-4f63-b9d9-70591d18af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add datasets attributes\n",
    "ATL15_dh.attrs['identifier_product_DOI'] = '10.5067/ATLAS/ATL15.003'\n",
    "ATL15_dh.attrs['shortName'] = 'ATL15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d571618-924b-410a-a202-29c72aced243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ATL15_dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0bae96-66b2-4c2e-8c49-5623c3fde835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scripps Grounding Line\n",
    "# https://doi.pangaea.de/10.1594/PANGAEA.819147\n",
    "Scripps_gl = gpd.read_file(DATA_DIR + \n",
    "    '/boundaries/Depoorter2013/Antarctica_masks/scripps_antarctica_polygons_v1.shp')\n",
    "\n",
    "# Isolate only land ice\n",
    "Scripps_landice = Scripps_gl[Scripps_gl['Id_text'] == 'Grounded ice or land']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ba0a3-7cc4-430c-ab7e-3604365c59cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import MODIS Mosaic of Antarctica surface imagery\n",
    "# https://nsidc.org/data/nsidc-0730/versions/1\n",
    "# Relocate to data_dir\n",
    "# Open into an xarray.DataArray\n",
    "# moa_lowres = DATA_DIR + '/surface_imagery/MODIS_MOA/2014/moa750_2014_hp1_v01.tif' \n",
    "# moa_lowres_da = rioxarray.open_rasterio(moa_lowres)\n",
    "\n",
    "moa_highres = DATA_DIR + '/surface_imagery/MODIS_MOA/2014/moa125_2014_hp1_v01.tif' \n",
    "moa_highres_da = rioxarray.open_rasterio(moa_highres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a112e9f7-d8bc-4010-b547-3b63a05a4948",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df19fcb6-5ea0-4d70-946d-53e960a9bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip altimetry datasets to grounding line to limit analysis to only below grounded ice\n",
    "CS2_Smith2017.rio.write_crs(3031, inplace=True)\n",
    "CS2_Smith2017 = CS2_Smith2017.rio.clip(Scripps_landice.geometry.values, Scripps_landice.crs, drop=False)\n",
    "ATL15_dh.rio.write_crs(3031, inplace=True)\n",
    "ATL15_dh = ATL15_dh.rio.clip(Scripps_landice.geometry.values, Scripps_landice.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05b1c92-b737-4461-8341-5b7dfbd39349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset1=CS2_Smith2017\n",
    "dataset2=ATL15_dh\n",
    "\n",
    "# Create empty lists to store data\n",
    "cyc_start_dates = []\n",
    "cyc_end_dates = []\n",
    "midcyc_dates = []\n",
    "dataset = []\n",
    "\n",
    "for idx in range(len(dataset1.delta_h[:33])):\n",
    "    # CryoSat-2 SARIn data - Smith and others, 2017 method\n",
    "    if dataset1.identifier_product_DOI == 'doi:10.5194/tc-11-451-2017':\n",
    "        cyc_start_date = dataset1.time.values[idx]\n",
    "        cyc_end_date = dataset1.time.values[idx+1]\n",
    "        midcyc_days = cyc_end_date - cyc_start_date\n",
    "        midcyc_date = cyc_start_date + midcyc_days/2\n",
    "        cyc_start_dates += [cyc_start_date]\n",
    "        cyc_end_dates += [cyc_end_date]\n",
    "        midcyc_dates += [midcyc_date]\n",
    "        dataset += ['CS2_Smith2017']\n",
    "    # CryoSat-2 SARIn data - Cryo-TEMPO-EOLIS Swath Thematic Gridded Product\n",
    "    elif dataset1.Title == 'Land Ice Elevation Thematic Gridded Product':\n",
    "        date_time_str = '70-01-01'\n",
    "        date_time_obj = datetime.datetime.strptime(date_time_str, '%y-%m-%d')\n",
    "        cyc_start_date = date_time_obj + datetime.timedelta(seconds = ds_sub.time.values[idx].astype(float))\n",
    "        cyc_end_date = date_time_obj + datetime.timedelta(seconds = ds_sub.time.values[idx+1].astype(float))\n",
    "        midcyc_days = cyc_end_date - cyc_start_date\n",
    "        midcyc_date = cyc_start_date + midcyc_days/2\n",
    "        cyc_start_dates += [cyc_start_date]\n",
    "        cyc_end_dates += [cyc_end_date]\n",
    "        midcyc_dates += [midcyc_date]\n",
    "        dataset += ['CS2_Cryo-TEMPO-EOLIS']\n",
    "for idx in range(len(dataset2.delta_h)-1):\n",
    "    # ICESat-2 ATL15 r003\n",
    "    if dataset2.identifier_product_DOI == '10.5067/ATLAS/ATL15.003':    \n",
    "        cyc_start_date = dataset2.time.values[idx]\n",
    "        cyc_end_date = dataset2.time.values[idx+1]\n",
    "        midcyc_days = cyc_end_date - cyc_start_date\n",
    "        midcyc_date = cyc_start_date + midcyc_days/2\n",
    "        cyc_start_dates += [cyc_start_date]\n",
    "        cyc_end_dates += [cyc_end_date]\n",
    "        midcyc_dates += [midcyc_date]\n",
    "        dataset += ['IS2_ATL15']\n",
    "\n",
    "# Concatenate list into pandas dataframe\n",
    "cyc_dates = pd.DataFrame({'cyc_start_dates': cyc_start_dates, 'midcyc_dates': midcyc_dates, 'cyc_end_dates': cyc_end_dates, 'dataset': dataset})\n",
    "\n",
    "# Store cycle dates list as csv for use in other notebooks\n",
    "cyc_dates.to_csv('output/cycle_dates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f638644a-e8ec-4dcb-97cf-726c97744575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Ensure dates are correct\n",
    "# for idx in range(len(midcyc_dates)):\n",
    "#     if idx <= 32:\n",
    "#         print('idx:', idx, 'CS2 era')\n",
    "#         print('cyc_start_date:', CS2_Smith2017.time[idx].values)\n",
    "#         print('mid_cyc_date:', midcyc_dates[idx])\n",
    "#         print('cyc_end_date:', CS2_Smith2017.time[idx+1].values)\n",
    "#     elif idx > 32:\n",
    "#         print('idx:', idx, 'IS2 era')\n",
    "#         # Subtract 33 (32th idx because 0th idx) from idx to start over with new dataset\n",
    "#         print('cyc_start_date:', ATL15_dh.time[idx-33].values)\n",
    "#         print('mid_start_date:', midcyc_dates[idx])\n",
    "#         print('cyc_end_date:', ATL15_dh.time[(idx+1)-33].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596962e3-e1e3-47c9-b19e-62d368182982",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analysis of previously identified lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "429e1ca6-74fb-4945-b353-206a57285013",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Mars\n",
      "Folder '/home/jovyan/1_outlines_candidates/output/FigS1_lake_reexamination_methods.ipynb/plot_data_counts' already exists.\n",
      "Folder '/home/jovyan/1_outlines_candidates/output/FigS1_lake_reexamination_methods.ipynb/plot_data_counts/Mars' created successfully.\n",
      "Now creating/saving plots...\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Plot data counts for all the lakes to determine if there is adequate data coverage for analysis \n",
    "\n",
    "# List all folders in target directory\n",
    "folder_path = '/home/jovyan/1_outlines_candidates/output/FigS1_lake_reexamination_methods.ipynb/plot_data_counts'\n",
    "\n",
    "# Store folder names that are non-empty (in case a folder was created, but output wasn't stored to it)\n",
    "folders = [name for name in os.listdir(folder_path) \n",
    "           if os.path.isdir(os.path.join(folder_path, name)) \n",
    "           and os.listdir(os.path.join(folder_path, name))]\n",
    "\n",
    "# Filter the GeoDataFrame to only include rows where the 'name' is not in folders\n",
    "lakes_gdf_filtered = lakes_gdf[~lakes_gdf['name'].isin(folders)]\n",
    "\n",
    "# Now, run your loop on the filtered GeoDataFrame\n",
    "for idx, row in lakes_gdf_filtered.iterrows():\n",
    "\n",
    "    # Isolate lake from lakes_gdf as geodataframe using slicing\n",
    "    lake_gdf = lakes_gdf.iloc[idx:idx+1]\n",
    "    print('Working on', lake_gdf['name'].values[0])\n",
    "    \n",
    "    # Plot data counts\n",
    "    plot_data_counts(lake_gdf=lake_gdf, dataset1=CS2_Smith2017, dataset2=ATL15_dh)\n",
    "    \n",
    "    # Clear the output of each index\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c95430-57cb-46cc-ac0e-7a63c6c6fa8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot height changes at each time slice\n",
    "\n",
    "# List all folders in target directory\n",
    "folder_path = '/home/jovyan/1_outlines_candidates/output/FigS1_lake_reexamination_methods.ipynb/plot_height_changes'\n",
    "\n",
    "# Store folder names that are non-empty (in case a folder was created, but output wasn't stored to it)\n",
    "folders = [name for name in os.listdir(folder_path) \n",
    "           if os.path.isdir(os.path.join(folder_path, name)) \n",
    "           and os.listdir(os.path.join(folder_path, name))]\n",
    "\n",
    "# Filter the GeoDataFrame to only include rows where the 'name' is not in folders\n",
    "lakes_gdf_filtered = lakes_gdf[~lakes_gdf['name'].isin(folders)]\n",
    "\n",
    "# Now, run your loop on the filtered GeoDataFrame\n",
    "for idx, row in lakes_gdf_filtered.iterrows():\n",
    "\n",
    "    # Isolate lake from lakes_gdf as geodataframe using slicing\n",
    "    lake_gdf = lakes_gdf.iloc[idx:idx+1]\n",
    "    print('Working on', lake_gdf['name'].values[0])\n",
    "    \n",
    "    # Plot height changes\n",
    "    plot_height_changes(lake_gdf=lake_gdf, dataset1=CS2_Smith2017, dataset2=ATL15_dh)\n",
    "    \n",
    "    # Clear the output of each index\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd526c8-8d96-4967-b870-94335dff0fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find evolving outlines at various search extents and levels for each lake\n",
    "\n",
    "# List all files in your target directory\n",
    "folder_path = '/home/jovyan/1_outlines_candidates/Sauthoff-2024-J.Glaciol./output/lake_outlines/search_extents_levels'\n",
    "\n",
    "# List all files in folder_path and strip extensions\n",
    "names = {os.path.splitext(name)[0] for name in os.listdir(folder_path) \n",
    "                 if os.path.isfile(os.path.join(folder_path, name))}\n",
    "\n",
    "# Filter the GeoDataFrame to only include rows that have not been created in target directory yet\n",
    "lakes_gdf_filtered = lakes_gdf[~lakes_gdf['name'].isin(names)]\n",
    "\n",
    "# Now, run loop on the filtered GeoDataFrame\n",
    "for idx, row in lakes_gdf_filtered.iterrows():\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    initial_level = 0.1  # Starting levels\n",
    "    area_multiple_search_extents = range(2, 16)  # From 2 to 15 inclusive\n",
    "\n",
    "    # Assign dataset; during CS2 era, assign based on SARIn coverage time period\n",
    "    if lakes_gdf['CS2_SARIn_time_period'][idx] == np.nan:\n",
    "        dataset1='none'\n",
    "    elif lakes_gdf['CS2_SARIn_time_period'][idx] == '2013.75-2018.75':\n",
    "        dataset1=CS2_Smith2017.sel(time=slice(np.datetime64('2013-10-01T22:30:00.000000000'), None))\n",
    "    elif lakes_gdf['CS2_SARIn_time_period'][idx] == '2010.5-2018.75':\n",
    "        dataset1=CS2_Smith2017\n",
    "    dataset2=ATL15_dh\n",
    "\n",
    "    # Select lake\n",
    "    lake_gdf = lakes_gdf.iloc[idx:idx+1]\n",
    "    lake_name = lake_gdf.name.values[0]\n",
    "    print('Working on', lake_name)\n",
    "\n",
    "    # Initialize DataFrame to store results\n",
    "    search_extents_levels_df = pd.DataFrame(columns=['area_multiple_search_extent', 'level'])\n",
    "\n",
    "    for area_multiple_search_extent in area_multiple_search_extents:\n",
    "        level = initial_level  # Reset level for each new area_multiple_search_extent\n",
    "        within_fraction = 0.0  # Reset within_fraction for each new area_multiple_search_extent\n",
    "        level_increment = 0.05  # Initial level increment\n",
    "\n",
    "        while within_fraction < 0.90 and level <= 2.0:\n",
    "            # Find evolving outlines\n",
    "            outlines_gdf = find_evolving_outlines(lake_gdf=lake_gdf, \n",
    "                area_multiple_search_extent=area_multiple_search_extent, level=level, \n",
    "                dataset1=dataset1, dataset2=dataset2)\n",
    "\n",
    "            # Define lake polygon and buffered geodataframe as before\n",
    "            lake_poly = lake_gdf.iloc[0].geometry\n",
    "            buffered_lake_poly = muliple_area_buffer(polygon=lake_poly, \n",
    "                area_multiple=area_multiple_search_extent)\n",
    "            buffered_lake_gdf = gpd.GeoDataFrame(geometry=gpd.GeoSeries([buffered_lake_poly]), crs=3031)\n",
    "            \n",
    "            # Check which evolving outlines are within or overlap\n",
    "            within = gpd.sjoin(outlines_gdf, buffered_lake_gdf, predicate='within')\n",
    "            overlaps = gpd.sjoin(outlines_gdf, buffered_lake_gdf, predicate='overlaps')\n",
    "\n",
    "            # Calculate within_fraction as before\n",
    "            if (len(within) + len(overlaps)) > 0:\n",
    "                within_fraction = np.round((len(within) / (len(within) + len(overlaps))), 2)\n",
    "            else:\n",
    "                print('No outlines found at this level')\n",
    "                break\n",
    "\n",
    "            print(f\"Extent: {area_multiple_search_extent}, Level: {level}, Within: {round(within_fraction*100)}%\")\n",
    "\n",
    "            # Once within_fraction is greater specified percent,\n",
    "            if within_fraction >= 0.75:\n",
    "                level_increment = 0.01  # Increase level increment\n",
    "\n",
    "            # Once within_fraction is greater specified percent,\n",
    "            if within_fraction >= 0.90:\n",
    "                # Store the results in a new row and then concatenate into the results DataFrame only if within_fraction is > 90%\n",
    "                new_row = pd.DataFrame({'area_multiple_search_extent': [area_multiple_search_extent], 'level': [level]})\n",
    "                search_extents_levels_df = pd.concat([search_extents_levels_df, new_row], ignore_index=True)\n",
    "                break  # Exit loop if condition is met\n",
    "\n",
    "            level += level_increment\n",
    "            level = np.round(level, 2)\n",
    "\n",
    "    # Store the DataFrame as geojson file for later use\n",
    "    search_extents_levels_df.to_csv('output/search_extents_levels/{}.csv'.format(lake_gdf['name'].values[0]), index=False)\n",
    "\n",
    "    # Clear the output of each index\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5497fa-6d56-4d3b-afc0-6d29e572f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize outlines for each lake and plot the outlines\n",
    "\n",
    "# List all files in your target directory\n",
    "folder_path = '/home/jovyan/1_outlines_candidates/Sauthoff-2024-J.Glaciol./output/lake_outlines/evolving_outlines'\n",
    "\n",
    "# List all files in folder_path2 and strip extensions\n",
    "names = {os.path.splitext(name)[0] for name in os.listdir(folder_path) \n",
    "                 if os.path.isfile(os.path.join(folder_path, name))}\n",
    "\n",
    "# Filter the GeoDataFrame to only include rows where the 'name' is in unique_folders_path1\n",
    "lakes_gdf_filtered = lakes_gdf[lakes_gdf['name'].isin(names)]\n",
    "\n",
    "# Now, run loop on the filtered GeoDataFrame\n",
    "for idx, row in lakes_gdf_filtered.iterrows():\n",
    "\n",
    "    # Select lake\n",
    "    lake_gdf = lakes_gdf.iloc[idx:idx+1]\n",
    "    lake_name = lake_gdf.name.values[0]\n",
    "    print('Working on', lake_name)\n",
    "\n",
    "    # Select dataframe the row with the minimum level to capture the most signal\n",
    "    # If there are multiple rows with the same minimum level, select the row with \n",
    "    # the smaller area_multiple_search_extent\n",
    "    \n",
    "    # Sort the DataFrame first by 'Resulting_Level' and then by 'area_multiple_search_extent', \n",
    "    # both in ascending order\n",
    "    search_extents_levels_df = pd.read_csv('output/search_extents_levels/{}.csv'.format(lake_gdf['name'].values[0]))\n",
    "    sorted_df = search_extents_levels_df.sort_values(by=['level', 'area_multiple_search_extent'], \n",
    "        ascending=[True, True])\n",
    "    \n",
    "    # Select the first row of the sorted DataFrame\n",
    "    selected_row = sorted_df.iloc[0]\n",
    "    \n",
    "    # Display the selected row\n",
    "    print(selected_row)\n",
    "    \n",
    "    # Repeat with the selected level (first make output folders)\n",
    "    create_folder(OUTPUT_DIR + '/{}'.format('find_evolving_outlines'))\n",
    "    create_folder(OUTPUT_DIR + '/find_evolving_outlines/{}'.format(lake_name))\n",
    "    outlines_gdf = find_evolving_outlines(lake_gdf=lake_gdf, \n",
    "        area_multiple_search_extent=selected_row['area_multiple_search_extent'],\n",
    "        level=selected_row['level'], dataset1=dataset1, dataset2=dataset2, plot=True)\n",
    "   \n",
    "    # Clean up outlines by just looking at those intersecting with original static lake outline or evolving outlines that do so\n",
    "    filtered_gdf = extract_intersecting_polygons(outlines_gdf, lake_gdf['geometry'].values[0])\n",
    "    \n",
    "    # Plot and save fig of evolving outlines\n",
    "    create_folder(OUTPUT_DIR + '/{}'.format('plot_evolving_outlines'))\n",
    "    plot_evolving_outlines(lake_gdf=lake_gdf, outlines_gdf=filtered_gdf)\n",
    "    \n",
    "    # # Simplify centroid column to string to allow GeoJSON export\n",
    "    # for column in filtered_gdf.columns:\n",
    "    #     if column != 'geometry':  # Exclude the geometry column\n",
    "    #         filtered_gdf[column] = filtered_gdf[column].astype(str)\n",
    "    \n",
    "    # Ensure there are outlines in filtered_gdf\n",
    "    if not filtered_gdf.empty:\n",
    "        # Export evolving outlines GeoDataFrame to GeoJSON\n",
    "        filtered_gdf.to_file(filename='output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_gdf['name'].values[0]), driver='GeoJSON')\n",
    "    elif filtered_gdf.empty:\n",
    "        # Write a text file indicating no evolving outlines\n",
    "        with open('output/lake_outlines/evolving_outlines/{}.txt'.format(lake_gdf['name'].values[0]), 'w') as f:\n",
    "            f.write(\"There are no evolving outlines for this lake.\")\n",
    "        # Otherwise move onto next lake\n",
    "        continue\n",
    "    \n",
    "    # # Clear the output of each index\n",
    "    # clear_output(wait=True)\n",
    "    \n",
    "    # # Print cell duration\n",
    "    # end_time = time.time()\n",
    "    # duration = end_time - start_time\n",
    "    # print(f\"The cell ran in {np.round(duration/60,1)} mins.\")\n",
    "    \n",
    "    # Play sound to indicate a lake is complete\n",
    "    # play_sound()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4f63832-b1e7-4458-a3db-5942206f7bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "151\n",
      "145\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "# Ensure all lakes complete\n",
    "print(len(os.listdir('/home/jovyan/1_outlines_candidates/output/FigS1_lake_reexamination_methods.ipynb/find_evolving_outlines')))\n",
    "print(len(os.listdir('/home/jovyan/1_outlines_candidates/Sauthoff-2025-J.Glaciol./output/lake_outlines/evolving_outlines')))\n",
    "print(len(os.listdir('/home/jovyan/1_outlines_candidates/Sauthoff-2025-J.Glaciol./output/lake_outlines/compare_evolving_and_static_outlines')))\n",
    "print(len(os.listdir('/home/jovyan/1_outlines_candidates/output/FigS1_lake_reexamination_methods.ipynb/plot_evolving_and_static_comparison/')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9536a1-0f27-41b1-b60c-b14943739870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp\n",
    "# Redoing lakes to save search extents and levels df's\n",
    "\n",
    "# # List all folders in your target directory\n",
    "# folder_path = '/home/jovyan/1_outlines_candidates/output/FigS1_lake_reexamination_methods.ipynb/find_evolving_outlines'\n",
    "\n",
    "# # Store folder names that are non-empty (in case a folder was created, but output wasn't stored to it)\n",
    "# names = [name for name in os.listdir(folder_path) \n",
    "#            if os.path.isdir(os.path.join(folder_path, name)) \n",
    "#            and os.listdir(os.path.join(folder_path, name))]\n",
    "\n",
    "# List all files in your target directory\n",
    "folder_path = '/home/jovyan/1_outlines_candidates/Sauthoff-2024-J.Glaciol./output/search_extents_levels'\n",
    "\n",
    "# List all files in folder_path and strip extensions\n",
    "names = {os.path.splitext(name)[0] for name in os.listdir(folder_path) \n",
    "                 if os.path.isfile(os.path.join(folder_path, name))}\n",
    "\n",
    "# Filter the GeoDataFrame to only include rows where the 'name' is in unique_folders_path1\n",
    "lakes_gdf_filtered = lakes_gdf[~lakes_gdf['name'].isin(names)]\n",
    "\n",
    "# Now, run loop on the filtered GeoDataFrame\n",
    "for idx, row in lakes_gdf_filtered.iterrows():\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    initial_level = 0.1  # Starting levels\n",
    "    area_multiple_search_extents = range(2, 16)  # From 2 to 15 inclusive\n",
    "\n",
    "    # Assign dataset; during CS2 era, assign based on SARIn coverage time period\n",
    "    if lakes_gdf['CS2_SARIn_time_period'][idx] == np.nan:\n",
    "        dataset1='none'\n",
    "    elif lakes_gdf['CS2_SARIn_time_period'][idx] == '2013.75-2018.75':\n",
    "        dataset1=CS2_Smith2017.sel(time=slice(np.datetime64('2013-10-01T22:30:00.000000000'), None))\n",
    "    elif lakes_gdf['CS2_SARIn_time_period'][idx] == '2010.5-2018.75':\n",
    "        dataset1=CS2_Smith2017\n",
    "    dataset2=ATL15_dh\n",
    "\n",
    "    # Select lake\n",
    "    lake_gdf = lakes_gdf.iloc[idx:idx+1]\n",
    "    lake_name = lake_gdf.name.values[0]\n",
    "    print('Working on', lake_name)\n",
    "\n",
    "    # Initialize DataFrame to store results\n",
    "    search_extents_levels_df = pd.DataFrame(columns=['area_multiple_search_extent', 'level'])\n",
    "\n",
    "    for area_multiple_search_extent in area_multiple_search_extents:\n",
    "        level = initial_level  # Reset level for each new area_multiple_search_extent\n",
    "        within_fraction = 0.0  # Reset within_fraction for each new area_multiple_search_extent\n",
    "        level_increment = 0.05  # Initial level increment\n",
    "\n",
    "        while within_fraction < 0.90 and level <= 2.0:\n",
    "            # Find evolving outlines\n",
    "            outlines_gdf = find_evolving_outlines(lake_gdf=lake_gdf, \n",
    "                area_multiple_search_extent=area_multiple_search_extent, level=level, \n",
    "                dataset1=dataset1, dataset2=dataset2)\n",
    "\n",
    "            # Define lake polygon and buffered geodataframe as before\n",
    "            lake_poly = lake_gdf.iloc[0].geometry\n",
    "            buffered_lake_poly = muliple_area_buffer(polygon=lake_poly, \n",
    "                area_multiple=area_multiple_search_extent)\n",
    "            buffered_lake_gdf = gpd.GeoDataFrame(geometry=gpd.GeoSeries([buffered_lake_poly]), crs=3031)\n",
    "            \n",
    "            # Check which evolving outlines are within or overlap\n",
    "            within = gpd.sjoin(outlines_gdf, buffered_lake_gdf, predicate='within')\n",
    "            overlaps = gpd.sjoin(outlines_gdf, buffered_lake_gdf, predicate='overlaps')\n",
    "\n",
    "            # Calculate within_fraction as before\n",
    "            if (len(within) + len(overlaps)) > 0:\n",
    "                within_fraction = np.round((len(within) / (len(within) + len(overlaps))), 2)\n",
    "            else:\n",
    "                print('No outlines found at this level')\n",
    "                break\n",
    "\n",
    "            print(f\"Extent: {area_multiple_search_extent}, Level: {level}, Within: {round(within_fraction*100)}%\")\n",
    "\n",
    "            # Once within_fraction is greater specified percent,\n",
    "            if within_fraction >= 0.75:\n",
    "                level_increment = 0.01  # Increase level increment\n",
    "\n",
    "            # Once within_fraction is greater specified percent,\n",
    "            if within_fraction >= 0.90:\n",
    "                # Store the results in a new row and then concatenate into the results DataFrame only if within_fraction is > 90%\n",
    "                new_row = pd.DataFrame({'area_multiple_search_extent': [area_multiple_search_extent], 'level': [level]})\n",
    "                search_extents_levels_df = pd.concat([search_extents_levels_df, new_row], ignore_index=True)\n",
    "                break  # Exit loop if condition is met\n",
    "\n",
    "            level += level_increment\n",
    "            level = np.round(level, 2)\n",
    "\n",
    "    # Store the DataFrame as geojson file for later use\n",
    "    search_extents_levels_df.to_csv('output/search_extents_levels/{}.csv'.format(lake_gdf['name'].values[0]), index=False)\n",
    "\n",
    "    # Clear the output of each index\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb81064d-3d23-4e9a-a5f0-fe770d92ead0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59c5f54-ffce-459a-9360-3e244f5a6fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some lake may behave as a complex of interconnected lakes that are sometimes separate but other times lobes of one lake\n",
    "# We'll investigate those here\n",
    "# Mac1-3\n",
    "# Slessor4-5\n",
    "# Sites B-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7e4cee-1d30-4051-af7d-57e15348b3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rec2/4/5, Mac2/3 have strange search extents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690572f5-a515-489e-9575-42e1a21f1b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rec7 invalid geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1443137c-072e-41f8-975a-28ad1ad91ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Byrd_s6 appears clipped\n",
    "# Byrd_s7 appears clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938bcd2d-3328-4568-b951-e5fa1fcf978d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0d7ce093-8427-4dd8-8cb3-c39ba331cdb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/io/file.py:383: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  as_dt = pd.to_datetime(df[k], errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Wilkes_2\n",
      "Working on 2010-08-17 06:45:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2010-11-16 11:15:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2011-02-15 15:45:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2011-05-17 23:15:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2011-08-17 06:45:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2011-11-16 11:15:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2012-02-15 15:45:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2012-05-16 23:15:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2012-08-16 06:45:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2012-11-15 23:15:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2013-02-15 15:45:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2013-05-17 23:15:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2013-08-17 06:45:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2013-11-16 11:15:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2014-02-15 15:45:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2014-05-17 23:15:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2014-08-17 06:45:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2014-11-16 11:15:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2015-02-15 15:45:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2015-05-17 23:15:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2015-08-17 06:45:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2015-11-16 11:15:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2016-02-15 15:45:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2016-05-16 23:15:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2016-08-16 06:45:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2016-11-15 23:15:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2017-02-15 15:45:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2017-05-17 23:15:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2017-08-17 06:45:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2017-11-16 11:15:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2018-02-15 15:45:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2018-05-17 23:15:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2018-08-17 06:45:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2018-11-16 14:15:00\n",
      "Working on 2019-02-15 21:45:00\n",
      "Working on 2019-05-18 05:15:00\n",
      "Working on 2019-08-17 12:45:00\n",
      "Working on 2019-11-16 20:15:00\n",
      "Working on 2020-02-16 03:45:00\n",
      "Working on 2020-05-17 11:15:00\n",
      "Working on 2020-08-16 18:45:00\n",
      "Working on 2020-11-16 02:15:00\n",
      "Working on 2021-02-15 09:45:00\n",
      "Working on 2021-05-17 17:15:00\n",
      "Working on 2021-08-17 00:45:00\n",
      "Working on 2021-11-16 08:15:00\n",
      "Working on 2022-02-15 15:45:00\n",
      "Working on 2022-05-17 23:15:00\n",
      "Working on 2022-08-17 06:45:00\n",
      "Working on 2022-11-16 14:15:00\n",
      "Working on 2023-02-15 21:45:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Calc darea, dh, and dvol for evolving and static outlines and compare using\n",
    "# bias (evolving-static) for each lake\n",
    "\n",
    "# List all files in your target directory\n",
    "folder_path = '/home/jovyan/1_outlines_candidates/Sauthoff-2024-J.Glaciol./output/lake_outlines/compare_evolving_and_static_outlines'\n",
    "\n",
    "# List all files in your target directory without their extensions\n",
    "file_names_without_extension = [os.path.splitext(name)[0] for name in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, name))]\n",
    "\n",
    "# Filter the GeoDataFrame to only include rows where the 'name' is not in folder\n",
    "lakes_gdf_filtered = lakes_gdf[~lakes_gdf['name'].isin(file_names_without_extension)]\n",
    "\n",
    "# Now run loop on the filtered GeoDataFrame\n",
    "for idx, row in lakes_gdf_filtered.iterrows():    \n",
    "\n",
    "    # Assign dataset; during CS2 era, assign based on SARIn coverage time period\n",
    "    if lakes_gdf_filtered['CS2_SARIn_time_period'][idx] == 'nan':\n",
    "        dataset1='none'\n",
    "    elif lakes_gdf_filtered['CS2_SARIn_time_period'][idx] == '2013.75-2018.75':\n",
    "        dataset1=CS2_Smith2017.sel(time=slice(np.datetime64('2013-10-01T22:30:00.000000000'), None))\n",
    "    elif lakes_gdf_filtered['CS2_SARIn_time_period'][idx] == '2010.5-2018.75':\n",
    "        dataset1=CS2_Smith2017\n",
    "    dataset2=ATL15_dh\n",
    "    \n",
    "    # Isolate lake from lakes_gdf as geodataframe using slicing\n",
    "    lake_gdf = lakes_gdf_filtered.loc[idx:idx]\n",
    "    print('Working on', lake_gdf['name'].values[0])\n",
    "    \n",
    "    # Load evolving outlines geodataframe from store geojson\n",
    "    file_path = 'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_gdf['name'].values[0])\n",
    "    if os.path.exists(file_path):  # Check if the file exists\n",
    "        outlines_gdf = gpd.read_file(file_path)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Compare static and evolving outlines\n",
    "    compare_evolving_and_static_outlines(lake_gdf=lake_gdf, outlines_gdf=outlines_gdf, \n",
    "        dataset1=dataset1, dataset2=dataset2)\n",
    "    \n",
    "    # Clear the output of each index\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "017f9f11-268d-4ca1-9e49-312879277f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that combines all the evolving/static lake geometric calc's\n",
    "# and bias comparisons into one dataframe for continentally integrated numbers\n",
    "\n",
    "# Define your directory\n",
    "directory = 'output/lake_outlines/compare_evolving_and_static_outlines'\n",
    "dataframes = []  # List to store each DataFrame temporarily\n",
    "\n",
    "# List and read each CSV file\n",
    "for file in os.listdir(directory):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(directory, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Group by 'midcyc_datetime' if needed and sum the data\n",
    "combined_df = combined_df.groupby('midcyc_datetime').sum().reset_index()\n",
    "\n",
    "# Save the combined DataFrame to csv file\n",
    "combined_df.to_csv('output/lake_outlines/compare_evolving_and_static_outlines/all_previously_identified_lakes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b801477-c029-4543-96c9-b77cb5f0d9bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot evolving and static lakes comparison plots\n",
    "\n",
    "# List all files in your target directory\n",
    "folder_path = '/home/jovyan/1_outlines_candidates/output/FigS1_lake_reexamination_methods.ipynb/plot_evolving_and_static_comparison'\n",
    "\n",
    "# List all files in your target directory without their extensions\n",
    "file_names_without_extension = [os.path.splitext(name)[0] for name in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, name))]\n",
    "\n",
    "# Filter the GeoDataFrame to only include rows where the 'name' is not in folders\n",
    "# lakes_gdf_filtered = lakes_gdf[~lakes_gdf['name'].isin(file_names_without_extension)]\n",
    "lakes_gdf_filtered = lakes_gdf[~lakes_gdf['name'].isin(file_names_without_extension)]\n",
    "\n",
    "# Now run loop on the filtered GeoDataFrame\n",
    "for idx, row in lakes_gdf_filtered.iterrows():    \n",
    "    idx=idx+1\n",
    "    plot_evolving_and_static_comparison(lakes_gdf_filtered.iloc[idx]['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d329cc-e71a-46de-91c9-e25f47f1704d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d19013-5b78-48fc-8fe6-ea69b5e22170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to improve legend line for evolving outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ae662e-feed-4ebb-a5bb-da223d4ed7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerLine2D, HandlerTuple\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Define a custom legend handler\n",
    "class HandlerLine2DScatter(HandlerLine2D):\n",
    "    def create_artists(self, legend, orig_handle,\n",
    "                       xdescent, ydescent, width, height, fontsize, trans):\n",
    "        # Create line (use the same properties as your LineCollection)\n",
    "        line = Line2D([xdescent + width / 2], [ydescent + height / 2], linestyle=\"dotted\", color=\"turquoise\", linewidth=2)\n",
    "        \n",
    "        # Create scatter (adjust color, marker, etc. as per your scatter plot)\n",
    "        scatter = plt.Line2D([xdescent + width / 3, xdescent + 2 * width / 3], \n",
    "                             [ydescent + height / 2, ydescent + height / 2], \n",
    "                             linestyle=(0, (1, 1)), color=\"turquoise\", marker='o')\n",
    "        \n",
    "        return [line, scatter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b7897-abdf-4a65-8a87-91c3a5179515",
   "metadata": {},
   "outputs": [],
   "source": [
    "lake_name = 'Bindschadler_2'\n",
    "\n",
    "fig, ax = plt.subplots(1,4, figsize=(12,5))\n",
    "\n",
    "# Define colors and linestyles that will be reused and create lines for legend\n",
    "# S09_color = 'paleturquoise'\n",
    "SF18_color  = 'turquoise'\n",
    "# lake_locations_postSF18_color = 'darkturquoise'\n",
    "# S09_linestyle=(0, (1, 2))\n",
    "SF18_linestyle=(0, (1, 1))\n",
    "# S09_line = plt.Line2D((0, 1), (0, 0), color=S09_color, linestyle=S09_linestyle, linewidth=2)\n",
    "SF18_line = plt.Line2D((0, 1), (0, 0), color=SF18_color, linestyle='solid', linewidth=1)\n",
    "\n",
    "# Panel A - evolving outlines ------------------------------------------------------\n",
    "# Plot static and evolving outlines onto MOA surface imagery\n",
    "# Open static outline(s) and evolving outlines and geometric calculation comparison geodataframes for plotting\n",
    "# S09_lake_gdf = S09_outlines[S09_outlines['Name'] == 'Whillans_4']\n",
    "lake_gdf = lakes_gdf[lakes_gdf['name'] == lake_name]\n",
    "# Attempt to open the evolving outlines GeoJSON file\n",
    "try:\n",
    "    evolving_outlines_gdf = gpd.read_file(os.path.join(\n",
    "        os.getcwd(), 'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)))\n",
    "except fiona.errors.DriverError:\n",
    "    print(f\"File for {lake_name} not found. Skipping...\")\n",
    "    # return  # Skip the rest of the function if the file doesn't exist\n",
    "\n",
    "# Attempt to open the geometric calculations CSV file\n",
    "try:\n",
    "    geom_calcs_df = pd.read_csv(os.path.join(\n",
    "        os.getcwd(), 'output/lake_outlines/compare_evolving_and_static_outlines/{}.csv'.format(lake_name)))\n",
    "except FileNotFoundError:\n",
    "    print(f\"CSV file for {lake_name} not found. Skipping...\")\n",
    "    # return  # Skip the rest of the function if the file doesn't exist\n",
    "\n",
    "# Convert of strings to datetime\n",
    "geom_calcs_df['midcyc_datetime'] = pd.to_datetime(geom_calcs_df['midcyc_datetime'])\n",
    "\n",
    "# Combine static outline(s) with evolving outlines in unary union to plot all within bounds of plot\n",
    "# Use .buffer(0) to fix any invalid geometries\n",
    "# all_outlines_unary_union = unary_union([S09_lake_gdf.geometry.iloc[0], SF18_lake_gdf.geometry.iloc[0]] + list(evolving_outlines_gdf.geometry))\n",
    "all_outlines_unary_union = unary_union([lake_gdf.geometry.iloc[0].buffer(0)] + list(evolving_outlines_gdf.geometry.buffer(0)))\n",
    "x_min, y_min, x_max, y_max = all_outlines_unary_union.bounds\n",
    "buffer_frac = 0.2\n",
    "x_buffer = abs(x_max-x_min)*buffer_frac\n",
    "y_buffer = abs(y_max-y_min)*buffer_frac\n",
    "mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "ax[0].imshow(moa_highres_da_subset[0,:,:], cmap=\"gray\", clim=[14000, 17000], extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "\n",
    "# Pick colormap and make continuous cmap discrete for evolving outlines\n",
    "colormap = 'plasma'\n",
    "continuous_cmap = matplotlib.colormaps[colormap]\n",
    "discrete_cmap = colors.ListedColormap(continuous_cmap(np.linspace(0, 1, len(cyc_dates['midcyc_dates'])-1)))\n",
    "\n",
    "# Norm to time variable\n",
    "norm = plt.Normalize(mdates.date2num(cyc_dates['midcyc_dates'].iloc[0]), \n",
    "                     mdates.date2num(cyc_dates['midcyc_dates'].iloc[-1]))\n",
    "\n",
    "# Use for loop to store each time slice as line segment to use in legend\n",
    "# And plot each evolving outline in the geodataframe color by date\n",
    "lines = []  # list of lines to be used for the legend\n",
    "for idx, dt in enumerate(cyc_dates['midcyc_dates']):\n",
    "    x = 1; y = 1\n",
    "    line, = ax[0].plot(x, y, color=discrete_cmap(norm(mdates.date2num(cyc_dates['midcyc_dates'][idx]))), linewidth=1)\n",
    "    lines.append(line)\n",
    "    \n",
    "    # Filter rows that match the current time slice\n",
    "    evolving_outlines_gdf_dt_sub = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "\n",
    "    # Plotting the subset if not empty\n",
    "    if not evolving_outlines_gdf_dt_sub.empty:\n",
    "        evolving_outlines_gdf_dt_sub.boundary.plot(ax=ax[0], color=discrete_cmap(norm(mdates.date2num(cyc_dates['midcyc_dates'][idx]))), linewidth=1)\n",
    "\n",
    "# Plot static outline\n",
    "lake_gdf.boundary.plot(ax=ax[0], color=SF18_color, linewidth=1)\n",
    "\n",
    "# Plot inset map\n",
    "axIns = ax[0].inset_axes([0.8, 0.8, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "axIns.set_aspect('equal')\n",
    "moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "axIns.axis('off')\n",
    "\n",
    "# Plot red star to indicate location\n",
    "axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "    linewidth=1, edgecolor='k', facecolor='r', s=50, zorder=3)\n",
    "\n",
    "# Plot legend\n",
    "legend = ax[0].legend([tuple(lines), SF18_line], ['evolving outlines', 'static outline'],\n",
    "    handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper left')\n",
    "# legend.get_frame().set_linewidth(0.0)\n",
    "ax[0].patch.set_alpha(1)\n",
    "\n",
    "# Create colorbar \n",
    "m = plt.cm.ScalarMappable(cmap=discrete_cmap)\n",
    "m.set_array(np.array([timestamp_to_fractional_year(date) for date in cyc_dates['midcyc_dates']]))\n",
    "cax = inset_axes(ax[0],\n",
    "                 width=\"100%\",\n",
    "                 height=\"3%\",\n",
    "                 loc=3,\n",
    "                 bbox_to_anchor=[0,-0.2,1,1],\n",
    "                 bbox_transform=ax[0].transAxes,\n",
    "                 borderpad=0,\n",
    "                 )\n",
    "cbar=fig.colorbar(m, ticks=np.array([2010,2012,2014,2016,2018,2020,2022]), \n",
    "             cax=cax, orientation='horizontal')#.set_label('evolving outline year', size=15)\n",
    "\n",
    "# Set the label for the colorbar and adjust its size\n",
    "cbar.set_label('evolving outline year', size=10, labelpad=5)\n",
    "\n",
    "# Change polar stereographic m to km\n",
    "km_scale = 1e3\n",
    "ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "ax[0].xaxis.set_major_formatter(ticks_x)\n",
    "ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "ax[0].yaxis.set_major_formatter(ticks_y)\n",
    "\n",
    "# Set axes limit, title, and axis label\n",
    "ax[0].set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "ax[0].set_title('evolving and static\\noutline comparison', size=12, pad=8)\n",
    "ax[0].set_xlabel('X [km]')\n",
    "ax[0].set_ylabel('Y [km]')\n",
    "\n",
    "# Panel - da/dt ---------------------------------------------\n",
    "# Plot multi-colored line and scatter for data points\n",
    "x=mdates.date2num(geom_calcs_df['midcyc_datetime'])\n",
    "y=np.cumsum(np.divide(geom_calcs_df['evolving_outlines_darea (m^2)'], 1e6))\n",
    "\n",
    "# Create points and segments for LineCollection\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "\n",
    "# Create a LineCollection, using the discrete colormap and norm\n",
    "lc = LineCollection(segments, cmap=discrete_cmap, norm=norm, linestyle=(0, (2, 1)))\n",
    "\n",
    "# Set the values used for colormapping, using matplotlib dates for colors\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(2)\n",
    "line = ax[1].add_collection(lc)\n",
    "\n",
    "# Scatter plot, using the discrete colormap and norm for coloring\n",
    "# Convert x values (fractional years) back to matplotlib dates for consistent coloring\n",
    "scatter = ax[1].scatter(x, y, c=x, cmap=discrete_cmap, norm=norm)\n",
    "\n",
    "# Plot static outline area\n",
    "ax[1].axhline(np.divide(lake_gdf['area (m^2)'], 1e6).values, color=SF18_color, linestyle='solid', linewidth=1)\n",
    "\n",
    "# Plot bias\n",
    "ax[1].plot(mdates.date2num(geom_calcs_df['midcyc_datetime']),\n",
    "    np.divide(geom_calcs_df['bias_area (m^2)'], 1e6), color='red', linestyle='solid', linewidth=1)\n",
    "\n",
    "# Add legend\n",
    "bias = plt.Line2D((0, 1), (0, 0), color='red', linestyle='solid', linewidth=1)\n",
    "legend = ax[1].legend([tuple(lines), SF18_line, bias],\n",
    "    ['evolving outlines', 'static outline', 'bias (evolving - static)'], \n",
    "    handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper left')\n",
    "\n",
    "# Format the x-axis to display years only\n",
    "ax[1].xaxis.set_major_locator(mdates.YearLocator())  # Tick every year\n",
    "ax[1].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Display ticks as years\n",
    "\n",
    "# # Set x-axis limits\n",
    "# start_date = geom_calcs_df['midcyc_datetime'].min()\n",
    "# end_date = geom_calcs_df['midcyc_datetime'].max()\n",
    "# ax[1].set_xlim(mdates.date2num(start_date), mdates.date2num(end_date))\n",
    "\n",
    "# Set title and axis label\n",
    "ax[1].set_title('wetted area [km$^2$]', size=12, pad=8)\n",
    "ax[1].set_xlabel('year')\n",
    "\n",
    "# Panel C - dh/dt -------------------------------------------------------\n",
    "# Plot horizontal zero line for reference\n",
    "ax[2].axhline(0, color='k', linestyle='solid', linewidth=1)\n",
    "\n",
    "# Plot multi-colored line and scatter for data points\n",
    "x=mdates.date2num(geom_calcs_df['midcyc_datetime'])\n",
    "y=np.cumsum(geom_calcs_df['evolving_outlines_dh_corr (m)'])\n",
    "\n",
    "# Create points and segments for LineCollection\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "\n",
    "# Create a LineCollection, using the discrete colormap and norm\n",
    "lc = LineCollection(segments, cmap=discrete_cmap, norm=norm, linestyle=(0, (2, 1)))\n",
    "\n",
    "# Set the values used for colormapping, using matplotlib dates for colors\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(2)\n",
    "line = ax[2].add_collection(lc)\n",
    "\n",
    "# Scatter plot, using the discrete colormap and norm for coloring\n",
    "# Convert x values (fractional years) back to matplotlib dates for consistent coloring\n",
    "scatter = ax[2].scatter(x, y, c=x, cmap=discrete_cmap, norm=norm)\n",
    "\n",
    "# Plot static outline time series\n",
    "ax[2].plot(mdates.date2num(geom_calcs_df['midcyc_datetime']), \n",
    "    np.cumsum(geom_calcs_df['static_outline_dh_corr (m)']), color=SF18_color, linestyle='solid', linewidth=1)\n",
    "\n",
    "# Plot bias\n",
    "ax[2].plot(mdates.date2num(geom_calcs_df['midcyc_datetime']),\n",
    "    np.cumsum(geom_calcs_df['bias_outlines_dh_corr (m)']), color='red', linestyle='solid', linewidth=1)\n",
    "\n",
    "# Format the x-axis to display years only\n",
    "ax[2].xaxis.set_major_locator(mdates.YearLocator())  # Tick every year\n",
    "ax[2].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Display ticks as years\n",
    "\n",
    "# # Set x-axis limits\n",
    "# start_date = geom_calcs_df['midcyc_datetime'].min()\n",
    "# end_date = geom_calcs_df['midcyc_datetime'].max()\n",
    "# ax[2].set_xlim(mdates.date2num(start_date), mdates.date2num(end_date))\n",
    "\n",
    "# Set title and axis label\n",
    "ax[2].set_title('cumulative\\nheight change [m]', size=12, pad=8)\n",
    "ax[2].set_xlabel('year')\n",
    "\n",
    "# Panel D - dv/dt --------------------------------------------------\n",
    "# Plot horizontal line at zero for reference\n",
    "ax[3].axhline(0, color='k', linestyle='solid', linewidth=1)\n",
    "\n",
    "# Plot static outline time series\n",
    "ax[3].plot(mdates.date2num(geom_calcs_df['midcyc_datetime']), \n",
    "    np.divide(np.cumsum(geom_calcs_df['static_outline_dvol_corr (m^3)']), 1e9), \n",
    "    color=SF18_color, linestyle='solid', linewidth=1)\n",
    "\n",
    "# Plot multi-colored line and scatter for data points\n",
    "x=mdates.date2num(geom_calcs_df['midcyc_datetime'])\n",
    "y=np.cumsum(np.divide(geom_calcs_df['evolving_outlines_dvol_corr (m^3)'], 1e9))\n",
    "\n",
    "# Create points and segments for LineCollection\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "\n",
    "# Create a LineCollection, using the discrete colormap and norm\n",
    "lc = LineCollection(segments, cmap=discrete_cmap, norm=norm, linestyle=(0, (2, 1)))\n",
    "\n",
    "# Set the values used for colormapping, using matplotlib dates for colors\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(2)\n",
    "line = ax[3].add_collection(lc)\n",
    "\n",
    "# Scatter plot, using the discrete colormap and norm for coloring\n",
    "scatter = ax[3].scatter(x, y, c=x, cmap=discrete_cmap, norm=norm)\n",
    "\n",
    "# Plot bias\n",
    "ax[3].plot(mdates.date2num(geom_calcs_df['midcyc_datetime']),\n",
    "    np.cumsum(np.divide(geom_calcs_df['bias_dvol_corr (m^3)'], 1e9)), color='red', linestyle='solid', linewidth=1)\n",
    "\n",
    "# Format the x-axis to display years only\n",
    "ax[3].xaxis.set_major_locator(mdates.YearLocator())  # Tick every year\n",
    "ax[3].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Display ticks as years\n",
    "\n",
    "# # Set x-axis limits\n",
    "# start_date = geom_calcs_df['midcyc_datetime'].min()\n",
    "# end_date = geom_calcs_df['midcyc_datetime'].max()\n",
    "# ax[3].set_xlim(mdates.date2num(start_date), mdates.date2num(end_date))\n",
    "\n",
    "# Set title and axis label\n",
    "ax[3].set_title('cumulative ice volume\\ndisplacement [km$^3$]', size=12, pad=8)\n",
    "ax[3].set_xlabel('year')\n",
    "\n",
    "# Display plot\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a583f25d-9678-4996-850a-58a0e3c45262",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae0b045-fee3-4cc4-ad17-8f77d0f8b101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c059b478-b0d9-4cc2-b7a3-a195305ef531",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fig. S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89d2ef4-2675-46b9-a0d3-109671221ab0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROI_poly = buffered_poly\n",
    "print(type(buffered_poly))\n",
    "print(type(ROI_poly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a4007e-064b-4b7a-9163-705fa3fb5927",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROI_poly = buffered_poly\n",
    "dataset1 = CS2_dh\n",
    "dataset2 = ATL15_dh\n",
    "\n",
    "# Clipping datasets\n",
    "dataset1_clipped = dataset1.rio.clip(ROI_poly, dataset1.rio.crs)\n",
    "dataset2_clipped = dataset2.rio.clip(ROI_poly, dataset2.rio.crs)\n",
    "\n",
    "# Extract min and max of x and y for dataset1\n",
    "min_x1 = dataset1_clipped.x.min().item()\n",
    "max_x1 = dataset1_clipped.x.max().item()\n",
    "min_y1 = dataset1_clipped.y.min().item()\n",
    "max_y1 = dataset1_clipped.y.max().item()\n",
    "\n",
    "# Extract min and max of x and y for dataset2\n",
    "min_x2 = dataset2_clipped.x.min().item()\n",
    "max_x2 = dataset2_clipped.x.max().item()\n",
    "min_y2 = dataset2_clipped.y.min().item()\n",
    "max_y2 = dataset2_clipped.y.max().item()\n",
    "\n",
    "# Check if the coordinates match\n",
    "if min_x1 != min_x2 or max_x1 != max_x2 or min_y1 != min_y2 or max_y1 != max_y2:\n",
    "    raise ValueError(\"Dataset1 and Dataset2 do not have matching x, y min, max coordinates\")\n",
    "\n",
    "# Establish x_min, x_max, y_min, y_max, \n",
    "x_min = min_x1\n",
    "x_max = max_x1\n",
    "y_min = min_y1\n",
    "y_max = max_y1\n",
    "\n",
    "# Subsetting datasets\n",
    "# Subset datasets to region of interest for plotting\n",
    "buffer = 4000\n",
    "mask_x = (dataset1.x >= x_min-buffer) & (dataset1.x <= x_max+buffer)\n",
    "mask_y = (dataset1.y >= y_min-buffer) & (dataset1.y <= y_max+buffer)\n",
    "dataset1_subset = dataset1.where(mask_x & mask_y, drop=True)\n",
    "mask_x = (dataset2.x >= x_min-buffer) & (dataset2.x <= x_max+buffer)\n",
    "mask_y = (dataset2.y >= y_min-buffer) & (dataset2.y <= y_max+buffer)\n",
    "dataset2_subset = dataset2.where(mask_x & mask_y, drop=True)\n",
    "mask_x = (moa_highres_da.x >= x_min-buffer) & (moa_highres_da.x <= x_max+buffer)\n",
    "mask_y = (moa_highres_da.y >= y_min-buffer) & (moa_highres_da.y <= y_max+buffer)\n",
    "moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7937af91-7689-4a40-b36c-fa8c58a1808e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modify find contours function to achieve something like less than 5% of contours intersecting with buffer\n",
    "# gdf = find_evolving_outlines(ROI['name'].values[0], buffered_poly, 0.5, CS2_dh, ATL15_dh)\n",
    "# gdf = find_evolving_outlines(ROI['name'].values[0], buffered_poly, 0.5, CS2_dh, ATL15_dh)\n",
    "gdf = find_evolving_outlines('Slessor_2_3_23', buffered_poly, 0.5, CS2_dh, ATL15_dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f68e0c-a180-4b49-a3a9-7faeb8ee5e61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Try other lakes\n",
    "\n",
    "# Plot Fig. 2\n",
    "fig, ax = plt.subplots(3,1, sharex=True, figsize=(5.5,16.5))\n",
    "\n",
    "\n",
    "# Panel A - Plot uplift filling event\n",
    "# Specify the time value you want to plot\n",
    "specified_date = datetime.date(2010, 8, 17)\n",
    "gdf_subset[(gdf_subset['date'] == specified_date) & (gdf_subset['dh (m)'] > 0)].boundary.plot(ax=ax[0], color='blue')\n",
    "gdf_subset[(gdf_subset['date'] == specified_date) & (gdf_subset['dh (m)'] < 0)].boundary.plot(ax=ax[0], color='red')\n",
    "\n",
    "# Calculate the absolute difference between each time in the dataset and the specified time\n",
    "time_diff = np.abs(midcyc_dates - np.datetime64(specified_date))\n",
    "# Find the index of the minimum difference\n",
    "nearest_time_index = time_diff.argmin().item()\n",
    "if nearest_time_index <= 32:\n",
    "    dhdt = dataset1_subset.cyc_to_cyc_delta_h[nearest_time_index,:,:]\n",
    "elif nearest_time_index > 32:\n",
    "    # Subtract 33 from idx to start over with new dataset\n",
    "    dhdt = dataset2_subset.cyc_to_cyc_delta_h[(nearest_time_index-33),:,:]\n",
    "\n",
    "# Plot gridded height change data\n",
    "divnorm=colors.TwoSlopeNorm(vmin=-1.5, vcenter=0., vmax=1.5)  \n",
    "img = ax[0].imshow(dhdt, extent=[x_min-buffer, x_max+buffer, y_min-buffer, y_max+buffer], origin='upper', cmap='coolwarm_r', \n",
    "                   # norm=colors.CenteredNorm(),\n",
    "                   norm=divnorm)\n",
    "\n",
    "# Plot buffered polygon showing extent of evolving outline search\n",
    "ROI_color = 'magenta'\n",
    "gpd.GeoSeries(buffered_poly).boundary.plot(ax=ax[0], color=ROI_color)\n",
    "\n",
    "# Create an axes on the right side of ax1 for the colorbar\n",
    "cax = fig.add_axes([ax[0].get_position().x1 + 0.15, ax[0].get_position().y0 - 0.092, 0.03, ax[0].get_position().height])\n",
    "fig.colorbar(img, cax=cax).set_label('height change [m]', size=12)\n",
    "\n",
    "# Annotate time slice\n",
    "ax[0].annotate('height change: {} to {}'.format(datetime64_to_fractional_year(cyc_start_dates[nearest_time_index]),\n",
    "    datetime64_to_fractional_year(cyc_end_dates[nearest_time_index])), \n",
    "    xy=(-421e3,1009e3), xycoords='data', fontsize=14)\n",
    "\n",
    "\n",
    "# Panel B - Plot subsidence draining event\n",
    "# specified_date = datetime.date(2021, 2, 15)\n",
    "specified_date = datetime.date(2020, 2, 16)\n",
    "# specified_date = datetime.date(2015, 8, 17)\n",
    "\n",
    "gdf_subset[(gdf_subset['date'] == specified_date) & (gdf_subset['dh (m)'] > 0)].boundary.plot(ax=ax[1], color='blue')\n",
    "gdf_subset[(gdf_subset['date'] == specified_date) & (gdf_subset['dh (m)'] < 0)].boundary.plot(ax=ax[1], color='red')\n",
    "\n",
    "# Calculate the absolute difference between each time in the dataset and the specified time\n",
    "time_diff = np.abs(midcyc_dates - np.datetime64(specified_date))\n",
    "# Find the index of the minimum difference\n",
    "nearest_time_index = time_diff.argmin().item()\n",
    "if nearest_time_index <= 32:\n",
    "    dhdt = dataset1_subset.delta_h[nearest_time_index+1,:,:]-dataset1_subset.delta_h[nearest_time_index,:,:]\n",
    "elif nearest_time_index > 32:\n",
    "    # Subtract 33 from idx to start over with new dataset\n",
    "    dhdt = dataset2_subset.delta_h[(nearest_time_index-33)+1,:,:]-dataset2_subset.delta_h[(nearest_time_index-33),:,:]\n",
    "\n",
    "# Plot gridded height change data\n",
    "img = ax[1].imshow(dhdt, extent=[x_min-buffer, x_max+buffer, y_min-buffer, y_max+buffer], origin='lower', cmap='coolwarm_r', \n",
    "                   # norm=colors.CenteredNorm(),\n",
    "                   norm=divnorm)\n",
    "gpd.GeoSeries(buffered_poly).boundary.plot(ax=ax[1], color=ROI_color)\n",
    "\n",
    "ax[1].annotate('height change: {} to {}'.format(datetime64_to_fractional_year(cyc_start_dates[nearest_time_index]),\n",
    "    datetime64_to_fractional_year(cyc_end_dates[nearest_time_index])), \n",
    "    xy=(-421e3,1009e3), xycoords='data', fontsize=14)\n",
    "\n",
    "\n",
    "# Panel C - Plot outlines in aggregate vs. two past static delineations\n",
    "# Plot MOA imagery  \n",
    "ax[2].imshow(moa_highres_da_subset[0,:,:], cmap=\"gray\", clim=[14000, 17000], extent=[x_min-buffer, x_max+buffer, y_min-buffer, y_max+buffer])\n",
    "\n",
    "# Pick colormap and make continuous cmap discrete for evolving outlines\n",
    "colormap = 'plasma'\n",
    "continuous_cmap = matplotlib.colormaps[colormap]\n",
    "discrete_cmap = colors.ListedColormap(continuous_cmap(np.linspace(0, 1, len(midcyc_dates)-1)))\n",
    "\n",
    "# Norm to time variable\n",
    "norm = plt.Normalize(mdates.date2num(midcyc_dates[0]), \n",
    "                     mdates.date2num(midcyc_dates[-1]))\n",
    "\n",
    "# Use for loop to store each time slice as line segment to use in legend\n",
    "# And plot each outline in the geopandas dataframe and color by date\n",
    "lines = []  # list of lines to be used for the legend\n",
    "for idx, dt in enumerate(midcyc_dates, 0):\n",
    "    x = 1; y = 1\n",
    "    line, = ax[2].plot(x, y, color=discrete_cmap(norm(mdates.date2num(midcyc_dates[idx]))), linewidth=3)\n",
    "    lines.append(line)\n",
    "    \n",
    "    # Filter rows that match the current time slice\n",
    "    gdf_subset_dt = gdf_subset[gdf_subset['datetime'] == dt]\n",
    "\n",
    "    # Plotting the subset\n",
    "    gdf_subset_dt.plot(ax=ax[2], edgecolor=discrete_cmap(norm(mdates.date2num(midcyc_dates[idx]))), facecolor='none')\n",
    "\n",
    "    \n",
    "# All panels\n",
    "# Label axes\n",
    "ax[2].set_xlabel('x [km]', size=16)\n",
    "ax[1].set_ylabel('y [km]', size=16)\n",
    "\n",
    "# ax[0].annotate('A', xy=(-425e3,1049e3), xycoords='data', fontsize=30)\n",
    "# ax[1].annotate('B', xy=(-425e3,1049e3), xycoords='data', fontsize=30)\n",
    "# ax[2].annotate('C', xy=(-425e3,1049e3), xycoords='data', fontsize=30)\n",
    "ax[0].annotate('A', xy=(0.02, 0.9), xycoords='axes fraction', fontsize=30)\n",
    "ax[1].annotate('B', xy=(0.02, 0.9), xycoords='axes fraction', fontsize=30)\n",
    "ax[2].annotate('C', xy=(0.02, 0.9), xycoords='axes fraction', fontsize=30)\n",
    "\n",
    "# Create lines for legend\n",
    "S09_color = 'lightseagreen'\n",
    "SF18_color = 'teal'\n",
    "Smith2009 = plt.Line2D((0, 1), (0, 0), color=S09_color, linestyle=(0, (1, 2)), linewidth=3)\n",
    "SiegfriedFricker2018 = plt.Line2D((0, 1), (0, 0), color=SF18_color, linestyle=(0, (1, 1)), linewidth=3)\n",
    "ROI = plt.Line2D((0, 1), (0, 0), color=ROI_color, linestyle='solid', linewidth=3)\n",
    "uplift = plt.Line2D((0, 1), (0, 0), color='blue', linewidth=3)\n",
    "subsidence = plt.Line2D((0, 1), (0, 0), color='red', linewidth=3)\n",
    "\n",
    "# Create legends\n",
    "ax[0].legend([Smith2009, SiegfriedFricker2018, ROI, uplift], \n",
    "           ['static outline [10]',\n",
    "            'static outline [13]', \n",
    "            # 'evolving outline ({} m threshold)'.format(threshold)], \n",
    "            'evolving outline search limit [this study]',\n",
    "            'evolving outline - uplift [this study]'], \n",
    "             loc='upper right') \n",
    "\n",
    "ax[1].legend([subsidence],\n",
    "           ['evolving outline - subsidence [this study]'], \n",
    "             loc='upper right')\n",
    "\n",
    "legend = ax[2].legend([tuple(lines)], ['evolving outlines [this study]'],\n",
    "    handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper center')\n",
    "legend.get_frame().set_linewidth(0.0)\n",
    "ax[2].patch.set_alpha(1)\n",
    "\n",
    "# Create colorbar \n",
    "m = plt.cm.ScalarMappable(cmap=discrete_cmap)\n",
    "m.set_array(np.array([datetime64_to_fractional_year(date) for date in midcyc_dates[0:]]))\n",
    "cax = inset_axes(ax[2],\n",
    "                 width=\"100%\",\n",
    "                 height=\"2.5%\",\n",
    "                 loc=3,\n",
    "                 bbox_to_anchor=[0,-0.14,1,1],\n",
    "                 bbox_transform=ax[2].transAxes,\n",
    "                 borderpad=0,\n",
    "                 )\n",
    "cbar=fig.colorbar(m, ticks=np.array([2010,2012,2014,2016,2018,2020,2022]), \n",
    "             cax=cax, orientation='horizontal').set_label('evolving outline year', size=15)\n",
    "\n",
    "# Plot inset map\n",
    "axIns = ax[0].inset_axes([0.01, 0.01, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "axIns.set_aspect('equal')\n",
    "moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "axIns.axis('off')\n",
    "# # Plot black rectangle to indicate location\n",
    "# rect = Rectangle((x_min, y_min), (x_max-x_min), (y_max-y_min), fill=False, linewidth=2, color='k', zorder=3)\n",
    "# axIns.add_artist(rect)\n",
    "# Plot red star to indicate location\n",
    "axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "    linewidth=1, edgecolor='k', facecolor='r', s=100, zorder=3)\n",
    "\n",
    "# # Add annotation to the opposite side of the colorbar\n",
    "# cbar.ax.text(1.1, 0.5, 'CryoSat-2 era', va='center', ha='left', transform=cbar.ax.transAxes)\n",
    "\n",
    "for i in ax: \n",
    "    S09_outlines.boundary.plot(ax=i, edgecolor=S09_color, facecolor='none', linestyle=(0, (1, 2)), linewidth=3, alpha=1, zorder=0)\n",
    "    SF18_outlines.boundary.plot(ax=i, edgecolor=SF18_color, facecolor='none', linestyle=(0, (1, 1)), linewidth=3, alpha=1, zorder=0)\n",
    "    \n",
    "    # Change polar stereographic m to km\n",
    "    km_scale = 1e3\n",
    "    ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    i.xaxis.set_major_formatter(ticks_x)\n",
    "    ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    i.yaxis.set_major_formatter(ticks_y)\n",
    "    \n",
    "    # i.set(xlim=(x_min-buffer, x_max+buffer), ylim=(y_min-buffer, y_max+buffer))   \n",
    "    i.set(xlim=(x_min-buffer, x_max+buffer), ylim=(y_min-buffer, y_max+buffer))   \n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9029d96-c400-4346-abab-fd6ce96b5474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a382fc-3e70-4dd8-bb49-ef539b386c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b587357-145a-48d9-96b0-7aac94ccedbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe41da-fd2d-4890-a858-d6ac61e90535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I can't figure out why data_counts plot looks sparse when you plot at continental scale\n",
    "plt.close()\n",
    "for idx in range(145, len(lakes_gdf)):\n",
    "\n",
    "    # Define lake name and polygon\n",
    "    lake_name = lake_gdf.name.values[0]\n",
    "    lake_poly = lake_gdf.iloc[0].geometry\n",
    "\n",
    "    # # Create buffered polygons for various multiples of lake area to find which\n",
    "    # # best emcompasses the height change signals at previously identified lakes\n",
    "    buffered_poly_2x = muliple_area_buffer(lake_poly, 2)\n",
    "    # buffered_poly_3x = muliple_area_buffer(lake_poly, 3)\n",
    "    # buffered_poly_4x = muliple_area_buffer(lake_poly, 4)\n",
    "    # buffered_poly_5x = muliple_area_buffer(lake_poly, 5)\n",
    "\n",
    "    # Establish x_min, x_max, y_min, y_max\n",
    "    x_min, y_min, x_max, y_max = buffered_poly_2x.iloc[0].geometry.bounds\n",
    "    x_buffer = abs(x_max-x_min)*10\n",
    "    y_buffer = abs(y_max-y_min)*10\n",
    "\n",
    "    # Subsetting datasets\n",
    "    mask_x = (dataset1.x >= x_min - x_buffer) & (dataset1.x <= x_max + x_buffer)\n",
    "    mask_y = (dataset1.y >= y_min - y_buffer) & (dataset1.y <= y_max + y_buffer)\n",
    "    dataset1_subset = dataset1.where(mask_x & mask_y, drop=True)\n",
    "    mask_x = (dataset2.x >= x_min - x_buffer) & (dataset2.x <= x_max + x_buffer)\n",
    "    mask_y = (dataset2.y >= y_min - y_buffer) & (dataset2.y <= y_max + y_buffer)\n",
    "    dataset2_subset = dataset2.where(mask_x & mask_y, drop=True)\n",
    "\n",
    "    # Make output folders\n",
    "    create_folder(OUTPUT_DIR + '/{}'.format('plot_data_counts'))\n",
    "    create_folder(OUTPUT_DIR + '/plot_data_counts/{}'.format(lake_name))\n",
    "    print('Now creating/saving plots...')\n",
    "\n",
    "    # Create lines for legend\n",
    "    S09_color = 'cyan'\n",
    "    SF18_color  = 'darkcyan'\n",
    "    lakes_gdf_postSF18_color = 'deepskyblue'\n",
    "    Smith2009 = plt.Line2D((0, 1), (0, 0), color=S09_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "    SiegfriedFricker2018 = plt.Line2D((0, 1), (0, 0), color=SF18_color, linestyle=(0, (1, 2)), linewidth=2)\n",
    "    lakes_gdf_postSF18 = plt.Line2D((0, 1), (0, 0), color=lakes_gdf_postSF18_color, linestyle=(0, (1, 1)), linewidth=2)\n",
    "\n",
    "    # Calculate cycle-to-cycle dHeight at each cycle of the spliced data sets\n",
    "    for idx in range(33,34):#len(midcyc_dates)):\n",
    "        # For midcyc_dates indexes <= 32, use CryoSat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the CryoSat-2 era before ICESat-2 launch (2010-08-17 to 2018-08-17)\n",
    "        if idx <= 32:\n",
    "            count_subset = dataset1_subset['data_count'][idx,:,:]\n",
    "            count_subset.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "        # For midcyc_dates indexes > 32, use ICESat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the ICESat-2 era (2018-11-16 to most recently available data)\n",
    "        elif idx > 32:\n",
    "            # Subtract 33 from idx to start over at index zero with new dataset\n",
    "            count_subset = dataset2_subset['data_count'][(idx-33),:,:]\n",
    "            count_subset.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "        # Only plot arrays that have data in them; some time slices have no data because no CryoSat-2 SARIn coverage\n",
    "        # Returns True if there is at least one non-NaN value in data_stacked, and False if all values are NaN\n",
    "        # if np.any(~np.isnan(count_subset)):\n",
    "        # Create fig, ax\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        # Plot figure\n",
    "        img = ax.imshow(count_subset, extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer], \n",
    "            origin='lower', cmap='viridis')\n",
    "\n",
    "        buffered_poly_2x.boundary.plot(ax=ax, edgecolor='r', facecolor='none', linewidth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3de926-fc8f-4f81-bcc5-8d005970cb74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only plot arrays that have data in them; some time slices have no data because no CryoSat-2 SARIn coverage\n",
    "# Returns True if there is at least one non-NaN value in data_stacked, and False if all values are NaN\n",
    "# if np.any(~np.isnan(count_subset)):\n",
    "plt.close()\n",
    "# Create fig, ax\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "idx=44\n",
    "\n",
    "# Plot figure\n",
    "img = ax.imshow(dataset2['data_count'][(idx-33),:,:], \n",
    "    # extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer],\n",
    "    extent=[dataset2.x.min(), dataset2.x.max(), dataset2.y.min(), dataset2.y.max()], \n",
    "    origin='lower', \n",
    "    # cmap='viridis'\n",
    "    )\n",
    "\n",
    "buffered_poly_2x.boundary.plot(ax=ax, edgecolor='r', facecolor='none', linewidth=5)\n",
    "plt.show()\n",
    "Scripps_landice.boundary.plot(ax=ax, edgecolor='blue', facecolor='none', linewidth=0.5)\n",
    "\n",
    "# Add colorbar \n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "fig.colorbar(img, cax=cax).set_label('data counts', size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac66a1-541e-4dc1-8e1d-54ada1b4ea59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
