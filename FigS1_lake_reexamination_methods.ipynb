{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9a180750-d435-44ea-94f0-c96b9dbea099",
   "metadata": {
    "user_expressions": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Alter outline index to x/y coordinate mapping so that outlines appear a half pixel upward on y and rightward in the x\n",
    "# might want to make toy example to be sure\n",
    "\n",
    "# Investigate why algo says outlines are >95% within but have many that intersect the search_extent\n",
    "# Is this line the issue?\n",
    "                buffered_poly=get_buffered_poly(lake_gdf, selected_row['area_multiple_search_extent']),\n",
    "# Shoudl it be:\n",
    "                buffered_poly=get_buffered_poly(lake_gdf, selected_row['area_multiple_search_extent']+1),\n",
    "\n",
    "\n",
    "# Save discarded outlines to non-git output for plotting purposes? and for troubleshooting algo with matt\n",
    "\n",
    "# Change outline filtering function to only keep outlines if intersecting with another outline that is a temporal neighbor\n",
    "\n",
    "# Fig. S1 - Add something along the lines of code below to illustrate lake area/outline:\n",
    "# # Plot polygons in the GeoDataFrame\n",
    "# gdf.plot(ax=ax, color='lightblue', edgecolor='black', linewidth=1, label='Polygon Area')\n",
    "# gdf.boundary.plot(ax=ax, color='red', linewidth=2, label='Polygon Boundary')\n",
    "\n",
    "# Simplify subsetting code:\n",
    "# CS2_dh_sub = CS2_dh.sel(y=slice(y_max, y_min), x=slice(x_min, x_max))\n",
    "# ATL15_dh_sub = ATL15_dh.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "\n",
    "# Simplify codes to makes lines:\n",
    "# line_3_1 = mlines.Line2D([], [], color='red', linewidth=2, label='v3.1')\n",
    "\n",
    "# # Saved geojson's behave differently: \n",
    "# # evolving_outlines_gdfs have midcyc_datetime open as timestamp whereas \n",
    "# # compare_evolving_stationary_outlines_gdfs have midcyc_datetime open as string\n",
    "# evolving_outlines_gdf = gpd.read_file(os.path.join(os.getcwd(), 'output/lake_outlines/evolving_outlines/{}.geojson'.format(SF18_lake_gdf['name'].values[0])))\n",
    "# print(type(evolving_outlines_gdf['midcyc_datetime'][0]))\n",
    "# geom_calcs_df = pd.read_csv(os.path.join(\n",
    "#     os.getcwd(), 'output/lake_outlines/compare_evolving_stationary_outlines/{}.csv'.format(SF18_lake_gdf['name'].values[0])))\n",
    "# print(type(geom_calcs_df['midcyc_datetime'][0]))\n",
    "\n",
    "# Within plot_data_counts and plot_height_changes func's add handling of lakes with shortened CS2 time period\n",
    "\n",
    "# To extract_intersecting_polygons add while loop to do all polygons within\n",
    "\n",
    "# Fix issues with evolving_lakes_gdf \n",
    "# lake systems; lower beginning threshold for EAP lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d85c17-6d3e-4cc1-bd29-4a6fa7f7c7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c199d3a-7236-40c4-b1ed-33f946685e07",
   "metadata": {},
   "source": [
    "Code to do data analysis of re-examined active subglacial lakes and create Fig. S1 in Sauthoff and others, 202X, _Journal_.\n",
    "\n",
    "Written 2023-07-11 by W. Sauthoff (wsauthoff.github.io)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff75638-a801-40a3-bd59-7d6536dbc054",
   "metadata": {},
   "source": [
    "# Set up computing environment\n",
    "\n",
    "This code runs continental-scale operations on multiple datasets and requires a ~32 GB server or local memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67b2a28f-a575-4f1e-b1c9-f601327149aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openpyxl --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ffcf4c9-ea9f-4c3c-b28a-e5b7c358bd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad47cdb0-00aa-4268-8691-9e44b3bfda2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import datetime\n",
    "import earthaccess\n",
    "import fiona\n",
    "import gc\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from pyproj import CRS, Geod, Transformer\n",
    "import rioxarray\n",
    "from rioxarray.exceptions import NoDataInBounds\n",
    "from shapely.geometry import box, MultiPolygon, Point, Polygon\n",
    "from shapely.ops import unary_union\n",
    "from shapely.validation import make_valid\n",
    "# import shutil\n",
    "from skimage import measure\n",
    "import time\n",
    "import traceback\n",
    "import xarray as xr\n",
    "\n",
    "# Magic functions\n",
    "%matplotlib widget\n",
    "# %matplotlib inline\n",
    "\n",
    "# Define data directories dependent on home environment\n",
    "# Replace with your directory file paths\n",
    "if os.getenv('HOME') == '/home/jovyan':\n",
    "    DATA_DIR = '/home/jovyan/data'\n",
    "    OUTPUT_DIR = '/home/jovyan/1_evolving_lakes/output/FigS1_lake_reexamination_methods.ipynb'\n",
    "    OUTPUT_DIR_GIT = '/home/jovyan/1_evolving_lakes/Sauthoff-202X-evolving-lakes/output'\n",
    "\n",
    "# Define constants and coordinate transforms for the geodesic area calculation\n",
    "CRS_LL = \"EPSG:4326\" # wgs84 in lon,lat\n",
    "GEOD = CRS(CRS_LL).get_geod() # geod object for calculating geodesic area on defined ellipsoid\n",
    "CRS_XY = \"EPSG:3031\" # Antarctic Polar Stereographic in x, y\n",
    "XY_TO_LL = Transformer.from_crs(CRS_XY, CRS_LL, always_xy = True) # make coord transformer\n",
    "geod = Geod(ellps=\"WGS84\") # Create a Geod object for calculating area on the WGS84 ellipsoid\n",
    "\n",
    "# Change default font to increase font size\n",
    "plt.rcParams.update({'font.size': 8})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4adef9-57d4-4ab4-b066-ba4a4ebb38a5",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac48698c-9656-4c01-b62d-37c460a384e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_quarter_year(date):\n",
    "    \"\"\"Convert datetime64 to year.quarter.\"\"\"\n",
    "    if isinstance(date, np.datetime64):\n",
    "        date = pd.Timestamp(date)\n",
    "    \n",
    "    return date.year + (date.quarter - 1) * 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a93123a-6923-44f9-9498-3d8bef3017dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_area_buffer(polygon, area_multiple, precision=100):\n",
    "    '''\n",
    "    This function takes a polygon and returns a polygon with a buffer such that the area of the buffered polygon\n",
    "    is approximately the specified multiple the area of the original polygon.\n",
    "\n",
    "    Inputs\n",
    "    * param polygon: Shapely Polygon object\n",
    "    * param area_multiple: the multiple of the original polygon area you wish the buffered polygon to be\n",
    "    * param precision: Precision for the iterative process to find the buffer distance\n",
    "    * return: Buffered Polygon\n",
    "\n",
    "    # Example usage\n",
    "    # Define a simple square polygon\n",
    "    square = Polygon([(0, 0), (1, 0), (1, 1), (0, 1)])\n",
    "    # Apply the function to find the buffered polygon area and bounds\n",
    "    buffered_poly = multiple_area_buffer(square, 2)\n",
    "    '''\n",
    "    original_area = polygon.area\n",
    "    target_area = area_multiple * original_area\n",
    "    buffer_distance = 0\n",
    "    buffered_polygon = polygon\n",
    "\n",
    "    while buffered_polygon.area < target_area:\n",
    "        buffer_distance += precision\n",
    "        buffered_polygon = polygon.buffer(buffer_distance)\n",
    "    \n",
    "    return buffered_polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0968f459-d9c3-436f-911d-d9643aba8a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_gdf_by_folder_contents(gdf, folder_path, exclude=True):\n",
    "    '''\n",
    "    Filter the GeoDataFrame based on whether the 'name' is in the folder_path directories or files.\n",
    "    \n",
    "    Args:\n",
    "    gdf: GeoDataFrame to be filtered.\n",
    "    folder_path: Path to the directory containing files and/or subdirectories.\n",
    "    exclude: If True, excludes rows where the 'name' is in the folder_path directories or files.\n",
    "             If False, includes only rows where the 'name' is in the folder_path directories or files.\n",
    "\n",
    "    Returns:\n",
    "    GeoDataFrame filtered based on the presence of 'name' in folder_path.\n",
    "    '''\n",
    "    names_in_folder = {os.path.splitext(name)[0].lower().strip() for name in os.listdir(folder_path)}\n",
    "\n",
    "    # Filter without adding and then dropping a new column\n",
    "    gdf_filtered = gdf[gdf['name'].str.lower().str.strip().apply(lambda x: (x not in names_in_folder) if exclude else (x in names_in_folder))]\n",
    "\n",
    "    return gdf_filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abeb0f11-15ad-455d-807f-33dbc73a7e80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_data_counts(lake_gdf, dataset1, dataset2): \n",
    "    '''\n",
    "    Create planview plots of counts going into gridded ice-surface height change (dh/dt) products\n",
    "    \n",
    "    Inputs:\n",
    "    * lake_gdf: geodataframe of lake to be analyzed\n",
    "    * dataset1: dataset1 to be analyzed\n",
    "    * dataset2: dataset2 to be analyzed in conjunction with dataset1; currently configured to \n",
    "    splice CryoSat-2 and ICESat-2 eras\n",
    "    \n",
    "    Outputs:\n",
    "    * Sequence of planview data count visuals of lake around surrounding area\n",
    "    '''\n",
    "    # Define lake name and polygon\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "    \n",
    "    # Create buffered polygons for various multiples of lake area to find which\n",
    "    # best emcompasses the height change signals at previously identified lakes\n",
    "    search_extent_poly = multiple_area_buffer(lake_poly, 15)\n",
    "    \n",
    "    # Establish x_min, x_max, y_min, y_max\n",
    "    # x_min, y_min, x_max, y_max = search_extent_poly.iloc[0].geometry.bounds\n",
    "    x_min, y_min, x_max, y_max = search_extent_poly.bounds\n",
    "    x_buffer = abs(x_max-x_min)*0.5\n",
    "    y_buffer = abs(y_max-y_min)*0.5\n",
    "\n",
    "    # Subsetting datasets\n",
    "    mask_x = (dataset1.x >= x_min - x_buffer) & (dataset1.x <= x_max + x_buffer)\n",
    "    mask_y = (dataset1.y >= y_min - y_buffer) & (dataset1.y <= y_max + y_buffer)\n",
    "    dataset1_subset = dataset1.where(mask_x & mask_y, drop=True)\n",
    "    mask_x = (dataset2.x >= x_min - x_buffer) & (dataset2.x <= x_max + x_buffer)\n",
    "    mask_y = (dataset2.y >= y_min - y_buffer) & (dataset2.y <= y_max + y_buffer)\n",
    "    dataset2_subset = dataset2.where(mask_x & mask_y, drop=True)\n",
    "    \n",
    "    # Make output folders\n",
    "    os.makedirs(OUTPUT_DIR + '/{}'.format('plot_data_counts'), exist_ok=True)\n",
    "    os.makedirs(OUTPUT_DIR + '/plot_data_counts/{}'.format(lake_name), exist_ok=True)\n",
    "    print('Now creating/saving plots...')\n",
    "\n",
    "    # Create lines for legend\n",
    "    stationary_lakes_color = 'turquoise'\n",
    "    stationary_lakes_line = plt.Line2D([], [], color=stationary_lakes_color, linestyle=(0, (1, 1)), linewidth=2)\n",
    "    search_extent_line = plt.Line2D([], [], color='red', linestyle='solid', linewidth=2)\n",
    "\n",
    "    # Calculate cycle-to-cycle dHeight at each cycle of the spliced data sets\n",
    "    for idx in range(len(mid_cyc_dates)):\n",
    "        # For mid_cyc_dates indexes <= 32, use CryoSat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the CryoSat-2 era before ICESat-2 launch (2010-08-17 to 2018-08-17)\n",
    "        if idx <= 32:\n",
    "            count_subset = dataset1_subset['data_count'][idx,:,:]\n",
    "            count_subset.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "        # For mid_cyc_dates indexes > 32, use ICESat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the ICESat-2 era (2018-11-16 to most recently available data)\n",
    "        elif idx > 32:\n",
    "            # Subtract 33 from idx to start over at index zero with new dataset\n",
    "            count_subset = dataset2_subset['data_count'][(idx-33),:,:]\n",
    "            count_subset.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "            \n",
    "        # Only plot arrays that have data in them; some time slices have no data because no CryoSat-2 SARIn coverage\n",
    "        # Returns True if there is at least one non-NaN value in data_stacked, and False if all values are NaN\n",
    "        if np.any(~np.isnan(count_subset)):\n",
    "            # Create fig, ax\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "            # Plot figure\n",
    "            img = ax.imshow(count_subset, extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer], \n",
    "                origin='lower', cmap='viridis')\n",
    "\n",
    "            # Plot buffered polygons\n",
    "            gpd.GeoDataFrame(geometry=[search_extent_poly]).boundary.plot(ax=ax, edgecolor='r', facecolor='none', linewidth=2)\n",
    "            \n",
    "            # Overlay published active lake outlines for visual comparison and grounding line\n",
    "            stationary_lakes_gdf.boundary.plot(ax=ax, edgecolor=stationary_lakes_color, linestyle=(0, (1, 1)), linewidth=2)          \n",
    "            \n",
    "            # Change polar stereographic m to km\n",
    "            km_scale = 1e3\n",
    "            ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "            ax.xaxis.set_major_formatter(ticks_x)\n",
    "            ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "            ax.yaxis.set_major_formatter(ticks_y)  \n",
    "\n",
    "            # Label axes, set limits, and set title\n",
    "            ax.set_xlabel('x [km]', size=15)\n",
    "            ax.set_ylabel('y [km]', size=15) \n",
    "            ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "            # Plot inset map\n",
    "            axIns = ax.inset_axes([0.01, 0.01, 0.2, 0.2]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "            axIns.set_aspect('equal')\n",
    "            moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "            moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "            axIns.axis('off')\n",
    "\n",
    "            # Plot red star to indicate location\n",
    "            axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "                linewidth=1, edgecolor='k', facecolor='r', s=100, zorder=3)\n",
    "\n",
    "            # Add colorbar \n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "            fig.colorbar(img, cax=cax).set_label('data counts', size=15)\n",
    "\n",
    "            # Add legend\n",
    "            ax.legend([Smith2009, \n",
    "                       SiegfriedFricker2018, \n",
    "                       stationary_lakes_line,\n",
    "                       search_extent_line                       \n",
    "                      ],\n",
    "                ['stationary outline (S09)', \n",
    "                 'stationary outline (SF18)', \n",
    "                 'other',\n",
    "                 'search extent'\n",
    "                ], \n",
    "                loc='upper center')\n",
    "            \n",
    "            # Set a title for the axes\n",
    "            ax.set_title('Data counts from from {} to {}'.format(cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)))\n",
    "            \n",
    "            # Save and close fig\n",
    "            plt.savefig(OUTPUT_DIR + \n",
    "                '/plot_data_counts/{}/plot_data_counts_{}_{}-{}.png'\n",
    "                .format(lake_name, lake_name, cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "    print('Complete')\n",
    "        \n",
    "# # Example usage\n",
    "# plot_data_counts(lake_gdf=lake_gdf, dataset1=CS2_Smith2017, dataset2=ATL15_dh): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9970b6cb-b3b0-494c-b6c6-290679b5194d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_height_changes(lake_gdf, dataset1, dataset2): \n",
    "    '''\n",
    "    Create planview plots of ice surface height changes (dh/dt)\n",
    "    \n",
    "    Inputs:\n",
    "    * lake_gdf: geodataframe of lake to be analyzed\n",
    "    * dataset1: dataset1 to be analyzed\n",
    "    * dataset2: dataset2 to be analyzed in conjunction with dataset1; currently configured to \n",
    "    splice CryoSat-2 and ICESat-2 eras\n",
    "    \n",
    "    Outputs: \n",
    "    * Sequence of planview delta height visuals of lake around surrounding area\n",
    "    '''\n",
    "    # Define lake name and polygon\n",
    "    lake_name = lake_gdf.name.values[0]\n",
    "    stationary_outline = lake_gdf.iloc[0].geometry\n",
    "    \n",
    "    # Create buffered polygons for various multiples of lake area to find which\n",
    "    # best emcompasses the height change signals at previously identified lakes\n",
    "    search_extent_poly = multiple_area_buffer(stationary_outline, 15)\n",
    "\n",
    "    # Establish x_min, x_max, y_min, y_max\n",
    "    # x_min, y_min, x_max, y_max = search_extent_poly.iloc[0].geometry.bounds\n",
    "    x_min, y_min, x_max, y_max = search_extent_poly.bounds\n",
    "    x_buffer = abs(x_max-x_min)*0.2\n",
    "    y_buffer = abs(y_max-y_min)*0.2\n",
    "    \n",
    "    # Subsetting datasets\n",
    "    mask_x = (dataset1.x >= x_min - x_buffer) & (dataset1.x <= x_max + x_buffer)\n",
    "    mask_y = (dataset1.y >= y_min - y_buffer) & (dataset1.y <= y_max + y_buffer)\n",
    "    dataset1_subset = dataset1.where(mask_x & mask_y, drop=True)\n",
    "    mask_x = (dataset2.x >= x_min - x_buffer) & (dataset2.x <= x_max + x_buffer)\n",
    "    mask_y = (dataset2.y >= y_min - y_buffer) & (dataset2.y <= y_max + y_buffer)\n",
    "    dataset2_subset = dataset2.where(mask_x & mask_y, drop=True)\n",
    "    \n",
    "    # Find magnitude of ice surface deformation in bounding box to create appropriate color map scale\n",
    "    # Create empty lists to store data\n",
    "    height_anom_pos = []\n",
    "    height_anom_neg = []\n",
    "    for idx in range(len(mid_cyc_dates)):\n",
    "        if idx <= 32:\n",
    "            dhdt_subset = dataset1_subset.delta_h[idx+1,:,:]-dataset1_subset.delta_h[idx,:,:]\n",
    "        elif idx > 32:\n",
    "            # Subtract 33 from idx to start over at index zero with new dataset\n",
    "            dhdt_subset = dataset2_subset.delta_h[(idx-33)+1,:,:]-dataset2_subset.delta_h[(idx-33),:,:]\n",
    "        if np.any(~np.isnan(dhdt_subset)):       \n",
    "            pos = np.nanmax(dhdt_subset)\n",
    "            neg = np.nanmin(dhdt_subset)\n",
    "            height_anom_pos += [pos]\n",
    "            height_anom_neg += [neg]\n",
    "    \n",
    "    # Store max pos/neg height anomalies from all cycles to create colorbar bounds later\n",
    "    max_height_anom_pos = max(height_anom_pos)\n",
    "    max_height_anom_neg = min(height_anom_neg)\n",
    "    \n",
    "    # Establish diverging colorbar\n",
    "    divnorm=colors.TwoSlopeNorm(vmin=max_height_anom_neg, vcenter=0., vmax=max_height_anom_pos)\n",
    "   \n",
    "    # Make output folders\n",
    "    os.makedirs(OUTPUT_DIR + '/{}'.format('plot_height_changes'), exist_ok=True)\n",
    "    os.makedirs(OUTPUT_DIR + '/plot_height_changes/{}'.format(lake_name), exist_ok=True)\n",
    "    print('Now creating/saving plots...')\n",
    "\n",
    "    # Create lines for legend\n",
    "    stationary_lakes_color = 'turquoise'\n",
    "    stationary_lakes_line = plt.Line2D([], [], color=stationary_lakes_color, linestyle=(0, (1, 1)), linewidth=2)\n",
    "    search_extent_line = plt.Line2D([], [], color='red', linestyle='solid', linewidth=2)\n",
    "\n",
    "    # Calculate cycle-to-cycle dHeight at each cycle of the spliced data sets\n",
    "    for idx in range(len(mid_cyc_dates)):\n",
    "        # For mid_cyc_dates indexes <= 32, use CryoSat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the CryoSat-2 era before ICESat-2 era (2010-08-17 to 2018-08-17)\n",
    "        if idx <= 32:\n",
    "            dhdt_subset = dataset1_subset.delta_h[idx+1,:,:]-dataset1_subset.delta_h[idx,:,:]\n",
    "            # dhdt_subset.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "        # For mid_cyc_dates indexes > 32, use ICESat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the ICESat-2 era (2018-11-16 to most recently available data)\n",
    "        elif idx > 32:\n",
    "            # Subtract 33 from idx to reset idx to zero for new dataset\n",
    "            dhdt_subset = dataset2_subset.delta_h[(idx-33)+1,:,:]-dataset2_subset.delta_h[(idx-33),:,:]\n",
    "            # dhdt_subset.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "        # Only plot arrays that have data in them; some time slices have no data because no CryoSat-2 SARIn coverage\n",
    "        # Returns True if there is at least one non-NaN value in data_stacked, and False if all values are NaN\n",
    "        if np.any(~np.isnan(dhdt_subset)):\n",
    "            # Create fig, ax\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "            # Plot figure\n",
    "            img = ax.imshow(dhdt_subset, extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer],\n",
    "                origin='lower', cmap='coolwarm_r', \n",
    "                norm=divnorm)\n",
    "\n",
    "            # Plot buffered polygons\n",
    "            gpd.GeoDataFrame(geometry=[search_extent_poly]).boundary.plot(ax=ax, edgecolor='red', facecolor='none', linewidth=2)\n",
    "            \n",
    "            # Overlay published active lake outlines for visual comparison and grounding line\n",
    "            S09_outlines.boundary.plot(ax=ax, edgecolor=S09_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "            SF18_outlines.boundary.plot(ax=ax, edgecolor=SF18_color, linestyle=(0, (1, 2)), linewidth=2)\n",
    "            stationary_lakes_gdf_postSF18.boundary.plot(ax=ax, edgecolor=stationary_lakes_color, linestyle=(0, (1, 1)), linewidth=2)          \n",
    "            \n",
    "            # Change polar stereographic m to km\n",
    "            km_scale = 1e3\n",
    "            ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "            ax.xaxis.set_major_formatter(ticks_x)\n",
    "            ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "            ax.yaxis.set_major_formatter(ticks_y)  \n",
    "\n",
    "            # Label axes, set limits, and set title\n",
    "            ax.set_xlabel('x [km]', size=15)\n",
    "            ax.set_ylabel('y [km]', size=15) \n",
    "            ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "            # Plot inset map\n",
    "            axIns = ax.inset_axes([0.01, 0.01, 0.2, 0.2]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "            axIns.set_aspect('equal')\n",
    "            moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "            moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "            axIns.axis('off')\n",
    "\n",
    "            # Plot red star to indicate location\n",
    "            axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "                linewidth=1, edgecolor='k', facecolor='r', s=100, zorder=3)\n",
    "\n",
    "            # Add colorbar \n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "            fig.colorbar(img, cax=cax).set_label('height change (dh) [m]', size=15)\n",
    "\n",
    "            # Add legend\n",
    "            ax.legend([stationary_lakes_line,\n",
    "                       search_extent_line\n",
    "                      ],\n",
    "                ['stationary outline',\n",
    "                 'search extent'\n",
    "                ], \n",
    "                loc='upper center')\n",
    "            \n",
    "            # Set a title for the axes\n",
    "            ax.set_title('Height change from from {} to {}'.format(cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)))\n",
    "            \n",
    "            # Save and close fig\n",
    "            plt.savefig(OUTPUT_DIR + \n",
    "                '/plot_height_changes/{}/plot_height_changes_{}_{}-{}.png'\n",
    "                .format(lake_name, lake_name, cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "    print('Complete')\n",
    "        \n",
    "# # Example usage\n",
    "# plot_height_changes(lake_gdf=lake_gdf, dataset1=CS2_Smith2017, dataset2=ATL15_dh): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57434eae-877a-4332-979b-fb76c3919fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_search_extents_and_levels(stationary_lakes_gdf):\n",
    "    '''\n",
    "    Analyze and determine the search extents and level increments for stationary lakes using evolving outlines.\n",
    "\n",
    "    This function iterates through each lake in the given GeoDataFrame, `stationary_lakes_gdf`, to identify the\n",
    "    appropriate search extents and level increments for analyzing lake dynamics. The process involves checking\n",
    "    various area multiples and incrementing levels until a target fraction of the evolving outlines falls within\n",
    "    the search extent boundary.\n",
    "\n",
    "    Parameters:\n",
    "    stationary_lakes_gdf (GeoDataFrame): A GeoDataFrame containing stationary lakes with relevant attributes such as\n",
    "                                         'CS2_SARIn_time_period' and 'geometry'. Each row represents a different lake.\n",
    "\n",
    "    Returns:\n",
    "    None: The results are saved as CSV files in the 'output/search_extents_levels/' directory with filenames corresponding\n",
    "          to the lake names.\n",
    "\n",
    "    Process:\n",
    "    1. Iterate through each lake in the GeoDataFrame.\n",
    "    2. For each lake, determine the appropriate temporal subset of the CryoSat-2 data set to use based on the 'CS2_SARIn_time_period'.\n",
    "    3. Initialize a DataFrame to store results for search extents and levels.\n",
    "    4. For each area multiple (from 2 to 15), reset the level and level increment variables.\n",
    "    5. Calculate the evolving outlines for the current level and area multiple search extent.\n",
    "    6. Buffer the stationary lake outline by the current area multiple search extent.\n",
    "    7. Calculate the fraction of evolving outlines within the area multiple search extent.\n",
    "    8. Store results if the within fraction exceeds the target within percentage; otherwise increase the level by the specified increment.\n",
    "    9. After trying all the search extents, save the results for each lake as a CSV file in the specified directory.\n",
    "    10. Clear the output for the next iteration.\n",
    "    \n",
    "    Example:\n",
    "    >>> stationary_lakes_gdf = gpd.read_file('path_to_stationary_lakes.geojson')\n",
    "    >>> find_search_extents_and_levels(stationary_lakes_gdf)\n",
    "    '''\n",
    "    for idx, row in stationary_lakes_gdf.iterrows():\n",
    "\n",
    "        # Define parameters of search extents and levels\n",
    "        area_multiple_search_extents = range(2, 3)  # From 2 to 15 inclusive\n",
    "        initial_level = 0.01\n",
    "        level_increment = 0.01\n",
    "        within_fraction_target = 0.95\n",
    "   \n",
    "        # Assign dataset; during CS2 era, assign based on SARIn coverage time period\n",
    "        CS2_SARIn_time_period = stationary_lakes_gdf['CS2_SARIn_time_period'][idx]\n",
    "        dataset1 = None\n",
    "        dataset1_doi = None\n",
    "        \n",
    "        if not pd.isna(CS2_SARIn_time_period) and CS2_SARIn_time_period != '<NA>':\n",
    "            if CS2_SARIn_time_period == '2013.75-2018.75':\n",
    "                dataset1 = CS2_Smith2017.sel(time=slice(np.datetime64('2013-10-01T22:30:00.000000000'), None))\n",
    "                dataset1_doi = dataset1.attrs.get('identifier_product_DOI', 'Unknown')\n",
    "            elif CS2_SARIn_time_period == '2010.5-2018.75':\n",
    "                dataset1 = CS2_Smith2017\n",
    "                dataset1_doi = dataset1.attrs.get('identifier_product_DOI', 'Unknown')\n",
    "                \n",
    "        dataset2 = ATL15_dh\n",
    "        dataset2_doi = dataset2.attrs.get('identifier_product_DOI', 'Unknown')\n",
    "\n",
    "        # Select lake\n",
    "        lake_gdf = stationary_lakes_gdf.loc[idx]\n",
    "        print('Finding optimal levels at search extents for', lake_gdf['name'])\n",
    "    \n",
    "        # Initialize DataFrame to store results\n",
    "        search_extents_levels_df = pd.DataFrame(columns=['area_multiple_search_extent', 'level', 'dataset_dois'])\n",
    "    \n",
    "        for area_multiple_search_extent in area_multiple_search_extents:\n",
    "            # Mask data sets\n",
    "            lake_poly = lake_gdf['geometry']\n",
    "            buffered_poly = multiple_area_buffer(lake_poly, area_multiple_search_extent+1)\n",
    "            x_min, y_min, x_max, y_max = buffered_poly.bounds\n",
    "            \n",
    "            dataset1_masked = None\n",
    "            if dataset1 is not None:\n",
    "                dataset1_buffered_poly_sub = dataset1.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "                dataset1_mask = np.array([[buffered_poly.contains(Point(x, y)) for x in dataset1_buffered_poly_sub['x'].values] for y in dataset1_buffered_poly_sub['y'].values])\n",
    "                dataset1_mask_da = xr.DataArray(dataset1_mask, coords=[dataset1_buffered_poly_sub.y, dataset1_buffered_poly_sub.x], dims=[\"y\", \"x\"])\n",
    "                dataset1_masked = dataset1.where(dataset1_mask_da, drop=True)\n",
    "            \n",
    "            dataset2_buffered_poly_sub = dataset2.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "            dataset2_mask = np.array([[buffered_poly.contains(Point(x, y)) for x in dataset2_buffered_poly_sub['x'].values] for y in dataset2_buffered_poly_sub['y'].values])\n",
    "            dataset2_mask_da = xr.DataArray(dataset2_mask, coords=[dataset2_buffered_poly_sub.y, dataset2_buffered_poly_sub.x], dims=[\"y\", \"x\"])\n",
    "            dataset2_masked = dataset2.where(dataset2_mask_da, drop=True)\n",
    "\n",
    "            # Reset level and within_fraction for each area_multiple_search_extent iteration\n",
    "            level = initial_level\n",
    "            within_fraction = 0.0\n",
    "\n",
    "            while within_fraction < within_fraction_target and level <= 2.0:\n",
    "                # Find evolving outlines\n",
    "                outlines_gdf = find_evolving_outlines(\n",
    "                    lake_gdf=lake_gdf, \n",
    "                    area_multiple_search_extent=area_multiple_search_extent, \n",
    "                    level=level, \n",
    "                    dataset1_masked=dataset1_masked,\n",
    "                    dataset2_masked=dataset2_masked,\n",
    "                    buffered_poly=buffered_poly,\n",
    "                    plot=False\n",
    "                )\n",
    "    \n",
    "                # Define lake polygon and buffered geodataframe as before\n",
    "                stationary_outline = lake_gdf['geometry']\n",
    "                buffered_stationary_outline = multiple_area_buffer(polygon=stationary_outline, \n",
    "                    area_multiple=area_multiple_search_extent)\n",
    "                buffered_lake_gdf = gpd.GeoDataFrame(geometry=gpd.GeoSeries([buffered_stationary_outline]), crs=3031)\n",
    "\n",
    "                # Ensure geometries are valid and not empty\n",
    "                outlines_gdf = outlines_gdf[outlines_gdf.is_valid & ~outlines_gdf.is_empty]\n",
    "                buffered_lake_gdf = buffered_lake_gdf[buffered_lake_gdf.is_valid & ~buffered_lake_gdf.is_empty]\n",
    "\n",
    "                # Convert to Shapely object before spatial join\n",
    "                outlines_gdf['geometry'] = outlines_gdf['geometry'].apply(lambda geom: geom if isinstance(geom, Polygon) else Polygon())\n",
    "                buffered_lake_gdf['geometry'] = buffered_lake_gdf['geometry'].apply(lambda geom: geom if isinstance(geom, Polygon) else Polygon())\n",
    "\n",
    "                # Check which evolving outlines are within or overlap\n",
    "                within = gpd.sjoin(outlines_gdf, buffered_lake_gdf, predicate='within')\n",
    "                overlaps = gpd.sjoin(outlines_gdf, buffered_lake_gdf, predicate='overlaps')\n",
    "    \n",
    "                # Calculate within_fraction\n",
    "                if (len(within) + len(overlaps)) > 0:\n",
    "                    within_fraction = np.round((len(within) / (len(within) + len(overlaps))), 2)\n",
    "                else:\n",
    "                    print('No outlines found at this level')\n",
    "                    break\n",
    "    \n",
    "                print(f\"Extent: {area_multiple_search_extent}, Level: {level}, Within: {round(within_fraction*100)}%\")\n",
    "\n",
    "                # Once within_fraction is greater specified percent\n",
    "                if within_fraction >= within_fraction_target:\n",
    "            \n",
    "                    # Clean up outlines to remove any that are off-lake\n",
    "                    filtered_gdf = extract_intersecting_polygons_recursive(outlines_gdf, lake_gdf['geometry'])\n",
    "                        \n",
    "                    # Save results and plot\n",
    "                    if not filtered_gdf.empty:\n",
    "                        # Prepare dataset DOIs\n",
    "                        dois = [doi for doi in [dataset1_doi, dataset2_doi] if doi is not None]\n",
    "                        dois_str = ', '.join(dois)\n",
    "                        \n",
    "                        # Create new row\n",
    "                        new_row = pd.DataFrame({\n",
    "                            'area_multiple_search_extent': [area_multiple_search_extent],\n",
    "                            'level': [level],\n",
    "                            'within_percent': [within_fraction * 100],\n",
    "                            'dataset_dois': [dois_str]\n",
    "                        })\n",
    "                        \n",
    "                        # Check if new_row has any non-NA values before concatenating\n",
    "                        if not new_row.isna().all().all():\n",
    "                            search_extents_levels_df = pd.concat(\n",
    "                                [df for df in [search_extents_levels_df, new_row] if not df.empty],\n",
    "                                ignore_index=True)\n",
    "                            \n",
    "                    break  # Exit loop if condition is met\n",
    "\n",
    "                # Increment level if within_fraction not met\n",
    "                level += level_increment\n",
    "                level = np.round(level, 2)\n",
    "\n",
    "                # Clear memory\n",
    "                del outlines_gdf, buffered_lake_gdf, within, overlaps\n",
    "                gc.collect()\n",
    "    \n",
    "        # Store the DataFrame as geojson file for later use only if it's not empty\n",
    "        if not search_extents_levels_df.empty:\n",
    "            search_extents_levels_df.to_csv(OUTPUT_DIR + '/search_extents_levels/{}.csv'.format(lake_gdf['name']), index=False)\n",
    "        else:\n",
    "            # Write no outlines found file\n",
    "            filepath = OUTPUT_DIR + '/search_extents_levels/{}.txt'.format(lake_gdf['name'])\n",
    "            write_no_outlines(filepath)\n",
    "            filepath = OUTPUT_DIR_GIT + '/lake_outlines/evolving_outlines/{}.txt'.format(lake_gdf['name'])\n",
    "            write_no_outlines(filepath)\n",
    "            \n",
    "        # Clear memory\n",
    "        del dataset1, dataset2\n",
    "        gc.collect()\n",
    "    \n",
    "        # Clear the output of each index\n",
    "        clear_output(wait=True)\n",
    "\n",
    "def write_no_outlines(filepath):\n",
    "    \"\"\"Write file indicating no outlines found\"\"\"\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write(\"There are no evolving outlines for this lake.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e4b51f-de2c-46a5-ac71-78654df2f47d",
   "metadata": {},
   "source": [
    "## find_search_extents_and_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56ff4a18-e7d5-4626-b4e7-e7dfd0f6f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_search_extents_and_levels(stationary_lakes_gdf):\n",
    "    '''\n",
    "    Analyze and determine the search extents and level increments for stationary lakes using evolving outlines.\n",
    "\n",
    "    This function iterates through each lake in the given GeoDataFrame, `stationary_lakes_gdf`, to identify the\n",
    "    appropriate search extents and level increments for analyzing lake dynamics. The process involves checking\n",
    "    various area multiples and incrementing levels until a target fraction of the evolving outlines falls within\n",
    "    the search extent boundary.\n",
    "\n",
    "    Parameters:\n",
    "    stationary_lakes_gdf (GeoDataFrame): A GeoDataFrame containing stationary lakes with relevant attributes such as\n",
    "                                         'CS2_SARIn_time_period' and 'geometry'. Each row represents a different lake.\n",
    "\n",
    "    Returns:\n",
    "    None: The results are saved as CSV files in the 'output/search_extents_levels/' directory with filenames corresponding\n",
    "          to the lake names.\n",
    "\n",
    "    Process:\n",
    "    1. Iterate through each lake in the GeoDataFrame.\n",
    "    2. For each lake, determine the appropriate temporal subset of the CryoSat-2 data set to use based on the 'CS2_SARIn_time_period'.\n",
    "    3. Initialize a DataFrame to store results for search extents and levels.\n",
    "    4. For each area multiple (from 2 to 15), reset the level and level increment variables.\n",
    "    5. Calculate the evolving outlines for the current level and area multiple search extent.\n",
    "    6. Buffer the stationary lake outline by the current area multiple search extent.\n",
    "    7. Calculate the fraction of evolving outlines within the area multiple search extent.\n",
    "    8. Store results if the within fraction exceeds the target within percentage; otherwise increase the level by the specified increment.\n",
    "    9. After trying all the search extents, save the results for each lake as a CSV file in the specified directory.\n",
    "    10. Clear the output for the next iteration.\n",
    "    \n",
    "    Example:\n",
    "    >>> stationary_lakes_gdf = gpd.read_file('path_to_stationary_lakes.geojson')\n",
    "    >>> find_search_extents_and_levels(stationary_lakes_gdf)\n",
    "    '''\n",
    "def find_search_extents_and_levels(stationary_lakes_gdf):\n",
    "    '''\n",
    "    Analyze and determine search extents and level increments with proper validation.\n",
    "    '''\n",
    "    for idx, row in stationary_lakes_gdf.iterrows():\n",
    "        # Define parameters\n",
    "        area_multiple_search_extents = range(2, 16)  # From 2 to 15 inclusive\n",
    "        initial_level = 0.01\n",
    "        level_increment = 0.01\n",
    "        within_fraction_target = 0.95\n",
    "   \n",
    "        # Assign dataset\n",
    "        CS2_SARIn_time_period = stationary_lakes_gdf['CS2_SARIn_time_period'][idx]\n",
    "        dataset1 = None\n",
    "        dataset1_doi = None\n",
    "        \n",
    "        if not pd.isna(CS2_SARIn_time_period) and CS2_SARIn_time_period != '<NA>':\n",
    "            if CS2_SARIn_time_period == '2013.75-2018.75':\n",
    "                dataset1 = CS2_Smith2017.sel(time=slice(np.datetime64('2013-10-01T22:30:00.000000000'), None))\n",
    "                dataset1_doi = dataset1.attrs.get('identifier_product_DOI', 'Unknown')\n",
    "            elif CS2_SARIn_time_period == '2010.5-2018.75':\n",
    "                dataset1 = CS2_Smith2017\n",
    "                dataset1_doi = dataset1.attrs.get('identifier_product_DOI', 'Unknown')\n",
    "                \n",
    "        dataset2 = ATL15_dh\n",
    "        dataset2_doi = dataset2.attrs.get('identifier_product_DOI', 'Unknown')\n",
    "\n",
    "        # Select lake\n",
    "        lake_gdf = stationary_lakes_gdf.loc[idx]\n",
    "        print('Finding optimal levels at search extents for', lake_gdf['name'])\n",
    "    \n",
    "        # Initialize DataFrame\n",
    "        search_extents_levels_df = pd.DataFrame(\n",
    "            columns=['area_multiple_search_extent', 'level', 'within_percent', 'dataset_dois'])\n",
    "    \n",
    "        try:\n",
    "            for area_multiple_search_extent in area_multiple_search_extents:\n",
    "                try:\n",
    "                    # Mask datasets\n",
    "                    lake_poly = lake_gdf['geometry']\n",
    "                    buffered_poly = multiple_area_buffer(lake_poly, area_multiple_search_extent+1)\n",
    "                    x_min, y_min, x_max, y_max = buffered_poly.bounds\n",
    "                    \n",
    "                    dataset1_masked = None\n",
    "                    if dataset1 is not None:\n",
    "                        dataset1_buffered_poly_sub = dataset1.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "                        dataset1_mask = np.array([[buffered_poly.contains(Point(x, y)) \n",
    "                                                for x in dataset1_buffered_poly_sub['x'].values] \n",
    "                                                for y in dataset1_buffered_poly_sub['y'].values])\n",
    "                        dataset1_mask_da = xr.DataArray(dataset1_mask, \n",
    "                                                    coords=[dataset1_buffered_poly_sub.y, dataset1_buffered_poly_sub.x], \n",
    "                                                    dims=[\"y\", \"x\"])\n",
    "                        dataset1_masked = dataset1.where(dataset1_mask_da, drop=True)\n",
    "                    \n",
    "                    dataset2_buffered_poly_sub = dataset2.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "                    dataset2_mask = np.array([[buffered_poly.contains(Point(x, y)) \n",
    "                                            for x in dataset2_buffered_poly_sub['x'].values] \n",
    "                                            for y in dataset2_buffered_poly_sub['y'].values])\n",
    "                    dataset2_mask_da = xr.DataArray(dataset2_mask, \n",
    "                                                coords=[dataset2_buffered_poly_sub.y, dataset2_buffered_poly_sub.x], \n",
    "                                                dims=[\"y\", \"x\"])\n",
    "                    dataset2_masked = dataset2.where(dataset2_mask_da, drop=True)\n",
    "\n",
    "                    # Reset level and within_fraction\n",
    "                    level = initial_level\n",
    "                    within_fraction = 0.0\n",
    "\n",
    "                    while within_fraction < within_fraction_target and level <= 2.0:\n",
    "                        try:\n",
    "                            # Find evolving outlines\n",
    "                            outlines_gdf = find_evolving_outlines(\n",
    "                                lake_gdf=lake_gdf, \n",
    "                                area_multiple_search_extent=area_multiple_search_extent, \n",
    "                                level=level, \n",
    "                                dataset1_masked=dataset1_masked,\n",
    "                                dataset2_masked=dataset2_masked,\n",
    "                                buffered_poly=buffered_poly,\n",
    "                                plot=False\n",
    "                            )\n",
    "                            \n",
    "                            # Check if outlines_gdf has entries\n",
    "                            if outlines_gdf is None or outlines_gdf.empty:\n",
    "                                print(f'No outlines found at level:{level} m')\n",
    "                                break  # Break this loop iteration to abort finding outlines at this search_extent and try next\n",
    "        \n",
    "                            # Process geometries\n",
    "                            stationary_outline = lake_gdf['geometry']\n",
    "                            buffered_stationary_outline = multiple_area_buffer(\n",
    "                                polygon=stationary_outline, \n",
    "                                area_multiple=area_multiple_search_extent)\n",
    "                            buffered_lake_gdf = gpd.GeoDataFrame(\n",
    "                                geometry=gpd.GeoSeries([buffered_stationary_outline]), \n",
    "                                crs=3031)\n",
    "\n",
    "                            # Validate geometries with proper copying\n",
    "                            valid_outlines = outlines_gdf.loc[outlines_gdf.is_valid & ~outlines_gdf.is_empty].copy()\n",
    "                            valid_buffer = buffered_lake_gdf.loc[buffered_lake_gdf.is_valid & ~buffered_lake_gdf.is_empty].copy()\n",
    "                            \n",
    "                            if valid_outlines.empty or valid_buffer.empty:\n",
    "                                print(f'No valid outlines found at level:{level} m')\n",
    "                                break\n",
    "\n",
    "                            # Convert geometries using .loc\n",
    "                            valid_outlines.loc[:, 'geometry'] = valid_outlines['geometry'].apply(\n",
    "                                lambda geom: geom if isinstance(geom, Polygon) else Polygon())\n",
    "                            valid_buffer.loc[:, 'geometry'] = valid_buffer['geometry'].apply(\n",
    "                                lambda geom: geom if isinstance(geom, Polygon) else Polygon())\n",
    "\n",
    "                            # Spatial analysis\n",
    "                            within = gpd.sjoin(valid_outlines, valid_buffer, predicate='within')\n",
    "                            overlaps = gpd.sjoin(valid_outlines, valid_buffer, predicate='overlaps')\n",
    "        \n",
    "                            # Calculate within fraction\n",
    "                            if (len(within) + len(overlaps)) > 0:\n",
    "                                within_fraction = np.round((len(within) / (len(within) + len(overlaps))), 2)\n",
    "                                print(f\"Extent: {area_multiple_search_extent}, Level: {level}, Within: {round(within_fraction*100)}%\")\n",
    "                            else:\n",
    "                                print(f'No valid intersections at level {level}')\n",
    "                                break\n",
    "        \n",
    "                            # Process if target reached\n",
    "                            if within_fraction >= within_fraction_target:\n",
    "                                filtered_gdf = extract_intersecting_polygons_recursive(\n",
    "                                    valid_outlines, lake_gdf['geometry'])\n",
    "                                    \n",
    "                                if not filtered_gdf.empty:\n",
    "                                    # Prepare DOIs\n",
    "                                    dois = [doi for doi in [dataset1_doi, dataset2_doi] if doi is not None]\n",
    "                                    dois_str = ', '.join(dois)\n",
    "                                    \n",
    "                                    # Create new row\n",
    "                                    new_row = pd.DataFrame({\n",
    "                                        'area_multiple_search_extent': [area_multiple_search_extent],\n",
    "                                        'level': [level],\n",
    "                                        'within_percent': [within_fraction * 100],\n",
    "                                        'dataset_dois': [dois_str]\n",
    "                                    })\n",
    "                                    \n",
    "                                    search_extents_levels_df = pd.concat(\n",
    "                                        [search_extents_levels_df, new_row],\n",
    "                                        ignore_index=True)\n",
    "                                    \n",
    "                                break\n",
    "\n",
    "                            # Increment level\n",
    "                            level += level_increment\n",
    "                            level = np.round(level, 2)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error at level {level}: {str(e)}\")\n",
    "                            level += level_increment\n",
    "                            level = np.round(level, 2)\n",
    "                            continue\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Error at extent {area_multiple_search_extent}: {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "            # Save results\n",
    "            os.makedirs(OUTPUT_DIR + '/search_extents_levels', exist_ok=True)\n",
    "            \n",
    "            if not search_extents_levels_df.empty:\n",
    "                search_extents_levels_df.to_csv(\n",
    "                    OUTPUT_DIR + '/search_extents_levels/{}.csv'.format(lake_gdf['name']), \n",
    "                    index=False)\n",
    "                print(f\"Saved results for {lake_gdf['name']}\")\n",
    "            else:\n",
    "                print(f\"No outlines found for {lake_gdf['name']}\")\n",
    "                # Write no outlines found files\n",
    "                write_no_outlines(OUTPUT_DIR + '/search_extents_levels/{}.txt'.format(lake_gdf['name']))\n",
    "                write_no_outlines(OUTPUT_DIR_GIT + '/lake_outlines/evolving_outlines/{}.txt'.format(lake_gdf['name']))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing lake {lake_gdf['name']}: {str(e)}\")\n",
    "        finally:\n",
    "            # Cleanup\n",
    "            if 'dataset1' in locals(): del dataset1\n",
    "            if 'dataset2' in locals(): del dataset2\n",
    "            gc.collect()\n",
    "            clear_output(wait=True)\n",
    "\n",
    "def write_no_outlines(filepath):\n",
    "    \"\"\"Write file indicating no outlines found\"\"\"\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write(\"There are no evolving outlines for this lake.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cdd99ea-abb5-4bc7-864f-ca20075a1905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_evolving_outlines(lake_gdf, area_multiple_search_extent, level, dataset1_masked, dataset2_masked, buffered_poly, plot=False): \n",
    "#     '''\n",
    "#     Create time-variable outlines using skimage contour to plot evolving outlines as polygons.\n",
    "#     If plot=True, create planview dh/dt plots of ice surface height changes with evolving outlines found,\n",
    "#     along with a subplot showing data counts.\n",
    "\n",
    "#     Inputs:\n",
    "#     * lake_gdf: GeoDataFrame containing lake information\n",
    "#     * area_multiple_search_extent: Factor to multiply lake area for search extent\n",
    "#     * level: vertical distance in meters to delineate ice surface deformation contour\n",
    "#     * dataset1_masked: masked dataset1 to be analyzed\n",
    "#     * dataset2_masked: masked dataset2 to be analyzed\n",
    "#     * buffered_poly: buffered polygon defining the search extent\n",
    "#     * plot: boolean, if True, create and save plots\n",
    "    \n",
    "#     Outputs: \n",
    "#     * geopandas geodataframe of polygons created at each step\n",
    "#     * If plot=True, sequence of planview delta height visuals of ICESat-2 ATL15 with variable ice surface \n",
    "#     deformation contours plotted to delineate evolving lake boundaries, along with data count subplots.\n",
    "\n",
    "#     # Example usage\n",
    "#     >>> outlines_gdf = find_evolving_outlines(lake_gdf=lake_gdf, area_multiple_search_extent=2, level=0.1, \n",
    "#         dataset1_masked=dataset1_masked, dataset2_masked=dataset2_masked, plot=False)\n",
    "#     '''    \n",
    "#     # Define lake name and polygon\n",
    "#     lake_name = lake_gdf['name']\n",
    "#     lake_poly = lake_gdf['geometry']\n",
    "\n",
    "#     # Establish x_min, x_max, y_min, y_max\n",
    "#     x_min, y_min, x_max, y_max = buffered_poly.bounds\n",
    "#     x_buffer = abs(x_max-x_min)*0.2\n",
    "#     y_buffer = abs(y_max-y_min)*0.2\n",
    "    \n",
    "#     # Create empty lists to store polygons, areas, dh's, dvol's and dates\n",
    "#     polys = []\n",
    "#     areas = []\n",
    "#     dhs = []\n",
    "#     dvols =[]\n",
    "#     midcyc_datetimes = []\n",
    "\n",
    "#     # Only proceed with plotting if plot=True\n",
    "#     if plot:\n",
    "#         # Find magnitude of ice surface deformation and maximum data counts to create appropriate color map scales\n",
    "#         height_anom_pos = []\n",
    "#         height_anom_neg = []\n",
    "#         max_counts = []\n",
    "        \n",
    "#         for idx in range(len(mid_cyc_dates)-1):\n",
    "#             if lake_gdf['CS2_SARIn_time_period'] == '2013.75-2018.75' and 20 <= idx <= 32:\n",
    "#                 continue\n",
    "#             if idx <= 32:\n",
    "#                 if dataset1_masked is not None:\n",
    "#                     dhdt_masked = dataset1_masked['delta_h'][idx+1,:,:]-dataset1_masked['delta_h'][idx,:,:]\n",
    "#                     count_masked = dataset1_masked['data_count'][idx,:,:]\n",
    "#                 else:\n",
    "#                     continue\n",
    "#             elif idx > 32:\n",
    "#                 dhdt_masked = dataset2_masked['delta_h'][(idx-33)+1,:,:]-dataset2_masked['delta_h'][(idx-33),:,:]\n",
    "#                 count_masked = dataset2_masked['data_count'][(idx-33),:,:]\n",
    "            \n",
    "#             if np.any(~np.isnan(dhdt_masked)):       \n",
    "#                 pos = np.nanmax(dhdt_masked)\n",
    "#                 neg = np.nanmin(dhdt_masked)\n",
    "#                 height_anom_pos += [pos]\n",
    "#                 height_anom_neg += [neg]\n",
    "                \n",
    "#                 # Find maximum data count for this cycle\n",
    "#                 max_count = np.nanmax(count_masked)\n",
    "#                 max_counts += [max_count]\n",
    "\n",
    "#         # Maximum pos/neg dh anomalies and data_counts across all cycles\n",
    "#         max_height_anom_pos = max(height_anom_pos)\n",
    "#         max_height_anom_neg = min(height_anom_neg)\n",
    "#         max_count_overall = max(max_counts)\n",
    "\n",
    "#         # Add checks for empty sequences\n",
    "#         if not height_anom_pos or not height_anom_neg or not max_counts:\n",
    "#             print(f\"No valid data found for {lake_gdf['name']}\")\n",
    "#             return None\n",
    "\n",
    "#         divnorm = colors.TwoSlopeNorm(vmin=max_height_anom_neg, vcenter=0., vmax=max_height_anom_pos)\n",
    "#         countnorm = colors.Normalize(vmin=0, vmax=max_count_overall)  # Normalize counts from 0 to max\n",
    "\n",
    "#         # Create lines for legend\n",
    "#         stationary_lakes_color = 'darkturquoise'\n",
    "#         stationary_lakes_line = plt.Line2D((0, 1), (0, 0), color=stationary_lakes_color, linestyle=(0, (1, 1)), linewidth=2)\n",
    "#         uplift = plt.Line2D((0, 1), (0, 0), color='blue', linestyle=(0, (3, 5, 1, 5)), linewidth=2)\n",
    "#         subsidence = plt.Line2D((0, 1), (0, 0), color='red', linestyle=(0, (3, 5, 1, 5, 1, 5)), linewidth=2)\n",
    "#         search_extent_line = plt.Line2D((0, 1), (0, 0), color='magenta', linestyle='solid', linewidth=2)\n",
    "    \n",
    "#     # Calculate cycle-to-cycle dh at each cycle of the spliced data sets\n",
    "#     for idx in range(len(mid_cyc_dates)-1):\n",
    "#         if lake_gdf['CS2_SARIn_time_period'] == '2013.75-2018.75' and 20 <= idx <= 32:\n",
    "#             continue\n",
    "        \n",
    "#         if idx <= 32:\n",
    "#             if dataset1_masked is not None:\n",
    "#                 dhdt_masked = dataset1_masked['delta_h'][idx+1,:,:]-dataset1_masked['delta_h'][idx,:,:]\n",
    "#                 dhdt_masked.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "#                 count_masked = dataset1_masked['data_count'][idx,:,:]\n",
    "#             else:\n",
    "#                 continue\n",
    "#         elif idx > 32:\n",
    "#             dhdt_masked = dataset2_masked['delta_h'][(idx-33)+1,:,:]-dataset2_masked['delta_h'][(idx-33),:,:]\n",
    "#             dhdt_masked.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "#             count_masked = dataset2_masked['data_count'][(idx-33),:,:]\n",
    "\n",
    "#         count_masked.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "#         x_conv = (x_max-x_min)/dhdt_masked.shape[1]\n",
    "#         y_conv = (y_max-y_min)/dhdt_masked.shape[0]\n",
    "\n",
    "#         if np.any(~np.isnan(dhdt_masked)):\n",
    "#             if plot:\n",
    "#                 # Create fig with two subplots\n",
    "#                 fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "#                 # Plot data counts\n",
    "#                 img1 = ax1.imshow(count_masked, extent=[x_min, x_max, y_min, y_max], \n",
    "#                     origin='lower', cmap='Greys', norm=countnorm)\n",
    "\n",
    "#                 # Plot height change\n",
    "#                 img2 = ax2.imshow(dhdt_masked, extent=[x_min, x_max, y_min, y_max], \n",
    "#                     origin='lower', cmap='coolwarm_r', \n",
    "#                     norm=divnorm)\n",
    "\n",
    "#                 # Plot the boundary of the evolving outline search extent on both subplots\n",
    "#                 gpd.GeoSeries(buffered_poly).boundary.plot(ax=ax1, color='magenta')\n",
    "#                 gpd.GeoSeries(buffered_poly).boundary.plot(ax=ax2, color='magenta')\n",
    "\n",
    "#             # Create contours and plot them (only on the height change subplot)\n",
    "#             contours_pos = []\n",
    "#             contours_neg = []\n",
    "\n",
    "#             if np.any(~np.isnan(count_masked)): #TESTING\n",
    "#                 contour_pos = measure.find_contours(dhdt_masked.values, level)\n",
    "#                 if len(contour_pos) > 0: \n",
    "#                     contours_pos += [contour_pos]\n",
    "#                 contour_neg = measure.find_contours(dhdt_masked.values, -level)\n",
    "#                 if len(contour_neg) > 0: \n",
    "#                     contours_neg += [contour_neg]\n",
    "\n",
    "#             for i in range(len(contours_pos)): \n",
    "#                 for j in range(len(contours_pos[i])):\n",
    "#                     x = x_min+contours_pos[i][j][:,1]*x_conv\n",
    "#                     y = y_min+contours_pos[i][j][:,0]*y_conv\n",
    "#                     if plot:\n",
    "#                         ax1.plot(x, y, color='blue', linestyle='dashdot', linewidth=1, label=level)\n",
    "#                         ax2.plot(x, y, color='blue', linestyle='dashdot', linewidth=1, label=level)\n",
    "\n",
    "#                     if len(contours_pos[i][j][:,1]) > 2: \n",
    "#                         poly = Polygon(list(zip(x, y)))\n",
    "#                         try:\n",
    "#                             dhdt_poly = dhdt_masked.rio.clip([poly])   \n",
    "#                             lon, lat = XY_TO_LL.transform(x,y)\n",
    "#                             poly_area = abs(GEOD.polygon_area_perimeter(lon, lat)[0])\n",
    "#                             if np.any(~np.isnan(dhdt_poly)):\n",
    "#                                 poly_dh = np.nanmean(dhdt_poly)\n",
    "#                                 poly_dvol = poly_dh*poly_area\n",
    "#                                 polys += [poly]\n",
    "#                                 areas += [poly_area]\n",
    "#                                 dhs += [poly_dh]\n",
    "#                                 dvols += [poly_dvol]\n",
    "#                                 midcyc_datetimes += [mid_cyc_dates[idx]]\n",
    "#                         except NoDataInBounds:\n",
    "#                             pass\n",
    "#                         except Exception as e:\n",
    "#                             raise\n",
    "\n",
    "#             for i in range(len(contours_neg)): \n",
    "#                 for j in range(len(contours_neg[i])):\n",
    "#                     x = x_min+contours_neg[i][j][:,1]*x_conv\n",
    "#                     y = y_min+contours_neg[i][j][:,0]*y_conv\n",
    "#                     if plot:\n",
    "#                         ax1.plot(x, y, color='red', linestyle=(0, (3, 1, 1, 1)), linewidth=1, label=-level)\n",
    "#                         ax2.plot(x, y, color='red', linestyle=(0, (3, 1, 1, 1)), linewidth=1, label=-level)\n",
    "\n",
    "#                     if len(contours_neg[i][j][:,1]) > 2: \n",
    "#                         poly = Polygon(list(zip(x, y)))\n",
    "#                         try:\n",
    "#                             dhdt_poly = dhdt_masked.rio.clip([poly]) \n",
    "#                             lon, lat = XY_TO_LL.transform(x,y)\n",
    "#                             poly_area = abs(GEOD.polygon_area_perimeter(lon, lat)[0])\n",
    "#                             if np.any(~np.isnan(dhdt_poly)):\n",
    "#                                 poly_dh = np.nanmean(dhdt_poly)\n",
    "#                                 poly_dvol = poly_dh*poly_area\n",
    "#                                 polys += [poly]\n",
    "#                                 areas += [poly_area]\n",
    "#                                 dhs += [poly_dh]\n",
    "#                                 dvols += [poly_dvol]\n",
    "#                                 midcyc_datetimes += [mid_cyc_dates[idx]]\n",
    "#                         except NoDataInBounds:\n",
    "#                             pass\n",
    "#                         except Exception as e:\n",
    "#                             raise\n",
    "\n",
    "#             if plot:\n",
    "#                 # Common plotting for both subplots\n",
    "#                 for ax in [ax1, ax2]:\n",
    "#                     stationary_lakes_gdf.boundary.plot(ax=ax, edgecolor=stationary_lakes_color, linestyle=(0, (1, 1)), linewidth=2)          \n",
    "\n",
    "#                     km_scale = 1e3\n",
    "#                     ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "#                     ax.xaxis.set_major_formatter(ticks_x)\n",
    "#                     ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "#                     ax.yaxis.set_major_formatter(ticks_y)  \n",
    "\n",
    "#                     ax.set_xlabel('x [km]', size=15)\n",
    "#                     ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "#                 # Plotting for ax1\n",
    "#                 ax1.set_ylabel('y [km]', size=15)\n",
    "\n",
    "#                 axIns = ax1.inset_axes([0.01, 0.01, 0.2, 0.2])\n",
    "#                 axIns.set_aspect('equal')\n",
    "#                 moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "#                 moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "#                 axIns.axis('off')\n",
    "\n",
    "#                 axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "#                     linewidth=1, edgecolor='k', facecolor='r', s=100, zorder=3)\n",
    "\n",
    "#                 # Add colorbars\n",
    "#                 divider1 = make_axes_locatable(ax1)\n",
    "#                 cax1 = divider1.append_axes('right', size='5%', pad=0.2)\n",
    "#                 fig.colorbar(img1, cax=cax1).set_label('data counts', size=15)\n",
    "\n",
    "#                 divider2 = make_axes_locatable(ax2)\n",
    "#                 cax2 = divider2.append_axes('right', size='5%', pad=0.2)\n",
    "#                 fig.colorbar(img2, cax=cax2).set_label('height change (dh) [m]', size=15)\n",
    "\n",
    "#                 # Add legend to height change subplot\n",
    "#                 ax1.legend([stationary_lakes_line,\n",
    "#                            uplift, \n",
    "#                            subsidence,\n",
    "#                            search_extent_line],\n",
    "#                     ['stationary outline ',\n",
    "#                      ('+ '+str(level)+' m evolving outline'), \n",
    "#                      (' '+str(level)+' m evolving outline'),\n",
    "#                      'search extent ('+str(area_multiple_search_extent)+'x)'], \n",
    "#                     loc='upper left')\n",
    "\n",
    "#                 ax2.legend([stationary_lakes_line,\n",
    "#                            uplift, \n",
    "#                            subsidence,\n",
    "#                            search_extent_line],\n",
    "#                     ['stationary outline ', \n",
    "#                      ('+ '+str(level)+' m evolving outline'), \n",
    "#                      (' '+str(level)+' m evolving outline'),\n",
    "#                      'search extent ('+str(area_multiple_search_extent)+'x)'], \n",
    "#                     loc='upper left')\n",
    "\n",
    "#                 # Set titles for subplots\n",
    "#                 ax1.set_title('Data counts')\n",
    "#                 ax2.set_title('Height change and evolving outlines')\n",
    "\n",
    "#                 # Set a common title for both subplots\n",
    "#                 fig.suptitle('From {} to {}'.format(cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)), fontsize=16)\n",
    "\n",
    "#                 # Adjust layout and save figure\n",
    "#                 plt.tight_layout()\n",
    "#                 plt.savefig(OUTPUT_DIR + \n",
    "#                     '/find_evolving_outlines/{}/find_evolving_outlines_{}_{}x-search-extent_{}m-level_{}-{}.png'\n",
    "#                     .format(lake_name, lake_name, area_multiple_search_extent, level, cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)), dpi=300, bbox_inches='tight')\n",
    "#                 plt.close()\n",
    "\n",
    "#     # Store optimal search extent and level information\n",
    "#     area_multiple_search_extents = [area_multiple_search_extent for _ in range(len(polys))]\n",
    "#     levels = [level for _ in range(len(polys))]\n",
    "\n",
    "#     # Store polygons in geopandas geodataframe for further analysis\n",
    "#     d = {'area_multiple_search_extent': area_multiple_search_extents,\n",
    "#          'level': levels,\n",
    "#          'geometry': polys, \n",
    "#          'area (m^2)': areas, \n",
    "#          'dh (m)': dhs, \n",
    "#          'vol (m^3)': dvols,\n",
    "#          'midcyc_datetime': midcyc_datetimes}\n",
    "#     gdf = gpd.GeoDataFrame(d, crs=\"EPSG:3031\")\n",
    "\n",
    "#     return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70c2dcbc-6147-4984-aa9e-94502aec6d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_evolving_outlines(lake_gdf, area_multiple_search_extent, level, dataset1_masked, dataset2_masked, buffered_poly, plot=False): \n",
    "#     '''\n",
    "#     Create time-variable outlines using skimage contour to plot evolving outlines as polygons.\n",
    "#     If plot=True, create planview dh/dt plots of ice surface height changes with evolving outlines found,\n",
    "#     along with a subplot showing data counts.\n",
    "\n",
    "#     Inputs:\n",
    "#     * lake_gdf: GeoDataFrame containing lake information\n",
    "#     * area_multiple_search_extent: Factor to multiply lake area for search extent\n",
    "#     * level: vertical distance in meters to delineate ice surface deformation contour\n",
    "#     * dataset1_masked: masked dataset1 to be analyzed\n",
    "#     * dataset2_masked: masked dataset2 to be analyzed\n",
    "#     * buffered_poly: buffered polygon defining the search extent\n",
    "#     * plot: boolean, if True, create and save plots\n",
    "    \n",
    "#     Outputs: \n",
    "#     * geopandas geodataframe of polygons created at each step\n",
    "#     * If plot=True, sequence of planview delta height visuals of ICESat-2 ATL15 with variable ice surface \n",
    "#     deformation contours plotted to delineate evolving lake boundaries, along with data count subplots.\n",
    "\n",
    "#     # Example usage\n",
    "#     >>> outlines_gdf = find_evolving_outlines(lake_gdf=lake_gdf, area_multiple_search_extent=2, level=0.1, \n",
    "#         dataset1_masked=dataset1_masked, dataset2_masked=dataset2_masked, plot=False)\n",
    "#     '''    \n",
    "#     # Define lake name and polygon\n",
    "#     lake_name = lake_gdf['name']\n",
    "#     lake_poly = lake_gdf['geometry']\n",
    "\n",
    "#     # Establish x_min, x_max, y_min, y_max\n",
    "#     x_min, y_min, x_max, y_max = buffered_poly.bounds\n",
    "#     x_buffer = abs(x_max-x_min)*0.2\n",
    "#     y_buffer = abs(y_max-y_min)*0.2\n",
    "\n",
    "#     # Initialize lists\n",
    "#     height_anom_pos = []\n",
    "#     height_anom_neg = []\n",
    "#     max_counts = []\n",
    "#     polys = []\n",
    "#     areas = []\n",
    "#     dhs = []\n",
    "#     dvols = []\n",
    "#     midcyc_datetimes = []\n",
    "\n",
    "#     # First pass to collect data for plotting\n",
    "#     data_found = False\n",
    "#     for idx in range(len(mid_cyc_dates)-1):\n",
    "#         if lake_gdf['CS2_SARIn_time_period'] == '2013.75-2018.75' and 20 <= idx <= 32:\n",
    "#             continue\n",
    "            \n",
    "#         # Get data from appropriate dataset\n",
    "#         dhdt_masked = None\n",
    "#         count_masked = None\n",
    "        \n",
    "#         if idx <= 32:\n",
    "#             if dataset1_masked is not None:\n",
    "#                 dhdt_masked = dataset1_masked['delta_h'][idx+1,:,:]-dataset1_masked['delta_h'][idx,:,:]\n",
    "#                 count_masked = dataset1_masked['data_count'][idx,:,:]\n",
    "#         elif idx > 32:\n",
    "#             if dataset2_masked is not None:\n",
    "#                 dhdt_masked = dataset2_masked['delta_h'][(idx-33)+1,:,:]-dataset2_masked['delta_h'][(idx-33),:,:]\n",
    "#                 count_masked = dataset2_masked['data_count'][(idx-33),:,:]\n",
    "\n",
    "#         if dhdt_masked is not None and count_masked is not None and np.any(~np.isnan(dhdt_masked)):\n",
    "#             pos = np.nanmax(dhdt_masked)\n",
    "#             neg = np.nanmin(dhdt_masked)\n",
    "#             height_anom_pos.append(pos)\n",
    "#             height_anom_neg.append(neg)\n",
    "#             max_counts.append(np.nanmax(count_masked))\n",
    "#             data_found = True\n",
    "\n",
    "#     # Check if we found any valid data\n",
    "#     if not data_found or not height_anom_pos or not height_anom_neg or not max_counts:\n",
    "#         print(f\"No valid data found at this search_extent\")\n",
    "#         return None\n",
    "\n",
    "#     # Now that we know we have data, set up plotting parameters\n",
    "#     if plot:\n",
    "#         max_height_anom_pos = max(height_anom_pos)\n",
    "#         max_height_anom_neg = min(height_anom_neg)\n",
    "#         max_count_overall = max(max_counts)\n",
    "        \n",
    "#         divnorm = colors.TwoSlopeNorm(vmin=max_height_anom_neg, vcenter=0., vmax=max_height_anom_pos)\n",
    "#         countnorm = colors.Normalize(vmin=0, vmax=max_count_overall)\n",
    "        \n",
    "#         # Create lines for legend\n",
    "#         stationary_lakes_color = 'darkturquoise'\n",
    "#         stationary_lakes_line = plt.Line2D((0, 1), (0, 0), color=stationary_lakes_color, linestyle='solid', linewidth=2)\n",
    "#         uplift = plt.Line2D((0, 1), (0, 0), color='blue', linestyle=(0, (3, 5, 1, 5)), linewidth=2)\n",
    "#         subsidence = plt.Line2D((0, 1), (0, 0), color='red', linestyle=(0, (3, 5, 1, 5, 1, 5)), linewidth=2)\n",
    "#         search_extent_line = plt.Line2D((0, 1), (0, 0), color='magenta', linestyle='solid', linewidth=2)\n",
    "    \n",
    "#     # Calculate cycle-to-cycle dh at each cycle of the spliced data sets\n",
    "#     for idx in range(len(mid_cyc_dates)-1):\n",
    "#         if lake_gdf['CS2_SARIn_time_period'] == '2013.75-2018.75' and 20 <= idx <= 32:\n",
    "#             continue\n",
    "        \n",
    "#         if idx <= 32:\n",
    "#             if dataset1_masked is not None:\n",
    "#                 dhdt_masked = dataset1_masked['delta_h'][idx+1,:,:]-dataset1_masked['delta_h'][idx,:,:]\n",
    "#                 dhdt_masked.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "#                 count_masked = dataset1_masked['data_count'][idx,:,:]\n",
    "#             else:\n",
    "#                 continue\n",
    "#         elif idx > 32:\n",
    "#             dhdt_masked = dataset2_masked['delta_h'][(idx-33)+1,:,:]-dataset2_masked['delta_h'][(idx-33),:,:]\n",
    "#             dhdt_masked.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "#             count_masked = dataset2_masked['data_count'][(idx-33),:,:]\n",
    "\n",
    "#         count_masked.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "#         x_conv = (x_max-x_min)/dhdt_masked.shape[1]\n",
    "#         y_conv = (y_max-y_min)/dhdt_masked.shape[0]\n",
    "\n",
    "#         if np.any(~np.isnan(dhdt_masked)):\n",
    "#             if plot:\n",
    "#                 # Create fig with two subplots\n",
    "#                 fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "#                 # Plot data counts\n",
    "#                 img1 = ax1.imshow(count_masked, extent=[x_min, x_max, y_min, y_max], \n",
    "#                     origin='lower', cmap='Greys', norm=countnorm)\n",
    "\n",
    "#                 # Plot height change\n",
    "#                 img2 = ax2.imshow(dhdt_masked, extent=[x_min, x_max, y_min, y_max], \n",
    "#                     origin='lower', cmap='coolwarm_r', \n",
    "#                     norm=divnorm)\n",
    "\n",
    "#                 # Plot the boundary of the evolving outline search extent on both subplots\n",
    "#                 gpd.GeoSeries(buffered_poly).boundary.plot(ax=ax1, color='magenta')\n",
    "#                 gpd.GeoSeries(buffered_poly).boundary.plot(ax=ax2, color='magenta')\n",
    "\n",
    "#             # Create contours and plot them (only on the height change subplot)\n",
    "#             contours_pos = []\n",
    "#             contours_neg = []\n",
    "\n",
    "#             if np.any(~np.isnan(count_masked)): #TESTING\n",
    "#                 contour_pos = measure.find_contours(dhdt_masked.values, level)\n",
    "#                 if len(contour_pos) > 0: \n",
    "#                     contours_pos += [contour_pos]\n",
    "#                 contour_neg = measure.find_contours(dhdt_masked.values, -level)\n",
    "#                 if len(contour_neg) > 0: \n",
    "#                     contours_neg += [contour_neg]\n",
    "\n",
    "#             for i in range(len(contours_pos)): \n",
    "#                 for j in range(len(contours_pos[i])):\n",
    "#                     x = x_min+contours_pos[i][j][:,1]*x_conv\n",
    "#                     y = y_min+contours_pos[i][j][:,0]*y_conv\n",
    "#                     if plot:\n",
    "#                         ax1.plot(x, y, color='blue', linestyle='dashdot', linewidth=1, label=level)\n",
    "#                         ax2.plot(x, y, color='blue', linestyle='dashdot', linewidth=1, label=level)\n",
    "\n",
    "#                     if len(contours_pos[i][j][:,1]) > 2: \n",
    "#                         poly = Polygon(list(zip(x, y)))\n",
    "#                         try:\n",
    "#                             dhdt_poly = dhdt_masked.rio.clip([poly])   \n",
    "#                             lon, lat = XY_TO_LL.transform(x,y)\n",
    "#                             poly_area = abs(GEOD.polygon_area_perimeter(lon, lat)[0])\n",
    "#                             if np.any(~np.isnan(dhdt_poly)):\n",
    "#                                 poly_dh = np.nanmean(dhdt_poly)\n",
    "#                                 poly_dvol = poly_dh*poly_area\n",
    "#                                 polys += [poly]\n",
    "#                                 areas += [poly_area]\n",
    "#                                 dhs += [poly_dh]\n",
    "#                                 dvols += [poly_dvol]\n",
    "#                                 midcyc_datetimes += [mid_cyc_dates[idx]]\n",
    "#                         except NoDataInBounds:\n",
    "#                             pass\n",
    "#                         except Exception as e:\n",
    "#                             raise\n",
    "\n",
    "#             for i in range(len(contours_neg)): \n",
    "#                 for j in range(len(contours_neg[i])):\n",
    "#                     x = x_min+contours_neg[i][j][:,1]*x_conv\n",
    "#                     y = y_min+contours_neg[i][j][:,0]*y_conv\n",
    "#                     if plot:\n",
    "#                         ax1.plot(x, y, color='red', linestyle=(0, (3, 1, 1, 1)), linewidth=1, label=-level)\n",
    "#                         ax2.plot(x, y, color='red', linestyle=(0, (3, 1, 1, 1)), linewidth=1, label=-level)\n",
    "\n",
    "#                     if len(contours_neg[i][j][:,1]) > 2: \n",
    "#                         poly = Polygon(list(zip(x, y)))\n",
    "#                         try:\n",
    "#                             dhdt_poly = dhdt_masked.rio.clip([poly]) \n",
    "#                             lon, lat = XY_TO_LL.transform(x,y)\n",
    "#                             poly_area = abs(GEOD.polygon_area_perimeter(lon, lat)[0])\n",
    "#                             if np.any(~np.isnan(dhdt_poly)):\n",
    "#                                 poly_dh = np.nanmean(dhdt_poly)\n",
    "#                                 poly_dvol = poly_dh*poly_area\n",
    "#                                 polys += [poly]\n",
    "#                                 areas += [poly_area]\n",
    "#                                 dhs += [poly_dh]\n",
    "#                                 dvols += [poly_dvol]\n",
    "#                                 midcyc_datetimes += [mid_cyc_dates[idx]]\n",
    "#                         except NoDataInBounds:\n",
    "#                             pass\n",
    "#                         except Exception as e:\n",
    "#                             raise\n",
    "\n",
    "#             if plot:\n",
    "#                 # Common plotting for both subplots\n",
    "#                 for ax in [ax1, ax2]:\n",
    "#                     stationary_lakes_gdf.boundary.plot(ax=ax, edgecolor=stationary_lakes_color, linestyle='solid', linewidth=2)          \n",
    "\n",
    "#                     km_scale = 1e3\n",
    "#                     ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "#                     ax.xaxis.set_major_formatter(ticks_x)\n",
    "#                     ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "#                     ax.yaxis.set_major_formatter(ticks_y)  \n",
    "\n",
    "#                     ax.set_xlabel('x [km]', size=15)\n",
    "#                     ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "#                 # Plotting for ax1\n",
    "#                 ax1.set_ylabel('y [km]', size=15)\n",
    "\n",
    "#                 axIns = ax1.inset_axes([0.01, 0.01, 0.2, 0.2])\n",
    "#                 axIns.set_aspect('equal')\n",
    "#                 moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "#                 moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "#                 axIns.axis('off')\n",
    "\n",
    "#                 axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "#                     linewidth=1, edgecolor='k', facecolor='r', s=100, zorder=3)\n",
    "\n",
    "#                 # Add colorbars\n",
    "#                 divider1 = make_axes_locatable(ax1)\n",
    "#                 cax1 = divider1.append_axes('right', size='5%', pad=0.2)\n",
    "#                 fig.colorbar(img1, cax=cax1).set_label('data counts', size=15)\n",
    "\n",
    "#                 divider2 = make_axes_locatable(ax2)\n",
    "#                 cax2 = divider2.append_axes('right', size='5%', pad=0.2)\n",
    "#                 fig.colorbar(img2, cax=cax2).set_label('height change (dh) [m]', size=15)\n",
    "\n",
    "#                 # Add legend to height change subplot\n",
    "#                 ax1.legend([stationary_lakes_line,\n",
    "#                            uplift, \n",
    "#                            subsidence,\n",
    "#                            search_extent_line],\n",
    "#                     ['stationary outline ',\n",
    "#                      ('+ '+str(level)+' m evolving outline'), \n",
    "#                      (' '+str(level)+' m evolving outline'),\n",
    "#                      'search extent ('+str(int(area_multiple_search_extent))+'x)'], \n",
    "#                     loc='upper left')\n",
    "\n",
    "#                 ax2.legend([stationary_lakes_line,\n",
    "#                            uplift, \n",
    "#                            subsidence,\n",
    "#                            search_extent_line],\n",
    "#                     ['stationary outline ', \n",
    "#                      ('+ '+str(level)+' m evolving outline'), \n",
    "#                      (' '+str(level)+' m evolving outline'),\n",
    "#                      'search extent ('+str(int(area_multiple_search_extent))+'x)'], \n",
    "#                     loc='upper left')\n",
    "\n",
    "#                 # Set titles for subplots\n",
    "#                 ax1.set_title('Data counts')\n",
    "#                 ax2.set_title('Height change')\n",
    "\n",
    "#                 # Set a common title for both subplots\n",
    "#                 fig.suptitle('From {} to {}'.format(cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)), fontsize=16)\n",
    "\n",
    "#                 # Adjust layout and save figure\n",
    "#                 plt.tight_layout()\n",
    "#                 plt.savefig(OUTPUT_DIR + \n",
    "#                     '/find_evolving_outlines/{}/find_evolving_outlines_{}_{}x-search-extent_{}m-level_{}-{}.png'\n",
    "#                     .format(lake_name, lake_name, int(area_multiple_search_extent), level, cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)), dpi=300, bbox_inches='tight')\n",
    "#                 plt.close()\n",
    "\n",
    "#     # Store optimal search extent and level information\n",
    "#     area_multiple_search_extents = [area_multiple_search_extent for _ in range(len(polys))]\n",
    "#     levels = [level for _ in range(len(polys))]\n",
    "\n",
    "#     # At the end, check if we found any polygons\n",
    "#     if not polys:\n",
    "#         print(f\"No outlines found at this search_extent and level\")\n",
    "#         return None\n",
    "\n",
    "#     # Create GeoDataFrame\n",
    "#     gdf = gpd.GeoDataFrame({\n",
    "#         'area_multiple_search_extent': area_multiple_search_extents,\n",
    "#         'level': levels,\n",
    "#         'geometry': polys, \n",
    "#         'area (m^2)': areas, \n",
    "#         'dh (m)': dhs, \n",
    "#         'vol (m^3)': dvols,\n",
    "#         'midcyc_datetime': midcyc_datetimes\n",
    "#     }, crs=\"EPSG:3031\")\n",
    "    \n",
    "#     return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17a89176-9c3a-433d-b4d6-4c7cfc55ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_evolving_outlines(lake_gdf, area_multiple_search_extent, level, dataset1_masked, dataset2_masked, buffered_poly, plot=False): \n",
    "#     '''\n",
    "#     Create time-variable outlines using skimage contour to plot evolving outlines as polygons.\n",
    "#     If plot=True, create planview dh/dt plots of ice surface height changes with evolving outlines found,\n",
    "#     along with a subplot showing data counts.\n",
    "\n",
    "#     Inputs:\n",
    "#     * lake_gdf: GeoDataFrame containing lake information\n",
    "#     * area_multiple_search_extent: Factor to multiply lake area for search extent\n",
    "#     * level: vertical distance in meters to delineate ice surface deformation contour\n",
    "#     * dataset1_masked: masked dataset1 to be analyzed\n",
    "#     * dataset2_masked: masked dataset2 to be analyzed\n",
    "#     * buffered_poly: buffered polygon defining the search extent\n",
    "#     * plot: boolean, if True, create and save plots\n",
    "    \n",
    "#     Outputs: \n",
    "#     * geopandas geodataframe of polygons created at each step\n",
    "#     * If plot=True, sequence of planview delta height visuals of ICESat-2 ATL15 with variable ice surface \n",
    "#     deformation contours plotted to delineate evolving lake boundaries, along with data count subplots.\n",
    "\n",
    "#     # Example usage\n",
    "#     >>> outlines_gdf = find_evolving_outlines(lake_gdf=lake_gdf, area_multiple_search_extent=2, level=0.1, \n",
    "#         dataset1_masked=dataset1_masked, dataset2_masked=dataset2_masked, plot=False)\n",
    "#     '''    \n",
    "#     # Define lake name and polygon\n",
    "#     lake_name = lake_gdf['name']\n",
    "#     lake_poly = lake_gdf['geometry']\n",
    "\n",
    "#     # Establish x_min, x_max, y_min, y_max\n",
    "#     x_min, y_min, x_max, y_max = buffered_poly.bounds\n",
    "#     x_buffer = abs(x_max-x_min)*0.2\n",
    "#     y_buffer = abs(y_max-y_min)*0.2\n",
    "    \n",
    "#     # Create empty lists to store polygons, areas, dh's, dvol's and dates\n",
    "#     polys = []\n",
    "#     areas = []\n",
    "#     dhs = []\n",
    "#     dvols = []\n",
    "#     midcyc_datetimes = []\n",
    "\n",
    "#     # Only proceed with plotting if plot=True\n",
    "#     if plot:\n",
    "#         # Find magnitude of ice surface deformation and maximum data counts\n",
    "#         height_anom_pos = []\n",
    "#         height_anom_neg = []\n",
    "#         max_counts = []\n",
    "        \n",
    "#         for idx in range(len(mid_cyc_dates)-1):\n",
    "#             # Get time period and check if we should skip\n",
    "#             time_period = lake_gdf['CS2_SARIn_time_period'].iloc[0] if isinstance(lake_gdf['CS2_SARIn_time_period'], pd.Series) else lake_gdf['CS2_SARIn_time_period']\n",
    "#             if pd.notna(time_period) and time_period == '2013.75-2018.75' and 20 <= idx <= 32:\n",
    "#                 continue\n",
    "\n",
    "#             if idx <= 32:\n",
    "#                 if dataset1_masked is not None:\n",
    "#                     dhdt_masked = dataset1_masked['delta_h'][idx+1,:,:]-dataset1_masked['delta_h'][idx,:,:]\n",
    "#                     count_masked = dataset1_masked['data_count'][idx,:,:]\n",
    "#                 else:\n",
    "#                     continue\n",
    "#             elif idx > 32:\n",
    "#                 dhdt_masked = dataset2_masked['delta_h'][(idx-33)+1,:,:]-dataset2_masked['delta_h'][(idx-33),:,:]\n",
    "#                 count_masked = dataset2_masked['data_count'][(idx-33),:,:]\n",
    "            \n",
    "#             if np.any(~np.isnan(dhdt_masked)):       \n",
    "#                 pos = np.nanmax(dhdt_masked)\n",
    "#                 neg = np.nanmin(dhdt_masked)\n",
    "#                 height_anom_pos += [pos]\n",
    "#                 height_anom_neg += [neg]\n",
    "                \n",
    "#                 # Find maximum data count for this cycle\n",
    "#                 max_count = np.nanmax(count_masked)\n",
    "#                 max_counts += [max_count]\n",
    "\n",
    "#         max_height_anom_pos = max(height_anom_pos)\n",
    "#         max_height_anom_neg = min(height_anom_neg)\n",
    "#         max_count_overall = max(max_counts)\n",
    "\n",
    "#         divnorm = colors.TwoSlopeNorm(vmin=max_height_anom_neg, vcenter=0., vmax=max_height_anom_pos)\n",
    "#         countnorm = colors.Normalize(vmin=0, vmax=max_count_overall)\n",
    "\n",
    "#         # Create lines for legend\n",
    "#         stationary_lakes_color = 'darkturquoise'\n",
    "#         stationary_lakes_line = plt.Line2D((0, 1), (0, 0), color=stationary_lakes_color, linestyle=(0, (1, 1)), linewidth=2)\n",
    "#         uplift = plt.Line2D((0, 1), (0, 0), color='mediumblue', linestyle=(0, (3, 5, 1, 5)), linewidth=2)\n",
    "#         subsidence = plt.Line2D((0, 1), (0, 0), color='maroon', linestyle=(0, (3, 5, 1, 5, 1, 5)), linewidth=2)\n",
    "#         search_extent_line = plt.Line2D((0, 1), (0, 0), color='red', linestyle='solid', linewidth=2)\n",
    "    \n",
    "#     # Calculate cycle-to-cycle dh at each cycle of the spliced data sets\n",
    "#     for idx in range(len(mid_cyc_dates)-1):\n",
    "#         # Get time period and check if we should skip\n",
    "#         time_period = lake_gdf['CS2_SARIn_time_period'].iloc[0] if isinstance(lake_gdf['CS2_SARIn_time_period'], pd.Series) else lake_gdf['CS2_SARIn_time_period']\n",
    "#         if pd.notna(time_period) and time_period == '2013.75-2018.75' and 20 <= idx <= 32:\n",
    "#             continue\n",
    "        \n",
    "#         if idx <= 32:\n",
    "#             if dataset1_masked is not None:\n",
    "#                 dhdt_masked = dataset1_masked['delta_h'][idx+1,:,:]-dataset1_masked['delta_h'][idx,:,:]\n",
    "#                 dhdt_masked.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "#                 count_masked = dataset1_masked['data_count'][idx,:,:]\n",
    "#             else:\n",
    "#                 continue\n",
    "#         elif idx > 32:\n",
    "#             dhdt_masked = dataset2_masked['delta_h'][(idx-33)+1,:,:]-dataset2_masked['delta_h'][(idx-33),:,:]\n",
    "#             dhdt_masked.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "#             count_masked = dataset2_masked['data_count'][(idx-33),:,:]\n",
    "\n",
    "#         count_masked.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "#         x_conv = (x_max-x_min)/dhdt_masked.shape[1]\n",
    "#         y_conv = (y_max-y_min)/dhdt_masked.shape[0]\n",
    "\n",
    "#         if np.any(~np.isnan(dhdt_masked)):\n",
    "#             if plot:\n",
    "#                 # Create fig with two subplots\n",
    "#                 fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "#                 # Plot data counts\n",
    "#                 img1 = ax1.imshow(count_masked, extent=[x_min, x_max, y_min, y_max], \n",
    "#                     origin='lower', cmap='Greys', norm=countnorm)\n",
    "\n",
    "#                 # Plot height change\n",
    "#                 img2 = ax2.imshow(dhdt_masked, extent=[x_min, x_max, y_min, y_max], \n",
    "#                     origin='lower', cmap='coolwarm_r', \n",
    "#                     norm=divnorm)\n",
    "\n",
    "#                 # Plot the boundary of the evolving outline search extent on both subplots\n",
    "#                 gpd.GeoSeries(buffered_poly).boundary.plot(ax=ax1, color='magenta')\n",
    "#                 gpd.GeoSeries(buffered_poly).boundary.plot(ax=ax2, color='magenta')\n",
    "\n",
    "#             # Create contours and plot them (only on the height change subplot)\n",
    "#             contours_pos = []\n",
    "#             contours_neg = []\n",
    "\n",
    "#             if np.any(~np.isnan(count_masked)):\n",
    "#                 contour_pos = measure.find_contours(dhdt_masked.values, level)\n",
    "#                 if len(contour_pos) > 0: \n",
    "#                     contours_pos += [contour_pos]\n",
    "#                 contour_neg = measure.find_contours(dhdt_masked.values, -level)\n",
    "#                 if len(contour_neg) > 0: \n",
    "#                     contours_neg += [contour_neg]\n",
    "\n",
    "#             for i in range(len(contours_pos)): \n",
    "#                 for j in range(len(contours_pos[i])):\n",
    "#                     x = x_min+contours_pos[i][j][:,1]*x_conv\n",
    "#                     y = y_min+contours_pos[i][j][:,0]*y_conv\n",
    "#                     if plot:\n",
    "#                         ax1.plot(x, y, color='blue', linestyle='dashdot', linewidth=1, label=level)\n",
    "#                         ax2.plot(x, y, color='blue', linestyle='dashdot', linewidth=1, label=level)\n",
    "\n",
    "#                     if len(contours_pos[i][j][:,1]) > 2: \n",
    "#                         poly = Polygon(list(zip(x, y)))\n",
    "#                         try:\n",
    "#                             dhdt_poly = dhdt_masked.rio.clip([poly])   \n",
    "#                             lon, lat = XY_TO_LL.transform(x,y)\n",
    "#                             poly_area = abs(GEOD.polygon_area_perimeter(lon, lat)[0])\n",
    "#                             if np.any(~np.isnan(dhdt_poly)):\n",
    "#                                 poly_dh = np.nanmean(dhdt_poly)\n",
    "#                                 poly_dvol = poly_dh*poly_area\n",
    "#                                 polys += [poly]\n",
    "#                                 areas += [poly_area]\n",
    "#                                 dhs += [poly_dh]\n",
    "#                                 dvols += [poly_dvol]\n",
    "#                                 midcyc_datetimes += [mid_cyc_dates[idx]]\n",
    "#                         except NoDataInBounds:\n",
    "#                             pass\n",
    "#                         except Exception as e:\n",
    "#                             raise\n",
    "\n",
    "#             for i in range(len(contours_neg)): \n",
    "#                 for j in range(len(contours_neg[i])):\n",
    "#                     x = x_min+contours_neg[i][j][:,1]*x_conv\n",
    "#                     y = y_min+contours_neg[i][j][:,0]*y_conv\n",
    "#                     if plot:\n",
    "#                         ax1.plot(x, y, color='red', linestyle=(0, (3, 1, 1, 1)), linewidth=1, label=-level)\n",
    "#                         ax2.plot(x, y, color='red', linestyle=(0, (3, 1, 1, 1)), linewidth=1, label=-level)\n",
    "\n",
    "#                     if len(contours_neg[i][j][:,1]) > 2: \n",
    "#                         poly = Polygon(list(zip(x, y)))\n",
    "#                         try:\n",
    "#                             dhdt_poly = dhdt_masked.rio.clip([poly]) \n",
    "#                             lon, lat = XY_TO_LL.transform(x,y)\n",
    "#                             poly_area = abs(GEOD.polygon_area_perimeter(lon, lat)[0])\n",
    "#                             if np.any(~np.isnan(dhdt_poly)):\n",
    "#                                 poly_dh = np.nanmean(dhdt_poly)\n",
    "#                                 poly_dvol = poly_dh*poly_area\n",
    "#                                 polys += [poly]\n",
    "#                                 areas += [poly_area]\n",
    "#                                 dhs += [poly_dh]\n",
    "#                                 dvols += [poly_dvol]\n",
    "#                                 midcyc_datetimes += [mid_cyc_dates[idx]]\n",
    "#                         except NoDataInBounds:\n",
    "#                             pass\n",
    "#                         except Exception as e:\n",
    "#                             raise\n",
    "\n",
    "#             if plot:\n",
    "#                 # Common plotting for both subplots\n",
    "#                 for ax in [ax1, ax2]:\n",
    "#                     stationary_lakes_gdf.boundary.plot(ax=ax, edgecolor=stationary_lakes_color, linestyle='solid', linewidth=2)          \n",
    "\n",
    "#                     km_scale = 1e3\n",
    "#                     ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "#                     ax.xaxis.set_major_formatter(ticks_x)\n",
    "#                     ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "#                     ax.yaxis.set_major_formatter(ticks_y)  \n",
    "\n",
    "#                     ax.set_xlabel('x [km]', size=15)\n",
    "#                     ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "#                 # Plotting for ax1\n",
    "#                 ax1.set_ylabel('y [km]', size=15)\n",
    "\n",
    "#                 axIns = ax1.inset_axes([0.01, 0.01, 0.2, 0.2])\n",
    "#                 axIns.set_aspect('equal')\n",
    "#                 moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "#                 moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "#                 axIns.axis('off')\n",
    "\n",
    "#                 axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "#                     linewidth=1, edgecolor='k', facecolor='r', s=100, zorder=3)\n",
    "\n",
    "#                 # Add colorbars\n",
    "#                 divider1 = make_axes_locatable(ax1)\n",
    "#                 cax1 = divider1.append_axes('right', size='5%', pad=0.2)\n",
    "#                 fig.colorbar(img1, cax=cax1).set_label('data counts', size=15)\n",
    "\n",
    "#                 divider2 = make_axes_locatable(ax2)\n",
    "#                 cax2 = divider2.append_axes('right', size='5%', pad=0.2)\n",
    "#                 fig.colorbar(img2, cax=cax2).set_label('height change (dh) [m]', size=15)\n",
    "\n",
    "#                 # Add legend to height change subplot\n",
    "#                 ax1.legend([stationary_lakes_line,\n",
    "#                            uplift, \n",
    "#                            subsidence,\n",
    "#                            search_extent_line],\n",
    "#                     ['stationary outline ',\n",
    "#                      ('+ '+str(level)+' m evolving outline'), \n",
    "#                      (' '+str(level)+' m evolving outline'),\n",
    "#                      'search extent ('+str(int(area_multiple_search_extent))+'x)'], \n",
    "#                     loc='upper left')\n",
    "\n",
    "#                 ax2.legend([stationary_lakes_line,\n",
    "#                            uplift, \n",
    "#                            subsidence,\n",
    "#                            search_extent_line],\n",
    "#                     ['stationary outline ', \n",
    "#                      ('+ '+str(level)+' m evolving outline'), \n",
    "#                      (' '+str(level)+' m evolving outline'),\n",
    "#                      'search extent ('+str(int(area_multiple_search_extent))+'x)'], \n",
    "#                     loc='upper left')\n",
    "\n",
    "#                 # Set titles for subplots\n",
    "#                 ax1.set_title('Data counts')\n",
    "#                 ax2.set_title('Height change')\n",
    "\n",
    "#                 # Set a common title for both subplots\n",
    "#                 fig.suptitle('From {} to {}'.format(cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)), fontsize=16)\n",
    "\n",
    "#                 # Adjust layout and save figure\n",
    "#                 plt.tight_layout()\n",
    "#                 plt.savefig(OUTPUT_DIR + \n",
    "#                     '/find_evolving_outlines/{}/find_evolving_outlines_{}_{}x-search-extent_{}m-level_{}-{}.png'\n",
    "#                     .format(lake_name, lake_name, int(area_multiple_search_extent), level, cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)), dpi=300, bbox_inches='tight')\n",
    "#                 plt.close()\n",
    "\n",
    "#     # Store optimal search extent and level information\n",
    "#     area_multiple_search_extents = [area_multiple_search_extent for _ in range(len(polys))]\n",
    "#     levels = [level for _ in range(len(polys))]\n",
    "\n",
    "#     # At the end, check if we found any polygons\n",
    "#     if not polys:\n",
    "#         return None\n",
    "\n",
    "#     # Create GeoDataFrame\n",
    "#     gdf = gpd.GeoDataFrame({\n",
    "#         'area_multiple_search_extent': area_multiple_search_extents,\n",
    "#         'level': levels,\n",
    "#         'geometry': polys, \n",
    "#         'area (m^2)': areas, \n",
    "#         'dh (m)': dhs, \n",
    "#         'vol (m^3)': dvols,\n",
    "#         'midcyc_datetime': midcyc_datetimes\n",
    "#     }, crs=\"EPSG:3031\")\n",
    "    \n",
    "#     return gdf\n",
    "\n",
    "# def should_skip_period(time_period, idx):\n",
    "#     \"\"\"\n",
    "#     Determine if we should skip this time period index based on the CS2_SARIn_time_period.\n",
    "#     Returns True if period should be skipped.\n",
    "#     \"\"\"\n",
    "#     if pd.isna(time_period):\n",
    "#         return False  # If no time period specified, don't skip any\n",
    "#     if time_period == '2013.75-2018.75' and 20 <= idx <= 32:\n",
    "#         return True\n",
    "#     return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408aed62-cd9b-4dcb-ab2b-944d49f0a0a1",
   "metadata": {},
   "source": [
    "## find_evolving_outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f3afa282-22d0-4897-8528-ef42e551f86b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (3992990441.py, line 135)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[85], line 135\u001b[0;36m\u001b[0m\n\u001b[0;31m    gpd.GeoSeries(multiple_area_buffer(lake_poly, area_multiple_search_extent).boundary.plot(ax=ax2, color='magenta')\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "# Trying to fix outline alignment issue\n",
    "# Trying to fix outlines labeled within but appear intersecting with search extent\n",
    "# Trying to optimize to use masking instead of clipping for outline poly dhdt calc\n",
    "\n",
    "def find_evolving_outlines(lake_gdf, area_multiple_search_extent, level, dataset1_masked, dataset2_masked, buffered_poly, plot=False): \n",
    "    '''\n",
    "    Create time-variable outlines using skimage contour to plot evolving outlines as polygons.\n",
    "    If plot=True, create planview dh/dt plots of ice surface height changes with evolving outlines found,\n",
    "    along with a subplot showing data counts.\n",
    "\n",
    "    Inputs:\n",
    "    * lake_gdf: GeoDataFrame containing lake information\n",
    "    * area_multiple_search_extent: Factor used elsewhere to multiply lake area for search extent boundary polygon\n",
    "    * level: vertical distance in meters to delineate ice surface deformation contour\n",
    "    * dataset1_masked: masked dataset1 to be analyzed\n",
    "    * dataset2_masked: masked dataset2 to be analyzed\n",
    "    * buffered_poly: buffered polygon that is 1x more area multiple than the search extent boundary to allow\n",
    "    outlines to be determined if they are intersected by the search extent\n",
    "    * plot: boolean, if True, create and save plots\n",
    "    \n",
    "    Outputs: \n",
    "    * geopandas geodataframe of polygons created at each step\n",
    "    * If plot=True, sequence of planview delta height visuals of ICESat-2 ATL15 with variable ice surface \n",
    "    deformation contours plotted to delineate evolving lake boundaries, along with data count subplots.\n",
    "\n",
    "    # Example usage\n",
    "    >>> outlines_gdf = find_evolving_outlines(lake_gdf=lake_gdf, area_multiple_search_extent=2, level=0.1, \n",
    "        dataset1_masked=dataset1_masked, dataset2_masked=dataset2_masked, plot=False)\n",
    "    '''    \n",
    "    # Define lake name and polygon\n",
    "    lake_name = lake_gdf['name']\n",
    "    lake_poly = lake_gdf['geometry']\n",
    "\n",
    "    # Establish x_min, x_max, y_min, y_max\n",
    "    x_min, y_min, x_max, y_max = buffered_poly.bounds\n",
    "    x_buffer = abs(x_max-x_min)*0.2\n",
    "    y_buffer = abs(y_max-y_min)*0.2\n",
    "    \n",
    "    # Create empty lists to store polygons, areas, dh's, dvol's and dates\n",
    "    polys = []\n",
    "    areas = []\n",
    "    dhs = []\n",
    "    dvols = []\n",
    "    midcyc_datetimes = []\n",
    "\n",
    "    # Only proceed with plotting if plot=True\n",
    "    if plot:\n",
    "        # Find magnitude of ice surface deformation and maximum data counts\n",
    "        height_anom_pos = []\n",
    "        height_anom_neg = []\n",
    "        max_counts = []\n",
    "        \n",
    "        for idx in range(len(mid_cyc_dates)-1):\n",
    "            # Get time period and check if we should skip\n",
    "            time_period = lake_gdf['CS2_SARIn_time_period'].iloc[0] if isinstance(lake_gdf['CS2_SARIn_time_period'], pd.Series) else lake_gdf['CS2_SARIn_time_period']\n",
    "            if pd.notna(time_period) and time_period == '2013.75-2018.75' and 20 <= idx <= 32:\n",
    "                continue\n",
    "\n",
    "            if idx <= 32:\n",
    "                if dataset1_masked is not None:\n",
    "                    dhdt_masked = dataset1_masked['delta_h'][idx+1,:,:]-dataset1_masked['delta_h'][idx,:,:]\n",
    "                    count_masked = dataset1_masked['data_count'][idx,:,:]\n",
    "                else:\n",
    "                    continue\n",
    "            elif idx > 32:\n",
    "                dhdt_masked = dataset2_masked['delta_h'][(idx-33)+1,:,:]-dataset2_masked['delta_h'][(idx-33),:,:]\n",
    "                count_masked = dataset2_masked['data_count'][(idx-33),:,:]\n",
    "            \n",
    "            if np.any(~np.isnan(dhdt_masked)):       \n",
    "                pos = np.nanmax(dhdt_masked)\n",
    "                neg = np.nanmin(dhdt_masked)\n",
    "                height_anom_pos += [pos]\n",
    "                height_anom_neg += [neg]\n",
    "                \n",
    "                # Find maximum data count for this cycle\n",
    "                max_count = np.nanmax(count_masked)\n",
    "                max_counts += [max_count]\n",
    "\n",
    "        max_height_anom_pos = max(height_anom_pos)\n",
    "        max_height_anom_neg = min(height_anom_neg)\n",
    "        max_count_overall = max(max_counts)\n",
    "\n",
    "        divnorm = colors.TwoSlopeNorm(vmin=max_height_anom_neg, vcenter=0., vmax=max_height_anom_pos)\n",
    "        countnorm = colors.Normalize(vmin=0, vmax=max_count_overall)\n",
    "\n",
    "        # Create lines for legend\n",
    "        stationary_lakes_color = 'darkturquoise'\n",
    "        stationary_lakes_line = plt.Line2D((0, 1), (0, 0), color=stationary_lakes_color, linestyle='solid', linewidth=2)\n",
    "        uplift = plt.Line2D((0, 1), (0, 0), color='mediumblue', linestyle=(0, (3, 5, 1, 5)), linewidth=2)\n",
    "        subsidence = plt.Line2D((0, 1), (0, 0), color='maroon', linestyle=(0, (3, 5, 1, 5, 1, 5)), linewidth=2)\n",
    "        search_extent_line = plt.Line2D((0, 1), (0, 0), color='magenta', linestyle='solid', linewidth=2)\n",
    "    \n",
    "    # Calculate cycle-to-cycle dh at each cycle of the spliced data sets\n",
    "    for idx in range(len(mid_cyc_dates)-1):\n",
    "        # Get time period and check if we should skip\n",
    "        time_period = lake_gdf['CS2_SARIn_time_period'].iloc[0] if isinstance(lake_gdf['CS2_SARIn_time_period'], pd.Series) else lake_gdf['CS2_SARIn_time_period']\n",
    "        if pd.notna(time_period) and time_period == '2013.75-2018.75' and 20 <= idx <= 32:\n",
    "            continue\n",
    "        \n",
    "        if idx <= 32:\n",
    "            if dataset1_masked is not None:\n",
    "                dhdt_masked = dataset1_masked['delta_h'][idx+1,:,:]-dataset1_masked['delta_h'][idx,:,:]\n",
    "                dhdt_masked.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "                count_masked = dataset1_masked['data_count'][idx,:,:]\n",
    "            else:\n",
    "                continue\n",
    "        elif idx > 32:\n",
    "            dhdt_masked = dataset2_masked['delta_h'][(idx-33)+1,:,:]-dataset2_masked['delta_h'][(idx-33),:,:]\n",
    "            dhdt_masked.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "            count_masked = dataset2_masked['data_count'][(idx-33),:,:]\n",
    "\n",
    "        count_masked.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "        x_conv = (x_max-x_min)/dhdt_masked.shape[1]\n",
    "        y_conv = (y_max-y_min)/dhdt_masked.shape[0]\n",
    "\n",
    "        if np.any(~np.isnan(dhdt_masked)):\n",
    "            if plot:\n",
    "                # Create fig with two subplots\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "                # Plot data counts\n",
    "                img1 = ax1.imshow(count_masked, extent=[x_min, x_max, y_min, y_max], \n",
    "                    origin='lower', cmap='Greys', norm=countnorm)\n",
    "\n",
    "                # Plot height change\n",
    "                img2 = ax2.imshow(dhdt_masked, extent=[x_min, x_max, y_min, y_max], \n",
    "                    origin='lower', cmap='coolwarm_r', \n",
    "                    norm=divnorm)\n",
    "\n",
    "                # Plot the boundary of the evolving outline search extent on both subplots\n",
    "                # gpd.GeoSeries(buffered_poly).boundary.plot(ax=ax1, color='magenta')\n",
    "                # gpd.GeoSeries(buffered_poly).boundary.plot(ax=ax2, color='magenta')\n",
    "                gpd.GeoSeries(multiple_area_buffer(lake_poly, area_multiple_search_extent).boundary.plot(ax=ax1, color='magenta')\n",
    "                gpd.GeoSeries(multiple_area_buffer(lake_poly, area_multiple_search_extent).boundary.plot(ax=ax2, color='magenta')\n",
    "                \n",
    "            # Create contours and plot them (only on the height change subplot)\n",
    "            contours_pos = []\n",
    "            contours_neg = []\n",
    "\n",
    "            if np.any(~np.isnan(count_masked)):\n",
    "                contour_pos = measure.find_contours(dhdt_masked.values, level)\n",
    "                if len(contour_pos) > 0: \n",
    "                    contours_pos += [contour_pos]\n",
    "                contour_neg = measure.find_contours(dhdt_masked.values, -level)\n",
    "                if len(contour_neg) > 0: \n",
    "                    contours_neg += [contour_neg]\n",
    "\n",
    "            for i in range(len(contours_pos)): \n",
    "                for j in range(len(contours_pos[i])):\n",
    "                    x = x_min+contours_pos[i][j][:,1]*x_conv+(0.5*x_conv)\n",
    "                    y = y_min+contours_pos[i][j][:,0]*y_conv+(0.5*y_conv)\n",
    "                    if plot:\n",
    "                        ax1.plot(x, y, color='blue', linestyle='dashdot', linewidth=1, label=level)\n",
    "                        ax2.plot(x, y, color='blue', linestyle='dashdot', linewidth=1, label=level)\n",
    "\n",
    "                    if len(contours_pos[i][j][:,1]) > 2: \n",
    "                        poly = Polygon(list(zip(x, y)))\n",
    "                        try:\n",
    "                            dhdt_poly = dhdt_masked.rio.clip([poly])   \n",
    "                            lon, lat = XY_TO_LL.transform(x,y)\n",
    "                            poly_area = abs(GEOD.polygon_area_perimeter(lon, lat)[0])\n",
    "                            if np.any(~np.isnan(dhdt_poly)):\n",
    "                                poly_dh = np.nanmean(dhdt_poly)\n",
    "                                poly_dvol = poly_dh*poly_area\n",
    "                                polys += [poly]\n",
    "                                areas += [poly_area]\n",
    "                                dhs += [poly_dh]\n",
    "                                dvols += [poly_dvol]\n",
    "                                midcyc_datetimes += [mid_cyc_dates[idx]]\n",
    "                        except NoDataInBounds:\n",
    "                            pass\n",
    "                        except Exception as e:\n",
    "                            raise\n",
    "\n",
    "            for i in range(len(contours_neg)): \n",
    "                for j in range(len(contours_neg[i])):\n",
    "                    x = x_min+contours_neg[i][j][:,1]*x_conv+(0.5*x_conv)\n",
    "                    y = y_min+contours_neg[i][j][:,0]*y_conv+(0.5*y_conv)\n",
    "                    if plot:\n",
    "                        ax1.plot(x, y, color='red', linestyle=(0, (3, 1, 1, 1)), linewidth=1, label=-level)\n",
    "                        ax2.plot(x, y, color='red', linestyle=(0, (3, 1, 1, 1)), linewidth=1, label=-level)\n",
    "\n",
    "                    if len(contours_neg[i][j][:,1]) > 2: \n",
    "                        poly = Polygon(list(zip(x, y)))\n",
    "                        try:\n",
    "                            dhdt_poly = dhdt_masked.rio.clip([poly]) \n",
    "                            lon, lat = XY_TO_LL.transform(x,y)\n",
    "                            poly_area = abs(GEOD.polygon_area_perimeter(lon, lat)[0])\n",
    "                            if np.any(~np.isnan(dhdt_poly)):\n",
    "                                poly_dh = np.nanmean(dhdt_poly)\n",
    "                                poly_dvol = poly_dh*poly_area\n",
    "                                polys += [poly]\n",
    "                                areas += [poly_area]\n",
    "                                dhs += [poly_dh]\n",
    "                                dvols += [poly_dvol]\n",
    "                                midcyc_datetimes += [mid_cyc_dates[idx]]\n",
    "                        except NoDataInBounds:\n",
    "                            pass\n",
    "                        except Exception as e:\n",
    "                            raise\n",
    "\n",
    "            if plot:\n",
    "                # Common plotting for both subplots\n",
    "                for ax in [ax1, ax2]:\n",
    "                    stationary_lakes_gdf.boundary.plot(ax=ax, edgecolor=stationary_lakes_color, linestyle='solid', linewidth=2)          \n",
    "\n",
    "                    km_scale = 1e3\n",
    "                    ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "                    ax.xaxis.set_major_formatter(ticks_x)\n",
    "                    ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "                    ax.yaxis.set_major_formatter(ticks_y)  \n",
    "\n",
    "                    ax.set_xlabel('x [km]', size=15)\n",
    "                    ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "                # Plotting for ax1\n",
    "                ax1.set_ylabel('y [km]', size=15)\n",
    "\n",
    "                axIns = ax1.inset_axes([0.01, 0.01, 0.2, 0.2])\n",
    "                axIns.set_aspect('equal')\n",
    "                moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "                moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "                axIns.axis('off')\n",
    "\n",
    "                axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "                    linewidth=1, edgecolor='k', facecolor='r', s=100, zorder=3)\n",
    "\n",
    "                # Add colorbars\n",
    "                divider1 = make_axes_locatable(ax1)\n",
    "                cax1 = divider1.append_axes('right', size='5%', pad=0.2)\n",
    "                fig.colorbar(img1, cax=cax1).set_label('data counts', size=15)\n",
    "\n",
    "                divider2 = make_axes_locatable(ax2)\n",
    "                cax2 = divider2.append_axes('right', size='5%', pad=0.2)\n",
    "                fig.colorbar(img2, cax=cax2).set_label('height change (dh) [m]', size=15)\n",
    "\n",
    "                # Add legend to height change subplot\n",
    "                ax1.legend([stationary_lakes_line,\n",
    "                           uplift, \n",
    "                           subsidence,\n",
    "                           search_extent_line],\n",
    "                    ['stationary outline ',\n",
    "                     ('+ '+str(level)+' m evolving outline'), \n",
    "                     (' '+str(level)+' m evolving outline'),\n",
    "                     'search extent ('+str(int(area_multiple_search_extent))+'x)'], \n",
    "                    loc='upper left')\n",
    "\n",
    "                ax2.legend([stationary_lakes_line,\n",
    "                           uplift, \n",
    "                           subsidence,\n",
    "                           search_extent_line],\n",
    "                    ['stationary outline ', \n",
    "                     ('+ '+str(level)+' m evolving outline'), \n",
    "                     (' '+str(level)+' m evolving outline'),\n",
    "                     'search extent ('+str(int(area_multiple_search_extent))+'x)'], \n",
    "                    loc='upper left')\n",
    "\n",
    "                # Set titles for subplots\n",
    "                ax1.set_title('Data counts')\n",
    "                ax2.set_title('Height change')\n",
    "\n",
    "                # Set a common title for both subplots\n",
    "                fig.suptitle('From {} to {}'.format(cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)), fontsize=16)\n",
    "\n",
    "                # Adjust layout and save figure\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(OUTPUT_DIR + \n",
    "                    '/find_evolving_outlines/{}/find_evolving_outlines_{}_{}x-search-extent_{}m-level_{}-{}.png'\n",
    "                    .format(lake_name, lake_name, int(area_multiple_search_extent), level, cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)), dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "\n",
    "    # Create arrays of search extent and level information for geodataframe creation\n",
    "    area_multiple_search_extents = [area_multiple_search_extent for _ in range(len(polys))]\n",
    "    levels = [level for _ in range(len(polys))]\n",
    "\n",
    "    # At the end, check if we found any polygons\n",
    "    if not polys:\n",
    "        return None\n",
    "\n",
    "    # Create GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame({\n",
    "        'area_multiple_search_extent': area_multiple_search_extents,\n",
    "        'level': levels,\n",
    "        'geometry': polys, \n",
    "        'area (m^2)': areas, \n",
    "        'dh (m)': dhs, \n",
    "        'vol (m^3)': dvols,\n",
    "        'midcyc_datetime': midcyc_datetimes\n",
    "    }, crs=\"EPSG:3031\")\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d18c01ba-ef36-49da-b3de-39fd4ce9692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def finalize_evolving_outlines(stationary_lakes_gdf):\n",
    "#     '''\n",
    "#     Finalize the evolving outlines for each stationary lake using optimal search extent and level.\n",
    "#     Includes topology validation and proper error handling.\n",
    "#     '''\n",
    "    \n",
    "#     for idx, row in stationary_lakes_gdf.iterrows():   \n",
    "#         # Select lake\n",
    "#         lake_gdf = stationary_lakes_gdf.loc[idx]\n",
    "#         lake_name = lake_gdf['name']\n",
    "#         print('Finalizing outlines for', lake_name)\n",
    "    \n",
    "#         try:\n",
    "#             # Load search extents and levels dataframe\n",
    "#             search_extents_levels_df = pd.read_csv(OUTPUT_DIR + '/search_extents_levels/{}.csv'.format(lake_name))\n",
    "#             if search_extents_levels_df.empty:\n",
    "#                 write_no_outlines(lake_name)\n",
    "#                 continue\n",
    "\n",
    "#             # Sort the DataFrame\n",
    "#             sorted_df = search_extents_levels_df.sort_values(\n",
    "#                 by=['level', 'area_multiple_search_extent'], \n",
    "#                 ascending=[True, True])\n",
    "            \n",
    "#             selected_row = sorted_df.iloc[0]\n",
    "#             print(selected_row)\n",
    "        \n",
    "#             # Assign dataset\n",
    "#             dataset1_masked, dataset2_masked = prepare_datasets(\n",
    "#                 lake_gdf, \n",
    "#                 selected_row['area_multiple_search_extent']\n",
    "#             )\n",
    "            \n",
    "#             # Create output folders\n",
    "#             os.makedirs(OUTPUT_DIR + '/find_evolving_outlines', exist_ok=True)\n",
    "#             os.makedirs(OUTPUT_DIR + f'/find_evolving_outlines/{lake_name}', exist_ok=True)\n",
    "#             os.makedirs(OUTPUT_DIR + '/plot_evolving_outlines', exist_ok=True)\n",
    "#             os.makedirs(OUTPUT_DIR + '/lake_outlines/evolving_outlines', exist_ok=True)\n",
    "\n",
    "#             # Find evolving outlines\n",
    "#             evolving_outlines_gdf = find_evolving_outlines(\n",
    "#                 lake_gdf=lake_gdf, \n",
    "#                 area_multiple_search_extent=selected_row['area_multiple_search_extent'], \n",
    "#                 level=selected_row['level'], \n",
    "#                 dataset1_masked=dataset1_masked,\n",
    "#                 dataset2_masked=dataset2_masked,\n",
    "#                 buffered_poly=get_buffered_poly(lake_gdf, selected_row['area_multiple_search_extent']),\n",
    "#                 plot=True\n",
    "#             )\n",
    "           \n",
    "#             if evolving_outlines_gdf is None or evolving_outlines_gdf.empty:\n",
    "#                 write_no_outlines(lake_name)\n",
    "#                 continue\n",
    "            \n",
    "#             # Clean geometries before filtering\n",
    "#             try:\n",
    "#                 print(f\"Cleaning geometries for {lake_name}...\")\n",
    "#                 # Clean evolving outlines geometries\n",
    "#                 evolving_outlines_gdf = evolving_outlines_gdf.copy()\n",
    "#                 evolving_outlines_gdf.loc[:, 'geometry'] = evolving_outlines_gdf['geometry'].apply(\n",
    "#                     lambda geom: make_valid(geom) if not geom.is_valid else geom\n",
    "#                 )\n",
    "                \n",
    "#                 # Clean reference geometry\n",
    "#                 reference_geometry = make_valid(lake_gdf['geometry']) if not lake_gdf['geometry'].is_valid else lake_gdf['geometry']\n",
    "                \n",
    "#                 # Extract intersecting polygons\n",
    "#                 filtered_gdf = extract_intersecting_polygons_recursive(\n",
    "#                     evolving_outlines_gdf, \n",
    "#                     reference_geometry\n",
    "#                 )\n",
    "                \n",
    "#                 if filtered_gdf is None or filtered_gdf.empty:\n",
    "#                     print(f\"No valid filtered outlines found for {lake_name}\")\n",
    "#                     write_no_outlines(lake_name)\n",
    "#                     continue\n",
    "                    \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error cleaning geometries for {lake_name}: {str(e)}\")\n",
    "#                 print(\"Attempting to continue with original geometries...\")\n",
    "#                 traceback.print_exc()\n",
    "                \n",
    "#                 try:\n",
    "#                     filtered_gdf = extract_intersecting_polygons_recursive(\n",
    "#                         evolving_outlines_gdf, \n",
    "#                         lake_gdf['geometry']\n",
    "#                     )\n",
    "                    \n",
    "#                     if filtered_gdf is None or filtered_gdf.empty:\n",
    "#                         print(f\"No valid filtered outlines found for {lake_name}\")\n",
    "#                         write_no_outlines(lake_name)\n",
    "#                         continue\n",
    "                        \n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error extracting polygons for {lake_name}: {str(e)}\")\n",
    "#                     traceback.print_exc()\n",
    "#                     write_no_outlines(lake_name)\n",
    "#                     continue\n",
    "                \n",
    "#             # Save results and plot\n",
    "#             if not filtered_gdf.empty:\n",
    "#                 try:\n",
    "#                     # Add metadata to filtered_gdf\n",
    "#                     filtered_gdf = filtered_gdf.copy()\n",
    "#                     filtered_gdf.loc[:, 'area_multiple_search_extent'] = selected_row['area_multiple_search_extent']\n",
    "#                     filtered_gdf.loc[:, 'level'] = selected_row['level']\n",
    "#                     filtered_gdf.loc[:, 'within_percent'] = selected_row['within_percent'] if 'within_percent' in selected_row else 100.0\n",
    "                    \n",
    "#                     # Export evolving outlines GeoDataFrame to GeoJSON\n",
    "#                     output_path = 'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)\n",
    "#                     filtered_gdf.to_file(filename=output_path, driver='GeoJSON')\n",
    "#                     print(f\"Saved outlines to: {output_path}\")\n",
    "                    \n",
    "#                     # Plot the outlines\n",
    "#                     try:\n",
    "#                         plot_evolving_outlines(\n",
    "#                             lake_gdf=lake_gdf,\n",
    "#                             evolving_outlines_gdf=filtered_gdf\n",
    "#                         )\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error plotting outlines for {lake_name}: {str(e)}\")\n",
    "#                         traceback.print_exc()\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error saving results for {lake_name}: {str(e)}\")\n",
    "#                     traceback.print_exc()\n",
    "#                     write_no_outlines(lake_name)\n",
    "#             else:\n",
    "#                 print(f\"No filtered outlines to save for {lake_name}\")\n",
    "#                 write_no_outlines(lake_name)\n",
    "\n",
    "#             # Clear memory\n",
    "#             gc.collect()\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {lake_name}: {str(e)}\")\n",
    "#             traceback.print_exc()\n",
    "#             continue\n",
    "            \n",
    "#         finally:\n",
    "#             # Clean up memory\n",
    "#             for var in ['dataset1_masked', 'dataset2_masked', 'evolving_outlines_gdf', 'filtered_gdf']:\n",
    "#                 if var in locals():\n",
    "#                     del locals()[var]\n",
    "#             gc.collect()\n",
    "#             clear_output(wait=True)\n",
    "\n",
    "# def write_no_outlines(filepath):\n",
    "#     \"\"\"Write file indicating no outlines found\"\"\"\n",
    "#     try:\n",
    "#         # Ensure directory exists\n",
    "#         os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "#         # Write file\n",
    "#         with open(filepath, 'w') as f:\n",
    "#             f.write(\"There are no evolving outlines for this lake.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error writing no outlines file to {filepath}: {str(e)}\")\n",
    "#         traceback.print_exc()\n",
    "\n",
    "# def prepare_datasets(lake_gdf, area_multiple):\n",
    "#     \"\"\"Prepare masked datasets based on lake parameters\"\"\"\n",
    "#     CS2_SARIn_time_period = lake_gdf['CS2_SARIn_time_period']\n",
    "    \n",
    "#     # Initialize dataset1\n",
    "#     if pd.isna(CS2_SARIn_time_period) or CS2_SARIn_time_period == '<NA>':\n",
    "#         dataset1 = None\n",
    "#     elif CS2_SARIn_time_period == '2013.75-2018.75':\n",
    "#         dataset1 = CS2_Smith2017.sel(time=slice(np.datetime64('2013-10-01T22:30:00.000000000'), None))\n",
    "#     elif CS2_SARIn_time_period == '2010.5-2018.75':\n",
    "#         dataset1 = CS2_Smith2017\n",
    "    \n",
    "#     dataset2 = ATL15_dh\n",
    "    \n",
    "#     # Mask datasets\n",
    "#     lake_poly = lake_gdf['geometry']\n",
    "#     buffered_poly = get_buffered_poly(lake_gdf, area_multiple)\n",
    "#     x_min, y_min, x_max, y_max = buffered_poly.bounds\n",
    "    \n",
    "#     return mask_datasets(dataset1, dataset2, buffered_poly, x_min, x_max, y_min, y_max)\n",
    "\n",
    "# def mask_datasets(dataset1, dataset2, buffered_poly, x_min, x_max, y_min, y_max):\n",
    "#     \"\"\"Apply masks to both datasets\"\"\"\n",
    "#     dataset1_masked = None\n",
    "#     if dataset1 is not None:\n",
    "#         dataset1_masked = apply_mask_to_dataset(dataset1, buffered_poly, x_min, x_max, y_min, y_max)\n",
    "    \n",
    "#     dataset2_masked = apply_mask_to_dataset(dataset2, buffered_poly, x_min, x_max, y_min, y_max)\n",
    "#     return dataset1_masked, dataset2_masked\n",
    "\n",
    "# def apply_mask_to_dataset(dataset, buffered_poly, x_min, x_max, y_min, y_max):\n",
    "#     \"\"\"Apply mask to a single dataset\"\"\"\n",
    "#     dataset_sub = dataset.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "#     mask = np.array([[buffered_poly.contains(Point(x, y)) \n",
    "#                      for x in dataset_sub['x'].values] \n",
    "#                      for y in dataset_sub['y'].values])\n",
    "#     mask_da = xr.DataArray(mask, coords=[dataset_sub.y, dataset_sub.x], dims=[\"y\", \"x\"])\n",
    "#     return dataset.where(mask_da, drop=True)\n",
    "\n",
    "# def get_buffered_poly(lake_gdf, area_multiple):\n",
    "#     \"\"\"Get buffered polygon for lake\"\"\"\n",
    "#     return multiple_area_buffer(lake_gdf['geometry'], area_multiple + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d9081-e42b-4ee5-90c2-fa0f5bc63ca2",
   "metadata": {},
   "source": [
    "## finalize_evolving_outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a37924e-eae5-4563-92e2-19fa0dba039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_evolving_outlines(stationary_lakes_gdf, row_index=0):\n",
    "    '''\n",
    "    Finalize the evolving outlines for each stationary lake using optimal search extent and level.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    stationary_lakes_gdf : GeoDataFrame\n",
    "        The dataset of stationary lakes\n",
    "    row_index : int, optional (default=0)\n",
    "        Index of the row to use from the sorted search_extents_levels_df dataframe.\n",
    "        0 gives the smallest level and area_multiple_search_extent,\n",
    "        -1 gives the largest level and area_multiple_search_extent.\n",
    "    '''\n",
    "    for idx, row in stationary_lakes_gdf.iterrows():   \n",
    "        # Select lake\n",
    "        lake_gdf = stationary_lakes_gdf.loc[idx]\n",
    "        lake_name = lake_gdf['name']\n",
    "        print('Finalizing outlines for', lake_name)\n",
    "    \n",
    "        try:\n",
    "            # Load search extents and levels dataframe\n",
    "            search_extents_levels_df = pd.read_csv(OUTPUT_DIR + '/search_extents_levels/{}.csv'.format(lake_name))\n",
    "            if search_extents_levels_df.empty:\n",
    "                write_no_outlines(lake_name)\n",
    "                continue\n",
    "\n",
    "            # Sort the DataFrame\n",
    "            sorted_df = search_extents_levels_df.sort_values(\n",
    "                by=['level', 'area_multiple_search_extent'], \n",
    "                ascending=[True, True])\n",
    "            \n",
    "            # Select row based on provided index\n",
    "            if abs(row_index) >= len(sorted_df):\n",
    "                print(f\"Warning: row_index {row_index} out of bounds for {lake_name}. Using first row.\")\n",
    "                selected_row = sorted_df.iloc[0]\n",
    "            else:\n",
    "                selected_row = sorted_df.iloc[row_index]\n",
    "            print(f\"Using parameters from row {row_index}:\")\n",
    "            print(selected_row)\n",
    "        \n",
    "            # Assign dataset\n",
    "            dataset1_masked, dataset2_masked = prepare_datasets(\n",
    "                lake_gdf, \n",
    "                selected_row['area_multiple_search_extent']\n",
    "            )\n",
    "            \n",
    "            # Create output folders\n",
    "            os.makedirs(OUTPUT_DIR + '/find_evolving_outlines', exist_ok=True)\n",
    "            os.makedirs(OUTPUT_DIR + f'/find_evolving_outlines/{lake_name}', exist_ok=True)\n",
    "            os.makedirs(OUTPUT_DIR + '/plot_evolving_outlines', exist_ok=True)\n",
    "            os.makedirs(OUTPUT_DIR_GIT + '/lake_outlines/evolving_outlines', exist_ok=True)\n",
    "\n",
    "            # Find evolving outlines\n",
    "            evolving_outlines_gdf = find_evolving_outlines(\n",
    "                lake_gdf=lake_gdf, \n",
    "                area_multiple_search_extent=selected_row['area_multiple_search_extent'], \n",
    "                level=selected_row['level'], \n",
    "                dataset1_masked=dataset1_masked,\n",
    "                dataset2_masked=dataset2_masked,\n",
    "                buffered_poly=get_buffered_poly(lake_gdf, selected_row['area_multiple_search_extent']),\n",
    "                plot=True\n",
    "            )\n",
    "           \n",
    "            if evolving_outlines_gdf is None or evolving_outlines_gdf.empty:\n",
    "                write_no_outlines(lake_name)\n",
    "                continue\n",
    "            \n",
    "            # Clean geometries before filtering\n",
    "            try:\n",
    "                print(f\"Cleaning geometries for {lake_name}...\")\n",
    "                # Clean evolving outlines geometries\n",
    "                evolving_outlines_gdf = evolving_outlines_gdf.copy()\n",
    "                evolving_outlines_gdf.loc[:, 'geometry'] = evolving_outlines_gdf['geometry'].apply(\n",
    "                    lambda geom: make_valid(geom) if not geom.is_valid else geom\n",
    "                )\n",
    "                \n",
    "                # Clean reference geometry\n",
    "                reference_geometry = make_valid(lake_gdf['geometry']) if not lake_gdf['geometry'].is_valid else lake_gdf['geometry']\n",
    "                \n",
    "                # Extract intersecting polygons\n",
    "                filtered_gdf = extract_intersecting_polygons_recursive(\n",
    "                    evolving_outlines_gdf, \n",
    "                    reference_geometry\n",
    "                )\n",
    "                \n",
    "                if filtered_gdf is None or filtered_gdf.empty:\n",
    "                    print(f\"No valid filtered outlines found for {lake_name}\")\n",
    "                    write_no_outlines(lake_name)\n",
    "                    continue\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error cleaning geometries for {lake_name}: {str(e)}\")\n",
    "                print(\"Attempting to continue with original geometries...\")\n",
    "                traceback.print_exc()\n",
    "                \n",
    "                try:\n",
    "                    filtered_gdf = extract_intersecting_polygons_recursive(\n",
    "                        evolving_outlines_gdf, \n",
    "                        lake_gdf['geometry']\n",
    "                    )\n",
    "                    \n",
    "                    if filtered_gdf is None or filtered_gdf.empty:\n",
    "                        print(f\"No valid filtered outlines found for {lake_name}\")\n",
    "                        write_no_outlines(lake_name)\n",
    "                        continue\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting polygons for {lake_name}: {str(e)}\")\n",
    "                    traceback.print_exc()\n",
    "                    write_no_outlines(lake_name)\n",
    "                    continue\n",
    "                \n",
    "            # Save results and plot\n",
    "            if not filtered_gdf.empty:\n",
    "                try:\n",
    "                    # Add metadata to filtered_gdf\n",
    "                    filtered_gdf = filtered_gdf.copy()\n",
    "                    filtered_gdf.loc[:, 'area_multiple_search_extent'] = selected_row['area_multiple_search_extent']\n",
    "                    filtered_gdf.loc[:, 'level'] = selected_row['level']\n",
    "                    filtered_gdf.loc[:, 'within_percent'] = selected_row['within_percent'] if 'within_percent' in selected_row else 100.0\n",
    "                    \n",
    "                    # Export evolving outlines GeoDataFrame to GeoJSON\n",
    "                    output_path = 'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)\n",
    "                    filtered_gdf.to_file(filename=output_path, driver='GeoJSON')\n",
    "                    print(f\"Saved outlines to: {output_path}\")\n",
    "                    \n",
    "                    # Plot the outlines\n",
    "                    try:\n",
    "                        plot_evolving_outlines(\n",
    "                            lake_gdf=lake_gdf,\n",
    "                            evolving_outlines_gdf=filtered_gdf\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error plotting outlines for {lake_name}: {str(e)}\")\n",
    "                        traceback.print_exc()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving results for {lake_name}: {str(e)}\")\n",
    "                    traceback.print_exc()\n",
    "                    write_no_outlines(lake_name)\n",
    "            else:\n",
    "                print(f\"No filtered outlines to save for {lake_name}\")\n",
    "                write_no_outlines(lake_name)\n",
    "\n",
    "            # Clear memory\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {lake_name}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "            \n",
    "        finally:\n",
    "            # Clean up memory\n",
    "            for var in ['dataset1_masked', 'dataset2_masked', 'evolving_outlines_gdf', 'filtered_gdf']:\n",
    "                if var in locals():\n",
    "                    del locals()[var]\n",
    "            gc.collect()\n",
    "            clear_output(wait=True)\n",
    "\n",
    "def write_no_outlines(filepath):\n",
    "    \"\"\"Write file indicating no outlines found\"\"\"\n",
    "    try:\n",
    "        # Ensure directory exists\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        # Write file\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(\"There are no evolving outlines for this lake.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing no outlines file to {filepath}: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "def prepare_datasets(lake_gdf, area_multiple):\n",
    "    \"\"\"Prepare masked datasets based on lake parameters\"\"\"\n",
    "    CS2_SARIn_time_period = lake_gdf['CS2_SARIn_time_period']\n",
    "    \n",
    "    # Initialize dataset1\n",
    "    if pd.isna(CS2_SARIn_time_period) or CS2_SARIn_time_period == '<NA>':\n",
    "        dataset1 = None\n",
    "    elif CS2_SARIn_time_period == '2013.75-2018.75':\n",
    "        dataset1 = CS2_Smith2017.sel(time=slice(np.datetime64('2013-10-01T22:30:00.000000000'), None))\n",
    "    elif CS2_SARIn_time_period == '2010.5-2018.75':\n",
    "        dataset1 = CS2_Smith2017\n",
    "    \n",
    "    dataset2 = ATL15_dh\n",
    "    \n",
    "    # Mask datasets\n",
    "    lake_poly = lake_gdf['geometry']\n",
    "    buffered_poly = get_buffered_poly(lake_gdf, area_multiple)\n",
    "    x_min, y_min, x_max, y_max = buffered_poly.bounds\n",
    "    \n",
    "    return mask_datasets(dataset1, dataset2, buffered_poly, x_min, x_max, y_min, y_max)\n",
    "\n",
    "def mask_datasets(dataset1, dataset2, buffered_poly, x_min, x_max, y_min, y_max):\n",
    "    \"\"\"Apply masks to both datasets\"\"\"\n",
    "    dataset1_masked = None\n",
    "    if dataset1 is not None:\n",
    "        dataset1_masked = apply_mask_to_dataset(dataset1, buffered_poly, x_min, x_max, y_min, y_max)\n",
    "    \n",
    "    dataset2_masked = apply_mask_to_dataset(dataset2, buffered_poly, x_min, x_max, y_min, y_max)\n",
    "    return dataset1_masked, dataset2_masked\n",
    "\n",
    "def apply_mask_to_dataset(dataset, buffered_poly, x_min, x_max, y_min, y_max):\n",
    "    \"\"\"Apply mask to a single dataset\"\"\"\n",
    "    dataset_sub = dataset.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "    mask = np.array([[buffered_poly.contains(Point(x, y)) \n",
    "                     for x in dataset_sub['x'].values] \n",
    "                     for y in dataset_sub['y'].values])\n",
    "    mask_da = xr.DataArray(mask, coords=[dataset_sub.y, dataset_sub.x], dims=[\"y\", \"x\"])\n",
    "    return dataset.where(mask_da, drop=True)\n",
    "\n",
    "def get_buffered_poly(lake_gdf, area_multiple):\n",
    "    \"\"\"Get buffered polygon for lake\"\"\"\n",
    "    return multiple_area_buffer(lake_gdf['geometry'], area_multiple + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96ef3593-61be-4cd5-8f21-211da5e6099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_intersecting_polygons_recursive(gdf, reference_geometry):\n",
    "#     \"\"\"Extract intersecting polygons using the updated union_all() method\"\"\"\n",
    "#     if gdf is None or gdf.empty:\n",
    "#         return gpd.GeoDataFrame(geometry=[])\n",
    "        \n",
    "#     # Initialize with polygons that directly intersect the reference\n",
    "#     directly_intersecting = gdf[gdf.geometry.intersects(reference_geometry)]\n",
    "#     if directly_intersecting.empty:\n",
    "#         return gpd.GeoDataFrame(geometry=[])\n",
    "        \n",
    "#     already_found = directly_intersecting.copy()\n",
    "#     while True:\n",
    "#         # Find polygons that intersect with any previously found polygons\n",
    "#         new_intersecting = gdf[gdf.geometry.intersects(already_found.geometry.union_all())]\n",
    "        \n",
    "#         if len(new_intersecting) == len(already_found):\n",
    "#             break\n",
    "            \n",
    "#         already_found = gpd.GeoDataFrame(pd.concat(\n",
    "#             [already_found, new_intersecting], \n",
    "#             ignore_index=True\n",
    "#         ).drop_duplicates(subset='geometry'))\n",
    "    \n",
    "#     return already_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de292a60-3f21-4b5a-934e-a2488d8c1291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_intersecting_polygons_recursive(gdf, reference_geometry):\n",
    "#     \"\"\"Extract intersecting polygons with proper copying\"\"\"\n",
    "#     if gdf is None or gdf.empty:\n",
    "#         return gpd.GeoDataFrame(geometry=[])\n",
    "        \n",
    "#     # Create copies of geodataframes\n",
    "#     gdf_copy = gdf.copy()\n",
    "    \n",
    "#     # Initialize with polygons that directly intersect the reference\n",
    "#     directly_intersecting = gdf_copy.loc[gdf_copy.geometry.intersects(reference_geometry)].copy()\n",
    "#     if directly_intersecting.empty:\n",
    "#         return gpd.GeoDataFrame(geometry=[])\n",
    "        \n",
    "#     already_found = directly_intersecting.copy()\n",
    "#     while True:\n",
    "#         # Find new intersecting polygons\n",
    "#         new_intersecting = gdf_copy.loc[\n",
    "#             gdf_copy.geometry.intersects(already_found.geometry.union_all())\n",
    "#         ].copy()\n",
    "        \n",
    "#         if len(new_intersecting) == len(already_found):\n",
    "#             break\n",
    "            \n",
    "#         # Combine results\n",
    "#         already_found = gpd.GeoDataFrame(\n",
    "#             pd.concat([already_found, new_intersecting], ignore_index=True)\n",
    "#         ).drop_duplicates(subset='geometry').copy()\n",
    "    \n",
    "#     return already_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9c5be45-ac4e-44ee-8d1b-50dc4725059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_intersecting_polygons_recursive(gdf, reference_geometry):\n",
    "    \"\"\"\n",
    "    Extract intersecting polygons with topology validation and cleaning\n",
    "    \"\"\"\n",
    "    import shapely.geometry\n",
    "    from shapely.validation import make_valid\n",
    "    \n",
    "    if gdf is None or gdf.empty:\n",
    "        return gpd.GeoDataFrame(geometry=[])\n",
    "        \n",
    "    try:\n",
    "        # Create a copy and clean geometries\n",
    "        gdf_copy = gdf.copy()\n",
    "        \n",
    "        # Clean reference geometry\n",
    "        if not reference_geometry.is_valid:\n",
    "            print(\"Cleaning reference geometry...\")\n",
    "            reference_geometry = make_valid(reference_geometry)\n",
    "            \n",
    "        # Clean all geometries in the GeoDataFrame\n",
    "        gdf_copy['geometry'] = gdf_copy['geometry'].apply(lambda geom: make_valid(geom) if not geom.is_valid else geom)\n",
    "        \n",
    "        # Find directly intersecting polygons\n",
    "        try:\n",
    "            directly_intersecting = gdf_copy.loc[gdf_copy.geometry.intersects(reference_geometry)].copy()\n",
    "        except Exception as e:\n",
    "            print(f\"Error in direct intersection: {str(e)}\")\n",
    "            return gpd.GeoDataFrame(geometry=[])\n",
    "            \n",
    "        if directly_intersecting.empty:\n",
    "            return gpd.GeoDataFrame(geometry=[])\n",
    "            \n",
    "        already_found = directly_intersecting.copy()\n",
    "        \n",
    "        # Try recursive intersection\n",
    "        try:\n",
    "            while True:\n",
    "                # Create union with buffer to handle small topology issues\n",
    "                union_geom = already_found.geometry.union_all()\n",
    "                if not union_geom.is_valid:\n",
    "                    print(\"Cleaning union geometry...\")\n",
    "                    union_geom = make_valid(union_geom)\n",
    "                \n",
    "                # Find new intersecting polygons\n",
    "                new_intersecting = gdf_copy.loc[gdf_copy.geometry.intersects(union_geom)].copy()\n",
    "                \n",
    "                if len(new_intersecting) == len(already_found):\n",
    "                    break\n",
    "                    \n",
    "                # Combine results with deduplication\n",
    "                already_found = gpd.GeoDataFrame(\n",
    "                    pd.concat([already_found, new_intersecting], ignore_index=True)\n",
    "                ).drop_duplicates(subset='geometry').copy()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error in recursive intersection: {str(e)}\")\n",
    "            print(\"Returning directly intersecting polygons only\")\n",
    "            return directly_intersecting\n",
    "        \n",
    "        return already_found\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_intersecting_polygons_recursive: {str(e)}\")\n",
    "        return gpd.GeoDataFrame(geometry=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cb31757-d7cb-47cb-bac9-2b6875b7f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_evolving_outlines(lake_gdf, evolving_outlines_gdf):\n",
    "#     '''\n",
    "#     Func to plot evolving outlines in aggregate\n",
    "#     '''\n",
    "#     try:\n",
    "#         # Define lake name and polygon\n",
    "#         lake_name = lake_gdf['name']\n",
    "#         print(f\"Starting plot for lake: {lake_name}\")\n",
    "        \n",
    "#         stationary_outline = lake_gdf['geometry']\n",
    "#         if stationary_outline is None:\n",
    "#             print(f\"Error: No geometry found for {lake_name}\")\n",
    "#             return\n",
    "\n",
    "#         # Verify output directory exists\n",
    "#         output_dir = os.path.join(OUTPUT_DIR, 'plot_evolving_outlines')\n",
    "#         if not os.path.exists(output_dir):\n",
    "#             print(f\"Creating output directory: {output_dir}\")\n",
    "#             os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#         # # Try to open the evolving outlines file\n",
    "#         # evolving_outlines_path = os.path.join(\n",
    "#         #     os.getcwd(), 'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name))\n",
    "        \n",
    "#         # if not os.path.exists(evolving_outlines_path):\n",
    "#         #     print(f\"File does not exist: {evolving_outlines_path}\")\n",
    "#         #     return\n",
    "        \n",
    "#         # evolving_outlines_gdf = gpd.read_file(evolving_outlines_path)\n",
    "#         # if evolving_outlines_gdf.empty:\n",
    "#         #     print(f\"No evolving outlines found in file for {lake_name}\")\n",
    "#         #     return\n",
    "            \n",
    "#         # print(f\"Successfully loaded evolving outlines for {lake_name}\")\n",
    "\n",
    "#         # Create buffered polygon\n",
    "#         search_extent_buffered_stationary_outline = multiple_area_buffer(\n",
    "#             stationary_outline, \n",
    "#             int(evolving_outlines_gdf['area_multiple_search_extent'].iloc[0]))\n",
    "\n",
    "#         # Create figure\n",
    "#         fig, ax = plt.subplots(figsize=(5,5))\n",
    "        \n",
    "#         # Plot buffered polygons\n",
    "#         gpd.GeoDataFrame(geometry=[search_extent_buffered_stationary_outline]).boundary.plot(\n",
    "#             ax=ax, edgecolor='magenta', facecolor='none', linewidth=1)\n",
    "\n",
    "#         # Set up colormap\n",
    "#         cmap = plt.get_cmap('plasma', len(mid_cyc_dates)-1)\n",
    "#         norm = plt.Normalize(\n",
    "#             date_to_quarter_year(mid_cyc_dates[0]), \n",
    "#             date_to_quarter_year(mid_cyc_dates[-1]))\n",
    "#         m = plt.cm.ScalarMappable(cmap=cmap)\n",
    "#         m.set_array(np.array([date_to_quarter_year(date) for date in mid_cyc_dates[0:-1]]))\n",
    "        \n",
    "#         # Add colorbar\n",
    "#         divider = make_axes_locatable(ax)\n",
    "#         cax = divider.append_axes('bottom', size='2.5%', pad=0.5)\n",
    "#         fig.colorbar(m, cax=cax, orientation='horizontal').set_label('evolving outline year', size=10)\n",
    "\n",
    "#         # Plot stationary lakes\n",
    "#         stationary_lake_color = 'darkturquoise'\n",
    "#         stationary_lakes_gdf.boundary.plot(\n",
    "#             ax=ax, facecolor=stationary_lake_color, linestyle='solid', linewidth=2, alpha=0.25)\n",
    "#         stationary_lakes_gdf.boundary.plot(\n",
    "#             ax=ax, edgecolor=stationary_lake_color, linestyle='solid', linewidth=2)\n",
    "\n",
    "#         # Plot evolving outlines\n",
    "#         lines = []\n",
    "#         for idx, dt in enumerate(mid_cyc_dates):\n",
    "#             x, y = 1, 1\n",
    "#             line, = ax.plot(x, y, color=cmap(norm(date_to_quarter_year(mid_cyc_dates[idx]))), linewidth=2)\n",
    "#             lines.append(line)\n",
    "            \n",
    "#             evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "#             if not evolving_outlines_dt.empty:\n",
    "#                 evolving_outlines_dt.boundary.plot(\n",
    "#                     ax=ax, \n",
    "#                     color=cmap(norm(date_to_quarter_year(mid_cyc_dates[idx]))), \n",
    "#                     linewidth=1)\n",
    "\n",
    "#         # Format axes\n",
    "#         km_scale = 1e3\n",
    "#         ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "#         ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "#         ax.set_aspect('equal')\n",
    "#         ax.set_xlabel('x [km]', size=10)\n",
    "#         ax.set_ylabel('y [km]', size=10)\n",
    "        \n",
    "#         # Set plot bounds\n",
    "#         x_min, y_min, x_max, y_max = search_extent_buffered_stationary_outline.bounds\n",
    "#         x_buffer = abs(x_max-x_min)*0.2\n",
    "#         y_buffer = abs(y_max-y_min)*0.2\n",
    "#         ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "#         # Add legend\n",
    "#         stationary_lakes_line = plt.Line2D([],[], color=stationary_lake_color, linestyle='solid', linewidth=2)\n",
    "#         search_extent = plt.Line2D([],[], color='magenta', linestyle='solid', linewidth=2)\n",
    "        \n",
    "#         ax.legend(\n",
    "#             handles=[stationary_lakes_line, tuple(lines), search_extent],\n",
    "#             labels=['stationary outline', 'evolving outline (this study)', \n",
    "#                     'search extent ({}x)'.format(int(evolving_outlines_gdf['area_multiple_search_extent'][0]))],\n",
    "#             handlelength=3,\n",
    "#             handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#             loc='upper left',\n",
    "#             bbox_to_anchor=(0, 1.35)\n",
    "#         )\n",
    "\n",
    "#         # Add inset map\n",
    "#         axIns = ax.inset_axes([0.02, 0.01, 0.25, 0.25])\n",
    "#         axIns.set_aspect('equal')\n",
    "#         moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "#         moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "#         axIns.scatter(\n",
    "#             ((x_max+x_min)/2), ((y_max+y_min)/2),\n",
    "#             marker='*', linewidth=1, edgecolor='k', facecolor='r', s=50, zorder=3\n",
    "#         )\n",
    "#         axIns.axis('off')\n",
    "\n",
    "#         # Generate output filename and save\n",
    "#         output_filename = os.path.join(\n",
    "#             OUTPUT_DIR,\n",
    "#             'plot_evolving_outlines',\n",
    "#             'plot_evolving_outlines_{}_{}x-search-extent_{}m-level_{}-withing-percent.png'.format(\n",
    "#                 lake_name,\n",
    "#                 evolving_outlines_gdf['area_multiple_search_extent'][0],\n",
    "#                 evolving_outlines_gdf['level'][0],\n",
    "#                 evolving_outlines_gdf['within_percent'][0]\n",
    "#             )\n",
    "#         )\n",
    "        \n",
    "#         plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "#         print(f\"Successfully saved plot to: {output_filename}\")\n",
    "        \n",
    "#         plt.close()\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in plot_evolving_outlines for {lake_name}:\")\n",
    "#         print(f\"Error message: {str(e)}\")\n",
    "#         print(\"Error traceback:\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "        \n",
    "#         # Try to close any open figures\n",
    "#         plt.close('all')\n",
    "#         return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fa8746-daae-4f37-b1df-361f3bd5c609",
   "metadata": {},
   "source": [
    "## plot_evolving_outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9e96a18-e31b-4901-9617-09c2f60b4ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evolving_outlines(lake_gdf, evolving_outlines_gdf):\n",
    "    '''\n",
    "    Plot evolving outlines with proper DataFrame handling\n",
    "    '''\n",
    "    try:\n",
    "        # Define lake name and polygon\n",
    "        lake_name = lake_gdf['name']\n",
    "        print(f\"Starting plot for lake: {lake_name}\")\n",
    "        \n",
    "        # Validate inputs\n",
    "        if evolving_outlines_gdf is None or evolving_outlines_gdf.empty:\n",
    "            print(f\"No evolving outlines provided for {lake_name}\")\n",
    "            return\n",
    "            \n",
    "        # Reset index to ensure proper access\n",
    "        evolving_outlines_gdf = evolving_outlines_gdf.reset_index(drop=True)\n",
    "        \n",
    "        # Get parameters using iloc\n",
    "        search_extent = evolving_outlines_gdf['area_multiple_search_extent'].iloc[0]\n",
    "        level = evolving_outlines_gdf['level'].iloc[0]\n",
    "        \n",
    "        print(f\"Parameters: search_extent={search_extent}, level={level}\")\n",
    "        \n",
    "        stationary_outline = lake_gdf['geometry']\n",
    "        if stationary_outline is None:\n",
    "            print(f\"Error: No geometry found for {lake_name}\")\n",
    "            return\n",
    "\n",
    "        # Create buffered polygon\n",
    "        search_extent_buffered_stationary_outline = multiple_area_buffer(\n",
    "            stationary_outline, \n",
    "            int(search_extent))\n",
    "\n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(5,5))\n",
    "        \n",
    "        # Plot buffered polygons\n",
    "        gpd.GeoDataFrame(geometry=[search_extent_buffered_stationary_outline]).boundary.plot(\n",
    "            ax=ax, edgecolor='magenta', facecolor='none', linewidth=1)\n",
    "\n",
    "        # Set up colormap\n",
    "        cmap = plt.get_cmap('plasma', len(mid_cyc_dates)-1)\n",
    "        norm = plt.Normalize(\n",
    "            date_to_quarter_year(mid_cyc_dates[0]), \n",
    "            date_to_quarter_year(mid_cyc_dates[-1]))\n",
    "        m = plt.cm.ScalarMappable(cmap=cmap)\n",
    "        m.set_array(np.array([date_to_quarter_year(date) for date in mid_cyc_dates[0:-1]]))\n",
    "        \n",
    "        # Add colorbar\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('bottom', size='2.5%', pad=0.5)\n",
    "        fig.colorbar(m, cax=cax, orientation='horizontal').set_label('evolving outline year', size=10)\n",
    "\n",
    "        # Plot stationary lakes\n",
    "        stationary_lake_color = 'darkturquoise'\n",
    "        stationary_lakes_gdf.boundary.plot(\n",
    "            ax=ax, facecolor=stationary_lake_color, linestyle='solid', linewidth=2, alpha=0.25)\n",
    "        stationary_lakes_gdf.boundary.plot(\n",
    "            ax=ax, edgecolor=stationary_lake_color, linestyle='solid', linewidth=2)\n",
    "\n",
    "        # Plot evolving outlines\n",
    "        lines = []\n",
    "        for idx, dt in enumerate(mid_cyc_dates):\n",
    "            x, y = 1, 1\n",
    "            line, = ax.plot(x, y, color=cmap(norm(date_to_quarter_year(mid_cyc_dates[idx]))), linewidth=2)\n",
    "            lines.append(line)\n",
    "            \n",
    "            evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "            if not evolving_outlines_dt.empty:\n",
    "                evolving_outlines_dt.boundary.plot(\n",
    "                    ax=ax, \n",
    "                    color=cmap(norm(date_to_quarter_year(mid_cyc_dates[idx]))), \n",
    "                    linewidth=1)\n",
    "\n",
    "        # Format axes\n",
    "        km_scale = 1e3\n",
    "        ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "        ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_xlabel('x [km]', size=10)\n",
    "        ax.set_ylabel('y [km]', size=10)\n",
    "        \n",
    "        # Set plot bounds\n",
    "        x_min, y_min, x_max, y_max = search_extent_buffered_stationary_outline.bounds\n",
    "        x_buffer = abs(x_max-x_min)*0.2\n",
    "        y_buffer = abs(y_max-y_min)*0.2\n",
    "        ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "        # Add legend\n",
    "        stationary_lakes_line = plt.Line2D([],[], color=stationary_lake_color, linestyle='solid', linewidth=2)\n",
    "        search_extent_line = plt.Line2D([],[], color='magenta', linestyle='solid', linewidth=2)\n",
    "        \n",
    "        ax.legend(\n",
    "            handles=[stationary_lakes_line, tuple(lines), search_extent_line],\n",
    "            labels=[\n",
    "                'stationary outline', \n",
    "                f'evolving outline ({level} m)', \n",
    "                f'search extent ({int(search_extent)}x)'\n",
    "            ],\n",
    "            handlelength=3,\n",
    "            handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "            loc='upper left',\n",
    "            bbox_to_anchor=(0, 1.2)\n",
    "        )\n",
    "\n",
    "        # Add inset map\n",
    "        axIns = ax.inset_axes([0.02, 0.01, 0.25, 0.25])\n",
    "        axIns.set_aspect('equal')\n",
    "        moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        axIns.scatter(\n",
    "            ((x_max+x_min)/2), ((y_max+y_min)/2),\n",
    "            marker='*', linewidth=0.1, edgecolor='k', facecolor='r', s=50, zorder=3\n",
    "        )\n",
    "        axIns.axis('off')\n",
    "\n",
    "        # Generate output filename and save\n",
    "        output_filename = os.path.join(\n",
    "            OUTPUT_DIR,\n",
    "            'plot_evolving_outlines',\n",
    "            f'plot_evolving_outlines_{lake_name}_{int(search_extent)}x-search-extent_{level:.2f}m-level.png'\n",
    "        )\n",
    "        \n",
    "        plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Successfully saved plot to: {output_filename}\")\n",
    "        \n",
    "        plt.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in plot_evolving_outlines for {lake_name}:\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        print(\"Error traceback:\")\n",
    "        traceback.print_exc()\n",
    "        plt.close('all')\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33d8541-f275-4cd5-a1fd-0db489fce5b8",
   "metadata": {},
   "source": [
    "## Functions to analyze lake groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab573138-9b16-4692-9b02-c5d86a1021d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_search_extents_and_levels_group(stationary_lakes_gdf, lake_group):\n",
    "    '''\n",
    "    Find optimal search extents and levels for a group of lakes analyzed together.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    stationary_lakes_gdf : GeoDataFrame\n",
    "        The complete dataset of stationary lakes\n",
    "    lake_group : list\n",
    "        List of lake names to be analyzed together, e.g. ['Byrd_1', 'Byrd_2', 'Byrd_s2']\n",
    "    '''\n",
    "    try:\n",
    "        # Filter GeoDataFrame for lakes in the group\n",
    "        group_gdf = stationary_lakes_gdf[stationary_lakes_gdf['name'].isin(lake_group)].copy()\n",
    "        if group_gdf.empty:\n",
    "            print(f\"No lakes found for group: {lake_group}\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Finding search extents and levels for lake group: {lake_group}\")\n",
    "        \n",
    "        # Create a combined name for the group\n",
    "        group_name = \"_\".join(lake_group)\n",
    "        \n",
    "        # Create a union of all lake geometries for the group\n",
    "        group_geometry = group_gdf.geometry.union_all()\n",
    "        if not group_geometry.is_valid:\n",
    "            print(\"Cleaning group geometry...\")\n",
    "            group_geometry = make_valid(group_geometry)\n",
    "        \n",
    "        # Determine the group's time period\n",
    "        group_time_period = determine_group_time_period(group_gdf['CS2_SARIn_time_period'])\n",
    "        print(f\"Group CryoSat-2 SARIn time period determined as: {group_time_period}\")\n",
    "        \n",
    "        # Create a GeoDataFrame for the group\n",
    "        group_single_gdf = gpd.GeoDataFrame(\n",
    "            {\n",
    "                'name': [group_name],\n",
    "                'geometry': [group_geometry],\n",
    "                'CS2_SARIn_time_period': [group_time_period]\n",
    "            },\n",
    "            crs=group_gdf.crs\n",
    "        )\n",
    "        \n",
    "        # Define parameters of search extents and levels\n",
    "        area_multiple_search_extents = range(5, 16)  # From 5 to 15 inclusive\n",
    "        initial_level = 0.01\n",
    "        level_increment = 0.01\n",
    "        within_fraction_target = 0.95\n",
    "   \n",
    "        # Initialize DataFrame to store results\n",
    "        search_extents_levels_df = pd.DataFrame(\n",
    "            columns=['area_multiple_search_extent', 'level', 'within_percent', 'dataset_dois'])\n",
    "    \n",
    "        try:\n",
    "            for area_multiple_search_extent in area_multiple_search_extents:\n",
    "                try:\n",
    "                    # Mask datasets for the group\n",
    "                    lake_poly = group_geometry\n",
    "                    buffered_poly = multiple_area_buffer(lake_poly, area_multiple_search_extent+1)\n",
    "                    x_min, y_min, x_max, y_max = buffered_poly.bounds\n",
    "                    \n",
    "                    # Assign dataset based on group time period\n",
    "                    dataset1 = None\n",
    "                    dataset1_doi = None\n",
    "                    \n",
    "                    if pd.notna(group_time_period):\n",
    "                        if group_time_period == '2013.75-2018.75':\n",
    "                            dataset1 = CS2_Smith2017.sel(time=slice(np.datetime64('2013-10-01T22:30:00.000000000'), None))\n",
    "                            dataset1_doi = dataset1.attrs.get('identifier_product_DOI', 'Unknown')\n",
    "                        elif group_time_period == '2010.5-2018.75':\n",
    "                            dataset1 = CS2_Smith2017\n",
    "                            dataset1_doi = dataset1.attrs.get('identifier_product_DOI', 'Unknown')\n",
    "                    \n",
    "                    dataset2 = ATL15_dh\n",
    "                    dataset2_doi = dataset2.attrs.get('identifier_product_DOI', 'Unknown')\n",
    "                    \n",
    "                    dataset1_masked, dataset2_masked = prepare_datasets(\n",
    "                        group_single_gdf.iloc[0], \n",
    "                        area_multiple_search_extent\n",
    "                    )\n",
    "\n",
    "                    # Reset level and within_fraction for each area_multiple_search_extent iteration\n",
    "                    level = initial_level\n",
    "                    within_fraction = 0.0\n",
    "\n",
    "                    while within_fraction < within_fraction_target and level <= 2.0:\n",
    "                        # Find evolving outlines\n",
    "                        outlines_gdf = find_evolving_outlines(\n",
    "                            lake_gdf=group_single_gdf.iloc[0], \n",
    "                            area_multiple_search_extent=area_multiple_search_extent, \n",
    "                            level=level, \n",
    "                            dataset1_masked=dataset1_masked,\n",
    "                            dataset2_masked=dataset2_masked,\n",
    "                            buffered_poly=buffered_poly,\n",
    "                            plot=False\n",
    "                        )\n",
    "    \n",
    "                        if outlines_gdf is None or outlines_gdf.empty:\n",
    "                            print(f'No outlines found at level {level}')\n",
    "                            break\n",
    "                            \n",
    "                        # Process geometries\n",
    "                        try:\n",
    "                            # Clean geometries\n",
    "                            outlines_gdf = outlines_gdf.copy()\n",
    "                            outlines_gdf.loc[:, 'geometry'] = outlines_gdf['geometry'].apply(\n",
    "                                lambda geom: make_valid(geom) if not geom.is_valid else geom\n",
    "                            )\n",
    "                            \n",
    "                            buffered_group_outline = multiple_area_buffer(\n",
    "                                group_geometry, \n",
    "                                area_multiple_search_extent)\n",
    "                            \n",
    "                            buffered_lake_gdf = gpd.GeoDataFrame(\n",
    "                                geometry=gpd.GeoSeries([buffered_group_outline]), \n",
    "                                crs=group_gdf.crs\n",
    "                            )\n",
    "\n",
    "                            # Ensure geometries are valid\n",
    "                            valid_outlines = outlines_gdf[outlines_gdf.is_valid & ~outlines_gdf.is_empty].copy()\n",
    "                            valid_buffer = buffered_lake_gdf[buffered_lake_gdf.is_valid & ~buffered_lake_gdf.is_empty].copy()\n",
    "\n",
    "                            # Spatial analysis\n",
    "                            within = gpd.sjoin(valid_outlines, valid_buffer, predicate='within')\n",
    "                            overlaps = gpd.sjoin(valid_outlines, valid_buffer, predicate='overlaps')\n",
    "            \n",
    "                            # Calculate fraction\n",
    "                            if (len(within) + len(overlaps)) > 0:\n",
    "                                within_fraction = np.round((len(within) / (len(within) + len(overlaps))), 2)\n",
    "                                print(f\"Extent: {area_multiple_search_extent}, Level: {level}, Within: {round(within_fraction*100)}%\")\n",
    "                            else:\n",
    "                                print('No valid intersections at this level')\n",
    "                                level += level_increment\n",
    "                                level = np.round(level, 2)\n",
    "                                continue\n",
    "            \n",
    "                            # Store results if target reached\n",
    "                            if within_fraction >= within_fraction_target:\n",
    "                                dois = [doi for doi in [dataset1_doi, dataset2_doi] if doi is not None]\n",
    "                                dois_str = ', '.join(dois)\n",
    "                                \n",
    "                                new_row = pd.DataFrame({\n",
    "                                    'area_multiple_search_extent': [area_multiple_search_extent],\n",
    "                                    'level': [level],\n",
    "                                    'within_percent': [within_fraction * 100],\n",
    "                                    'dataset_dois': [dois_str]\n",
    "                                })\n",
    "                                \n",
    "                                search_extents_levels_df = pd.concat(\n",
    "                                    [search_extents_levels_df, new_row],\n",
    "                                    ignore_index=True\n",
    "                                )\n",
    "                                break\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing geometries: {str(e)}\")\n",
    "                            traceback.print_exc()\n",
    "                            level += level_increment\n",
    "                            level = np.round(level, 2)\n",
    "                            continue\n",
    "\n",
    "                        # Increment level if needed\n",
    "                        level += level_increment\n",
    "                        level = np.round(level, 2)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing extent {area_multiple_search_extent}: {str(e)}\")\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "                    \n",
    "            # Save results\n",
    "            os.makedirs(OUTPUT_DIR + '/search_extents_levels', exist_ok=True)\n",
    "            \n",
    "            if not search_extents_levels_df.empty:\n",
    "                output_path = OUTPUT_DIR + f'/search_extents_levels/{group_name}.csv'\n",
    "                search_extents_levels_df.to_csv(output_path, index=False)\n",
    "                print(f\"Saved search extents and levels for group {group_name}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"No valid results found for group {group_name}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in analysis loop: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in find_search_extents_and_levels_group: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "    # Clear the output of each index\n",
    "    clear_output(wait=True)\n",
    "        \n",
    "\n",
    "def determine_group_time_period(time_periods):\n",
    "    '''\n",
    "    Determine the most exclusive time period for a group of lakes.\n",
    "    \n",
    "    Rules:\n",
    "    - If any lake has <NA>, group gets <NA>\n",
    "    - If all lakes have '2010.5-2018.75', group gets '2010.5-2018.75'\n",
    "    - If all lakes have either '2013.75-2018.75' or '2010.5-2018.75', group gets '2013.75-2018.75'\n",
    "    - Otherwise, group gets <NA>\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    time_periods : Series\n",
    "        Series of time periods from the group of lakes\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str or pd.NA\n",
    "        The determined time period for the group\n",
    "    '''\n",
    "    # If any lake has NA, group gets NA\n",
    "    if time_periods.isna().any():\n",
    "        return pd.NA\n",
    "        \n",
    "    # Convert to list and remove any NA values\n",
    "    periods = [p for p in time_periods if pd.notna(p)]\n",
    "    \n",
    "    # If all lakes have '2010.5-2018.75'\n",
    "    if all(p == '2010.5-2018.75' for p in periods):\n",
    "        return '2010.5-2018.75'\n",
    "        \n",
    "    # If all lakes have either '2013.75-2018.75' or '2010.5-2018.75'\n",
    "    valid_periods = {'2013.75-2018.75', '2010.5-2018.75'}\n",
    "    if all(p in valid_periods for p in periods):\n",
    "        return '2013.75-2018.75'\n",
    "        \n",
    "    # Default to NA for any other case\n",
    "    return pd.NA\n",
    "\n",
    "\n",
    "def finalize_evolving_outlines_group(stationary_lakes_gdf, lake_group):\n",
    "    '''\n",
    "    Finalize the evolving outlines for a group of lakes using parameters from group analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    stationary_lakes_gdf : GeoDataFrame\n",
    "        The complete dataset of stationary lakes\n",
    "    lake_group : list\n",
    "        List of lake names to be analyzed together, e.g. ['Byrd_1', 'Byrd_2', 'Byrd_s2']\n",
    "    '''\n",
    "    try:\n",
    "        # Filter GeoDataFrame for lakes in the group\n",
    "        group_gdf = stationary_lakes_gdf[stationary_lakes_gdf['name'].isin(lake_group)].copy()\n",
    "        if group_gdf.empty:\n",
    "            print(f\"No lakes found for group: {lake_group}\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Finalizing outlines for lake group: {lake_group}\")\n",
    "        \n",
    "        try:\n",
    "            # Create a combined name for the group\n",
    "            group_name = \"_\".join(lake_group)\n",
    "            \n",
    "            # Load search extents and levels dataframe for the group\n",
    "            search_extents_levels_df = pd.read_csv(OUTPUT_DIR + '/search_extents_levels/{}.csv'.format(group_name))\n",
    "            if search_extents_levels_df.empty:\n",
    "                write_no_outlines(OUTPUT_DIR + f'/search_extents_levels/{group_name}.txt')\n",
    "                write_no_outlines(OUTPUT_DIR_GIT + f'/lake_outlines/evolving_outlines/{group_name}.txt')\n",
    "                return\n",
    "\n",
    "            # Sort the DataFrame\n",
    "            sorted_df = search_extents_levels_df.sort_values(\n",
    "                by=['level', 'area_multiple_search_extent'], \n",
    "                ascending=[True, True])\n",
    "            \n",
    "            selected_row = sorted_df.iloc[0]\n",
    "            print(f\"Using parameters: {selected_row}\")\n",
    "        \n",
    "            # Create a union of all lake geometries for the group\n",
    "            group_geometry = group_gdf.geometry.union_all()\n",
    "            if not group_geometry.is_valid:\n",
    "                print(\"Cleaning group geometry...\")\n",
    "                group_geometry = make_valid(group_geometry)\n",
    "\n",
    "            # Determine the group's time period\n",
    "            group_time_period = determine_group_time_period(group_gdf['CS2_SARIn_time_period'])\n",
    "            print(f\"Group time period: {group_time_period}\")\n",
    "            \n",
    "            # Create a GeoDataFrame for the group\n",
    "            group_single_gdf = gpd.GeoDataFrame(\n",
    "                {\n",
    "                    'name': [group_name],\n",
    "                    'geometry': [group_geometry],\n",
    "                    'CS2_SARIn_time_period': [group_time_period]\n",
    "                },\n",
    "                crs=group_gdf.crs\n",
    "            )\n",
    "            \n",
    "            # Create output folders\n",
    "            os.makedirs(OUTPUT_DIR + '/find_evolving_outlines', exist_ok=True)\n",
    "            os.makedirs(OUTPUT_DIR + f'/find_evolving_outlines/{group_name}', exist_ok=True)\n",
    "            os.makedirs(OUTPUT_DIR + '/plot_evolving_outlines', exist_ok=True)\n",
    "            os.makedirs(OUTPUT_DIR_GIT + '/lake_outlines/evolving_outlines', exist_ok=True)\n",
    "\n",
    "            # Assign dataset\n",
    "            dataset1_masked, dataset2_masked = prepare_datasets(\n",
    "                group_single_gdf.iloc[0], \n",
    "                selected_row['area_multiple_search_extent']\n",
    "            )\n",
    "\n",
    "            # Find evolving outlines\n",
    "            evolving_outlines_gdf = find_evolving_outlines(\n",
    "                lake_gdf=group_single_gdf.iloc[0], \n",
    "                area_multiple_search_extent=selected_row['area_multiple_search_extent'], \n",
    "                level=selected_row['level'], \n",
    "                dataset1_masked=dataset1_masked,\n",
    "                dataset2_masked=dataset2_masked,\n",
    "                buffered_poly=get_buffered_poly(group_single_gdf.iloc[0], selected_row['area_multiple_search_extent']),\n",
    "                plot=True\n",
    "            )\n",
    "           \n",
    "            if evolving_outlines_gdf is None or evolving_outlines_gdf.empty:\n",
    "                print(f\"No evolving outlines found for group {group_name}\")\n",
    "                write_no_outlines(OUTPUT_DIR + f'/search_extents_levels/{group_name}.txt')\n",
    "                write_no_outlines(OUTPUT_DIR_GIT + f'/lake_outlines/evolving_outlines/{group_name}.txt')\n",
    "                return\n",
    "            \n",
    "            # Clean geometries before filtering\n",
    "            try:\n",
    "                print(f\"Cleaning geometries for group {group_name}...\")\n",
    "                evolving_outlines_gdf = evolving_outlines_gdf.copy()\n",
    "                evolving_outlines_gdf.loc[:, 'geometry'] = evolving_outlines_gdf['geometry'].apply(\n",
    "                    lambda geom: make_valid(geom) if not geom.is_valid else geom\n",
    "                )\n",
    "                \n",
    "                reference_geometry = make_valid(group_geometry) if not group_geometry.is_valid else group_geometry\n",
    "                \n",
    "                filtered_gdf = extract_intersecting_polygons_recursive(\n",
    "                    evolving_outlines_gdf, \n",
    "                    reference_geometry\n",
    "                )\n",
    "                \n",
    "                if filtered_gdf is None or filtered_gdf.empty:\n",
    "                    print(f\"No valid filtered outlines found for group {group_name}\")\n",
    "                    write_no_outlines(OUTPUT_DIR + f'/search_extents_levels/{group_name}.txt')\n",
    "                    write_no_outlines(OUTPUT_DIR_GIT + f'/lake_outlines/evolving_outlines/{group_name}.txt')\n",
    "                    return\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error cleaning geometries for group {group_name}: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "                return\n",
    "                \n",
    "            # Save results and plot\n",
    "            if not filtered_gdf.empty:\n",
    "                try:\n",
    "                    # Add metadata\n",
    "                    filtered_gdf = filtered_gdf.copy()\n",
    "                    filtered_gdf.loc[:, 'area_multiple_search_extent'] = selected_row['area_multiple_search_extent']\n",
    "                    filtered_gdf.loc[:, 'level'] = selected_row['level']\n",
    "                    filtered_gdf.loc[:, 'within_percent'] = selected_row['within_percent']\n",
    "                    filtered_gdf.loc[:, 'lake_group'] = str(lake_group)\n",
    "                    \n",
    "                    # Export evolving outlines\n",
    "                    output_path = f'output/lake_outlines/evolving_outlines/{group_name}.geojson'\n",
    "                    filtered_gdf.to_file(filename=output_path, driver='GeoJSON')\n",
    "                    print(f\"Saved group outlines to: {output_path}\")\n",
    "                    \n",
    "                    # Plot the outlines\n",
    "                    try:\n",
    "                        plot_evolving_outlines_group(\n",
    "                            group_gdf=group_gdf,\n",
    "                            evolving_outlines_gdf=filtered_gdf,\n",
    "                            group_name=group_name\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error plotting outlines for group {group_name}: {str(e)}\")\n",
    "                        traceback.print_exc()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving results for group {group_name}: {str(e)}\")\n",
    "                    traceback.print_exc()\n",
    "                    write_no_outlines(OUTPUT_DIR + f'/search_extents_levels/{group_name}.txt')\n",
    "                    write_no_outlines(OUTPUT_DIR_GIT + f'/lake_outlines/evolving_outlines/{group_name}.txt')\n",
    "            else:\n",
    "                print(f\"No filtered outlines to save for group {group_name}\")\n",
    "                write_no_outlines(OUTPUT_DIR + f'/search_extents_levels/{group_name}.txt')\n",
    "                write_no_outlines(OUTPUT_DIR_GIT + f'/lake_outlines/evolving_outlines/{group_name}.txt')\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing group {group_name}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            write_no_outlines(OUTPUT_DIR + f'/search_extents_levels/{group_name}.txt')\n",
    "            write_no_outlines(OUTPUT_DIR_GIT + f'/lake_outlines/evolving_outlines/{group_name}.txt')\n",
    "            \n",
    "        finally:\n",
    "            # Clean up memory\n",
    "            for var in ['dataset1_masked', 'dataset2_masked', 'evolving_outlines_gdf', 'filtered_gdf']:\n",
    "                if var in locals():\n",
    "                    del locals()[var]\n",
    "            gc.collect()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in finalize_evolving_outlines_group: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "def plot_evolving_outlines_group(group_gdf, evolving_outlines_gdf, group_name):\n",
    "    '''\n",
    "    Plot evolving outlines for a group of lakes.\n",
    "    '''\n",
    "    try:\n",
    "        print(f\"Starting plot for lake group: {group_name}\")\n",
    "        \n",
    "        # Reset index\n",
    "        evolving_outlines_gdf = evolving_outlines_gdf.reset_index(drop=True)\n",
    "        \n",
    "        # Get parameters\n",
    "        search_extent = float(evolving_outlines_gdf['area_multiple_search_extent'].iloc[0])\n",
    "        level = float(evolving_outlines_gdf['level'].iloc[0])\n",
    "        within_percent = float(evolving_outlines_gdf['within_percent'].iloc[0])\n",
    "        \n",
    "        # Create buffered polygon for whole group\n",
    "        group_geometry = group_gdf.geometry.union_all()\n",
    "        if not group_geometry.is_valid:\n",
    "            group_geometry = make_valid(group_geometry)\n",
    "            \n",
    "        search_extent_buffered = multiple_area_buffer(group_geometry, int(search_extent))\n",
    "        \n",
    "        # Create plot\n",
    "        fig, ax = plt.subplots(figsize=(10,10))  # Larger figure for group\n",
    "        \n",
    "        # Plot buffered boundary\n",
    "        gpd.GeoDataFrame(geometry=[search_extent_buffered]).boundary.plot(\n",
    "            ax=ax, edgecolor='magenta', facecolor='none', linewidth=1)\n",
    "\n",
    "        # Set up colormap\n",
    "        cmap = plt.get_cmap('plasma', len(mid_cyc_dates)-1)\n",
    "        norm = plt.Normalize(\n",
    "            date_to_quarter_year(mid_cyc_dates[0]), \n",
    "            date_to_quarter_year(mid_cyc_dates[-1]))\n",
    "        m = plt.cm.ScalarMappable(cmap=cmap)\n",
    "        m.set_array(np.array([date_to_quarter_year(date) for date in mid_cyc_dates[0:-1]]))\n",
    "        \n",
    "        # Add colorbar\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('bottom', size='2.5%', pad=0.5)\n",
    "        fig.colorbar(m, cax=cax, orientation='horizontal').set_label('evolving outline year', size=10)\n",
    "\n",
    "        # Plot original lake outlines\n",
    "        stationary_lake_color = 'darkturquoise'\n",
    "        for _, lake in group_gdf.iterrows():\n",
    "            gpd.GeoDataFrame(geometry=[lake.geometry]).boundary.plot(\n",
    "                ax=ax, color=stationary_lake_color, linestyle='solid', linewidth=2)\n",
    "\n",
    "        # Plot evolving outlines\n",
    "        lines = []\n",
    "        for idx, dt in enumerate(mid_cyc_dates):\n",
    "            x, y = 1, 1\n",
    "            line, = ax.plot(x, y, color=cmap(norm(date_to_quarter_year(mid_cyc_dates[idx]))), linewidth=2)\n",
    "            lines.append(line)\n",
    "            \n",
    "            evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "            if not evolving_outlines_dt.empty:\n",
    "                evolving_outlines_dt.boundary.plot(\n",
    "                    ax=ax, \n",
    "                    color=cmap(norm(date_to_quarter_year(mid_cyc_dates[idx]))), \n",
    "                    linewidth=1)\n",
    "\n",
    "        # Format axes\n",
    "        km_scale = 1e3\n",
    "        ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "        ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_xlabel('x [km]', size=10)\n",
    "        ax.set_ylabel('y [km]', size=10)\n",
    "        \n",
    "        # Set bounds\n",
    "        x_min, y_min, x_max, y_max = search_extent_buffered.bounds\n",
    "        x_buffer = abs(x_max-x_min)*0.2\n",
    "        y_buffer = abs(y_max-y_min)*0.2\n",
    "        ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "        # Add legend\n",
    "        stationary_lakes_line = plt.Line2D([],[], color=stationary_lake_color, linestyle='solid', linewidth=2)\n",
    "        search_extent_line = plt.Line2D([],[], color='magenta', linestyle='solid', linewidth=2)\n",
    "        \n",
    "        ax.legend(\n",
    "            handles=[stationary_lakes_line, tuple(lines), search_extent_line],\n",
    "            labels=[\n",
    "                'stationary outlines', \n",
    "                f'evolving outlines ({level:.2f}m)', \n",
    "                f'search extent ({int(search_extent)}x)'\n",
    "            ],\n",
    "            handlelength=3,\n",
    "            handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "            loc='upper left',\n",
    "            bbox_to_anchor=(0, 1.35)\n",
    "        )\n",
    "\n",
    "        # Add inset map\n",
    "        axIns = ax.inset_axes([0.02, 0.01, 0.25, 0.25])\n",
    "        axIns.set_aspect('equal')\n",
    "        moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        centroid = group_geometry.centroid\n",
    "        axIns.scatter(\n",
    "            centroid.x, centroid.y,\n",
    "            marker='*', linewidth=0.1, edgecolor='k', facecolor='r', s=50, zorder=3\n",
    "        )\n",
    "        axIns.axis('off')\n",
    "\n",
    "        # Save plot\n",
    "        output_filename = os.path.join(\n",
    "            OUTPUT_DIR,\n",
    "            'plot_evolving_outlines',\n",
    "            f'plot_evolving_outlines_{group_name}_{int(search_extent)}x-search-extent_{level:.2f}m-level_{within_percent:.0f}-within-percent.png'\n",
    "        )\n",
    "        \n",
    "        plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Successfully saved plot to: {output_filename}\")\n",
    "        \n",
    "        plt.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in plot_evolving_outlines_group for {group_name}:\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09a64b8c-09ac-424f-a6ed-225e5ff60791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def remove_outlier_polygon(gdf, axis='x', max_or_min='max'):\n",
    "#     '''\n",
    "#     Func to remove outline outliers by removing a polygon\n",
    "#     with the most extreme x or y coordinates\n",
    "    \n",
    "#     Inputs\n",
    "#     * geopandas geodataframe\n",
    "#     * axis indicates whether the x or y axis will \n",
    "#     have its extreme polygon removed\n",
    "#     * max_or_min indicates whether the max or min centroid axis\n",
    "#     value will be removed\n",
    "    \n",
    "#     Ouputs\n",
    "#     * geopandas geodataframe with one outline removed\n",
    "#     '''\n",
    "    \n",
    "#     # Check if the GeoDataFrame is empty\n",
    "#     if gdf.empty:\n",
    "#         print(\"GeoDataFrame is empty. Nothing to remove.\")\n",
    "#         return gdf\n",
    "    \n",
    "#     # Choose the axis and max or min for extreme value\n",
    "#     if axis not in ['x', 'y']:\n",
    "#         raise ValueError(\"Invalid axis. Use 'x' or 'y'.\")\n",
    "#     if max_or_min not in ['max', 'min']:\n",
    "#         raise ValueError(\"Invalid max_or_min. Use 'max' or 'min'.\")\n",
    "\n",
    "#     # Calculate the centroid and extreme value\n",
    "#     if axis == 'x':\n",
    "#         if max_or_min == 'max':\n",
    "#             idx_to_remove = gdf.geometry.bounds['maxx'].idxmax()\n",
    "#         else:\n",
    "#             idx_to_remove = gdf.geometry.bounds['minx'].idxmin()\n",
    "#     else:\n",
    "#         if max_or_min == 'max':\n",
    "#             idx_to_remove = gdf.geometry.bounds['maxy'].idxmax()\n",
    "#         else:\n",
    "#             idx_to_remove = gdf.geometry.bounds['miny'].idxmin()\n",
    "\n",
    "#     # Remove the polygon with the specified polygon index to remove\n",
    "#     gdf_filtered = gdf.drop(idx_to_remove)\n",
    "\n",
    "#     print(f\"Removed polygon with extreme {max_or_min} {axis}-value at index {idx_to_remove}.\")\n",
    "\n",
    "#     return gdf_filtered\n",
    "\n",
    "# # Example usage:\n",
    "# # Assuming 'gdf' is your GeoDataFrame\n",
    "# # new_gdf = remove_extreme_polygon(gdf, axis='x', positive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e38cbfea-ab55-48ad-83fe-719a5885ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_evolving_and_stationary_outlines(lake_ps, dataset1, dataset2): \n",
    "    '''\n",
    "    Create dataframe of dArea, dHeight, dVol calculations for evolving compared to stationary outlines\n",
    "    '''\n",
    "    # Define lake name and polygon and buffered polygon to use as off-lake region\n",
    "    print(type(lake_ps)) # temp\n",
    "    lake_name = lake_ps['name'].iloc[0]\n",
    "\n",
    "    # Open evolving outlines geodataframe\n",
    "    try:\n",
    "        evolving_outlines_gdf = gpd.read_file(os.path.join(\n",
    "            os.getcwd(), 'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)))\n",
    "    except fiona.errors.DriverError:\n",
    "        print(f\"File for {lake_name} not found. Skipping...\")\n",
    "        return  # Skip the rest of the function if the file doesn't exist\n",
    "    \n",
    "    # Ensure there are outlines in outines_gdf and deal with invalid polygon\n",
    "    if evolving_outlines_gdf.empty:\n",
    "        print('There are no evolving outlines in geodataframe.')\n",
    "        return  # Exit the function immediately if evolving_outlines_gdf is empty\n",
    "\n",
    "    # Define region of interest for slicing from evolving and stationary outlines\n",
    "    evolving_outlines_unary_union = evolving_outlines_gdf.unary_union\n",
    "    evolving_outlines_unary_union_buffered = multiple_area_buffer(evolving_outlines_unary_union, 2)\n",
    "    stationary_outline = lake_ps['geometry'].iloc[0]\n",
    "    stationary_outline_buffered = multiple_area_buffer(stationary_outline, 2)   \n",
    "\n",
    "    # Combine stationary outline with evolving outlines from outlines_gdf\n",
    "    combined_outline = unary_union([stationary_outline] + list(evolving_outlines_gdf.geometry))\n",
    "\n",
    "    # Establish x_min, x_max, y_min, y_max from the combined outline\n",
    "    x_min, y_min, x_max, y_max = combined_outline.bounds\n",
    "    del combined_outline\n",
    "    \n",
    "    # Clipping datasets\n",
    "    if dataset1 != None:\n",
    "        # Create and apply the mask to dataset variable\n",
    "        dataset1_combined_outline_sub = dataset1.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "        dataset1_stationary_outline_mask = np.array([[stationary_outline.contains(Point(x, y)) for x in dataset1_combined_outline_sub['x'].values] for y in dataset1_combined_outline_sub['y'].values])\n",
    "        dataset1_stationary_outline_mask_da = xr.DataArray(dataset1_stationary_outline_mask, coords=[dataset1_combined_outline_sub.y, dataset1_combined_outline_sub.x], dims=[\"y\", \"x\"])\n",
    "        dataset1_stationary_outline_masked_data = dataset1['delta_h'].where(dataset1_stationary_outline_mask_da, drop=True)\n",
    "\n",
    "        dataset1_stationary_outline_buffered_mask = np.array([[stationary_outline_buffered.contains(Point(x, y)) for x in dataset1_combined_outline_sub['x'].values] for y in dataset1_combined_outline_sub['y'].values])\n",
    "        dataset1_stationary_region_mask = dataset1_stationary_outline_buffered_mask & ~dataset1_stationary_outline_mask\n",
    "        dataset1_stationary_region_mask_da = xr.DataArray(dataset1_stationary_region_mask, coords=[dataset1_combined_outline_sub.y, dataset1_combined_outline_sub.x], dims=[\"y\", \"x\"])\n",
    "        dataset1_stationary_region_masked_data = dataset1['delta_h'].where(dataset1_stationary_region_mask_da, drop=True)\n",
    "\n",
    "        dataset1_evolving_outlines_unary_union_buffered_mask = np.array([[evolving_outlines_unary_union_buffered.contains(Point(x, y)) for x in dataset1_combined_outline_sub['x'].values] for y in dataset1_combined_outline_sub['y'].values])\n",
    "        dataset1_evolving_outlines_unary_union_mask = np.array([[evolving_outlines_unary_union.contains(Point(x, y)) for x in dataset1_combined_outline_sub['x'].values] for y in dataset1_combined_outline_sub['y'].values])\n",
    "        dataset1_evolving_outlines_region_mask = dataset1_evolving_outlines_unary_union_buffered_mask & ~dataset1_evolving_outlines_unary_union_mask\n",
    "        dataset1_evolving_outlines_region_mask_da = xr.DataArray(dataset1_evolving_outlines_region_mask, coords=[dataset1_combined_outline_sub.y, dataset1_combined_outline_sub.x], dims=[\"y\", \"x\"])\n",
    "        dataset1_evolving_outlines_region_masked_data = dataset1_combined_outline_sub['delta_h'].where(dataset1_evolving_outlines_region_mask_da, drop=True)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Do same for dataset2\n",
    "    dataset2_combined_outline_sub = dataset2.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "    dataset2_stationary_outline_mask = np.array([[stationary_outline.contains(Point(x, y)) for x in dataset2_combined_outline_sub['x'].values] for y in dataset2_combined_outline_sub['y'].values])\n",
    "    dataset2_stationary_outline_mask_da = xr.DataArray(dataset2_stationary_outline_mask, coords=[dataset2_combined_outline_sub.y, dataset2_combined_outline_sub.x], dims=[\"y\", \"x\"])\n",
    "    dataset2_stationary_outline_masked_data = dataset2['delta_h'].where(dataset2_stationary_outline_mask_da, drop=True)\n",
    "\n",
    "    dataset2_stationary_outline_buffered_mask = np.array([[stationary_outline_buffered.contains(Point(x, y)) for x in dataset2_combined_outline_sub['x'].values] for y in dataset2_combined_outline_sub['y'].values])\n",
    "    dataset2_stationary_region_mask = dataset2_stationary_outline_buffered_mask & ~dataset2_stationary_outline_mask\n",
    "    dataset2_stationary_region_mask_da = xr.DataArray(dataset2_stationary_region_mask, coords=[dataset2_combined_outline_sub.y, dataset2_combined_outline_sub.x], dims=[\"y\", \"x\"])\n",
    "    dataset2_stationary_region_masked_data = dataset2['delta_h'].where(dataset2_stationary_region_mask_da, drop=True)\n",
    "\n",
    "    dataset2_evolving_outlines_unary_union_buffered_mask = np.array([[evolving_outlines_unary_union_buffered.contains(Point(x, y)) for x in dataset2_combined_outline_sub['x'].values] for y in dataset2_combined_outline_sub['y'].values])\n",
    "    dataset2_evolving_outlines_unary_union_mask = np.array([[evolving_outlines_unary_union.contains(Point(x, y)) for x in dataset2_combined_outline_sub['x'].values] for y in dataset2_combined_outline_sub['y'].values])\n",
    "    dataset2_evolving_outlines_region_mask = dataset2_evolving_outlines_unary_union_buffered_mask & ~dataset2_evolving_outlines_unary_union_mask\n",
    "    dataset2_evolving_outlines_region_mask_da = xr.DataArray(dataset2_evolving_outlines_region_mask, coords=[dataset2_combined_outline_sub.y, dataset2_combined_outline_sub.x], dims=[\"y\", \"x\"])\n",
    "    dataset2_evolving_outlines_region_masked_data = dataset2_combined_outline_sub['delta_h'].where(dataset2_evolving_outlines_region_mask_da, drop=True)\n",
    "\n",
    "    # Create empty list to store lake and region dh, dh corr, dvol, areas, and dates\n",
    "    stationary_outline_dhs = []\n",
    "    stationary_outline_region_dhs = []\n",
    "    stationary_outline_dhs_corr = []\n",
    "    stationary_outline_dvols_corr = []\n",
    "    evolving_outlines_areas = []\n",
    "    evolving_outlines_dhs = []\n",
    "    evolving_outlines_region_dhs = []\n",
    "    evolving_outlines_dhs_corr = []\n",
    "    evolving_outlines_dvols_corr = []\n",
    "    midcyc_datetimes = []\n",
    "\n",
    "    # Calculate cycle-to-cycle dHeight at each cycle of the spliced data sets\n",
    "    for idx, dt in enumerate(mid_cyc_dates['mid_cyc_dates']):\n",
    "        # For mid_cyc_dates indexes <= 32, use CryoSat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the CryoSat-2 era before ICESat-2 era (2010-08-17 to 2018-08-17)\n",
    "        # Skip indices based on CS2_SARIn_time_period\n",
    "        if lake_gdf['CS2_SARIn_time_period'].iloc[0] == '2013.75-2018.75' and 20 <= idx <= 32:\n",
    "            continue  # Skip the rest of the loop for indexes 21-32 because this dataset has reduced number of time slices\n",
    "        \n",
    "        print('Working on', mid_cyc_dates['mid_cyc_dates'][idx])\n",
    "        if idx <= 32:\n",
    "            if dataset1 != None:\n",
    "                stationary_outline_dh = dataset1_stationary_outline_masked_data[idx+1,:,:]-dataset1_stationary_outline_masked_data[idx,:,:]\n",
    "                stationary_outline_region_dh = dataset1_stationary_region_masked_data[idx+1,:,:]-dataset1_stationary_region_masked_data[idx,:,:]\n",
    "    \n",
    "                # Filter rows that match the current time slice\n",
    "                evolving_outlines_gdf_dt_sub = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Check if evolving_outlines_gdf_dt_sub has any rows before proceeding\n",
    "            if not evolving_outlines_gdf_dt_sub.empty:\n",
    "                evolving_outlines_gdf_dt_sub_unary_union = evolving_outlines_gdf_dt_sub.unary_union\n",
    "                dataset1_evolving_outlines_dt_sub_mask = np.array([[evolving_outlines_gdf_dt_sub_unary_union.contains(Point(x, y)) for x in dataset1_combined_outline_sub['x'].values] for y in dataset1_combined_outline_sub['y'].values])\n",
    "                dataset1_evolving_outlines_dt_sub_mask_da = xr.DataArray(dataset1_evolving_outlines_dt_sub_mask, coords=[dataset1_combined_outline_sub.y, dataset1_combined_outline_sub.x], dims=[\"y\", \"x\"])\n",
    "                dataset1_evolving_outlines_dt_sub_masked_data = dataset1_combined_outline_sub['delta_h'].where(dataset1_evolving_outlines_dt_sub_mask_da, drop=True)\n",
    "                evolving_outlines_dh = dataset1_evolving_outlines_dt_sub_masked_data[idx+1,:,:]-dataset1_evolving_outlines_dt_sub_masked_data[idx,:,:]\n",
    "                evolving_outlines_region_dh = dataset1_evolving_outlines_region_masked_data[idx+1,:,:]-dataset1_evolving_outlines_region_masked_data[idx,:,:]\n",
    "            elif evolving_outlines_gdf_dt_sub.empty:\n",
    "                evolving_outlines_dh = 0\n",
    "                evolving_outlines_region_dh = 0\n",
    "\n",
    "        # For mid_cyc_dates indexes > 32, use ICESat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the ICESat-2 ATL06 v003 (2018-11-16 to 2023-04-02)\n",
    "        elif idx > 32:\n",
    "            # Subtract 33 from idx to start over at index zero with new dataset\n",
    "            stationary_outline_dh = dataset2_stationary_outline_masked_data[(idx-33)+1,:,:]-dataset2_stationary_outline_masked_data[(idx-33),:,:]\n",
    "            stationary_outline_region_dh = dataset2_stationary_region_masked_data[(idx-33)+1,:,:]-dataset2_stationary_region_masked_data[(idx-33),:,:]\n",
    "\n",
    "            # Filter rows that match the current time slice\n",
    "            evolving_outlines_gdf_dt_sub = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "\n",
    "            if not evolving_outlines_gdf_dt_sub.empty:\n",
    "                evolving_outlines_gdf_dt_sub_unary_union = evolving_outlines_gdf_dt_sub.unary_union\n",
    "                dataset2_evolving_outlines_dt_sub_mask = np.array([[evolving_outlines_gdf_dt_sub_unary_union.contains(Point(x, y)) for x in dataset2_combined_outline_sub['x'].values] for y in dataset2_combined_outline_sub['y'].values])\n",
    "                dataset2_evolving_outlines_dt_sub_mask_da = xr.DataArray(dataset2_evolving_outlines_dt_sub_mask, coords=[dataset2_combined_outline_sub.y, dataset2_combined_outline_sub.x], dims=[\"y\", \"x\"])\n",
    "                dataset2_evolving_outlines_dt_sub_masked_data = dataset2_combined_outline_sub['delta_h'].where(dataset2_evolving_outlines_dt_sub_mask_da, drop=True)\n",
    "                evolving_outlines_dh = dataset2_evolving_outlines_dt_sub_masked_data[(idx-33)+1,:,:]-dataset2_evolving_outlines_dt_sub_masked_data[(idx-33),:,:]\n",
    "                evolving_outlines_region_dh = dataset2_evolving_outlines_region_masked_data[(idx-33)+1,:,:]-dataset2_evolving_outlines_region_masked_data[(idx-33),:,:]\n",
    "            elif evolving_outlines_gdf_dt_sub.empty:\n",
    "                evolving_outlines_dh = 0\n",
    "                evolving_outlines_region_dh = 0\n",
    "\n",
    "        # Store data into lists\n",
    "        stationary_outline_dh_mean = np.nanmean(stationary_outline_dh)\n",
    "        stationary_outline_region_dh_mean = np.nanmean(stationary_outline_region_dh)\n",
    "        stationary_outline_dh_corr = stationary_outline_dh_mean - stationary_outline_region_dh_mean\n",
    "        stationary_outline_dvol_corr = stationary_outline_dh_corr*lake_gdf['area (m^2)'].iloc[0]\n",
    "\n",
    "        stationary_outline_dhs += [stationary_outline_dh_mean]\n",
    "        stationary_outline_region_dhs += [stationary_outline_region_dh_mean]\n",
    "        stationary_outline_dhs_corr += [stationary_outline_dh_corr]\n",
    "        stationary_outline_dvols_corr += [stationary_outline_dvol_corr]\n",
    "\n",
    "        evolving_outlines_gdf_dt_sub = evolving_outlines_gdf_dt_sub.to_crs('EPSG:4326')\n",
    "        evolving_outlines_area = sum(evolving_outlines_gdf_dt_sub['geometry'].apply(    \n",
    "            lambda poly: abs(geod.polygon_area_perimeter(\n",
    "            poly.exterior.coords.xy[0], poly.exterior.coords.xy[1])[0]) if poly is not None and poly.is_valid else None))\n",
    "\n",
    "        evolving_outlines_dh_mean = np.nanmean(evolving_outlines_dh)\n",
    "        evolving_outlines_region_dh_mean = np.nanmean(evolving_outlines_region_dh)\n",
    "        evolving_outlines_dh_corr = evolving_outlines_dh_mean - evolving_outlines_region_dh_mean\n",
    "        evolving_outlines_dvol_corr = evolving_outlines_dh_corr*evolving_outlines_area\n",
    "\n",
    "        evolving_outlines_areas += [evolving_outlines_area]\n",
    "        evolving_outlines_dhs += [evolving_outlines_dh_mean]\n",
    "        evolving_outlines_region_dhs += [evolving_outlines_region_dh_mean]\n",
    "        evolving_outlines_dhs_corr += [evolving_outlines_dh_corr]\n",
    "        evolving_outlines_dvols_corr += [evolving_outlines_dvol_corr]\n",
    "\n",
    "        midcyc_datetimes += [mid_cyc_dates[idx]]\n",
    "\n",
    "        # Clear the output of each index\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    # Store stationary outline area\n",
    "    stationary_outline_areas = [lake_gdf['area (m^2)'].iloc[0] for _ in range(len(midcyc_datetimes))]\n",
    "\n",
    "    # Store polygons in dataframe for further analysis\n",
    "    d = {'midcyc_datetime': midcyc_datetimes,\n",
    "        'evolving_outlines_area (m^2)': evolving_outlines_areas,\n",
    "        'evolving_outlines_dh (m)': evolving_outlines_dhs,\n",
    "        'evolving_outlines_region_dh (m)': evolving_outlines_region_dhs,\n",
    "        'evolving_outlines_dh_corr (m)': evolving_outlines_dhs_corr,\n",
    "        'evolving_outlines_dvol_corr (m^3)': evolving_outlines_dvols_corr,\n",
    "        'stationary_outline_area (m^2)': stationary_outline_areas,\n",
    "        'stationary_outline_dh (m)': stationary_outline_dhs, \n",
    "        'stationary_outline_region_dh (m)': stationary_outline_region_dhs,\n",
    "        'stationary_outline_dh_corr (m)': stationary_outline_dhs_corr,\n",
    "        'stationary_outline_dvol_corr (m^3)': stationary_outline_dvols_corr,\n",
    "        'bias_area (m^2)': [evolving - stationary for evolving, stationary in zip(evolving_outlines_areas, stationary_outline_areas)],\n",
    "        'bias_region_dh (m)': [evolving - stationary for evolving, stationary in zip(evolving_outlines_region_dhs, stationary_outline_region_dhs)],\n",
    "        'bias_outlines_dh_corr (m)': [evolving - stationary for evolving, stationary in zip(evolving_outlines_dhs_corr, stationary_outline_dhs_corr)],\n",
    "        'bias_dvol_corr (m^3)': [evolving - stationary for evolving, stationary in zip(evolving_outlines_dvols_corr, stationary_outline_dvols_corr)]}\n",
    "    df = pd.DataFrame(d)\n",
    "\n",
    "    # Export dataframe to csv\n",
    "    df.to_csv('output/lake_outlines/compare_evolving_and_stationary_outlines/{}.csv'.format(lake_name), index=False)\n",
    "\n",
    "    # Clear the output of each index\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e810a61-9054-477c-b708-a6f42bee4fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_evolving_union_and_stationary_outlines(stationary_lake_gdf, dataset1, dataset2, incl_stationary=True): \n",
    "    '''\n",
    "    Create dataframe of dArea, dHeight, dVol calculations for evolving union outline compared to stationary outline\n",
    "    '''\n",
    "    # Define lake name and polygon and buffered polygon to use as off-lake region\n",
    "    lake_name = str(stationary_lake_gdf['name'].iloc[0])\n",
    "\n",
    "    # Find appropriate evolving outlines union geodataframe row\n",
    "    if incl_stationary:\n",
    "        evolving_union_lake_gdf = evolving_stationary_outlines_union_gdf[evolving_stationary_outlines_union_gdf['name'] == lake_name]\n",
    "    else:\n",
    "        evolving_union_lake_gdf = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]\n",
    "    \n",
    "    # Define region of interest for slicing from evolving and stationary outlines\n",
    "    stationary_outline = stationary_lake_gdf['geometry'].iloc[0]\n",
    "    stationary_outline_buffered = multiple_area_buffer(stationary_outline, 2)   \n",
    "    evolving_outlines_union = evolving_union_lake_gdf['geometry'].iloc[0]\n",
    "    evolving_outlines_union_buffered = multiple_area_buffer(evolving_outlines_union, 2)   \n",
    "\n",
    "    # Combine stationary outline with evolving outlines from outlines_gdf\n",
    "    combined_outline = unary_union([evolving_outlines_union, stationary_outline])\n",
    "\n",
    "    # Establish x_min, x_max, y_min, y_max from the combined outline\n",
    "    x_min, y_min, x_max, y_max = combined_outline.bounds\n",
    "    del combined_outline\n",
    "    \n",
    "    # Clipping datasets\n",
    "    if dataset1 != None:\n",
    "        # Create and apply the mask to dataset variable\n",
    "        dataset1_combined_outline_sub = dataset1.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "        dataset1_stationary_outline_mask = np.array([[stationary_outline.contains(Point(x, y)) for x in dataset1_combined_outline_sub['x'].values] for y in dataset1_combined_outline_sub['y'].values])\n",
    "        dataset1_stationary_outline_mask_da = xr.DataArray(dataset1_stationary_outline_mask, coords=[dataset1_combined_outline_sub.y, dataset1_combined_outline_sub.x], dims=[\"y\", \"x\"])\n",
    "        dataset1_stationary_outline_masked_data = dataset1['delta_h'].where(dataset1_stationary_outline_mask_da, drop=True)\n",
    "\n",
    "        dataset1_stationary_outline_buffered_mask = np.array([[stationary_outline_buffered.contains(Point(x, y)) for x in dataset1_combined_outline_sub['x'].values] for y in dataset1_combined_outline_sub['y'].values])\n",
    "        dataset1_stationary_region_mask = dataset1_stationary_outline_buffered_mask & ~dataset1_stationary_outline_mask\n",
    "        dataset1_stationary_region_mask_da = xr.DataArray(dataset1_stationary_region_mask, coords=[dataset1_combined_outline_sub.y, dataset1_combined_outline_sub.x], dims=[\"y\", \"x\"])\n",
    "        dataset1_stationary_region_masked_data = dataset1['delta_h'].where(dataset1_stationary_region_mask_da, drop=True)\n",
    "\n",
    "        dataset1_evolving_outlines_union_buffered_mask = np.array([[evolving_outlines_union_buffered.contains(Point(x, y)) for x in dataset1_combined_outline_sub['x'].values] for y in dataset1_combined_outline_sub['y'].values])\n",
    "        dataset1_evolving_outlines_union_mask = np.array([[evolving_outlines_union.contains(Point(x, y)) for x in dataset1_combined_outline_sub['x'].values] for y in dataset1_combined_outline_sub['y'].values])\n",
    "        dataset1_evolving_outlines_region_mask = dataset1_evolving_outlines_union_buffered_mask & ~dataset1_evolving_outlines_union_mask\n",
    "        dataset1_evolving_outlines_region_mask_da = xr.DataArray(dataset1_evolving_outlines_region_mask, coords=[dataset1_combined_outline_sub.y, dataset1_combined_outline_sub.x], dims=[\"y\", \"x\"])\n",
    "        dataset1_evolving_outlines_region_masked_data = dataset1_combined_outline_sub['delta_h'].where(dataset1_evolving_outlines_region_mask_da, drop=True)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Do same for dataset2\n",
    "    dataset2_combined_outline_sub = dataset2.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "    dataset2_stationary_outline_mask = np.array([[stationary_outline.contains(Point(x, y)) for x in dataset2_combined_outline_sub['x'].values] for y in dataset2_combined_outline_sub['y'].values])\n",
    "    dataset2_stationary_outline_mask_da = xr.DataArray(dataset2_stationary_outline_mask, coords=[dataset2_combined_outline_sub.y, dataset2_combined_outline_sub.x], dims=[\"y\", \"x\"])\n",
    "    dataset2_stationary_outline_masked_data = dataset2['delta_h'].where(dataset2_stationary_outline_mask_da, drop=True)\n",
    "\n",
    "    dataset2_stationary_outline_buffered_mask = np.array([[stationary_outline_buffered.contains(Point(x, y)) for x in dataset2_combined_outline_sub['x'].values] for y in dataset2_combined_outline_sub['y'].values])\n",
    "    dataset2_stationary_region_mask = dataset2_stationary_outline_buffered_mask & ~dataset2_stationary_outline_mask\n",
    "    dataset2_stationary_region_mask_da = xr.DataArray(dataset2_stationary_region_mask, coords=[dataset2_combined_outline_sub.y, dataset2_combined_outline_sub.x], dims=[\"y\", \"x\"])\n",
    "    dataset2_stationary_region_masked_data = dataset2['delta_h'].where(dataset2_stationary_region_mask_da, drop=True)\n",
    "\n",
    "    dataset2_evolving_outlines_union_buffered_mask = np.array([[evolving_outlines_union_buffered.contains(Point(x, y)) for x in dataset2_combined_outline_sub['x'].values] for y in dataset2_combined_outline_sub['y'].values])\n",
    "    dataset2_evolving_outlines_union_mask = np.array([[evolving_outlines_union.contains(Point(x, y)) for x in dataset2_combined_outline_sub['x'].values] for y in dataset2_combined_outline_sub['y'].values])\n",
    "    dataset2_evolving_outlines_region_mask = dataset2_evolving_outlines_union_buffered_mask & ~dataset2_evolving_outlines_union_mask\n",
    "    dataset2_evolving_outlines_region_mask_da = xr.DataArray(dataset2_evolving_outlines_region_mask, coords=[dataset2_combined_outline_sub.y, dataset2_combined_outline_sub.x], dims=[\"y\", \"x\"])\n",
    "    dataset2_evolving_outlines_region_masked_data = dataset2_combined_outline_sub['delta_h'].where(dataset2_evolving_outlines_region_mask_da, drop=True)\n",
    "\n",
    "    # Create empty list to store lake and region dh, dh corr, dvol, areas, and dates\n",
    "    stationary_outline_dhs = []\n",
    "    stationary_outline_region_dhs = []\n",
    "    stationary_outline_dhs_corr = []\n",
    "    stationary_outline_dvols_corr = []\n",
    "    evolving_outlines_union_areas = []\n",
    "    evolving_outlines_union_dhs = []\n",
    "    evolving_outlines_union_region_dhs = []\n",
    "    evolving_outlines_union_dhs_corr = []\n",
    "    evolving_outlines_union_dvols_corr = []\n",
    "    midcyc_datetimes = []\n",
    "\n",
    "    # Calculate cycle-to-cycle dHeight at each cycle of the spliced data sets\n",
    "    for idx, dt in enumerate(mid_cyc_dates['mid_cyc_dates']):\n",
    "        # For mid_cyc_dates indexes <= 32, use CryoSat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the CryoSat-2 era before ICESat-2 era (2010-08-17 to 2018-08-17)\n",
    "        # Skip indices based on CS2_SARIn_time_period\n",
    "        if lake_gdf['CS2_SARIn_time_period'].iloc[0] == '2013.75-2018.75' and 20 <= idx <= 32:\n",
    "            continue  # Skip the rest of the loop for indexes 21-32 because this dataset has reduced number of time slices\n",
    "        \n",
    "        print('Working on', mid_cyc_dates['mid_cyc_dates'][idx])\n",
    "        if idx <= 32:\n",
    "            if dataset1 != None:\n",
    "                stationary_outline_dh = dataset1_stationary_outline_masked_data[idx+1,:,:]-dataset1_stationary_outline_masked_data[idx,:,:]\n",
    "                stationary_outline_region_dh = dataset1_stationary_region_masked_data[idx+1,:,:]-dataset1_stationary_region_masked_data[idx,:,:]\n",
    "                evolving_outlines_union_dh = dataset1_evolving_outlines_union_masked_data[idx+1,:,:]-dataset1_evolving_outlines_union_masked_data[idx,:,:]\n",
    "                evolving_outlines_union_region_dh = dataset1_evolving_region_masked_data[idx+1,:,:]-dataset1_evolving_region_masked_data[idx,:,:]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # For mid_cyc_dates indexes > 32, use ICESat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the ICESat-2 era (2018-11-16 to 2023-04-02)\n",
    "        elif idx > 32:\n",
    "            # Subtract 33 from idx to start over at index zero with new dataset\n",
    "            stationary_outline_dh = dataset2_stationary_outline_masked_data[(idx-33)+1,:,:]-dataset2_stationary_outline_masked_data[(idx-33),:,:]\n",
    "            stationary_outline_region_dh = dataset2_stationary_region_masked_data[(idx-33)+1,:,:]-dataset2_stationary_region_masked_data[(idx-33),:,:]\n",
    "            evolving_outlines_union_dh = dataset2_evolving_outlines_union_masked_data[(idx-33)+1,:,:]-dataset2_evolving_outlines_union__masked_data[(idx-33),:,:]\n",
    "            evolving_outlines_union_region_dh = dataset2_evolving_outlines_union__region_masked_data[(idx-33)+1,:,:]-dataset2_evolving_outlines_union_region_masked_data[(idx-33),:,:]\n",
    "\n",
    "        # Store data into lists\n",
    "        stationary_outline_union_dh_mean = np.nanmean(stationary_outline_dh)\n",
    "        stationary_outline_union_region_dh_mean = np.nanmean(stationary_outline_region_dh)\n",
    "        stationary_outline_union_dh_corr = stationary_outline_dh_mean - stationary_outline_region_dh_mean\n",
    "        stationary_outline_union_dvol_corr = stationary_outline_dh_corr*lake_gdf['area (m^2)'].iloc[0]\n",
    "\n",
    "        stationary_outline_dhs += [stationary_outline_dh_mean]\n",
    "        stationary_outline_region_dhs += [stationary_outline_region_dh_mean]\n",
    "        stationary_outline_dhs_corr += [stationary_outline_dh_corr]\n",
    "        stationary_outline_dvols_corr += [stationary_outline_dvol_corr]\n",
    "        \n",
    "        evolving_outlines_dh_mean = np.nanmean(evolving_outlines_dh)\n",
    "        evolving_outlines_region_dh_mean = np.nanmean(evolving_outlines_region_dh)\n",
    "        evolving_outlines_dh_corr = evolving_outlines_dh_mean - evolving_outlines_region_dh_mean\n",
    "        evolving_outlines_dvol_corr = evolving_outlines_dh_corr*evolving_outlines_area\n",
    "\n",
    "        evolving_outlines_areas += [evolving_outlines_area]\n",
    "        evolving_outlines_dhs += [evolving_outlines_dh_mean]\n",
    "        evolving_outlines_region_dhs += [evolving_outlines_region_dh_mean]\n",
    "        evolving_outlines_dhs_corr += [evolving_outlines_dh_corr]\n",
    "        evolving_outlines_dvols_corr += [evolving_outlines_dvol_corr]\n",
    "\n",
    "        midcyc_datetimes += [mid_cyc_dates[idx]]\n",
    "\n",
    "        # Clear the output of each index\n",
    "        # clear_output(wait=True)\n",
    "\n",
    "    # Store stationary outline area\n",
    "    stationary_outline_areas = [stationary_lake_ps['area (m^2)'].iloc[0] for _ in range(len(midcyc_datetimes))]\n",
    "    evolving_outlines_union_areas = [evolving_union_lake_ps['area (m^2)'].iloc[0] for _ in range(len(midcyc_datetimes))]\n",
    "\n",
    "    # Store polygons in dataframe for further analysis\n",
    "    d = {'midcyc_datetime': midcyc_datetimes,\n",
    "        'evolving_outlines_union_area (m^2)': evolving_union_outlines_areas,\n",
    "        'evolving_outlines_union_dh (m)': evolving_outlines_union_dhs,\n",
    "        'evolving_outlines_union_region_dh (m)': evolving_outlines_union_region_dhs,\n",
    "        'evolving_outlines_union_dh_corr (m)': evolving_outlines_union_dhs_corr,\n",
    "        'evolving_outlines_union_dvol_corr (m^3)': evolving_outlines_union_dvols_corr,\n",
    "        'stationary_outline_area (m^2)': stationary_outline_areas,\n",
    "        'stationary_outline_dh (m)': stationary_outline_dhs, \n",
    "        'stationary_outline_region_dh (m)': stationary_outline_region_dhs,\n",
    "        'stationary_outline_dh_corr (m)': stationary_outline_dhs_corr,\n",
    "        'stationary_outline_dvol_corr (m^3)': stationary_outline_dvols_corr,\n",
    "        'bias_area (m^2)': [evolving - stationary for evolving, stationary in zip(evolving_outlines_areas, stationary_outline_areas)],\n",
    "        'bias_region_dh (m)': [evolving - stationary for evolving, stationary in zip(evolving_outlines_region_dhs, stationary_outline_region_dhs)],\n",
    "        'bias_outlines_dh_corr (m)': [evolving - stationary for evolving, stationary in zip(evolving_outlines_dhs_corr, stationary_outline_dhs_corr)],\n",
    "        'bias_dvol_corr (m^3)': [evolving - stationary for evolving, stationary in zip(evolving_outlines_dvols_corr, stationary_outline_dvols_corr)]}\n",
    "    df = pd.DataFrame(d)\n",
    "\n",
    "    # Export dataframe to csv\n",
    "    if incl_stationary:\n",
    "        df.to_csv('output/lake_outlines/compare_evolving_union_and_stationary_outlines/{}.csv'.format(lake_name), index=False)\n",
    "    else:\n",
    "        df.to_csv('output/lake_outlines/compare_evolving_stationary_union_and_stationary_outlines/{}.csv'.format(lake_name), index=False)\n",
    "\n",
    "    # Clear the output of each index\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e184bf11-21e2-4afe-9a6b-1a5db1df1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_geodesic_area(poly):\n",
    "    '''\n",
    "    Calculate geodesic area of polygon or multipolygon. Polygon or multipolygon must extracted from geodataframe\n",
    "    that has CRS EPSG:4326.\n",
    "    '''\n",
    "    # Ensure geom exists and geom is valid\n",
    "    if poly is None or not poly.is_valid:\n",
    "        return None\n",
    "\n",
    "    # Calculate geodesic area and return it\n",
    "    if isinstance(poly, Polygon):\n",
    "        return abs(geod.polygon_area_perimeter(poly.exterior.coords.xy[0], poly.exterior.coords.xy[1])[0])\n",
    "    elif isinstance(poly, MultiPolygon):\n",
    "        total_area = 0\n",
    "        for part in poly.geoms:\n",
    "            total_area += abs(geod.polygon_area_perimeter(part.exterior.coords.xy[0], part.exterior.coords.xy[1])[0])\n",
    "        return total_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e16a960-6411-4725-b1bb-ff86f4ced3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_evolving_and_stationary_comparison(lake_name):\n",
    "#     '''\n",
    "#     Plot stationary and evolving outlines with dArea, dHeight, and dVolume\n",
    "#     '''\n",
    "#     print('working on {}'.format(lake_name))\n",
    "#     fig, ax = plt.subplots(1,4, figsize=(12,5))\n",
    "    \n",
    "#     # Define colors and linestyles that will be reused and create lines for legend\n",
    "#     # S09_color = 'paleturquoise'\n",
    "#     SF18_color  = 'turquoise'\n",
    "#     # lake_locations_postSF18_color = 'darkturquoise'\n",
    "#     # S09_linestyle=(0, (1, 2))\n",
    "#     SF18_linestyle=(0, (1, 1))\n",
    "#     # S09_line = plt.Line2D((0, 1), (0, 0), color=S09_color, linestyle=S09_linestyle, linewidth=2)\n",
    "#     SF18_line = plt.Line2D((0, 1), (0, 0), color=SF18_color, linestyle='solid', linewidth=1)\n",
    "    \n",
    "#     # Panel A - evolving outlines ------------------------------------------------------\n",
    "#     # Plot stationary and evolving outlines onto MOA surface imagery\n",
    "#     # Open stationary outline(s) and evolving outlines and geometric calculation comparison geodataframes for plotting\n",
    "#     # S09_lake_gdf = S09_outlines[S09_outlines['Name'] == 'Whillans_4']\n",
    "#     lake_gdf = stationary_lakes_gdf[stationary_lakes_gdf['name'] == lake_name]\n",
    "#     # Attempt to open the evolving outlines GeoJSON file\n",
    "#     try:\n",
    "#         evolving_outlines_gdf = gpd.read_file(os.path.join(\n",
    "#             os.getcwd(), 'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)))\n",
    "#     except fiona.errors.DriverError:\n",
    "#         print(f\"File for {lake_name} not found. Skipping...\")\n",
    "#         return  # Skip the rest of the function if the file doesn't exist\n",
    "    \n",
    "#     # Attempt to open the geometric calculations CSV file\n",
    "#     try:\n",
    "#         geom_calcs_df = pd.read_csv(os.path.join(\n",
    "#             os.getcwd(), 'output/lake_outlines/compare_evolving_and_stationary_outlines/{}.csv'.format(lake_name)))\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"CSV file for {lake_name} not found. Skipping...\")\n",
    "#         return  # Skip the rest of the function if the file doesn't exist\n",
    "    \n",
    "#     # Convert of strings to datetime\n",
    "#     geom_calcs_df['midcyc_datetime'] = pd.to_datetime(geom_calcs_df['midcyc_datetime'])\n",
    "\n",
    "#     # Create buffered polygon for the area multiple search extent select for this lake\n",
    "#     search_extent_poly = multiple_area_buffer(lake_gdf.geometry.iloc[0], evolving_outlines_gdf['area_multiple_search_extent'][0])    \n",
    "\n",
    "#     # Combine stationary outline(s) with evolving outlines in unary union to plot all within bounds of plot\n",
    "#     # Use .buffer(0) to fix any invalid geometries\n",
    "#     all_outlines_unary_union = unary_union([search_extent_poly, lake_gdf.geometry.iloc[0].buffer(0)] + list(evolving_outlines_gdf.geometry.buffer(0)))\n",
    "#     x_min, y_min, x_max, y_max = all_outlines_unary_union.bounds\n",
    "#     buffer_frac = 0.2\n",
    "#     x_buffer = abs(x_max-x_min)*buffer_frac\n",
    "#     y_buffer = abs(y_max-y_min)*buffer_frac\n",
    "#     mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "#     mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "#     moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "#     ax[0].imshow(moa_highres_da_subset[0,:,:], cmap=\"gray\", clim=[14000, 17000], extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "    \n",
    "#     # Pick colormap and make continuous cmap discrete for evolving outlines\n",
    "#     colormap = 'plasma'\n",
    "#     continuous_cmap = matplotlib.colormaps[colormap]\n",
    "#     discrete_cmap = colors.ListedColormap(continuous_cmap(np.linspace(0, 1, len(mid_cyc_dates['mid_cyc_dates'])-1)))\n",
    "    \n",
    "#     # Norm to time variable\n",
    "#     norm = plt.Normalize(mdates.date2num(mid_cyc_dates['mid_cyc_dates'].iloc[0]), \n",
    "#                          mdates.date2num(mid_cyc_dates['mid_cyc_dates'].iloc[-1]))\n",
    "    \n",
    "#     # Use for loop to store each time slice as line segment to use in legend\n",
    "#     # And plot each evolving outline in the geodataframe color by date\n",
    "#     lines = []  # list of lines to be used for the legend\n",
    "#     for idx, dt in enumerate(mid_cyc_dates['mid_cyc_dates']):\n",
    "#         x = 1; y = 1\n",
    "#         line, = ax[0].plot(x, y, color=discrete_cmap(norm(mdates.date2num(mid_cyc_dates['mid_cyc_dates'][idx]))), linewidth=1)\n",
    "#         lines.append(line)\n",
    "        \n",
    "#         # Filter rows that match the current time slice\n",
    "#         evolving_outlines_gdf_dt_sub = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "    \n",
    "#         # Plotting the subset if not empty\n",
    "#         if not evolving_outlines_gdf_dt_sub.empty:\n",
    "#             evolving_outlines_gdf_dt_sub.boundary.plot(ax=ax[0], color=discrete_cmap(norm(mdates.date2num(mid_cyc_dates['mid_cyc_dates'][idx]))), linewidth=1)\n",
    "    \n",
    "#     # Plot stationary outline\n",
    "#     lake_gdf.boundary.plot(ax=ax[0], color=SF18_color, linewidth=1)\n",
    "\n",
    "#     # Plot area multiple search extent\n",
    "#     gpd.GeoDataFrame(geometry=[search_extent_poly]).boundary.plot(ax=ax[0], edgecolor='magenta', facecolor='none', linewidth=1)\n",
    "    \n",
    "#     # Plot inset map\n",
    "#     axIns = ax[0].inset_axes([0.8, 0.8, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "#     axIns.set_aspect('equal')\n",
    "#     moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "#     moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "#     axIns.axis('off')\n",
    "    \n",
    "#     # Plot red star to indicate location\n",
    "#     axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "#         linewidth=1, edgecolor='k', facecolor='r', s=50, zorder=3)\n",
    "    \n",
    "#     # Plot legend\n",
    "#     search_extent_line = plt.Line2D((0, 1), (0, 0), color='magenta', linestyle='solid', linewidth=1)\n",
    "#     legend = ax[0].legend([tuple(lines), SF18_line, search_extent_line], ['evolving outlines', 'stationary outline', 'search extent'],\n",
    "#         handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#         loc='upper left')\n",
    "#     # legend.get_frame().set_linewidth(0.0)\n",
    "#     ax[0].patch.set_alpha(1)\n",
    "    \n",
    "#     # Create colorbar \n",
    "#     m = plt.cm.ScalarMappable(cmap=discrete_cmap)\n",
    "#     m.set_array(np.array([timestamp_to_fractional_year(date) for date in mid_cyc_dates['mid_cyc_dates']]))\n",
    "#     cax = inset_axes(ax[0],\n",
    "#                      width=\"100%\",\n",
    "#                      height=\"3%\",\n",
    "#                      loc=3,\n",
    "#                      bbox_to_anchor=[0,-0.2,1,1],\n",
    "#                      bbox_transform=ax[0].transAxes,\n",
    "#                      borderpad=0,\n",
    "#                      )\n",
    "#     cbar=fig.colorbar(m, ticks=np.array([2010,2012,2014,2016,2018,2020,2022]), \n",
    "#                  cax=cax, orientation='horizontal')#.set_label('evolving outline year', size=15)\n",
    "    \n",
    "#     # Set the label for the colorbar and adjust its size\n",
    "#     cbar.set_label('evolving outline year', size=10, labelpad=5)\n",
    "    \n",
    "#     # Change polar stereographic m to km\n",
    "#     km_scale = 1e3\n",
    "#     ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "#     ax[0].xaxis.set_major_formatter(ticks_x)\n",
    "#     ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "#     ax[0].yaxis.set_major_formatter(ticks_y)\n",
    "    \n",
    "#     # Set axes limit, title, and axis label\n",
    "#     ax[0].set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "#     ax[0].set_title('evolving and stationary\\noutline comparison', size=12, pad=8)\n",
    "#     ax[0].set_xlabel('X [km]')\n",
    "#     ax[0].set_ylabel('Y [km]')\n",
    "    \n",
    "#     # Panel - da/dt ---------------------------------------------\n",
    "#     # Plot multi-colored line and scatter for data points\n",
    "#     x=mdates.date2num(geom_calcs_df['midcyc_datetime'])\n",
    "#     y=np.divide(geom_calcs_df['evolving_outlines_area (m^2)'], 1e6)\n",
    "    \n",
    "#     # Create points and segments for LineCollection\n",
    "#     points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "#     segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    \n",
    "#     # Create a LineCollection, using the discrete colormap and norm\n",
    "#     lc = LineCollection(segments, cmap=discrete_cmap, norm=norm, linestyle=(0, (2, 1)))\n",
    "    \n",
    "#     # Set the values used for colormapping, using matplotlib dates for colors\n",
    "#     lc.set_array(x)\n",
    "#     lc.set_linewidth(2)\n",
    "#     line = ax[1].add_collection(lc)\n",
    "    \n",
    "#     # Scatter plot, using the discrete colormap and norm for coloring\n",
    "#     # Convert x values (fractional years) back to matplotlib dates for consistent coloring\n",
    "#     scatter = ax[1].scatter(x, y, c=x, cmap=discrete_cmap, norm=norm)\n",
    "    \n",
    "#     # Plot stationary outline area\n",
    "#     ax[1].axhline(np.divide(lake_gdf['area (m^2)'], 1e6).values, color=SF18_color, linestyle='solid', linewidth=1)\n",
    "    \n",
    "#     # # Plot bias\n",
    "#     # ax[1].plot(mdates.date2num(geom_calcs_df['midcyc_datetime']),\n",
    "#     #     np.divide(geom_calcs_df['bias_area (m^2)'], 1e6), color='red', linestyle='solid', linewidth=1)\n",
    "    \n",
    "#     # # Add legend\n",
    "#     # bias = plt.Line2D((0, 1), (0, 0), color='red', linestyle='solid', linewidth=1)\n",
    "#     # legend = ax[1].legend([tuple(lines), SF18_line, bias],\n",
    "#     #     ['evolving outlines', 'stationary outline', 'bias (evolving - stationary)'], \n",
    "#     #     handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#     #     loc='upper left')\n",
    "#     legend = ax[1].legend([tuple(lines), SF18_line],\n",
    "#         ['evolving outlines', 'stationary outline'], \n",
    "#         handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "#         loc='upper left')\n",
    "\n",
    "#     # Format the x-axis to display years only\n",
    "#     ax[1].xaxis.set_major_locator(mdates.YearLocator(base=2))  # Major ticks every other year\n",
    "#     ax[1].xaxis.set_minor_locator(mdates.YearLocator(base=1))  # Minor ticks every year\n",
    "#     ax[1].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Display major ticks as years\n",
    "    \n",
    "#     # # Set x-axis limits\n",
    "#     # start_date = geom_calcs_df['midcyc_datetime'].min()\n",
    "#     # end_date = geom_calcs_df['midcyc_datetime'].max()\n",
    "#     # ax[1].set_xlim(mdates.date2num(start_date), mdates.date2num(end_date))\n",
    "    \n",
    "#     # Set title and axis label\n",
    "#     ax[1].set_title('wetted area [km$^2$]', size=12, pad=8)\n",
    "#     ax[1].set_xlabel('year')\n",
    "    \n",
    "#     # Panel C - dh/dt -------------------------------------------------------\n",
    "#     # Plot horizontal zero line for reference\n",
    "#     ax[2].axhline(0, color='gray', linestyle='dotted', linewidth=1)\n",
    "    \n",
    "#     # Plot multi-colored line and scatter for data points\n",
    "#     x=mdates.date2num(geom_calcs_df['midcyc_datetime'])\n",
    "#     y=np.cumsum(geom_calcs_df['evolving_outlines_dh_corr (m)'])\n",
    "    \n",
    "#     # Create points and segments for LineCollection\n",
    "#     points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "#     segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    \n",
    "#     # Create a LineCollection, using the discrete colormap and norm\n",
    "#     lc = LineCollection(segments, cmap=discrete_cmap, norm=norm, linestyle=(0, (2, 1)))\n",
    "    \n",
    "#     # Set the values used for colormapping, using matplotlib dates for colors\n",
    "#     lc.set_array(x)\n",
    "#     lc.set_linewidth(2)\n",
    "#     line = ax[2].add_collection(lc)\n",
    "    \n",
    "#     # Scatter plot, using the discrete colormap and norm for coloring\n",
    "#     # Convert x values (fractional years) back to matplotlib dates for consistent coloring\n",
    "#     scatter = ax[2].scatter(x, y, c=x, cmap=discrete_cmap, norm=norm)\n",
    "    \n",
    "#     # Plot stationary outline time series\n",
    "#     ax[2].plot(mdates.date2num(geom_calcs_df['midcyc_datetime']), \n",
    "#         np.cumsum(geom_calcs_df['stationary_outline_dh_corr (m)']), color=SF18_color, linestyle='solid', linewidth=1)\n",
    "    \n",
    "#     # Plot bias\n",
    "#     ax[2].plot(mdates.date2num(geom_calcs_df['midcyc_datetime']),\n",
    "#         np.cumsum(geom_calcs_df['bias_outlines_dh_corr (m)']), color='red', linestyle='solid', linewidth=1)\n",
    "\n",
    "#     # Add legend\n",
    "#     bias = plt.Line2D((0, 1), (0, 0), color='red', linestyle='solid', linewidth=1)\n",
    "#     legend = ax[2].legend([bias],\n",
    "#         ['bias (evolving - stationary)'], \n",
    "#         loc='upper left')\n",
    "    \n",
    "#     # Format the x-axis to display years only\n",
    "#     ax[2].xaxis.set_major_locator(mdates.YearLocator(base=2))  # Major ticks every other year\n",
    "#     ax[2].xaxis.set_minor_locator(mdates.YearLocator(base=1))  # Minor ticks every year\n",
    "#     ax[2].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Display major ticks as years\n",
    "    \n",
    "#     # # Set x-axis limits\n",
    "#     # start_date = geom_calcs_df['midcyc_datetime'].min()\n",
    "#     # end_date = geom_calcs_df['midcyc_datetime'].max()\n",
    "#     # ax[2].set_xlim(mdates.date2num(start_date), mdates.date2num(end_date))\n",
    "    \n",
    "#     # Set title and axis label\n",
    "#     ax[2].set_title('cumulative\\nheight change [m]', size=12, pad=8)\n",
    "#     ax[2].set_xlabel('year')\n",
    "    \n",
    "#     # Panel D - dv/dt --------------------------------------------------\n",
    "#     # Plot horizontal line at zero for reference\n",
    "#     ax[3].axhline(0, color='gray', linestyle='dotted', linewidth=1)\n",
    "    \n",
    "#     # Plot stationary outline time series\n",
    "#     ax[3].plot(mdates.date2num(geom_calcs_df['midcyc_datetime']), \n",
    "#         np.divide(np.cumsum(geom_calcs_df['stationary_outline_dvol_corr (m^3)']), 1e9), \n",
    "#         color=SF18_color, linestyle='solid', linewidth=1)\n",
    "    \n",
    "#     # Plot multi-colored line and scatter for data points\n",
    "#     x=mdates.date2num(geom_calcs_df['midcyc_datetime'])\n",
    "#     y=np.cumsum(np.divide(geom_calcs_df['evolving_outlines_dvol_corr (m^3)'], 1e9))\n",
    "    \n",
    "#     # Create points and segments for LineCollection\n",
    "#     points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "#     segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    \n",
    "#     # Create a LineCollection, using the discrete colormap and norm\n",
    "#     lc = LineCollection(segments, cmap=discrete_cmap, norm=norm, linestyle=(0, (2, 1)))\n",
    "    \n",
    "#     # Set the values used for colormapping, using matplotlib dates for colors\n",
    "#     lc.set_array(x)\n",
    "#     lc.set_linewidth(2)\n",
    "#     line = ax[3].add_collection(lc)\n",
    "    \n",
    "#     # Scatter plot, using the discrete colormap and norm for coloring\n",
    "#     scatter = ax[3].scatter(x, y, c=x, cmap=discrete_cmap, norm=norm)\n",
    "    \n",
    "#     # Plot bias\n",
    "#     ax[3].plot(mdates.date2num(geom_calcs_df['midcyc_datetime']),\n",
    "#         np.cumsum(np.divide(geom_calcs_df['bias_dvol_corr (m^3)'], 1e9)), color='red', linestyle='solid', linewidth=1)\n",
    "    \n",
    "#     # Format the x-axis to display years only\n",
    "#     ax[3].xaxis.set_major_locator(mdates.YearLocator(base=2))  # Major ticks every other year\n",
    "#     ax[3].xaxis.set_minor_locator(mdates.YearLocator(base=1))  # Minor ticks every year\n",
    "#     ax[3].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Display major ticks as years\n",
    "    \n",
    "#     # # Set x-axis limits\n",
    "#     # start_date = geom_calcs_df['midcyc_datetime'].min()\n",
    "#     # end_date = geom_calcs_df['midcyc_datetime'].max()\n",
    "#     # ax[3].set_xlim(mdates.date2num(start_date), mdates.date2num(end_date))\n",
    "    \n",
    "#     # Set title and axis label\n",
    "#     ax[3].set_title('cumulative ice volume\\ndisplacement [km$^3$]', size=12, pad=8)\n",
    "#     ax[3].set_xlabel('year')\n",
    "    \n",
    "#     plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    \n",
    "#     # Save and close plot\n",
    "#     plt.savefig(OUTPUT_DIR + '/plot_evolving_and_stationary_comparison/{}.png'\n",
    "#         .format(lake_name),\n",
    "#                 dpi=300, bbox_inches='tight')\n",
    "    \n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6438f1f1-ae5e-4624-80ec-2f187a70cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evolving_and_stationary_comparison(lake_gdf):\n",
    "    '''\n",
    "    Plot and compare the evolving and stationary outlines of a lake along with the derived geometric calculations.\n",
    "\n",
    "    This function generates a set of comparison plots for a given lake, showing the differences between the evolving \n",
    "    and stationary outlines over time. It includes visualizations of the outlines on a map, as well as plots for \n",
    "    wetted area, cumulative height change, and cumulative volume displacement. The results are saved as a PNG file.\n",
    "\n",
    "    Parameters:\n",
    "    lake_gdf (GeoDataFrame): A GeoDataFrame containing a single lake's data with attributes such as 'name' and 'geometry'.\n",
    "                             The GeoDataFrame should have a single row corresponding to the lake.\n",
    "\n",
    "    Returns:\n",
    "    None: The results are saved as PNG files in the 'output/plot_evolving_and_stationary_comparison/' directory with filenames\n",
    "          corresponding to the lake names.\n",
    "\n",
    "    Process:\n",
    "    1. Extract the lake name and stationary outline from the input GeoDataFrame.\n",
    "    2. Attempt to open the evolving outlines GeoJSON file and the geometric calculations CSV file for the lake.\n",
    "       If either file is not found, skip the rest of the function for the current lake.\n",
    "    3. Convert the 'midcyc_datetime' column in the geometric calculations DataFrame to datetime format.\n",
    "    4. Set up a 4-panel plot with subplots for the evolving and stationary outlines, wetted area, cumulative height change,\n",
    "       and cumulative volume displacement.\n",
    "    5. For the first panel, plot the evolving and stationary outlines on a map, including a legend and an inset map showing \n",
    "       the lake's location.\n",
    "    6. For the second panel, plot the wetted area over time, using a multi-colored line to represent different time slices.\n",
    "    7. For the third panel, plot the cumulative height change over time, including corrections and bias.\n",
    "    8. For the fourth panel, plot the cumulative volume displacement over time, including corrections and bias.\n",
    "    9. Format the axes, titles, labels, and legends for each subplot.\n",
    "    10. Save the entire plot as a PNG file in the specified directory and close the plot to free up memory.\n",
    "\n",
    "    Example:\n",
    "    >>> lake_gdf = gpd.read_file('path_to_lake.geojson')\n",
    "    >>> plot_evolving_and_stationary_comparison(lake_gdf)\n",
    "    '''\n",
    "    # Define lake name and polygon and buffered polygon to use as off-lake region\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    stationary_outline = lake_gdf['geometry']\n",
    "    print('working on {}'.format(lake_name))\n",
    "    \n",
    "    # Open stationary outline(s) and evolving outlines and geometric calculation comparison geodataframes for plotting\n",
    "    # Attempt to open the evolving outlines GeoJSON file\n",
    "    try:\n",
    "        evolving_outlines_gdf = gpd.read_file(os.path.join(\n",
    "            os.getcwd(), 'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)))\n",
    "    except fiona.errors.DriverError:\n",
    "        print(f\"File for {lake_name} not found. Skipping...\")\n",
    "        return  # Skip the rest of the function if the file doesn't exist\n",
    "    \n",
    "    # Attempt to open the geometric calculations CSV file\n",
    "    try:\n",
    "        geom_calcs_df = pd.read_csv(os.path.join(\n",
    "            os.getcwd(), 'output/lake_outlines/compare_evolving_and_stationary_outlines/{}.csv'.format(lake_name)))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"CSV file for {lake_name} not found. Skipping...\")\n",
    "        return  # Skip the rest of the function if the file doesn't exist\n",
    "    \n",
    "    # Convert of strings to datetime\n",
    "    geom_calcs_df['midcyc_datetime'] = pd.to_datetime(geom_calcs_df['midcyc_datetime'])\n",
    "    \n",
    "    fig, ax = plt.subplots(1,4, figsize=(12,5))\n",
    "    \n",
    "    # Define colors and linestyles that will be reused and create lines for legend\n",
    "    stationary_color  = 'turquoise'\n",
    "    stationary_linestyle=(0, (1, 1))\n",
    "    stationary_line = plt.Line2D((0, 1), (0, 0), color=stationary_color, linestyle='solid', linewidth=1)\n",
    "    \n",
    "    # Panel - evolving outlines ------------------------------------------------------\n",
    "\n",
    "    # Create buffered polygon for the area multiple search extent select for this lake\n",
    "    search_extent_poly = multiple_area_buffer(lake_gdf['geometry'].iloc[0], evolving_outlines_gdf['area_multiple_search_extent'][0])    \n",
    "\n",
    "    # Combine stationary outline(s) with evolving outlines in unary union to plot all within bounds of plot\n",
    "    # Use .buffer(0) to fix any invalid geometries\n",
    "    all_outlines_unary_union = unary_union([search_extent_poly, lake_gdf['geometry'].iloc[0].buffer(0)] + list(evolving_outlines_gdf.geometry.buffer(0)))\n",
    "    x_min, y_min, x_max, y_max = all_outlines_unary_union.bounds\n",
    "    buffer_frac = 0.3\n",
    "    x_buffer = abs(x_max-x_min)*buffer_frac\n",
    "    y_buffer = abs(y_max-y_min)*buffer_frac\n",
    "    mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "    mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "    \n",
    "    # Plot stationary and evolving outlines onto MOA surface imagery\n",
    "    moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "    ax[0].imshow(moa_highres_da_subset[0,:,:], cmap=\"gray\", clim=[14000, 17000], extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "    \n",
    "    # Pick colormap and make continuous cmap discrete for evolving outlines\n",
    "    colormap = 'plasma'\n",
    "    continuous_cmap = matplotlib.colormaps[colormap]\n",
    "    discrete_cmap = colors.ListedColormap(continuous_cmap(np.linspace(0, 1, len(mid_cyc_dates['mid_cyc_dates'])-1)))\n",
    "    \n",
    "    # Norm to time variable\n",
    "    norm = plt.Normalize(mdates.date2num(mid_cyc_dates['mid_cyc_dates'].iloc[0]), \n",
    "                         mdates.date2num(mid_cyc_dates['mid_cyc_dates'].iloc[-1]))\n",
    "    \n",
    "    # Use for loop to store each time slice as line segment to use in legend\n",
    "    # And plot each evolving outline in the geodataframe color by date\n",
    "    lines = []  # list of lines to be used for the legend\n",
    "    for idx, dt in enumerate(mid_cyc_dates['mid_cyc_dates']):\n",
    "        x = 1; y = 1\n",
    "        line, = ax[0].plot(x, y, color=discrete_cmap(norm(mdates.date2num(mid_cyc_dates['mid_cyc_dates'][idx]))), linewidth=1)\n",
    "        lines.append(line)\n",
    "        \n",
    "        # Filter rows that match the current time slice\n",
    "        evolving_outlines_gdf_dt_sub = evolving_outlines_gdf[evolving_outlines_gdf['midcyc_datetime'] == dt]\n",
    "    \n",
    "        # Plotting the subset if not empty\n",
    "        if not evolving_outlines_gdf_dt_sub.empty:\n",
    "            evolving_outlines_gdf_dt_sub.boundary.plot(ax=ax[0], color=discrete_cmap(norm(mdates.date2num(mid_cyc_dates['mid_cyc_dates'][idx]))), linewidth=1)\n",
    "    \n",
    "    # Plot stationary outline\n",
    "    lake_gdf['geometry'].boundary.plot(ax=ax[0], color=stationary_color, linewidth=1)\n",
    "\n",
    "    # Plot area multiple search extent\n",
    "    gpd.GeoDataFrame(geometry=[search_extent_poly]).boundary.plot(ax=ax[0], edgecolor='magenta', facecolor='none', linewidth=1)\n",
    "\n",
    "    # Create evolving outlines unary union and plot\n",
    "    evolving_outlines_unary_union = unary_union(list(evolving_outlines_gdf.geometry))\n",
    "    evolving_outlines_unary_union_gdf = gpd.GeoDataFrame(geometry=[evolving_outlines_unary_union], crs='3031')\n",
    "    evolving_outlines_unary_union_gdf.boundary.plot(ax=ax[0], color='k', linestyle='dotted', linewidth=1)\n",
    "\n",
    "    # Convert GeoDataFrame to EPSG:4326 for geodesic area calculation\n",
    "    evolving_outlines_unary_union_gdf = evolving_outlines_unary_union_gdf.to_crs('4326')\n",
    "    \n",
    "    # Calculate the geodesic area for each polygon\n",
    "    evolving_outlines_unary_union_gdf['area (m^2)'] = evolving_outlines_unary_union_gdf['geometry'].apply(calculate_geodesic_area)\n",
    "\n",
    "    # Plot inset map\n",
    "    axIns = ax[0].inset_axes([0.8, -0.1, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "    axIns.set_aspect('equal')\n",
    "    moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    axIns.axis('off')\n",
    "    \n",
    "    # Plot red star to indicate location\n",
    "    axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "        linewidth=1, edgecolor='k', facecolor='r', s=50, zorder=3)\n",
    "    \n",
    "    # Plot legend\n",
    "    search_extent_line = plt.Line2D((0, 1), (0, 0), color='magenta', linestyle='solid', linewidth=1)\n",
    "    unary_union_line = plt.Line2D((0, 1), (0, 0), color='k', linestyle='dotted', linewidth=1)\n",
    "    legend = ax[0].legend([search_extent_line, tuple(lines), unary_union_line, stationary_line], \n",
    "        ['search extent ({}x)'.format(int(evolving_outlines_gdf['area_multiple_search_extent'][0])),\n",
    "         'evolving outlines ({} m)'.format(evolving_outlines_gdf['level'][0]),\n",
    "         'evolving outlines unary union',\n",
    "         'stationary outline'],\n",
    "         handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "         fontsize='small', loc='upper left')\n",
    "    # legend.get_frame().set_linewidth(0.0)\n",
    "    ax[0].patch.set_alpha(1)\n",
    "    \n",
    "    # Create colorbar\n",
    "    m = plt.cm.ScalarMappable(cmap=discrete_cmap)\n",
    "    m.set_array(np.array([mdates.date2num(date) for date in mid_cyc_dates['mid_cyc_dates']]))\n",
    "    cax = inset_axes(ax[0],\n",
    "                     width=\"100%\",\n",
    "                     height=\"3%\",\n",
    "                     loc=3,\n",
    "                     bbox_to_anchor=[0,-0.2,1,1],\n",
    "                     bbox_transform=ax[0].transAxes,\n",
    "                     borderpad=0,\n",
    "                     )\n",
    "    # Define years and convert to matplotlib date numbers\n",
    "    years = [2012, 2014, 2016, 2018, 2020, 2022]\n",
    "    year_dates = [mdates.date2num(datetime.datetime(year, 1, 1)) for year in years]\n",
    "    cbar=fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "    cbar.set_ticks(year_dates)  # Set ticks to the dates corresponding to the beginning of each year\n",
    "    cbar.set_ticklabels(years)  # Set tick labels as years\n",
    "    cbar.set_label('evolving outline year', size=10, labelpad=5)\n",
    "\n",
    "    # Change polar stereographic m to km\n",
    "    km_scale = 1e3\n",
    "    ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax[0].xaxis.set_major_formatter(ticks_x)\n",
    "    ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax[0].yaxis.set_major_formatter(ticks_y)\n",
    "    \n",
    "    # Set axes limit, title, and axis label\n",
    "    ax[0].set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "    ax[0].set_title('evolving and stationary\\noutline comparison\\n{}'.format(lake_name), size=12, pad=5)\n",
    "    ax[0].set_xlabel('X [km]')\n",
    "    ax[0].set_ylabel('Y [km]')\n",
    "    \n",
    "    # Panel - da/dt ---------------------------------------------\n",
    "    # Plot stationary outline and evolving outlines unary union areas\n",
    "    ax[1].axhline(np.divide(lake_gdf['area (m^2)'], 1e6).values, color=stationary_color, linestyle='solid', linewidth=1)\n",
    "    ax[1].axhline(np.divide(evolving_outlines_unary_union_gdf['area (m^2)'], 1e6).values, color='k', linestyle='dotted', linewidth=1)\n",
    "\n",
    "    # Plot multi-colored line and scatter for data points\n",
    "    x=mdates.date2num(geom_calcs_df['midcyc_datetime'])\n",
    "    y=np.divide(geom_calcs_df['evolving_outlines_area (m^2)'], 1e6)\n",
    "    \n",
    "    # Create points and segments for LineCollection\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    \n",
    "    # Create a LineCollection, using the discrete colormap and norm\n",
    "    lc = LineCollection(segments, cmap=discrete_cmap, norm=norm, linestyle='solid')\n",
    "    lc.set_array(x)  # Set the values used for colormapping, using matplotlib dates for colors\n",
    "    lc.set_linewidth(2)\n",
    "    line = ax[1].add_collection(lc)\n",
    "    scatter = ax[1].scatter(x, y, c=x, cmap=discrete_cmap, norm=norm, zorder=2)  # Scatter if I get legend line to work\n",
    "\n",
    "    # Add legend\n",
    "    legend = ax[1].legend([tuple(lines), unary_union_line, stationary_line],\n",
    "        ['evolving outlines darea', 'evolving outlines unary union area', 'stationary outline area'], \n",
    "        handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "        fontsize='small', loc='upper left')\n",
    "\n",
    "    # Format the x-axis to display years only\n",
    "    ax[1].xaxis.set_major_locator(mdates.YearLocator(base=2))  # Major ticks every other year\n",
    "    ax[1].xaxis.set_minor_locator(mdates.YearLocator(base=1))  # Minor ticks every year\n",
    "    ax[1].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Display major ticks as years\n",
    "    \n",
    "    # # Set x-axis limits\n",
    "    # start_date = geom_calcs_df['midcyc_datetime'].min()\n",
    "    # end_date = geom_calcs_df['midcyc_datetime'].max()\n",
    "    # ax[1].set_xlim(mdates.date2num(start_date), mdates.date2num(end_date))\n",
    "    \n",
    "    # Set title and axis label\n",
    "    ax[1].set_title('wetted area (darea) [km$^2$]', size=12, pad=8)\n",
    "    ax[1].set_xlabel('year')\n",
    "    \n",
    "    # Panel C - dh/dt -------------------------------------------------------\n",
    "    # Plot horizontal zero line for reference\n",
    "    ax[2].axhline(0, color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "    # Plot stationary outlines off-lake region dh\n",
    "    ax[2].plot(mdates.date2num(geom_calcs_df['midcyc_datetime']), \n",
    "        np.cumsum(geom_calcs_df['stationary_outline_region_dh (m)']), color='lightgray', linestyle='solid', linewidth=1)\n",
    "\n",
    "    # Plot evolving outlines off-lake region dh\n",
    "    ax[2].plot(mdates.date2num(geom_calcs_df['midcyc_datetime']), \n",
    "        np.cumsum(geom_calcs_df['evolving_outlines_region_dh (m)']), color='dimgray', linestyle='solid', linewidth=1)\n",
    "\n",
    "    # Plot stationary outline time series\n",
    "    ax[2].plot(mdates.date2num(geom_calcs_df['midcyc_datetime']), \n",
    "        np.cumsum(geom_calcs_df['stationary_outline_dh_corr (m)']), color=stationary_color, linestyle='solid', linewidth=1)\n",
    "\n",
    "    # Plot multi-colored line and scatter for data points\n",
    "    x=mdates.date2num(geom_calcs_df['midcyc_datetime'])\n",
    "    y=np.cumsum(geom_calcs_df['evolving_outlines_dh_corr (m)'])\n",
    "    \n",
    "    # Create points and segments for LineCollection\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    \n",
    "    # Create a LineCollection, using the discrete colormap and norm\n",
    "    lc = LineCollection(segments, cmap=discrete_cmap, norm=norm, linestyle='solid')\n",
    "    lc.set_array(x)\n",
    "    lc.set_linewidth(2)\n",
    "    line = ax[2].add_collection(lc)\n",
    "    scatter = ax[2].scatter(x, y, c=x, cmap=discrete_cmap, norm=norm, zorder=2)  # Scatter if I can figure out how to put in legend\n",
    "\n",
    "    # Plot bias\n",
    "    ax[2].plot(mdates.date2num(geom_calcs_df['midcyc_datetime']),\n",
    "        np.cumsum(geom_calcs_df['bias_outlines_dh_corr (m)']), color='red', linestyle='solid', linewidth=1)\n",
    "\n",
    "    # Add legend\n",
    "    evolving_region = plt.Line2D((0, 1), (0, 0), color='dimgray', linestyle='solid', linewidth=1)\n",
    "    stationary_region = plt.Line2D((0, 1), (0, 0), color='lightgray', linestyle='solid', linewidth=1)\n",
    "    bias = plt.Line2D((0, 1), (0, 0), color='red', linestyle='solid', linewidth=1)\n",
    "    legend = ax[2].legend(\n",
    "        [evolving_region,\n",
    "         stationary_region,\n",
    "         tuple(lines),\n",
    "         stationary_line,  \n",
    "         bias],\n",
    "        ['evolving outlines off-lake dh',\n",
    "         'stationary outline off-lake dh',\n",
    "         'evolving outlines dh$_{corr}$',\n",
    "         'stationary outline dh$_{corr}$', \n",
    "         'dh$_{corr}$ bias (evolving - stationary)'],\n",
    "         handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "         fontsize='small', loc='upper left')\n",
    "    \n",
    "    # Format the x-axis to display years only\n",
    "    ax[2].xaxis.set_major_locator(mdates.YearLocator(base=2))  # Major ticks every other year\n",
    "    ax[2].xaxis.set_minor_locator(mdates.YearLocator(base=1))  # Minor ticks every year\n",
    "    ax[2].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Display major ticks as years\n",
    "    \n",
    "    # # Set x-axis limits\n",
    "    # start_date = geom_calcs_df['midcyc_datetime'].min()\n",
    "    # end_date = geom_calcs_df['midcyc_datetime'].max()\n",
    "    # ax[2].set_xlim(mdates.date2num(start_date), mdates.date2num(end_date))\n",
    "    \n",
    "    # Set title and axis label\n",
    "    ax[2].set_title('cumulative height\\n change (dh) [m]', size=12, pad=8)\n",
    "    ax[2].set_xlabel('year')\n",
    "    \n",
    "    # Panel D - dvol/dt --------------------------------------------------\n",
    "    # Plot horizontal line at zero for reference\n",
    "    ax[3].axhline(0, color='k', linestyle='dashed', linewidth=1)\n",
    "    \n",
    "    # Plot stationary outline time series\n",
    "    ax[3].plot(mdates.date2num(geom_calcs_df['midcyc_datetime']), \n",
    "        np.divide(np.cumsum(geom_calcs_df['stationary_outline_dvol_corr (m^3)']), 1e9), \n",
    "        color=stationary_color, linestyle='solid', linewidth=1)\n",
    "    \n",
    "    # Plot multi-colored line and scatter for data points\n",
    "    x=mdates.date2num(geom_calcs_df['midcyc_datetime'])\n",
    "    y=np.cumsum(np.divide(geom_calcs_df['evolving_outlines_dvol_corr (m^3)'], 1e9))\n",
    "    \n",
    "    # Create points and segments for LineCollection\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    \n",
    "    # Create a LineCollection, using the discrete colormap and norm\n",
    "    lc = LineCollection(segments, cmap=discrete_cmap, norm=norm, linestyle='solid')\n",
    "    lc.set_array(x)\n",
    "    lc.set_linewidth(2)\n",
    "    line = ax[3].add_collection(lc)\n",
    "    scatter = ax[3].scatter(x, y, c=x, cmap=discrete_cmap, norm=norm, zorder=2)\n",
    "    \n",
    "    # Plot bias\n",
    "    ax[3].plot(mdates.date2num(geom_calcs_df['midcyc_datetime']),\n",
    "        np.cumsum(np.divide(geom_calcs_df['bias_dvol_corr (m^3)'], 1e9)), color='red', linestyle='solid', linewidth=1)\n",
    "\n",
    "    # Add legend\n",
    "    legend = ax[3].legend([tuple(lines), stationary_line, bias],\n",
    "        ['evolving outlines dvol$_{corr}$', \n",
    "         'stationary outline  dvol$_{corr}$',\n",
    "         'dvol$_{corr}$ bias (evolving - stationary)'], \n",
    "        handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "        fontsize='small', loc='upper left')\n",
    "    \n",
    "    # Format the x-axis to display years only\n",
    "    ax[3].xaxis.set_major_locator(mdates.YearLocator(base=2))  # Major ticks every other year\n",
    "    ax[3].xaxis.set_minor_locator(mdates.YearLocator(base=1))  # Minor ticks every year\n",
    "    ax[3].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Display major ticks as years\n",
    "    \n",
    "    # # Set x-axis limits\n",
    "    # start_date = geom_calcs_df['midcyc_datetime'].min()\n",
    "    # end_date = geom_calcs_df['midcyc_datetime'].max()\n",
    "    # ax[3].set_xlim(mdates.date2num(start_date), mdates.date2num(end_date))\n",
    "    \n",
    "    # Set title and axis label\n",
    "    ax[3].set_title('cumulative volume\\ndisplacement (dvol) [km$^3$]', size=12, pad=8)\n",
    "    ax[3].set_xlabel('year')\n",
    "    \n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    \n",
    "    # Save and close plot\n",
    "    plt.savefig(OUTPUT_DIR + '/plot_evolving_and_stationary_comparison/{}.png'\n",
    "        .format(lake_name),\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6aaf623-3e84-469e-94e0-f780b13cc099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_folders_in_directory(directory):\n",
    "    '''\n",
    "    Counts the number of folders in the given directory.\n",
    "    '''\n",
    "    folder_count = 0\n",
    "    for item in os.listdir(directory):\n",
    "        # Construct full path to the item\n",
    "        item_path = os.path.join(directory, item)\n",
    "        # Check if the item is a directory\n",
    "        if os.path.isdir(item_path):\n",
    "            folder_count += 1\n",
    "    return folder_count\n",
    "\n",
    "# # Example usage\n",
    "# directory_path = '/home/jovyan/1_outlines_candidates/output/FigS1_lake_reexamination_methods.ipynb/plot_height_changes'  # Replace this with the path to your directory\n",
    "# print(f'There are {count_folders_in_directory(directory_path)} folders in \"{directory_path}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "987cb664-0b99-4afb-91a9-f3561953a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_evolving_outlines_union(lake_ps, incl_stationary=True):\n",
    "    '''\n",
    "    Find the unary union of evolving outlines and return as a GeoSeries.\n",
    "    \n",
    "    Args:\n",
    "    lake_gdf: Pandas series of lake row from lakes geodataframe\n",
    "\n",
    "    Returns:\n",
    "    GeoSeries containing the unary union of the outlines, or None if an error occurs.\n",
    "    '''\n",
    "    lake_name = lake_ps['name']\n",
    "\n",
    "    # Attempt to open evolving outlines geojson\n",
    "    try:\n",
    "        evolving_outlines_gdf = gpd.read_file(os.path.join(\n",
    "            os.getcwd(), 'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)))\n",
    "    except Exception as e:  # Using a general exception to catch all file and driver errors\n",
    "        print(f\"File for {lake_name} not found or error reading file: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Create evolving outlines unary union\n",
    "    if incl_stationary:\n",
    "        outlines_unary_union = unary_union([lake_ps.geometry] + list(evolving_outlines_gdf.geometry))\n",
    "\n",
    "    else: \n",
    "        outlines_unary_union = unary_union(list(evolving_outlines_gdf.geometry))\n",
    "    \n",
    "    return gpd.GeoSeries([outlines_unary_union], index=[lake_ps.index[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5e0e1c-b26e-4ee3-87eb-ceea82bc5d73",
   "metadata": {},
   "source": [
    "# Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48d1542f-fb79-4965-a13f-3c84a3fa69fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import subglacial lake outlines \n",
    "# S09_outlines = gpd.read_file(os.path.join(os.getcwd(), 'output/lake_outlines/stationary_outlines/S09_outlines.geojson'))\n",
    "# SF18_outlines = gpd.read_file(os.path.join(os.getcwd(), 'output/lake_outlines/stationary_outlines/SF18_outlines.geojson'))\n",
    "stationary_lakes_gdf = gpd.read_file(os.path.join(os.getcwd(), 'output/lake_outlines/stationary_outlines/stationary_outlines_gdf.geojson'))\n",
    "# stationary_lakes_gdf_postSF18 = gpd.read_file(os.path.join(os.getcwd(), 'output/lake_outlines/stationary_outlines/stationary_outlines_notinSF18_gdf.geojson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1720b14b-ed5a-4870-b3f9-e9429a07e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CryoSat-2 SARIn mode mask\n",
    "# See 0_preprocess_data.ipynb for data source and pre-processing steps\n",
    "gdf_SARIn_3_1 = gpd.read_file('output/CS2_SARIn_mode_masks/gdf_SARIn_3_1.geojson')\n",
    "gdf_SARIn_3_1_3_6_diff= gpd.read_file('output/CS2_SARIn_mode_masks/gdf_SARIn_3_1_3_6_diff.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3c97b1e-bbd7-4f54-a26d-21d2aa3c9c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODIS MOA 2014 coastline and grounding line\n",
    "# https://nsidc.org/data/nsidc-0730/versions/1\n",
    "shp = DATA_DIR + '/boundaries/MODIS_MOA/2014/moa2014_coastline_v01.shp' \n",
    "moa_2014_coastline = gpd.read_file(shp)\n",
    "shp = DATA_DIR + '/boundaries/MODIS_MOA/2014/moa2014_grounding_line_v01.shp' \n",
    "moa_2014_groundingline = gpd.read_file(shp)\n",
    "# moa_2014_groundingline['geometry'] = moa_2014_groundingline.buffer(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e44c162d-b4d2-419f-81cd-945373956006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MODIS Mosaic of Antarctica (MOA) surface imagery\n",
    "# https://nsidc.org/data/nsidc-0730/versions/1\n",
    "# Relocate to data_dir\n",
    "# Open into an xarray.DataArray\n",
    "# moa_lowres = DATA_DIR + '/surface_imagery/MODIS_MOA/2014/moa750_2014_hp1_v01.tif' \n",
    "# moa_lowres_da = rioxarray.open_rasterio(moa_lowres)\n",
    "\n",
    "moa_highres = DATA_DIR + '/surface_imagery/MODIS_MOA/2014/moa125_2014_hp1_v01_fixtest.tif' #FIXME: delete old and rename new if working properly\n",
    "moa_highres_da = rioxarray.open_rasterio(moa_highres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89d4e7e3-0125-4c89-bf44-26fbb7f7c268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import Cryo-TEMPO-EOLIS and store as xarray dataset\n",
    "\n",
    "# # Specify the directory containing the NetCDF files\n",
    "# directory = DATA_DIR + '/altimetry/CryoSat2/CS2_SARIn_CryoTEMPO/Antarctica/*.nc'\n",
    "\n",
    "# # Get a list of all NetCDF file paths in the directory\n",
    "# file_paths = glob.glob(directory)\n",
    "\n",
    "# # Create an empty list to store the individual datasets\n",
    "# datasets = []\n",
    "\n",
    "# # Iterate over each file path\n",
    "# for file_path in file_paths:\n",
    "#     # Open the NetCDF file as an xarray dataset\n",
    "#     dataset = xr.open_dataset(file_path)\n",
    "#     # Append the dataset to the list\n",
    "#     datasets.append(dataset)\n",
    "\n",
    "# # Sort the datasets by their time variable in ascending order\n",
    "# datasets.sort(key=lambda ds: ds['time'].min().values)\n",
    "\n",
    "# # Combine the individual datasets into a single dataset\n",
    "# CS2_SARIn_CryoTEMPO = xr.concat(datasets, dim=\"time\")\n",
    "\n",
    "# # Transpose elevation variable to match other altimetry datasets and row, col order for imshow plotting\n",
    "# CS2_SARIn_CryoTEMPO['elevation'] = CS2_SARIn_CryoTEMPO['elevation'].transpose('time', 'y', 'x')\n",
    "\n",
    "# # View the new structure to confirm\n",
    "# CS2_SARIn_CryoTEMPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7fd6faa-738f-4689-abe0-38324f66d6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "html[data-theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 20GB\n",
       "Dimensions:      (y: 4451, x: 5451, time: 34)\n",
       "Coordinates:\n",
       "  * y            (y) float64 36kB -2.185e+06 -2.184e+06 ... 2.264e+06 2.265e+06\n",
       "  * x            (x) float64 44kB -2.665e+06 -2.664e+06 ... 2.784e+06 2.785e+06\n",
       "    spatial_ref  int64 8B 0\n",
       "  * time         (time) datetime64[ns] 272B 2010-07-02T15:00:00 ... 2018-10-0...\n",
       "Data variables:\n",
       "    mask         (y, x) float64 194MB ...\n",
       "    data_count   (time, y, x) float64 7GB ...\n",
       "    h            (time, y, x) float64 7GB ...\n",
       "    delta_h      (time, y, x) float64 7GB ...\n",
       "Attributes:\n",
       "    fileName:                mos_2010.5_2021.5.h5\n",
       "    shortName:               CS2-Smith-2017\n",
       "    identifier_product_DOI:  doi:10.5194/tc-11-451-2017</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-010e8a72-ed91-4107-9b05-92d8b34c9813' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-010e8a72-ed91-4107-9b05-92d8b34c9813' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>y</span>: 4451</li><li><span class='xr-has-index'>x</span>: 5451</li><li><span class='xr-has-index'>time</span>: 34</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-ae884b19-93a8-4126-ac6d-ce0af3618457' class='xr-section-summary-in' type='checkbox'  checked><label for='section-ae884b19-93a8-4126-ac6d-ce0af3618457' class='xr-section-summary' >Coordinates: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>y</span></div><div class='xr-var-dims'>(y)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-2.185e+06 -2.184e+06 ... 2.265e+06</div><input id='attrs-60d2caf2-6dd0-416e-9621-43fcd556e207' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-60d2caf2-6dd0-416e-9621-43fcd556e207' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-7275eb74-298d-4208-8f74-290386d365de' class='xr-var-data-in' type='checkbox'><label for='data-7275eb74-298d-4208-8f74-290386d365de' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([-2185000., -2184000., -2183000., ...,  2263000.,  2264000.,  2265000.])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>x</span></div><div class='xr-var-dims'>(x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-2.665e+06 -2.664e+06 ... 2.785e+06</div><input id='attrs-dd81a4e1-15f9-4994-8405-881d07513998' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-dd81a4e1-15f9-4994-8405-881d07513998' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-3f13ffbc-e9a1-4235-ad23-48830c41d743' class='xr-var-data-in' type='checkbox'><label for='data-3f13ffbc-e9a1-4235-ad23-48830c41d743' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([-2665000., -2664000., -2663000., ...,  2783000.,  2784000.,  2785000.])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>spatial_ref</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0</div><input id='attrs-b8f3b3be-8847-473a-b023-aa9404e12911' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-b8f3b3be-8847-473a-b023-aa9404e12911' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-f128c756-f340-44af-b1db-2b462a5c7157' class='xr-var-data-in' type='checkbox'><label for='data-f128c756-f340-44af-b1db-2b462a5c7157' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>crs_wkt :</span></dt><dd>PROJCS[&quot;WGS 84 / Antarctic Polar Stereographic&quot;,GEOGCS[&quot;WGS 84&quot;,DATUM[&quot;WGS_1984&quot;,SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563,AUTHORITY[&quot;EPSG&quot;,&quot;7030&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;6326&quot;]],PRIMEM[&quot;Greenwich&quot;,0,AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]],UNIT[&quot;degree&quot;,0.0174532925199433,AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]],PROJECTION[&quot;Polar_Stereographic&quot;],PARAMETER[&quot;latitude_of_origin&quot;,-71],PARAMETER[&quot;central_meridian&quot;,0],PARAMETER[&quot;false_easting&quot;,0],PARAMETER[&quot;false_northing&quot;,0],UNIT[&quot;metre&quot;,1,AUTHORITY[&quot;EPSG&quot;,&quot;9001&quot;]],AXIS[&quot;Easting&quot;,NORTH],AXIS[&quot;Northing&quot;,NORTH],AUTHORITY[&quot;EPSG&quot;,&quot;3031&quot;]]</dd><dt><span>semi_major_axis :</span></dt><dd>6378137.0</dd><dt><span>semi_minor_axis :</span></dt><dd>6356752.314245179</dd><dt><span>inverse_flattening :</span></dt><dd>298.257223563</dd><dt><span>reference_ellipsoid_name :</span></dt><dd>WGS 84</dd><dt><span>longitude_of_prime_meridian :</span></dt><dd>0.0</dd><dt><span>prime_meridian_name :</span></dt><dd>Greenwich</dd><dt><span>geographic_crs_name :</span></dt><dd>WGS 84</dd><dt><span>horizontal_datum_name :</span></dt><dd>World Geodetic System 1984</dd><dt><span>projected_crs_name :</span></dt><dd>WGS 84 / Antarctic Polar Stereographic</dd><dt><span>grid_mapping_name :</span></dt><dd>polar_stereographic</dd><dt><span>standard_parallel :</span></dt><dd>-71.0</dd><dt><span>straight_vertical_longitude_from_pole :</span></dt><dd>0.0</dd><dt><span>false_easting :</span></dt><dd>0.0</dd><dt><span>false_northing :</span></dt><dd>0.0</dd><dt><span>spatial_ref :</span></dt><dd>PROJCS[&quot;WGS 84 / Antarctic Polar Stereographic&quot;,GEOGCS[&quot;WGS 84&quot;,DATUM[&quot;WGS_1984&quot;,SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563,AUTHORITY[&quot;EPSG&quot;,&quot;7030&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;6326&quot;]],PRIMEM[&quot;Greenwich&quot;,0,AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]],UNIT[&quot;degree&quot;,0.0174532925199433,AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]],PROJECTION[&quot;Polar_Stereographic&quot;],PARAMETER[&quot;latitude_of_origin&quot;,-71],PARAMETER[&quot;central_meridian&quot;,0],PARAMETER[&quot;false_easting&quot;,0],PARAMETER[&quot;false_northing&quot;,0],UNIT[&quot;metre&quot;,1,AUTHORITY[&quot;EPSG&quot;,&quot;9001&quot;]],AXIS[&quot;Easting&quot;,NORTH],AXIS[&quot;Northing&quot;,NORTH],AUTHORITY[&quot;EPSG&quot;,&quot;3031&quot;]]</dd></dl></div><div class='xr-var-data'><pre>array(0)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2010-07-02T15:00:00 ... 2018-10-...</div><input id='attrs-c2f10cce-d22c-457f-94f8-1721cf3b0102' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-c2f10cce-d22c-457f-94f8-1721cf3b0102' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-2d0b244f-4fc9-4feb-bafc-41cd2e8f06ee' class='xr-var-data-in' type='checkbox'><label for='data-2d0b244f-4fc9-4feb-bafc-41cd2e8f06ee' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>description :</span></dt><dd>Time for each node</dd></dl></div><div class='xr-var-data'><pre>array([&#x27;2010-07-02T15:00:00.000000000&#x27;, &#x27;2010-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2011-01-01T00:00:00.000000000&#x27;, &#x27;2011-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2011-07-02T15:00:00.000000000&#x27;, &#x27;2011-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2012-01-01T00:00:00.000000000&#x27;, &#x27;2012-04-01T07:30:00.000000000&#x27;,\n",
       "       &#x27;2012-07-01T15:00:00.000000000&#x27;, &#x27;2012-09-30T22:30:00.000000000&#x27;,\n",
       "       &#x27;2013-01-01T00:00:00.000000000&#x27;, &#x27;2013-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2013-07-02T15:00:00.000000000&#x27;, &#x27;2013-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2014-01-01T00:00:00.000000000&#x27;, &#x27;2014-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2014-07-02T15:00:00.000000000&#x27;, &#x27;2014-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2015-01-01T00:00:00.000000000&#x27;, &#x27;2015-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2015-07-02T15:00:00.000000000&#x27;, &#x27;2015-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2016-01-01T00:00:00.000000000&#x27;, &#x27;2016-04-01T07:30:00.000000000&#x27;,\n",
       "       &#x27;2016-07-01T15:00:00.000000000&#x27;, &#x27;2016-09-30T22:30:00.000000000&#x27;,\n",
       "       &#x27;2017-01-01T00:00:00.000000000&#x27;, &#x27;2017-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2017-07-02T15:00:00.000000000&#x27;, &#x27;2017-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2018-01-01T00:00:00.000000000&#x27;, &#x27;2018-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2018-07-02T15:00:00.000000000&#x27;, &#x27;2018-10-01T22:30:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-a60eb66a-223e-4d55-a917-994c4b401f88' class='xr-section-summary-in' type='checkbox'  checked><label for='section-a60eb66a-223e-4d55-a917-994c4b401f88' class='xr-section-summary' >Data variables: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>mask</span></div><div class='xr-var-dims'>(y, x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-a8d724cf-7e91-4574-b470-733e1c0978d2' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-a8d724cf-7e91-4574-b470-733e1c0978d2' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-04df2462-ddd4-4d2b-b824-15386c20dc9b' class='xr-var-data-in' type='checkbox'><label for='data-04df2462-ddd4-4d2b-b824-15386c20dc9b' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>description: :</span></dt><dd>Data mask. 0: unknown, 1: unknown, nan: nan</dd></dl></div><div class='xr-var-data'><pre>[24262401 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>data_count</span></div><div class='xr-var-dims'>(time, y, x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-85bd9af3-5e8d-4271-b746-b5a8fa55a7da' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-85bd9af3-5e8d-4271-b746-b5a8fa55a7da' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-db6f3de2-7aba-4027-a384-1115b32bc9fb' class='xr-var-data-in' type='checkbox'><label for='data-db6f3de2-7aba-4027-a384-1115b32bc9fb' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>description: :</span></dt><dd>Data mask. 0: bare ground or ocean?, 1: ice?, nan: nan</dd></dl></div><div class='xr-var-data'><pre>[824921634 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>h</span></div><div class='xr-var-dims'>(time, y, x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-5e8c0864-6ea2-4335-b144-a2e4aa8334ef' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-5e8c0864-6ea2-4335-b144-a2e4aa8334ef' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-80dc5172-dcf7-4d11-82e4-507521377105' class='xr-var-data-in' type='checkbox'><label for='data-80dc5172-dcf7-4d11-82e4-507521377105' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[824921634 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>delta_h</span></div><div class='xr-var-dims'>(time, y, x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-9f257c1a-7b8b-4e73-8f66-86ff5d085d0a' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-9f257c1a-7b8b-4e73-8f66-86ff5d085d0a' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-7876ccfa-97df-4108-8eba-62c147bd575b' class='xr-var-data-in' type='checkbox'><label for='data-7876ccfa-97df-4108-8eba-62c147bd575b' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>description :</span></dt><dd>Height change relative to the ATL14 datum (Jan 1, 2020) surface</dd></dl></div><div class='xr-var-data'><pre>[824921634 values with dtype=float64]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-0b582c74-27e2-4631-be47-0d3f081d76dd' class='xr-section-summary-in' type='checkbox'  ><label for='section-0b582c74-27e2-4631-be47-0d3f081d76dd' class='xr-section-summary' >Indexes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>y</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-4e228ef9-b0d1-422b-85d3-e7c69ff462ff' class='xr-index-data-in' type='checkbox'/><label for='index-4e228ef9-b0d1-422b-85d3-e7c69ff462ff' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([-2185000.0, -2184000.0, -2183000.0, -2182000.0, -2181000.0, -2180000.0,\n",
       "       -2179000.0, -2178000.0, -2177000.0, -2176000.0,\n",
       "       ...\n",
       "        2256000.0,  2257000.0,  2258000.0,  2259000.0,  2260000.0,  2261000.0,\n",
       "        2262000.0,  2263000.0,  2264000.0,  2265000.0],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;y&#x27;, length=4451))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>x</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-c760f573-c56d-42a1-ac16-1d08f3795cfb' class='xr-index-data-in' type='checkbox'/><label for='index-c760f573-c56d-42a1-ac16-1d08f3795cfb' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([-2665000.0, -2664000.0, -2663000.0, -2662000.0, -2661000.0, -2660000.0,\n",
       "       -2659000.0, -2658000.0, -2657000.0, -2656000.0,\n",
       "       ...\n",
       "        2776000.0,  2777000.0,  2778000.0,  2779000.0,  2780000.0,  2781000.0,\n",
       "        2782000.0,  2783000.0,  2784000.0,  2785000.0],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;x&#x27;, length=5451))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>time</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-85ca12ee-2511-46ea-ae88-8964f6668b40' class='xr-index-data-in' type='checkbox'/><label for='index-85ca12ee-2511-46ea-ae88-8964f6668b40' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(DatetimeIndex([&#x27;2010-07-02 15:00:00&#x27;, &#x27;2010-10-01 22:30:00&#x27;,\n",
       "               &#x27;2011-01-01 00:00:00&#x27;, &#x27;2011-04-02 07:30:00&#x27;,\n",
       "               &#x27;2011-07-02 15:00:00&#x27;, &#x27;2011-10-01 22:30:00&#x27;,\n",
       "               &#x27;2012-01-01 00:00:00&#x27;, &#x27;2012-04-01 07:30:00&#x27;,\n",
       "               &#x27;2012-07-01 15:00:00&#x27;, &#x27;2012-09-30 22:30:00&#x27;,\n",
       "               &#x27;2013-01-01 00:00:00&#x27;, &#x27;2013-04-02 07:30:00&#x27;,\n",
       "               &#x27;2013-07-02 15:00:00&#x27;, &#x27;2013-10-01 22:30:00&#x27;,\n",
       "               &#x27;2014-01-01 00:00:00&#x27;, &#x27;2014-04-02 07:30:00&#x27;,\n",
       "               &#x27;2014-07-02 15:00:00&#x27;, &#x27;2014-10-01 22:30:00&#x27;,\n",
       "               &#x27;2015-01-01 00:00:00&#x27;, &#x27;2015-04-02 07:30:00&#x27;,\n",
       "               &#x27;2015-07-02 15:00:00&#x27;, &#x27;2015-10-01 22:30:00&#x27;,\n",
       "               &#x27;2016-01-01 00:00:00&#x27;, &#x27;2016-04-01 07:30:00&#x27;,\n",
       "               &#x27;2016-07-01 15:00:00&#x27;, &#x27;2016-09-30 22:30:00&#x27;,\n",
       "               &#x27;2017-01-01 00:00:00&#x27;, &#x27;2017-04-02 07:30:00&#x27;,\n",
       "               &#x27;2017-07-02 15:00:00&#x27;, &#x27;2017-10-01 22:30:00&#x27;,\n",
       "               &#x27;2018-01-01 00:00:00&#x27;, &#x27;2018-04-02 07:30:00&#x27;,\n",
       "               &#x27;2018-07-02 15:00:00&#x27;, &#x27;2018-10-01 22:30:00&#x27;],\n",
       "              dtype=&#x27;datetime64[ns]&#x27;, name=&#x27;time&#x27;, freq=None))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-ab9b562b-d081-43b6-ad8d-21cf60cdbf7f' class='xr-section-summary-in' type='checkbox'  checked><label for='section-ab9b562b-d081-43b6-ad8d-21cf60cdbf7f' class='xr-section-summary' >Attributes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>fileName :</span></dt><dd>mos_2010.5_2021.5.h5</dd><dt><span>shortName :</span></dt><dd>CS2-Smith-2017</dd><dt><span>identifier_product_DOI :</span></dt><dd>doi:10.5194/tc-11-451-2017</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset> Size: 20GB\n",
       "Dimensions:      (y: 4451, x: 5451, time: 34)\n",
       "Coordinates:\n",
       "  * y            (y) float64 36kB -2.185e+06 -2.184e+06 ... 2.264e+06 2.265e+06\n",
       "  * x            (x) float64 44kB -2.665e+06 -2.664e+06 ... 2.784e+06 2.785e+06\n",
       "    spatial_ref  int64 8B 0\n",
       "  * time         (time) datetime64[ns] 272B 2010-07-02T15:00:00 ... 2018-10-0...\n",
       "Data variables:\n",
       "    mask         (y, x) float64 194MB ...\n",
       "    data_count   (time, y, x) float64 7GB ...\n",
       "    h            (time, y, x) float64 7GB ...\n",
       "    delta_h      (time, y, x) float64 7GB ...\n",
       "Attributes:\n",
       "    fileName:                mos_2010.5_2021.5.h5\n",
       "    shortName:               CS2-Smith-2017\n",
       "    identifier_product_DOI:  doi:10.5194/tc-11-451-2017"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Smith and others, 2017, TC method CryoSat-2 SARIn height and dheight data (closed source aquired from Ben Smith)\n",
    "CS2_Smith2017 = xr.open_dataset(DATA_DIR + '/altimetry/CryoSat2/CS2_SARIn_Smith2017method/mos_2010.5_2019.0_relative_to_ATL14.nc')\n",
    "# Assign CRS\n",
    "CS2_Smith2017.rio.write_crs(\"EPSG:3031\", inplace=True)\n",
    "CS2_Smith2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3248a5a-3a1a-4855-a3a7-97e2050e1fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CS2_Smith2017_count = CS2_Smith2017.where(CS2_Smith2017['count'] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75f6efce-7341-400e-a660-2e42bbb24d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with ATL11 read in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79b17078-d831-4b2d-a16d-214e5c79b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Log into NASA Earthdata to search for datasets\n",
    "# earthaccess.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f33130e-0262-4ba9-a696-7f9bda24bf11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Find ICESat-2 ATL11 r003 data granules\n",
    "# results = earthaccess.search_data(\n",
    "#     doi='10.5067/ATLAS/ATL11.006',\n",
    "#     # short_name='ATL15',\n",
    "#     # version='003',\n",
    "#     bounding_box=(180, -90, -180, -60),  # (lower_left_lon, lower_left_lat , upper_right_lon, upper_right_lat))\n",
    "#     cloud_hosted=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1fc8217a-507e-4cbe-9e62-207236db2639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Open data granules as s3 files to stream\n",
    "# files = earthaccess.open(results)\n",
    "# # files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c38e74d8-78d9-44f9-8fe1-651cae14789a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0fae3032-84d5-4123-a67c-4daf2cf4643d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import datatree as dt\n",
    "\n",
    "# # Open the HDF5 file\n",
    "# with h5py.File(files[0], 'r') as h5file:\n",
    "#     # Load the entire HDF5 file into a DataTree\n",
    "#     data_tree = dt.DataTree.from_hdf5(h5file)\n",
    "\n",
    "# # Now `data_tree` is a DataTree object containing the structure and data of the HDF5 file\n",
    "# # You can navigate and manipulate this tree structure as needed\n",
    "\n",
    "# # For example, to print the contents of the DataTree\n",
    "# print(data_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "22bb7921-a63e-48c2-8215-20ceda095180",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # create empty lists\n",
    "# lon=[]\n",
    "# lat=[]\n",
    "# h_corr=[]\n",
    "# sigma_h=[]\n",
    "\n",
    "# # fill lists\n",
    "# for file in files:\n",
    "#     print(file)\n",
    "#     try:\n",
    "#         for pair in ['pt1','pt2','pt3']:\n",
    "#             lons, lats, hh, ss=read_ATL11(file, pair)\n",
    "#             lon += [lons]\n",
    "#             lat += [lats]\n",
    "#             h_corr += [hh]\n",
    "#             sigma_h += [ss]\n",
    "#     except Exception as E:\n",
    "#         pass\n",
    "\n",
    "# # concatenate lists\n",
    "# lon=np.concatenate(lon)\n",
    "# lat=np.concatenate(lat)\n",
    "# h_corr=np.concatenate(h_corr, axis=0)\n",
    "# sigma_h=np.concatenate(sigma_h, axis=0)\n",
    "# x,y=ll2ps(lon,lat) # transform geodetic lon, lat to polar stereographic x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d5dd4c7-cb4d-4e98-a6e5-2396278c2647",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcff003789aa4088b37b22a89da9b696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a5241675034af391d95bee0d000612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b819745c46dd437ca5fa71b0987cfa05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find ICESat-2 ATL15 v004 data granules\n",
    "results = earthaccess.search_data(\n",
    "    doi='10.5067/ATLAS/ATL15.004',\n",
    "    bounding_box=(180, -90, -180, -60),  # (lower_left_lon, lower_left_lat , upper_right_lon, upper_right_lat))\n",
    "    cloud_hosted=True,\n",
    ")\n",
    "\n",
    "# Open data granules as s3 files to stream\n",
    "files = earthaccess.open(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21ffb0a5-9651-474e-b47b-02340c93c87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A1_0322_01km_004_02.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A2_0322_01km_004_02.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A3_0322_01km_004_02.nc>,\n",
       " <File-like object HTTPFileSystem, https://n5eil01u.ecs.nsidc.org/DP8/ATLAS/ATL15.004/2019.01.01/ATL15_A4_0322_01km_004_02.nc>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter to 1-km resolution data sets\n",
    "filtered_files = [f for f in files if '01km' in str(f)]\n",
    "\n",
    "# Delete intermediary objects for memory conservation\n",
    "del results, files\n",
    "\n",
    "# Sort alphabetically by the data set file name\n",
    "filtered_files.sort(key=lambda x: str(x).split('/')[-1])\n",
    "\n",
    "# Display filtered list\n",
    "filtered_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "65a77402-26e0-404a-8c99-96343e1d7b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open each file, which are quadrants in polar stereographic coordinations \n",
    "# around the Geographic South Pole\n",
    "for i, file in enumerate(filtered_files):\n",
    "    # Create the variable name dynamically\n",
    "    var_name = f'ATL15_A{i+1}'  # Add one to convert from Python indexing\n",
    "    \n",
    "    # Use exec to create the variable with the dynamic name\n",
    "    # Opening the file with the specified group\n",
    "    exec(f\"{var_name} = xr.open_dataset(file, group='delta_h')\")\n",
    "\n",
    "del filtered_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8a8acc8a-b616-446b-bd14-41e6c26c46f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # After viewing files, index the files you wish to open\n",
    "# print(files[11])\n",
    "# print(files[0])\n",
    "# print(files[1])\n",
    "# print(files[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c0e8bdda-d8c0-415b-87b2-fae741baa41d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Open each file, which are quadrants in polar stereographic coordinations \n",
    "# # around the Geographic South Pole\n",
    "# ATL15_A1 = xr.open_dataset(files[11], group='delta_h')\n",
    "# ATL15_A2 = xr.open_dataset(files[0], group='delta_h')\n",
    "# ATL15_A3 = xr.open_dataset(files[1], group='delta_h')\n",
    "# ATL15_A4 = xr.open_dataset(files[13], group='delta_h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "da23c068-f494-45bc-814e-e76e22b4729b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Open locally stored files when NSIDC cloud access isn't working\n",
    "# ATL15_0 = xr.open_dataset(DATA_DIR + '/altimetry/ICESat2/ATL15.003-Ant/ATL15_A1_0318_01km_003_01.nc', group='delta_h')\n",
    "# ATL15_1 = xr.open_dataset(DATA_DIR + '/altimetry/ICESat2/ATL15.003-Ant/ATL15_A2_0318_01km_003_01.nc', group='delta_h')\n",
    "# ATL15_2 = xr.open_dataset(DATA_DIR + '/altimetry/ICESat2/ATL15.003-Ant/ATL15_A3_0318_01km_003_01.nc', group='delta_h')\n",
    "# ATL15_3 = xr.open_dataset(DATA_DIR + '/altimetry/ICESat2/ATL15.003-Ant/ATL15_A4_0318_01km_003_01.nc', group='delta_h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "69c4b4f9-6096-40de-92bb-12a1d9688890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the variables to keep\n",
    "variables_to_keep = ['time', 'y', 'x', 'delta_h', 'data_count']\n",
    "\n",
    "# List of xarray datasets\n",
    "datasets = [ATL15_A1, ATL15_A2, ATL15_A3, ATL15_A4]\n",
    "\n",
    "# Function to drop variables not in variables_to_keep from a dataset\n",
    "def drop_unwanted_variables(dataset):\n",
    "    variables_to_drop = [var for var in dataset.variables if var not in variables_to_keep]\n",
    "    return dataset.drop_vars(variables_to_drop)\n",
    "\n",
    "# Apply the function to each dataset\n",
    "ATL15_A1, ATL15_A2, ATL15_A3, ATL15_A4 = [drop_unwanted_variables(ds) for ds in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f875fb41-ec6f-4e03-a659-e7e7273b98df",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use xarray concatenation to stitch two quadrants togethers\n",
    "# Use xarray index selecting to occlude the duplicated x=0 vector of data\n",
    "ATL15_A12 = xr.concat([ATL15_A2.isel(x=slice(0,-1)), ATL15_A1], dim=\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "372eec1e-3eb5-42d8-94ee-a2de74316803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use xarray concatenation to stitch two quadrants togethers\n",
    "# Use xarray index selecting to occlude the duplicated x=0 vector of data\n",
    "ATL15_A34 = xr.concat([ATL15_A3.isel(x=slice(0,-1)), ATL15_A4], dim='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d14e491-fd5e-424b-ae91-75ba59381ec2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use xarray concatenation to stitch two quadrants togethers\n",
    "# Use xarray index selecting to occlude the duplicated x=0 vector of data\n",
    "ATL15_dh = xr.concat([ATL15_A34.isel(y=slice(0,-1)), ATL15_A12], dim='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d62934f-4e13-404d-b2b3-4338f498cfd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete variables to reduce memory consumption\n",
    "del ATL15_A1, ATL15_A12, ATL15_A2, ATL15_A3, ATL15_A34, ATL15_A4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e80b88e5-6957-4f63-b9d9-70591d18af2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "html[data-theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 4GB\n",
       "Dimensions:     (time: 22, y: 4461, x: 5461)\n",
       "Coordinates:\n",
       "  * time        (time) datetime64[ns] 176B 2019-01-01T06:00:00 ... 2024-04-01...\n",
       "  * x           (x) float64 44kB -2.67e+06 -2.669e+06 ... 2.789e+06 2.79e+06\n",
       "  * y           (y) float64 36kB -2.19e+06 -2.189e+06 ... 2.269e+06 2.27e+06\n",
       "Data variables:\n",
       "    delta_h     (time, y, x) float32 2GB nan nan nan nan nan ... nan nan nan nan\n",
       "    data_count  (time, y, x) float32 2GB nan nan nan nan nan ... nan nan nan nan\n",
       "Attributes:\n",
       "    description:             delta_h group includes variables describing heig...\n",
       "    identifier_product_DOI:  doi:10.5067/ATLAS/ATL15.004\n",
       "    shortName:               ATL15</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-958fa807-e983-4e01-b6af-6172108c1e8a' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-958fa807-e983-4e01-b6af-6172108c1e8a' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>time</span>: 22</li><li><span class='xr-has-index'>y</span>: 4461</li><li><span class='xr-has-index'>x</span>: 5461</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-a979eb1d-0f3f-4644-8e77-db346b87f52d' class='xr-section-summary-in' type='checkbox'  checked><label for='section-a979eb1d-0f3f-4644-8e77-db346b87f52d' class='xr-section-summary' >Coordinates: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2019-01-01T06:00:00 ... 2024-04-...</div><input id='attrs-205c9f59-1de1-4aff-80ff-53ae2ff502cf' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-205c9f59-1de1-4aff-80ff-53ae2ff502cf' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-cb339803-8056-403e-9359-7ca83b997681' class='xr-var-data-in' type='checkbox'><label for='data-cb339803-8056-403e-9359-7ca83b997681' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>dimensions :</span></dt><dd>time</dd><dt><span>datatype :</span></dt><dd>float64</dd><dt><span>least_significant_digit :</span></dt><dd>None</dd><dt><span>description :</span></dt><dd>Time for each node, in days since 2018-01-01:T00.00.00 UTC</dd><dt><span>long_name :</span></dt><dd>quarterly h(t) time</dd><dt><span>source :</span></dt><dd>ATBD section 4.2</dd></dl></div><div class='xr-var-data'><pre>array([&#x27;2019-01-01T06:00:00.000000000&#x27;, &#x27;2019-04-02T13:30:00.000000000&#x27;,\n",
       "       &#x27;2019-07-02T21:00:00.000000000&#x27;, &#x27;2019-10-02T04:30:00.000000000&#x27;,\n",
       "       &#x27;2020-01-01T12:00:00.000000000&#x27;, &#x27;2020-04-01T19:30:00.000000000&#x27;,\n",
       "       &#x27;2020-07-02T03:00:00.000000000&#x27;, &#x27;2020-10-01T10:30:00.000000000&#x27;,\n",
       "       &#x27;2020-12-31T18:00:00.000000000&#x27;, &#x27;2021-04-02T01:30:00.000000000&#x27;,\n",
       "       &#x27;2021-07-02T09:00:00.000000000&#x27;, &#x27;2021-10-01T16:30:00.000000000&#x27;,\n",
       "       &#x27;2022-01-01T00:00:00.000000000&#x27;, &#x27;2022-04-02T07:30:00.000000000&#x27;,\n",
       "       &#x27;2022-07-02T15:00:00.000000000&#x27;, &#x27;2022-10-01T22:30:00.000000000&#x27;,\n",
       "       &#x27;2023-01-01T06:00:00.000000000&#x27;, &#x27;2023-04-02T13:30:00.000000000&#x27;,\n",
       "       &#x27;2023-07-02T21:00:00.000000000&#x27;, &#x27;2023-10-02T04:30:00.000000000&#x27;,\n",
       "       &#x27;2024-01-01T12:00:00.000000000&#x27;, &#x27;2024-04-01T19:30:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>x</span></div><div class='xr-var-dims'>(x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-2.67e+06 -2.669e+06 ... 2.79e+06</div><input id='attrs-4ebd2efc-9515-4bac-acf3-4b419a87edcd' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-4ebd2efc-9515-4bac-acf3-4b419a87edcd' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-cb3f9913-a326-404b-821b-e45d968eb468' class='xr-var-data-in' type='checkbox'><label for='data-cb3f9913-a326-404b-821b-e45d968eb468' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>meters</dd><dt><span>dimensions :</span></dt><dd>x</dd><dt><span>datatype :</span></dt><dd>float64</dd><dt><span>least_significant_digit :</span></dt><dd>None</dd><dt><span>description :</span></dt><dd>x coordinate of the 1-km cell centers, in projected coordinates</dd><dt><span>long_name :</span></dt><dd>polar stereographic x at 1km</dd><dt><span>source :</span></dt><dd>ATBD section 3.2</dd><dt><span>grid_mapping :</span></dt><dd>Polar_Stereographic</dd><dt><span>standard_name :</span></dt><dd>projection_x_coordinate</dd></dl></div><div class='xr-var-data'><pre>array([-2670000., -2669000., -2668000., ...,  2788000.,  2789000.,  2790000.])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>y</span></div><div class='xr-var-dims'>(y)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-2.19e+06 -2.189e+06 ... 2.27e+06</div><input id='attrs-8bca80d7-5f99-4c7f-8467-fec0df875c82' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-8bca80d7-5f99-4c7f-8467-fec0df875c82' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-40a95199-c401-47fc-8a1e-fcb68a5ac5bb' class='xr-var-data-in' type='checkbox'><label for='data-40a95199-c401-47fc-8a1e-fcb68a5ac5bb' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>meters</dd><dt><span>dimensions :</span></dt><dd>y</dd><dt><span>datatype :</span></dt><dd>float64</dd><dt><span>least_significant_digit :</span></dt><dd>None</dd><dt><span>description :</span></dt><dd>y coordinate of the 1-km cell centers, in projected coordinates</dd><dt><span>long_name :</span></dt><dd>polar stereographic y at 1km</dd><dt><span>source :</span></dt><dd>ATBD section 3.2</dd><dt><span>grid_mapping :</span></dt><dd>Polar_Stereographic</dd><dt><span>standard_name :</span></dt><dd>projection_y_coordinate</dd></dl></div><div class='xr-var-data'><pre>array([-2190000., -2189000., -2188000., ...,  2268000.,  2269000.,  2270000.])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-6f0d8ef2-73e7-4ed1-a63a-1a4f8fc7e435' class='xr-section-summary-in' type='checkbox'  checked><label for='section-6f0d8ef2-73e7-4ed1-a63a-1a4f8fc7e435' class='xr-section-summary' >Data variables: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>delta_h</span></div><div class='xr-var-dims'>(time, y, x)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>nan nan nan nan ... nan nan nan nan</div><input id='attrs-bc6a2169-ef51-44f0-a34b-6b3def0a0926' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-bc6a2169-ef51-44f0-a34b-6b3def0a0926' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-eec18e6d-b0c6-447f-a068-3613666dadc7' class='xr-var-data-in' type='checkbox'><label for='data-eec18e6d-b0c6-447f-a068-3613666dadc7' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>least_significant_digit :</span></dt><dd>4</dd><dt><span>units :</span></dt><dd>meters</dd><dt><span>dimensions :</span></dt><dd>time,y,x</dd><dt><span>datatype :</span></dt><dd>float32</dd><dt><span>description :</span></dt><dd>Height change relative to the datum (Jan 1, 2020) surface</dd><dt><span>long_name :</span></dt><dd>height change  at 1 km</dd><dt><span>source :</span></dt><dd>ATBD section 3.4</dd><dt><span>grid_mapping :</span></dt><dd>Polar_Stereographic</dd></dl></div><div class='xr-var-data'><pre>array([[[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "...\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>data_count</span></div><div class='xr-var-dims'>(time, y, x)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>nan nan nan nan ... nan nan nan nan</div><input id='attrs-01ad6044-1fa8-4b80-b6f2-aa046f048c90' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-01ad6044-1fa8-4b80-b6f2-aa046f048c90' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-aba0953c-0eae-463f-a6f4-8692191eac75' class='xr-var-data-in' type='checkbox'><label for='data-aba0953c-0eae-463f-a6f4-8692191eac75' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>least_significant_digit :</span></dt><dd>4</dd><dt><span>units :</span></dt><dd>counts</dd><dt><span>dimensions :</span></dt><dd>time,y,x</dd><dt><span>datatype :</span></dt><dd>float32</dd><dt><span>description :</span></dt><dd>Weighted number of data contributing to each node in the 1-km height-change grid</dd><dt><span>long_name :</span></dt><dd>data count </dd><dt><span>source :</span></dt><dd>ATBD section 5.2.4.4</dd><dt><span>grid_mapping :</span></dt><dd>Polar_Stereographic</dd></dl></div><div class='xr-var-data'><pre>array([[[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "...\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-e3fbceea-593f-4fbd-a784-fe79e5450d0c' class='xr-section-summary-in' type='checkbox'  ><label for='section-e3fbceea-593f-4fbd-a784-fe79e5450d0c' class='xr-section-summary' >Indexes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>time</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-c887a580-7dab-49f7-bff4-127508925f12' class='xr-index-data-in' type='checkbox'/><label for='index-c887a580-7dab-49f7-bff4-127508925f12' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(DatetimeIndex([&#x27;2019-01-01 06:00:00&#x27;, &#x27;2019-04-02 13:30:00&#x27;,\n",
       "               &#x27;2019-07-02 21:00:00&#x27;, &#x27;2019-10-02 04:30:00&#x27;,\n",
       "               &#x27;2020-01-01 12:00:00&#x27;, &#x27;2020-04-01 19:30:00&#x27;,\n",
       "               &#x27;2020-07-02 03:00:00&#x27;, &#x27;2020-10-01 10:30:00&#x27;,\n",
       "               &#x27;2020-12-31 18:00:00&#x27;, &#x27;2021-04-02 01:30:00&#x27;,\n",
       "               &#x27;2021-07-02 09:00:00&#x27;, &#x27;2021-10-01 16:30:00&#x27;,\n",
       "               &#x27;2022-01-01 00:00:00&#x27;, &#x27;2022-04-02 07:30:00&#x27;,\n",
       "               &#x27;2022-07-02 15:00:00&#x27;, &#x27;2022-10-01 22:30:00&#x27;,\n",
       "               &#x27;2023-01-01 06:00:00&#x27;, &#x27;2023-04-02 13:30:00&#x27;,\n",
       "               &#x27;2023-07-02 21:00:00&#x27;, &#x27;2023-10-02 04:30:00&#x27;,\n",
       "               &#x27;2024-01-01 12:00:00&#x27;, &#x27;2024-04-01 19:30:00&#x27;],\n",
       "              dtype=&#x27;datetime64[ns]&#x27;, name=&#x27;time&#x27;, freq=None))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>x</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-34a976cf-7d14-4dcb-bcbd-2de12c0eb971' class='xr-index-data-in' type='checkbox'/><label for='index-34a976cf-7d14-4dcb-bcbd-2de12c0eb971' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([-2670000.0, -2669000.0, -2668000.0, -2667000.0, -2666000.0, -2665000.0,\n",
       "       -2664000.0, -2663000.0, -2662000.0, -2661000.0,\n",
       "       ...\n",
       "        2781000.0,  2782000.0,  2783000.0,  2784000.0,  2785000.0,  2786000.0,\n",
       "        2787000.0,  2788000.0,  2789000.0,  2790000.0],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;x&#x27;, length=5461))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>y</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-c7e5c2a6-4e30-4f17-a9cd-55b618991a97' class='xr-index-data-in' type='checkbox'/><label for='index-c7e5c2a6-4e30-4f17-a9cd-55b618991a97' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([-2190000.0, -2189000.0, -2188000.0, -2187000.0, -2186000.0, -2185000.0,\n",
       "       -2184000.0, -2183000.0, -2182000.0, -2181000.0,\n",
       "       ...\n",
       "        2261000.0,  2262000.0,  2263000.0,  2264000.0,  2265000.0,  2266000.0,\n",
       "        2267000.0,  2268000.0,  2269000.0,  2270000.0],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;y&#x27;, length=4461))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-d4fca3ee-b745-4191-9f14-c97bb8fa044c' class='xr-section-summary-in' type='checkbox'  checked><label for='section-d4fca3ee-b745-4191-9f14-c97bb8fa044c' class='xr-section-summary' >Attributes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>description :</span></dt><dd>delta_h group includes variables describing height differences between the model surface at any time and the DEM surface at a resolution of 1 km.</dd><dt><span>identifier_product_DOI :</span></dt><dd>doi:10.5067/ATLAS/ATL15.004</dd><dt><span>shortName :</span></dt><dd>ATL15</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset> Size: 4GB\n",
       "Dimensions:     (time: 22, y: 4461, x: 5461)\n",
       "Coordinates:\n",
       "  * time        (time) datetime64[ns] 176B 2019-01-01T06:00:00 ... 2024-04-01...\n",
       "  * x           (x) float64 44kB -2.67e+06 -2.669e+06 ... 2.789e+06 2.79e+06\n",
       "  * y           (y) float64 36kB -2.19e+06 -2.189e+06 ... 2.269e+06 2.27e+06\n",
       "Data variables:\n",
       "    delta_h     (time, y, x) float32 2GB nan nan nan nan nan ... nan nan nan nan\n",
       "    data_count  (time, y, x) float32 2GB nan nan nan nan nan ... nan nan nan nan\n",
       "Attributes:\n",
       "    description:             delta_h group includes variables describing heig...\n",
       "    identifier_product_DOI:  doi:10.5067/ATLAS/ATL15.004\n",
       "    shortName:               ATL15"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add datasets attributes\n",
    "ATL15_dh.attrs['identifier_product_DOI'] = 'doi:10.5067/ATLAS/ATL15.004'\n",
    "ATL15_dh.attrs['shortName'] = 'ATL15'\n",
    "\n",
    "# View data set\n",
    "ATL15_dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1b0bae96-66b2-4c2e-8c49-5623c3fde835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scripps Grounding Line\n",
    "# https://doi.pangaea.de/10.1594/PANGAEA.819147\n",
    "Scripps_gl = gpd.read_file(DATA_DIR + \n",
    "    '/boundaries/Depoorter2013/Antarctica_masks/scripps_antarctica_polygons_v1.shp')\n",
    "\n",
    "# Isolate only land ice\n",
    "Scripps_landice = Scripps_gl[Scripps_gl['Id_text'] == 'Grounded ice or land']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "df19fcb6-5ea0-4d70-946d-53e960a9bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip altimetry datasets to grounding line to limit analysis to below grounded ice\n",
    "CS2_Smith2017.rio.write_crs(3031, inplace=True)\n",
    "CS2_Smith2017 = CS2_Smith2017.rio.clip(Scripps_landice.geometry.values, Scripps_landice.crs, drop=False)\n",
    "ATL15_dh.rio.write_crs(3031, inplace=True)\n",
    "ATL15_dh = ATL15_dh.rio.clip(Scripps_landice.geometry.values, Scripps_landice.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "955c5399-d3c1-47b0-90df-ba9ecaa637a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cyc_start_dates</th>\n",
       "      <th>mid_cyc_dates</th>\n",
       "      <th>cyc_end_dates</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-07-02 15:00:00</td>\n",
       "      <td>2010-08-17 06:45:00</td>\n",
       "      <td>2010-10-01 22:30:00</td>\n",
       "      <td>CS2_Smith2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-10-01 22:30:00</td>\n",
       "      <td>2010-11-16 11:15:00</td>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>CS2_Smith2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>2011-02-15 15:45:00</td>\n",
       "      <td>2011-04-02 07:30:00</td>\n",
       "      <td>CS2_Smith2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-04-02 07:30:00</td>\n",
       "      <td>2011-05-17 23:15:00</td>\n",
       "      <td>2011-07-02 15:00:00</td>\n",
       "      <td>CS2_Smith2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-07-02 15:00:00</td>\n",
       "      <td>2011-08-17 06:45:00</td>\n",
       "      <td>2011-10-01 22:30:00</td>\n",
       "      <td>CS2_Smith2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cyc_start_dates       mid_cyc_dates       cyc_end_dates        dataset\n",
       "0 2010-07-02 15:00:00 2010-08-17 06:45:00 2010-10-01 22:30:00  CS2_Smith2017\n",
       "1 2010-10-01 22:30:00 2010-11-16 11:15:00 2011-01-01 00:00:00  CS2_Smith2017\n",
       "2 2011-01-01 00:00:00 2011-02-15 15:45:00 2011-04-02 07:30:00  CS2_Smith2017\n",
       "3 2011-04-02 07:30:00 2011-05-17 23:15:00 2011-07-02 15:00:00  CS2_Smith2017\n",
       "4 2011-07-02 15:00:00 2011-08-17 06:45:00 2011-10-01 22:30:00  CS2_Smith2017"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import mid_cyc_dates to ensure writing worked properly \n",
    "cyc_dates = pd.read_csv('output/cycle_dates.csv', parse_dates=['cyc_start_dates', 'mid_cyc_dates', 'cyc_end_dates'])\n",
    "\n",
    "# View dates\n",
    "cyc_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0ad65559-91e2-4419-86ca-66b37ac09980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the cyc_dates columns as a np array with datetime64[ns] data type\n",
    "cyc_start_dates = [np.datetime64(ts) for ts in cyc_dates['cyc_start_dates']]\n",
    "mid_cyc_dates = [np.datetime64(ts) for ts in cyc_dates['mid_cyc_dates']]\n",
    "cyc_end_dates = [np.datetime64(ts) for ts in cyc_dates['cyc_end_dates']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596962e3-e1e3-47c9-b19e-62d368182982",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analysis of previously identified lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b30ea060-bfba-4d69-80b2-fc98940c83fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find evolving outlines at various search extents and levels for each lake\n",
    "\n",
    "# Filter stationary_lakes_gdf based on destination folder contents\n",
    "folder_path = OUTPUT_DIR + '/search_extents_levels'\n",
    "# folder_path = OUTPUT_DIR_GIT + '/lake_outlines/evolving_outlines'  # Folder path to verify all outlines were exported\n",
    "# folder_path = OUTPUT_DIR + '/plot_evolving_outlines'  # Folder path to verify all outlines were plotted in aggregate for each lake\n",
    "\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "stationary_lakes_gdf_filtered = filter_gdf_by_folder_contents(stationary_lakes_gdf, folder_path)\n",
    "\n",
    "# # If you wanted to run a second server instance runing through the lakes backwards to speed up analysis\n",
    "# stationary_lakes_gdf_filtered = stationary_lakes_gdf_filtered.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "for idx in range(len(stationary_lakes_gdf_filtered)):\n",
    "\n",
    "    # Find search extents and levels\n",
    "    # find_search_extents_and_levels(stationary_lakes_gdf_filtered.iloc[idx:idx+1])\n",
    "\n",
    "    # Finalize evolving outlines\n",
    "    finalize_evolving_outlines(stationary_lakes_gdf_filtered.iloc[idx:idx+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "53a66e2a-5705-4c18-9b2d-9b1199642c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>area (m^2)</th>\n",
       "      <th>cite</th>\n",
       "      <th>CS2_SARIn_time_period</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty GeoDataFrame\n",
       "Columns: [name, area (m^2), cite, CS2_SARIn_time_period, geometry]\n",
       "Index: []"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find evolving outlines at various search extents and levels for each lake\n",
    "\n",
    "# Filter stationary_lakes_gdf based on destination folder contents\n",
    "folder_path = OUTPUT_DIR + '/search_extents_levels'\n",
    "# folder_path = OUTPUT_DIR_GIT + '/lake_outlines/evolving_outlines'  # Folder path to verify all outlines were exported\n",
    "# folder_path = OUTPUT_DIR + '/plot_evolving_outlines'  # Folder path to verify all outlines were plotted in aggregate for each lake\n",
    "\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "stationary_lakes_gdf_filtered = filter_gdf_by_folder_contents(stationary_lakes_gdf, folder_path)\n",
    "stationary_lakes_gdf_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0380a4bb-a043-479f-bb63-9c8ee6fbb451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>area (m^2)</th>\n",
       "      <th>cite</th>\n",
       "      <th>CS2_SARIn_time_period</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EAP_9</td>\n",
       "      <td>1.854869e+08</td>\n",
       "      <td>Smith and others, 2009, J. Glac., doi:10.3189/...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>POLYGON ((1078870.769 -1101380.036, 1079946.46...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Foundation_1</td>\n",
       "      <td>1.961027e+08</td>\n",
       "      <td>Smith and others, 2009, J. Glac., doi:10.3189/...</td>\n",
       "      <td>2013.75-2018.75</td>\n",
       "      <td>POLYGON ((-558303.98 312113.15, -556309.653 31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Foundation_4</td>\n",
       "      <td>7.930457e+07</td>\n",
       "      <td>Smith and others, 2009, J. Glac., doi:10.3189/...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>POLYGON ((-470982.992 316376.719, -471176.081 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Foundation_7</td>\n",
       "      <td>9.990824e+07</td>\n",
       "      <td>Smith and others, 2009, J. Glac., doi:10.3189/...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>POLYGON ((-391118.045 288666.358, -389303.391 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LowerConwaySubglacialLake</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>Siegfried and Fricker, 2021, Geophys. Res. Let...</td>\n",
       "      <td>2010.5-2018.75</td>\n",
       "      <td>POLYGON ((-306215.876 -509000, -306224.467 -50...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mulock_1</td>\n",
       "      <td>2.232099e+08</td>\n",
       "      <td>Smith and others, 2009, J. Glac., doi:10.3189/...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>POLYGON ((659290.633 -1109327.77, 659862.086 -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TL122</td>\n",
       "      <td>1.264163e+07</td>\n",
       "      <td>Hoffman and others, 2020, Cryosphere, doi:10.5...</td>\n",
       "      <td>2010.5-2018.75</td>\n",
       "      <td>POLYGON ((-1420016.468 -529499.367, -1419896.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>U1</td>\n",
       "      <td>2.000000e+08</td>\n",
       "      <td>Wingham and others, 2006, Nature, doi:10.1038/...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>POLYGON ((1093427.271 -1085448.426, 1093388.85...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>U2</td>\n",
       "      <td>2.000000e+08</td>\n",
       "      <td>Wingham and others, 2006, Nature, doi:10.1038/...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>POLYGON ((1047996.562 -1068353.903, 1047958.14...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name    area (m^2)  \\\n",
       "0                      EAP_9  1.854869e+08   \n",
       "1               Foundation_1  1.961027e+08   \n",
       "2               Foundation_4  7.930457e+07   \n",
       "3               Foundation_7  9.990824e+07   \n",
       "4  LowerConwaySubglacialLake  1.000000e+07   \n",
       "5                   Mulock_1  2.232099e+08   \n",
       "6                      TL122  1.264163e+07   \n",
       "7                         U1  2.000000e+08   \n",
       "8                         U2  2.000000e+08   \n",
       "\n",
       "                                                cite CS2_SARIn_time_period  \\\n",
       "0  Smith and others, 2009, J. Glac., doi:10.3189/...                  <NA>   \n",
       "1  Smith and others, 2009, J. Glac., doi:10.3189/...       2013.75-2018.75   \n",
       "2  Smith and others, 2009, J. Glac., doi:10.3189/...                  <NA>   \n",
       "3  Smith and others, 2009, J. Glac., doi:10.3189/...                  <NA>   \n",
       "4  Siegfried and Fricker, 2021, Geophys. Res. Let...        2010.5-2018.75   \n",
       "5  Smith and others, 2009, J. Glac., doi:10.3189/...                  <NA>   \n",
       "6  Hoffman and others, 2020, Cryosphere, doi:10.5...        2010.5-2018.75   \n",
       "7  Wingham and others, 2006, Nature, doi:10.1038/...                  <NA>   \n",
       "8  Wingham and others, 2006, Nature, doi:10.1038/...                  <NA>   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((1078870.769 -1101380.036, 1079946.46...  \n",
       "1  POLYGON ((-558303.98 312113.15, -556309.653 31...  \n",
       "2  POLYGON ((-470982.992 316376.719, -471176.081 ...  \n",
       "3  POLYGON ((-391118.045 288666.358, -389303.391 ...  \n",
       "4  POLYGON ((-306215.876 -509000, -306224.467 -50...  \n",
       "5  POLYGON ((659290.633 -1109327.77, 659862.086 -...  \n",
       "6  POLYGON ((-1420016.468 -529499.367, -1419896.4...  \n",
       "7  POLYGON ((1093427.271 -1085448.426, 1093388.85...  \n",
       "8  POLYGON ((1047996.562 -1068353.903, 1047958.14...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def extract_lake_name_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract lake name from filename by removing prefix and suffix.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): Filename like 'plot_evolving_outlines_Bindschadler_2_7.0x-search-extent_0.23m-level'\n",
    "    \n",
    "    Returns:\n",
    "        str: Lake name (e.g., 'Bindschadler_2')\n",
    "    \"\"\"\n",
    "    # Remove the prefix 'plot_evolving_outlines_'\n",
    "    without_prefix = filename.replace('plot_evolving_outlines_', '')\n",
    "    \n",
    "    # Use regex to find the pattern: digits followed by '.0x'\n",
    "    pattern = r'_\\d+x-search-extent'\n",
    "    \n",
    "    # Split at the pattern and take the first part\n",
    "    lake_name = re.split(pattern, without_prefix)[0]\n",
    "    \n",
    "    return lake_name\n",
    "\n",
    "def filter_gdf_by_folder_contents_with_name_processing(gdf, folder_path, exclude=True):\n",
    "    '''\n",
    "    Filter the GeoDataFrame based on processed lake names from the folder contents.\n",
    "    \n",
    "    Args:\n",
    "    gdf: GeoDataFrame to be filtered.\n",
    "    folder_path: Path to the directory containing files and/or subdirectories.\n",
    "    exclude: If True, excludes rows where the 'name' is in the folder_path directories or files.\n",
    "             If False, includes only rows where the 'name' is in the folder_path directories or files.\n",
    "    Returns:\n",
    "    GeoDataFrame filtered based on the presence of 'name' in folder_path.\n",
    "    '''\n",
    "    # Process filenames to get lake names\n",
    "    names_in_folder = {\n",
    "        extract_lake_name_from_filename(name).lower().strip() \n",
    "        for name in os.listdir(folder_path)\n",
    "    }\n",
    "    \n",
    "    # Filter without adding and then dropping a new column\n",
    "    gdf_filtered = gdf[gdf['name'].str.lower().str.strip().apply(\n",
    "        lambda x: (x not in names_in_folder) if exclude else (x in names_in_folder)\n",
    "    )]\n",
    "    \n",
    "    return gdf_filtered.reset_index(drop=True)\n",
    "\n",
    "# Example usage:\n",
    "# gdf_filtered = filter_gdf_by_folder_contents_with_name_processing(gdf, folder_path)\n",
    "\n",
    "# # Test the extraction function\n",
    "# test_filenames = [\n",
    "#     \"plot_evolving_outlines_Bindschadler_2_7.0x-search-extent_0.23m-level\",\n",
    "#     \"plot_evolving_outlines_Byrd_s11_5.0x-search-extent_0.12m-level\"\n",
    "# ]\n",
    "\n",
    "# # Print test results\n",
    "# for filename in test_filenames:\n",
    "#     lake_name = extract_lake_name_from_filename(filename)\n",
    "#     print(f\"Original: {filename}\")\n",
    "#     print(f\"Extracted: {lake_name}\\n\")\n",
    "\n",
    "folder_path = OUTPUT_DIR + '/plot_evolving_outlines'  # Folder path to verify all outlines were plotted in aggregate for each lake\n",
    "# names_in_folder = {\n",
    "#         extract_lake_name_from_filename(name).lower().strip() \n",
    "#         for name in os.listdir(folder_path)\n",
    "#     }\n",
    "# names_in_folder\n",
    "gdf_filtered = filter_gdf_by_folder_contents_with_name_processing(stationary_lakes_gdf, folder_path)\n",
    "gdf_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6f298a96-9268-4ad8-a645-f1f39dc53770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(len(stationary_lakes_gdf_filtered)):\n",
    "# # for idx in range(len(gdf_filtered)):\n",
    "\n",
    "#     # Find search extents and levels\n",
    "#     find_search_extents_and_levels(stationary_lakes_gdf_filtered.iloc[idx:idx+1])\n",
    "#     # find_search_extents_and_levels(gdf_filtered.iloc[idx:idx+1])\n",
    "\n",
    "#     # Finalize evolving outlines\n",
    "#     finalize_evolving_outlines(stationary_lakes_gdf_filtered.iloc[idx:idx+1])\n",
    "#     # finalize_evolving_outlines(gdf_filtered.iloc[idx:idx+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7dcf67f2-59b5-4f5b-a110-be5db1344eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(stationary_lakes_gdf_filtered)):\n",
    "\n",
    "    # First find search extents and levels for the group\n",
    "    if  find_search_extents_and_levels(stationary_lakes_gdf_filtered.iloc[idx:idx+1]):\n",
    "        \n",
    "        # Then finalize the evolving outlines using these parameters\n",
    "        finalize_evolving_outlines(stationary_lakes_gdf_filtered.iloc[idx:idx+1], row_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5cab0b76-5294-4d83-a54b-bd621274f2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make videos from the data_counts and dh plots made using the find_evolving_outlines function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d725d06-64e9-4c64-b3ee-24502c1cc4b4",
   "metadata": {},
   "source": [
    "# Quality check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e05fcb-1dcf-41ae-bc12-f65c3912f31a",
   "metadata": {},
   "source": [
    "In your OUTPUT_DIR, navigate to the `plot_evolving_outlines` folder. There you will see the evolving outlines plotted in aggregate for each lake. Some will have jagged boundaries where some of the outlines generated terminated at array edges vs. being a closed outline within the data array (gridded dh data). While this gives us an idea of where the lake's surface dh expression occurs, it gives a false of where it ends because the outline is prematurely clipped by the search extent boundary. We can confirm this is occurring by additionally looking at the data_counts, dh and evolving outline plot for each time slice in the `find_evolving_outlines` folder or the watching the movies of the time series for each lake.\n",
    "\n",
    "Doing this step, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a48919-2d95-4d3c-bb28-c89f6a472228",
   "metadata": {},
   "source": [
    "# Redo lakes with flawed analysis\n",
    "\n",
    "We start with lakes that have neighbor lake neighbors and appear to interact with that neighbor, so we analyze those lake groupings as lake systems where two or more lakes are analyzed together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a82e94-0e8e-449c-8339-dff8e6130e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing outlines for lake group: ['LowerMercerSubglacialLake', 'MercerSubglacialLake']\n",
      "Using parameters: area_multiple_search_extent                                                    5\n",
      "level                                                                       0.31\n",
      "within_percent                                                              95.0\n",
      "dataset_dois                   doi:10.5194/tc-11-451-2017, doi:10.5067/ATLAS/...\n",
      "Name: 0, dtype: object\n",
      "Group time period: 2010.5-2018.75\n"
     ]
    }
   ],
   "source": [
    "# Define your lake groups\n",
    "lake_groups = [\n",
    "    # ['Byrd_1', 'Byrd_2', 'Byrd_s2'],\n",
    "    # ['Byrd_s4', 'Byrd_s5', 'Byrd_s6', 'Byrd_s7', 'Byrd_s8'],\n",
    "    # ['Byrd_s5', 'Byrd_s6', 'Byrd_s7', 'Byrd_s8'],\n",
    "    # ['JG_D1_a', 'JG_D1_b', 'JG_D2_a'],\n",
    "    # ['Kamb_3', 'Kamb_4'],\n",
    "    # ['Mac1', 'Mac2'],\n",
    "    # ['Mac4', 'Mac5'],\n",
    "    ['LowerMercerSubglacialLake', 'MercerSubglacialLake'],\n",
    "    # ['Rec5', 'Rec6'],\n",
    "    # ['Site_B', 'Site_C'],\n",
    "    # ['Slessor_4', 'Slessor_5'],\n",
    "    # ['Thw_70', 'Thw_124', 'Thw_142', 'Thw_170'],\n",
    "    # ['Thw_124', 'Thw_142', 'Thw_170'],\n",
    "    # ['Thw_124', 'Thw_142'],\n",
    "    # ['TL96', 'TL108', 'TL115', 'TL122'],\n",
    "    # ['TL108', 'TL115', 'TL122']\n",
    "]\n",
    "\n",
    "# Process each group\n",
    "for lake_group in lake_groups:\n",
    "    \n",
    "    # First find search extents and levels for the group\n",
    "    # if find_search_extents_and_levels_group(stationary_lakes_gdf, lake_group):\n",
    "        \n",
    "        # Then finalize the evolving outlines using these parameters\n",
    "    finalize_evolving_outlines_group(stationary_lakes_gdf, lake_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2dd8c5-9c0b-4cbe-ad88-8581d3390bc2",
   "metadata": {},
   "source": [
    "Next we address the singular lakes that are not near a lake neighbor but have finalized evolving outlines that appear flawed because some of the outlines are prematurely cut by the search extent selected. We address this by selecting the next most optimal level/search_extent combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eb7944e3-6fe1-46a0-b332-78dfcccce345",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_redux = [\n",
    "    # 'Bindschadler_3',\n",
    "    # 'Byrd_s2',\n",
    "    # 'Byrd_s6',\n",
    "    # 'Byrd_s7',\n",
    "    # 'Cook_E2',\n",
    "    # 'David_1',\n",
    "    # 'David_s1',\n",
    "    # 'Foundation_6',\n",
    "    # 'Foundation_11',\n",
    "    # 'Foundation_15',\n",
    "    # 'Institute_E2',\n",
    "    # 'Institute_W2',\n",
    "    # 'JG_Combined_E2_F2',\n",
    "    # 'JG_D1_a',\n",
    "    # 'JG_D1_b',\n",
    "    # 'JG_F1',\n",
    "    # 'Kamb_3',\n",
    "    # 'Kamb_4',\n",
    "    # 'Kamb_9',\n",
    "    # 'KT1',\n",
    "    # 'Lake10',\n",
    "    # 'Lake12',\n",
    "    # 'Lake78',\n",
    "    # 'LennoxKing_1',\n",
    "    # 'LowerMercerSubglacialLake',\n",
    "    # 'M1',\n",
    "    # 'M2',\n",
    "    # 'Mac3',\n",
    "    # 'MercerSubglacialLake',\n",
    "    'Ninnis_1',\n",
    "    # 'R1',\n",
    "    # 'R3',\n",
    "    # 'Raymond_1',\n",
    "    # 'Rec2',\n",
    "    # 'Rutford_1',\n",
    "    # 'Slessor_1',\n",
    "    # 'TL108',\n",
    "    # 'TL115',\n",
    "    # 'Totten_2',\n",
    "    # 'UpperSubglacialLakeConway',\n",
    "    # 'V1',\n",
    "    # 'Whillans_6',\n",
    "    # 'Whillans_8',\n",
    "    # 'WT',\n",
    "    # 'Wilkes_2',\n",
    "]\n",
    "\n",
    "lake_grps_redux=[\n",
    "    ['Slessor_4', 'Slessor_5'],\n",
    "    ['Thw_70', 'Thw_124', 'Thw_142', 'Thw_170']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "86fdb5fd-df60-4789-b453-460d70cab135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to delete the data_counts/dh plots generated by the find_evolving_outlines function\n",
    "# We reviewed these and determined that outlines were prematurely cut by the search_extext, so we will\n",
    "# use the next optimal level/search_extent combination to see if it better characterizes the lake's activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "386faa19-453a-47f8-9d6c-316a54af98be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared directory contents of Thw_124_Thw_142_Thw_170\n"
     ]
    }
   ],
   "source": [
    "# For each lake name in the list\n",
    "for lake_name in lakes_redux:\n",
    "    # Construct the full directory path\n",
    "    lake_dir = os.path.join(OUTPUT_DIR, 'find_evolving_outlines', lake_name)\n",
    "    \n",
    "    # Check if directory exists before attempting to delete contents\n",
    "    if os.path.exists(lake_dir):\n",
    "        # Delete all contents in the directory\n",
    "        for item in os.listdir(lake_dir):\n",
    "            item_path = os.path.join(lake_dir, item)\n",
    "            try:\n",
    "                if os.path.isfile(item_path):\n",
    "                    os.unlink(item_path)\n",
    "                elif os.path.isdir(item_path):\n",
    "                    shutil.rmtree(item_path)\n",
    "            except Exception as e:\n",
    "                print(f'Failed to delete {item_path}. Reason: {e}')\n",
    "        print(f'Cleared directory contents of {lake_name}')\n",
    "    else:\n",
    "        print(f'Directory does not exist: {lake_dir}')\n",
    "\n",
    "    # Clear the output of each index\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8e5054e4-40b6-46c1-a154-793384f69a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing outlines for Ninnis_1\n",
      "Warning: row_index 1 out of bounds for Ninnis_1. Using first row.\n",
      "Using parameters from row 1:\n",
      "area_multiple_search_extent                                                    2\n",
      "level                                                                       0.54\n",
      "within_percent                                                             100.0\n",
      "dataset_dois                   doi:10.5194/tc-11-451-2017, doi:10.5067/ATLAS/...\n",
      "Name: 0, dtype: object\n",
      "Cleaning geometries for Ninnis_1...\n",
      "Saved outlines to: output/lake_outlines/evolving_outlines/Ninnis_1.geojson\n",
      "Starting plot for lake: Ninnis_1\n",
      "Parameters: search_extent=2, level=0.54\n",
      "Successfully saved plot to: /home/jovyan/1_evolving_lakes/output/FigS1_lake_reexamination_methods.ipynb/plot_evolving_outlines/plot_evolving_outlines_Ninnis_1_2x-search-extent_0.54m-level.png\n"
     ]
    }
   ],
   "source": [
    "stationary_lakes_gdf_filtered = stationary_lakes_gdf[stationary_lakes_gdf['name'].isin(lakes_redux)]\n",
    "\n",
    "for idx in range(len(stationary_lakes_gdf_filtered)):\n",
    "\n",
    "    # Then finalize the evolving outlines using these parameters\n",
    "    finalize_evolving_outlines(stationary_lakes_gdf_filtered.iloc[idx:idx+1], row_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1935efd5-31cc-4f1c-951b-ff289d60a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(lake_grps_redux)):\n",
    "    finalize_evolving_outlines_group(stationary_lakes_gdf, lake_grps_redux[idx], row_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b0c3de-0e93-4807-a435-ccb45d6601e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5ef840-6632-464b-9875-e661d5c7d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_redux = [\n",
    "    'Byrd_s2',\n",
    "    # 'Cook_E2',\n",
    "    'David_s1',\n",
    "    'Foundation_11',\n",
    "    'Foundation_15',\n",
    "    'Institute_E2',\n",
    "    'JG_Combined_E2_F2',\n",
    "    'JG_D1_a',\n",
    "    'JG_D1_b',\n",
    "    'JG_F1',\n",
    "    'Kamb_3',\n",
    "    'Kamb_9',\n",
    "    'KT1',\n",
    "    'Lake10',\n",
    "    'Lake78',\n",
    "    'LennoxKing_1',\n",
    "    'LowerMercerSubglacialLake',\n",
    "    'M1',\n",
    "    'Mac3',\n",
    "    'Ninnis_1',\n",
    "    'R1',\n",
    "    'R3',\n",
    "    'Raymond_1',\n",
    "    'Slessor_1',\n",
    "    'TL115',\n",
    "    'V1',\n",
    "    'Whillans_6',\n",
    "    'Whillans_8',\n",
    "    'Wilkes_2',\n",
    "    'WT',\n",
    "]\n",
    "\n",
    "lake_grps_redux=[\n",
    "    ['Slessor_4', 'Slessor_5'],\n",
    "    ['Thw_70', 'Thw_124', 'Thw_142', 'Thw_170']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a355ae6-51b0-476a-bb07-99542f31e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each lake name in the list\n",
    "for lake_name in lakes_redux:\n",
    "    # Construct the full directory path\n",
    "    lake_dir = os.path.join(OUTPUT_DIR, 'find_evolving_outlines', lake_name)\n",
    "    \n",
    "    # Check if directory exists before attempting to delete contents\n",
    "    if os.path.exists(lake_dir):\n",
    "        # Delete all contents in the directory\n",
    "        for item in os.listdir(lake_dir):\n",
    "            item_path = os.path.join(lake_dir, item)\n",
    "            try:\n",
    "                if os.path.isfile(item_path):\n",
    "                    os.unlink(item_path)\n",
    "                elif os.path.isdir(item_path):\n",
    "                    shutil.rmtree(item_path)\n",
    "            except Exception as e:\n",
    "                print(f'Failed to delete {item_path}. Reason: {e}')\n",
    "        print(f'Cleared directory contents of {lake_name}')\n",
    "    else:\n",
    "        print(f'Directory does not exist: {lake_dir}')\n",
    "\n",
    "    # Clear the output of each index\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f803c8-c512-4a03-a7db-f24945135268",
   "metadata": {},
   "outputs": [],
   "source": [
    "stationary_lakes_gdf_filtered = stationary_lakes_gdf[stationary_lakes_gdf['name'].isin(lakes_redux)]\n",
    "\n",
    "for idx in range(len(stationary_lakes_gdf_filtered)):\n",
    "\n",
    "    # Then finalize the evolving outlines using these parameters\n",
    "    finalize_evolving_outlines(stationary_lakes_gdf_filtered.iloc[idx:idx+1], row_index=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1912df48-84f8-489b-bf30-e190a856d389",
   "metadata": {},
   "source": [
    "# Final quality check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363d9a64-0782-4f6f-8169-7a02f4983150",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create new lakes geodataframe that has the unary union of the evolving outlines for each lake\n",
    "\n",
    "# List all folders in target directory\n",
    "folder_path = '/home/jovyan/1_evolving_Ant_subglacial_hydro/Sauthoff-2025-J.Glaciol./output/lake_outlines/evolving_outlines'\n",
    "\n",
    "# Filter stationary_lakes_gdf based on folder contents\n",
    "stationary_lakes_gdf_filtered = filter_gdf_by_folder_contents(stationary_lakes_gdf, folder_path, exclude=False)\n",
    "\n",
    "# Copy stationary_lakes_gdf_filtered to create evolving_lakes_gdf that will contain \n",
    "# the unary unions of the evolving outlines for each lake as their geometry\n",
    "union_evolving_lakes_gdf = stationary_lakes_gdf_filtered.copy(deep=True)\n",
    "\n",
    "for idx, row in stationary_lakes_gdf_filtered.iterrows():\n",
    "    # Select lake as pandas series\n",
    "    lake_ps = stationary_lakes_gdf_filtered.loc[idx]\n",
    "\n",
    "    # Perform unary union using function that returns as geoseries\n",
    "    evolving_outlines_union_gs = find_evolving_outlines_unary_union(lake_ps)\n",
    "\n",
    "    # Store polygon from geoseries in a geodataframe with CRS\n",
    "    evolving_outlines_union_gdf = gpd.GeoDataFrame(index=[0], crs='EPSG:3031', geometry=[evolving_outlines_union_gs[0]])\n",
    "\n",
    "    # Convert GeoDataFrame to EPSG:4326 CRS for geodesic area calculation\n",
    "    evolving_outlines_union_gdf = evolving_outlines_union_gdf.to_crs('4326')\n",
    "\n",
    "    # Replace stationary geometry with the union of evolving outlines at each lake\n",
    "    if evolving_outlines_union_gs is not None:\n",
    "        union_evolving_lakes_gdf.at[idx, 'geometry'] = evolving_outlines_union_gs[0]\n",
    "        union_evolving_lakes_gdf.at[idx, 'area (m^2)'] = calculate_area(evolving_outlines_union_gdf['geometry'].iloc[0])\n",
    "\n",
    "# Reproject GeoDataFrame to EPSG:3031\n",
    "union_evolving_lakes_gdf = union_evolving_lakes_gdf.to_crs('3031')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3660f3a-1655-4a59-91d9-7c161f0fbde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_evolving_lakes_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5b3fd-50f1-4dc8-973e-13b6e6c69ed6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create two new lakes geodataframe that have the unary union of \n",
    "# 1) the evolving outlines for each lake\n",
    "# 2) the evolving outlines and the stationary outline for each lake\n",
    "\n",
    "# List all folders in target directory\n",
    "folder_path = '/home/jovyan/1_evolving_Ant_subglacial_hydro/Sauthoff-2025-J.Glaciol./output/lake_outlines/evolving_outlines'\n",
    "\n",
    "# Filter stationary_lakes_gdf to only include lakes that have evolving outlines found at them\n",
    "stationary_lakes_gdf_filtered = filter_gdf_by_folder_contents(stationary_lakes_gdf, folder_path, exclude=False)\n",
    "\n",
    "# Copy stationary_lakes_gdf_filtered to create two new geodataframes\n",
    "evolving_stationary_outlines_union_gdf = stationary_lakes_gdf_filtered.copy(deep=True)\n",
    "evolving_outlines_union_gdf = stationary_lakes_gdf_filtered.copy(deep=True)\n",
    "\n",
    "for idx, row in stationary_lakes_gdf_filtered.iterrows():\n",
    "    # Select lake as pandas series\n",
    "    lake_ps = stationary_lakes_gdf_filtered.loc[idx]\n",
    "\n",
    "    # Perform unary union using function that returns as geoseries\n",
    "    evolving_stationary_outlines_union_gs = find_evolving_outlines_union(lake_ps, incl_stationary=True)\n",
    "    evolving_outlines_union_gs = find_evolving_outlines_union(lake_ps, incl_stationary=False)\n",
    "\n",
    "    # Store polygon from geoseries in a geodataframe with CRS\n",
    "    evolving_stationary_outlines_union_gdf_idx = gpd.GeoDataFrame(index=[0], crs='EPSG:3031', geometry=[evolving_stationary_outlines_union_gs[0]])\n",
    "    evolving_outlines_union_gdf_idx = gpd.GeoDataFrame(index=[0], crs='EPSG:3031', geometry=[evolving_outlines_union_gs[0]])\n",
    "\n",
    "    # Convert GeoDataFrame to EPSG:4326 CRS for geodesic area calculation\n",
    "    evolving_stationary_outlines_union_gdf_idx = evolving_stationary_outlines_union_gdf_idx.to_crs('4326')\n",
    "    evolving_outlines_union_gdf_idx = evolving_outlines_union_gdf_idx.to_crs('4326')\n",
    "\n",
    "    # Replace stationary geometry with the union of evolving outlines at each lake\n",
    "    if evolving_outlines_union_gs is not None:\n",
    "        evolving_stationary_outlines_union_gdf.at[idx, 'geometry'] = evolving_outlines_union_gs[0]\n",
    "        evolving_outlines_union_gdf.at[idx, 'geometry'] = evolving_stationary_outlines_union_gs[0]\n",
    "        evolving_stationary_outlines_union_gdf.at[idx, 'area (m^2)'] = calculate_geodesic_area(evolving_stationary_outlines_union_gdf_idx['geometry'].iloc[0])\n",
    "        evolving_outlines_union_gdf.at[idx, 'area (m^2)'] = calculate_geodesic_area(evolving_outlines_union_gdf_idx['geometry'].iloc[0])\n",
    "\n",
    "# Reproject GeoDataFrame to EPSG:3031\n",
    "evolving_stationary_outlines_union_gdf = evolving_stationary_outlines_union_gdf.to_crs('3031')\n",
    "evolving_outlines_union_gdf = evolving_outlines_union_gdf.to_crs('3031')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86835b9e-64dc-441e-b049-a46e928be65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "stationary_lakes_gdf.boundary.plot(ax=ax, color='red')\n",
    "union_evolving_lakes_gdf.boundary.plot(ax=ax, color='blue')\n",
    "union_evolving_stationary_lakes_gdf.boundary.plot(ax=ax, color='green')\n",
    "Scripps_landice.boundary.plot(ax=ax, lw=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b801477-c029-4543-96c9-b77cb5f0d9bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot evolving and stationary lakes comparison plots\n",
    "\n",
    "# List all files in your target directory\n",
    "folder_path = '/home/jovyan/1_evolving_Ant_subglacial_hydro/output/FigS1_lake_reexamination_methods.ipynb/plot_evolving_and_stationary_comparison'\n",
    "\n",
    "# List all files in your target directory without their extensions\n",
    "file_names_without_extension = [os.path.splitext(name)[0] for name in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, name))]\n",
    "\n",
    "# Filter the GeoDataFrame to only include rows where the 'name' is not in folders\n",
    "stationary_lakes_gdf_filtered = stationary_lakes_gdf[~stationary_lakes_gdf['name'].isin(file_names_without_extension)]\n",
    "\n",
    "# Now run loop on the filtered GeoDataFrame\n",
    "for idx, row in stationary_lakes_gdf_filtered.iterrows():\n",
    "    # plot_evolving_and_stationary_comparison(stationary_lakes_gdf_filtered.loc[idx:idx])\n",
    "    plot_evolving_and_stationary_comparison(stationary_lakes_gdf_filtered.loc[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db029eca-b1f0-4792-b946-713933e3d290",
   "metadata": {},
   "source": [
    "# Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11879f2-dfcf-44ef-afe1-c4e11f3cbbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foundation_5 has outlines during CS2 but geometric variables just during IS2\n",
    "# Institute_W1 has strange gap in geometric variable calc's\n",
    "# Nimrod_1 has CS2 era data despite partial coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b029c1eb-4a63-4e4b-9ac7-1813e114f944",
   "metadata": {},
   "source": [
    "# Spot check and export evolving outlines union lakes geodataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f100e4c8-4e31-41d3-837a-a49b6dbd3b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "stationary_lakes_gdf.boundary.plot(ax=ax, color='red')\n",
    "union_evolving_lakes_gdf.boundary.plot(ax=ax, color='blue')\n",
    "union_evolving_stationary_lakes_gdf.boundary.plot(ax=ax, color='green')\n",
    "Scripps_landice.boundary.plot(ax=ax, lw=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e4551-0588-40cb-8f8f-48bb9c4dde2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to gdf\n",
    "evolving_stationary_outlines_union_gdf.to_file('output/lake_outlines/evolving_stationary_outlines_union_gdf.geojson', driver='GeoJSON')\n",
    "evolving_outlines_union_gdf.to_file('output/lake_outlines/evolving_outlines_union_gdf.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa2fb01-bcc2-46f6-80cf-ae565921127f",
   "metadata": {},
   "source": [
    "# Run geometric calculation and plotting functions on finalized evolving outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95fc51e-03fb-43cb-ae2b-d2431f91d95a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List all files in your target directory\n",
    "folder_path = '/home/jovyan/1_evolving_Ant_subglacial_hydro/Sauthoff-2025-J.Glaciol./output/lake_outlines/evolving_outlines'\n",
    "\n",
    "# List all files in your target directory without their extensions\n",
    "file_names_without_extension = [os.path.splitext(name)[0] for name in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, name))]\n",
    "\n",
    "# Filter the GeoDataFrame to only include rows where the 'name' is not in folders\n",
    "# stationary_lakes_gdf_filtered = stationary_lakes_gdf[~stationary_lakes_gdf['name'].isin(file_names_without_extension)]\n",
    "\n",
    "# Filter stationary_lakes_gdf based on folder contents\n",
    "stationary_lakes_gdf_filtered = filter_gdf_by_folder_contents(stationary_lakes_gdf, folder_path, exclude=False)\n",
    "\n",
    "# Now run loop on the filtered GeoDataFrame\n",
    "for idx in range(len(stationary_lakes_gdf_filtered)):\n",
    "\n",
    "    # Isolate lake from stationary_lakes_gdf as geodataframe using slicing\n",
    "    lake_gdf = stationary_lakes_gdf_filtered.loc[idx:idx]\n",
    "    print('Working on', lake_gdf['name'].iloc[0])\n",
    "\n",
    "    # Assign dataset; during CS2 era, assign based on SARIn coverage time period\n",
    "    cs2_time_period = stationary_lakes_gdf['CS2_SARIn_time_period'][idx]\n",
    "    if cs2_time_period == None:\n",
    "        dataset1 = None\n",
    "    elif cs2_time_period == '2013.75-2018.75':\n",
    "        dataset1 = CS2_Smith2017.sel(time=slice(np.datetime64('2013-10-01T22:30:00.000000000'), None))\n",
    "    elif cs2_time_period == '2010.5-2018.75':\n",
    "        dataset1 = CS2_Smith2017\n",
    "    dataset2=ATL15_dh\n",
    "\n",
    "    # Calc darea, dh, and dvol for evolving and stationary outlines and compare using bias (evolving-stationary) for each lake\n",
    "    # compare_evolving_and_stationary_outlines(lake_ps=lake_ps, dataset1=dataset1, dataset2=dataset2)\n",
    "    compare_evolving_union_and_stationary_outlines(stationary_lake_gdf=lake_ps, dataset1=dataset1, dataset2=dataset2, incl_stationary=True)\n",
    "    compare_evolving_union_and_stationary_outlines(stationary_lake_gdf=lake_ps, dataset1=dataset1, dataset2=dataset2, incl_stationary=False)\n",
    "\n",
    "    # Clear the output of each index\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ec28f-ac81-48e7-ad44-b6c924ab8015",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stationary_lakes_gdf_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa4b58-625d-47ae-a82c-ba00ae9271b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_names_without_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211c9765-93f5-4bdd-b122-0880f557a8cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List all files in your target directory\n",
    "folder_path = '/home/jovyan/1_evolving_Ant_subglacial_hydro/Sauthoff-2025-J.Glaciol./output/lake_outlines/evolving_outlines'\n",
    "\n",
    "# List all files in your target directory without their extensions\n",
    "file_names_without_extension = [os.path.splitext(name)[0] for name in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, name))]\n",
    "\n",
    "# Filter the GeoDataFrame to only include rows where the 'name' is not in folders\n",
    "stationary_lakes_gdf_filtered = stationary_lakes_gdf[~stationary_lakes_gdf['name'].isin(file_names_without_extension)]\n",
    "\n",
    "# Now run loop on the filtered GeoDataFrame\n",
    "for idx in range(len(stationary_lakes_gdf)):\n",
    "\n",
    "    # Isolate lake from stationary_lakes_gdf as geodataframe using slicing\n",
    "    lake_gdf = stationary_lakes_gdf.loc[idx:idx]\n",
    "    print('Working on', lake_gdf['name'].iloc[0])\n",
    "\n",
    "    # Assign dataset; during CS2 era, assign based on SARIn coverage time period\n",
    "    cs2_time_period = stationary_lakes_gdf['CS2_SARIn_time_period'][idx]\n",
    "    if cs2_time_period == None:\n",
    "        dataset1 = None\n",
    "    elif cs2_time_period == '2013.75-2018.75':\n",
    "        dataset1 = CS2_Smith2017.sel(time=slice(np.datetime64('2013-10-01T22:30:00.000000000'), None))\n",
    "    elif cs2_time_period == '2010.5-2018.75':\n",
    "        dataset1 = CS2_Smith2017\n",
    "    dataset2=ATL15_dh\n",
    "\n",
    "    # Plot the outlines in aggregate\n",
    "    plot_evolving_outlines(lake_gdf=lake_gdf)\n",
    "\n",
    "    # Clear the output of each index\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acbd330-9999-466f-b8c9-64f97341c029",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List all files in your target directory\n",
    "folder_path = '/home/jovyan/1_evolving_Ant_subglacial_hydro/Sauthoff-2025-J.Glaciol./output/lake_outlines/evolving_outlines'\n",
    "\n",
    "# List all files in your target directory without their extensions\n",
    "file_names_without_extension = [os.path.splitext(name)[0] for name in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, name))]\n",
    "\n",
    "# Filter the GeoDataFrame to only include rows where the 'name' is not in folders\n",
    "stationary_lakes_gdf_filtered = stationary_lakes_gdf[~stationary_lakes_gdf['name'].isin(file_names_without_extension)]\n",
    "\n",
    "# Now run loop on the filtered GeoDataFrame\n",
    "for idx in range(len(stationary_lakes_gdf)):\n",
    "\n",
    "    # Isolate lake from stationary_lakes_gdf as geodataframe using slicing\n",
    "    lake_gdf = stationary_lakes_gdf.loc[idx:idx]\n",
    "    print('Working on', lake_gdf['name'].iloc[0])\n",
    "\n",
    "    # Assign dataset; during CS2 era, assign based on SARIn coverage time period\n",
    "    cs2_time_period = stationary_lakes_gdf['CS2_SARIn_time_period'][idx]\n",
    "    if cs2_time_period == None:\n",
    "        dataset1 = None\n",
    "    elif cs2_time_period == '2013.75-2018.75':\n",
    "        dataset1 = CS2_Smith2017.sel(time=slice(np.datetime64('2013-10-01T22:30:00.000000000'), None))\n",
    "    elif cs2_time_period == '2010.5-2018.75':\n",
    "        dataset1 = CS2_Smith2017\n",
    "    dataset2=ATL15_dh\n",
    "\n",
    "    # Plot stationary and evolving outlines darea, dh, dvol comparison\n",
    "    plot_evolving_and_stationary_comparison(lake_gdf=lake_gdf)\n",
    "\n",
    "    # Clear the output of each index\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f63832-b1e7-4458-a3db-5942206f7bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all lakes complete\n",
    "print(len(os.listdir('/home/jovyan/1_outlines_candidates/output/FigS1_lake_reexamination_methods.ipynb/find_evolving_outlines')))\n",
    "print(len(os.listdir('/home/jovyan/1_outlines_candidates/Sauthoff-2025-J.Glaciol./output/lake_outlines/evolving_outlines')))\n",
    "print(len(os.listdir('/home/jovyan/1_outlines_candidates/output/FigS1_lake_reexamination_methods.ipynb/plot_evolving_outlines/')))\n",
    "print(len(os.listdir('/home/jovyan/1_outlines_candidates/Sauthoff-2025-J.Glaciol./output/lake_outlines/compare_evolving_and_stationary_outlines')))\n",
    "print(len(os.listdir('/home/jovyan/1_outlines_candidates/output/FigS1_lake_reexamination_methods.ipynb/plot_evolving_and_stationary_comparison/')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb81064d-3d23-4e9a-a5f0-fe770d92ead0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6af699-cd85-4583-82ea-8b5e3396c018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e827cad2-3dd4-41bd-ad48-6db295419f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that combines all the evolving and stationary lake geometric calculations\n",
    "# and bias comparisons into one dataframe for continentally integrated numbers\n",
    "\n",
    "# Define your directory\n",
    "directory = 'output/lake_outlines/compare_evolving_and_stationary_outlines'\n",
    "dataframes_all_lakes = []\n",
    "dataframes_CS2_IS2 = []\n",
    "dataframes_IS2 = []\n",
    "\n",
    "# List and read each CSV file\n",
    "for file in os.listdir(directory):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(directory, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check conditions to append to specific lists\n",
    "        lake_name = os.path.splitext(file)[0]  # Remove '.csv' from filename to create the lake name\n",
    "        lake_row = stationary_lakes_gdf[stationary_lakes_gdf['name'] == lake_name]\n",
    "        if not lake_row.empty:\n",
    "            dataframes_all_lakes.append(df)\n",
    "            \n",
    "            time_period = lake_row['CS2_SARIn_time_period'].values[0]\n",
    "            if time_period in ['2010.5-2018.75', '2013.75-2018.75']:\n",
    "                dataframes_CS2_IS2.append(df)\n",
    "            elif pd.isna(time_period):\n",
    "                dataframes_IS2.append(df)\n",
    "            else:\n",
    "                print('Cryosat-2 time period not one of expected values')\n",
    "\n",
    "# Concatenate DataFrames in the list\n",
    "combined_all_lakes_df = pd.concat(dataframes_all_lakes, ignore_index=True)\n",
    "combined_CS2_IS2_df = pd.concat(dataframes_CS2_IS2, ignore_index=True)\n",
    "combined_IS2_df = pd.concat(dataframes_IS2, ignore_index=True)\n",
    "\n",
    "# Group by 'midcyc_datetime' to make spatially integrated along time axis\n",
    "combined_all_lakes_df = combined_all_lakes_df.groupby('midcyc_datetime').sum().reset_index()\n",
    "combined_CS2_IS2_df = combined_CS2_IS2_df.groupby('midcyc_datetime').sum().reset_index()\n",
    "combined_IS2_df = combined_IS2_df.groupby('midcyc_datetime').sum().reset_index()\n",
    "\n",
    "# For all lakes df, remove CryoSat-2 era since only some of the lakes have that coverage\n",
    "# Ensure the midcyc_datetime column is in datetime format\n",
    "combined_all_lakes_df['midcyc_datetime'] = pd.to_datetime(combined_all_lakes_df['midcyc_datetime'])\n",
    "\n",
    "# Define the datetime when ICESat time series begins\n",
    "threshold = pd.Timestamp('2018-11-16 14:15:00')\n",
    "\n",
    "# Filter the DataFrame\n",
    "combined_all_lakes_df_filtered = combined_all_lakes_df[combined_all_lakes_df['midcyc_datetime'] >= threshold]\n",
    "\n",
    "# Reset index\n",
    "combined_all_lakes_df_filtered = combined_all_lakes_df_filtered.reset_index(drop=True)\n",
    "\n",
    "# Save the combined DataFrame to csv file\n",
    "combined_all_lakes_df_filtered.to_csv('output/lake_outlines/compare_evolving_and_stationary_outlines/all_lakes_during_IS2_spatially_integrated.csv', index=False) \n",
    "combined_CS2_IS2_df.to_csv('output/lake_outlines/compare_evolving_and_stationary_outlines/all_CS2_IS2_lakes_spatially_integrated.csv', index=False) \n",
    "combined_IS2_df.to_csv('output/lake_outlines/compare_evolving_and_stationary_outlines/all_IS2_lakes_spatially_integrated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c059b478-b0d9-4cc2-b7a3-a195305ef531",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fig. S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462e3a8-67e5-429b-988f-3608ae40bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remaking fig\n",
    "# -trellis plot\n",
    "# -panel of cryotempoeolis gridded data (show that it is point data at many lakes);\n",
    "# perhaps pick a lake at SARIn boundary to show that you didn't include those in the CS2 era analysis\n",
    "# -data_count panel (use example of lake at boundary of CS2 SARIn coverage)\n",
    "# -panel showing all 15 area multiple search extents\n",
    "# -panels of pos and neg dh with contours created (and level listed)\n",
    "# -aggregate plot of evolving outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f7aab3-8bd8-43b6-9713-7eb01755fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import mid_cyc_dates\n",
    "mid_cyc_dates = pd.read_csv('output/cycle_dates.csv', parse_dates=['cyc_start_dates', 'mid_cyc_dates', 'cyc_end_dates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580139d0-21a0-41cd-8a89-6ff37d8b04b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_cyc_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e2412f-1e9f-4d81-8415-d1b8751547b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Cryo-TEMPO-EOLIS\n",
    "\n",
    "for i in range(len(CS2_SARIn_CryoTEMPO['time'])):\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    \n",
    "    # Panel - CryoSat-2 SARIn boundary lake with Cryo-TEMPO-EOLIS data ---------------------------------------------\n",
    "    lake_gdf = stationary_lakes_gdf[stationary_lakes_gdf['name'] == 'Rec2']\n",
    "    \n",
    "    # Define lake name and polygon\n",
    "    lake_name = lake_gdf['name']\n",
    "    lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "    \n",
    "    # Create buffered polygons for various multiples of lake area to find which\n",
    "    # best emcompasses the height change signals at previously identified lakes\n",
    "    search_extent_poly = multiple_area_buffer(lake_poly, 2)\n",
    "    \n",
    "    # Establish x_min, x_max, y_min, y_max\n",
    "    # x_min, y_min, x_max, y_max = search_extent_poly.iloc[0].geometry.bounds\n",
    "    x_min, y_min, x_max, y_max = lake_poly.bounds\n",
    "    x_buffer = abs(x_max-x_min)*0.3\n",
    "    y_buffer = abs(y_max-y_min)*0.3\n",
    "    \n",
    "    # Subsetting dataset\n",
    "    dataset1 = CS2_SARIn_CryoTEMPO\n",
    "    mask_x = (dataset1.x >= x_min - x_buffer) & (dataset1.x <= x_max + x_buffer)\n",
    "    mask_y = (dataset1.y >= y_min - y_buffer) & (dataset1.y <= y_max + y_buffer)\n",
    "    dataset1_subset = dataset1.where(mask_x & mask_y, drop=True)\n",
    "    \n",
    "    # Plot figure\n",
    "    img = ax.imshow(dataset1_subset['elevation'][i,:,:], extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer], \n",
    "        origin='lower', cmap='viridis')\n",
    "    \n",
    "    # Overlay published active lake outlines for visual comparison and grounding line\n",
    "    S09_outlines_color = 'paleturquoise'\n",
    "    stationary_lakes_gdf_color  = 'turquoise'\n",
    "    # S09_outlines.boundary.plot(ax=ax, edgecolor=S09_outlines_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "    S09_outlines.boundary.plot(ax=ax, edgecolor=S09_outlines_color, linestyle='solid', linewidth=2)\n",
    "    # stationary_lakes_gdf.boundary.plot(ax=ax, edgecolor=stationary_lakes_color, linestyle=(0, (1, 1)), linewidth=2)          \n",
    "    stationary_lakes_gdf.boundary.plot(ax=ax, edgecolor=stationary_lakes_color, linestyle='solid', linewidth=2)          \n",
    "    \n",
    "    # Change polar stereographic m to km\n",
    "    km_scale = 1e3\n",
    "    ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax.xaxis.set_major_formatter(ticks_x)\n",
    "    ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax.yaxis.set_major_formatter(ticks_y)  \n",
    "    \n",
    "    # Label axes, set limits, and set title\n",
    "    ax.set_xlabel('x [km]', size=15)\n",
    "    ax.set_ylabel('y [km]', size=15) \n",
    "    ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "    \n",
    "    \n",
    "    # Panel - data counts ---------------------------------------------\n",
    "    \n",
    "    # Panel - area multiple search extents ---------------------------------------------\n",
    "    \n",
    "    # Panel - CryoSat-2 era time slice with evolving outlines ---------------------------------------------\n",
    "    \n",
    "    # Panel - ICESat-2 era time slice with evolving outlines (opposite sign) ---------------------------------------------\n",
    "    \n",
    "    # Panel - aggregated evolving outlines ---------------------------------------------\n",
    "\n",
    "    # Save and close plot\n",
    "    plt.savefig(OUTPUT_DIR + '/temp/fig{}.png'.format(i),\n",
    "        dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab7052a-956a-4a91-a9bc-d5b0d35a9eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(CS2_SARIn_CryoTEMPO['time'])):\n",
    "    # fig, ax = plt.subplots(3, 2, figsize=(15, 10), constrained_layout=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 5), constrained_layout=True)\n",
    "    \n",
    "    # Panel - CryoSat-2 SARIn boundary lake with Cryo-TEMPO-EOLIS data ---------------------------------------------\n",
    "    lake_gdf = stationary_lakes_gdf[stationary_lakes_gdf['name'] == 'Rec2']\n",
    "    \n",
    "    # Define lake name and polygon\n",
    "    lake_name = lake_gdf['name']\n",
    "    lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "    \n",
    "    # Create buffered polygons for various multiples of lake area to find which\n",
    "    # best emcompasses the height change signals at previously identified lakes\n",
    "    search_extent_poly = multiple_area_buffer(lake_poly, 2)\n",
    "    \n",
    "    # Establish x_min, x_max, y_min, y_max\n",
    "    # x_min, y_min, x_max, y_max = search_extent_poly.iloc[0].geometry.bounds\n",
    "    x_min, y_min, x_max, y_max = lake_poly.bounds\n",
    "    x_buffer = abs(x_max-x_min)*0.3\n",
    "    y_buffer = abs(y_max-y_min)*0.3\n",
    "    \n",
    "    # Subsetting dataset\n",
    "    dataset1 = CS2_SARIn_CryoTEMPO\n",
    "    mask_x = (dataset1.x >= x_min - x_buffer) & (dataset1.x <= x_max + x_buffer)\n",
    "    mask_y = (dataset1.y >= y_min - y_buffer) & (dataset1.y <= y_max + y_buffer)\n",
    "    dataset1_subset = dataset1.where(mask_x & mask_y, drop=True)\n",
    "    \n",
    "    # Plot CryoSat-2 SARIn mode masks\n",
    "    gdf_SARIn_3_1.plot(ax=ax, edgecolor='k', facecolor='yellow', alpha=0.25)\n",
    "    gdf_SARIn_3_1_3_6_diff.plot(ax=ax, edgecolor='k', facecolor='yellow', lw=1, alpha=0.25)\n",
    "    \n",
    "    # Plot elevation data\n",
    "    # idx = 33  # Select timeslice to show\n",
    "    img = ax.imshow(dataset1_subset['elevation'][idx+1,:,:]-dataset1_subset['elevation'][idx,:,:], \n",
    "        extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer], \n",
    "        origin='lower', cmap='RdBu', norm=colors.CenteredNorm(vcenter=0), zorder=1)\n",
    "    \n",
    "    # Overlay published active lake outlines for visual comparison and grounding line\n",
    "    S09_outlines_color = 'paleturquoise'\n",
    "    stationary_lakes_gdf_color = 'darkturquoise'\n",
    "    # S09_outlines.boundary.plot(ax=ax, edgecolor=S09_outlines_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "    S09_outlines.boundary.plot(ax=ax, edgecolor=S09_outlines_color, linestyle='solid', linewidth=2)\n",
    "    # stationary_lakes_gdf.boundary.plot(ax=ax, edgecolor=stationary_lakes_color, linestyle=(0, (1, 1)), linewidth=2)          \n",
    "    stationary_lakes_gdf.boundary.plot(ax=ax, edgecolor=stationary_lakes_gdf_color, linestyle='solid', linewidth=2)          \n",
    "    \n",
    "    # Plot inset map\n",
    "    axIns = ax.inset_axes([0.61, 0.01, 0.5, 0.5]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "    axIns.set_aspect('equal')\n",
    "    moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    axIns.axis('off')\n",
    "    gdf_SARIn_3_1.plot(ax=axIns, edgecolor='k', facecolor='yellow', alpha=0.25, zorder=3)\n",
    "    gdf_SARIn_3_1_3_6_diff.plot(ax=axIns, edgecolor='k', facecolor='yellow', alpha=0.25, zorder=3)\n",
    "    \n",
    "    # Plot red star to indicate location\n",
    "    axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "        linewidth=1, edgecolor='k', facecolor='r', s=100, zorder=3)\n",
    "    \n",
    "    # Change polar stereographic m to km\n",
    "    km_scale = 1e3\n",
    "    ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax.xaxis.set_major_formatter(ticks_x)\n",
    "    ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax.yaxis.set_major_formatter(ticks_y)  \n",
    "    \n",
    "    # Label axes, set limits, and set title\n",
    "    ax.set_xlabel('x [km]', size=12)\n",
    "    ax.set_ylabel('y [km]', size=12) \n",
    "    ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "    \n",
    "    # Creating custom legend entries\n",
    "    S09_line = plt.Line2D((0, 1), (0, 0), color=S09_outlines_color, linestyle='solid', linewidth=2)\n",
    "    S18_line = plt.Line2D((0, 1), (0, 0), color=stationary_lakes_gdf_color, linestyle='solid', linewidth=2)\n",
    "    CS2_SARIn_patch = mpatches.Patch(edgecolor='k', facecolor='yellow', alpha=0.25)\n",
    "    \n",
    "    # Plot colorbar\n",
    "    axins = inset_axes(ax, width=\"100%\", height=\"100%\", loc='center right',\n",
    "                       bbox_to_anchor=(1.01, 0, 0.02, 1),\n",
    "                       bbox_transform=ax.transAxes,\n",
    "                       borderpad=0)\n",
    "    cbar = fig.colorbar(img, cax=axins)#, label='height change [m quarter${^-1}$]', orientation='vertical')\n",
    "    cbar.ax.tick_params(labelsize=12)  # Increase font size of colorbar tick labels\n",
    "    cbar.ax.set_ylabel('height change [m quarter$^{-1}$]', fontsize=14)  # Increase font size of colorbar label\n",
    "    \n",
    "    # Set a title for the axes\n",
    "    # title_text = 'Height change from from {} to {}'.format(mid_cyc_dates['cyc_end_dates'][idx+1].astype('datetime64[D]').astype(str), mid_cyc_dates['cyc_start_dates'][idx].astype('datetime64[D]').astype(str))\n",
    "    title_text = 'Height change from {} to {}'.format(mid_cyc_dates['cyc_start_dates'][idx].strftime('%Y-%m-%d'), mid_cyc_dates['cyc_start_dates'][idx+1].strftime('%Y-%m-%d'))\n",
    "    ax.set_title(title_text, fontsize=16, pad=67)\n",
    "    \n",
    "    # Create legend\n",
    "    ax.legend([S09_line, S18_line, CS2_SARIn_patch],\n",
    "        ['Recovery_2 active lake stationary outline', 'Recovery_2 active lake redelineated', 'CryoSat-2 SARIn coverage'], \n",
    "        loc='upper center', bbox_to_anchor=(0.5, 1.28), fontsize=12)\n",
    "    \n",
    "    # Increase tick label font sizes\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "    # Panel - data counts ---------------------------------------------\n",
    "    \n",
    "    # Panel - area multiple search extents ---------------------------------------------\n",
    "    \n",
    "    # Panel - CryoSat-2 era time slice with evolving outlines ---------------------------------------------\n",
    "    \n",
    "    # Panel - ICESat-2 era time slice with evolving outlines (opposite sign) ---------------------------------------------\n",
    "    \n",
    "    # Panel - aggregated evolving outlines ---------------------------------------------\n",
    "    \n",
    "    \n",
    "    # plt.show()\n",
    "    # Save and close plot\n",
    "    plt.savefig(OUTPUT_DIR + '/temp/fig{}.png'.format(idx),\n",
    "        dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400ed53d-703d-4411-a0db-b6791a9a69d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same but for Ben's CS2 gridded data\n",
    "for idx in range(len(CS2_Smith2017['time'])):\n",
    "    # fig, ax = plt.subplots(3, 2, figsize=(15, 10), constrained_layout=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 5), constrained_layout=True)\n",
    "    \n",
    "    # Panel - CryoSat-2 SARIn boundary lake with Cryo-TEMPO-EOLIS data ---------------------------------------------\n",
    "    lake_gdf = stationary_lakes_gdf[stationary_lakes_gdf['name'] == 'Rec2']\n",
    "    \n",
    "    # Define lake name and polygon\n",
    "    lake_name = lake_gdf['name']\n",
    "    lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "    \n",
    "    # Create buffered polygons for various multiples of lake area to find which\n",
    "    # best emcompasses the height change signals at previously identified lakes\n",
    "    search_extent_poly = multiple_area_buffer(lake_poly, 2)\n",
    "    \n",
    "    # Establish x_min, x_max, y_min, y_max\n",
    "    # x_min, y_min, x_max, y_max = search_extent_poly.iloc[0].geometry.bounds\n",
    "    x_min, y_min, x_max, y_max = lake_poly.bounds\n",
    "    x_buffer = abs(x_max-x_min)*0.3\n",
    "    y_buffer = abs(y_max-y_min)*0.3\n",
    "    \n",
    "    # Subsetting dataset\n",
    "    dataset1 = CS2_Smith2017\n",
    "    mask_x = (dataset1.x >= x_min - x_buffer) & (dataset1.x <= x_max + x_buffer)\n",
    "    mask_y = (dataset1.y >= y_min - y_buffer) & (dataset1.y <= y_max + y_buffer)\n",
    "    dataset1_subset = dataset1.where(mask_x & mask_y, drop=True)\n",
    "    dataset1_subset = dataset1_subset.where(dataset1_subset.data_count > 1, drop=True)\n",
    "    \n",
    "    # Plot CryoSat-2 SARIn mode masks\n",
    "    gdf_SARIn_3_1.plot(ax=ax, edgecolor='k', facecolor='yellow', alpha=0.25)\n",
    "    gdf_SARIn_3_1_3_6_diff.plot(ax=ax, edgecolor='k', facecolor='yellow', lw=1, alpha=0.25)\n",
    "    \n",
    "    # Plot elevation data\n",
    "    # idx = 33  # Select timeslice to show\n",
    "    img = ax.imshow(dataset1_subset['delta_h'][idx+1,:,:]-dataset1_subset['delta_h'][idx,:,:], \n",
    "        extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer], \n",
    "        origin='lower', cmap='RdBu', norm=colors.CenteredNorm(vcenter=0), zorder=1)\n",
    "    \n",
    "    # Overlay published active lake outlines for visual comparison and grounding line\n",
    "    S09_outlines_color = 'paleturquoise'\n",
    "    stationary_lakes_gdf_color = 'darkturquoise'\n",
    "    # S09_outlines.boundary.plot(ax=ax, edgecolor=S09_outlines_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "    S09_outlines.boundary.plot(ax=ax, edgecolor=S09_outlines_color, linestyle='solid', linewidth=2)\n",
    "    # stationary_lakes_gdf.boundary.plot(ax=ax, edgecolor=stationary_lakes_color, linestyle=(0, (1, 1)), linewidth=2)          \n",
    "    stationary_lakes_gdf.boundary.plot(ax=ax, edgecolor=stationary_lakes_gdf_color, linestyle='solid', linewidth=2)          \n",
    "    \n",
    "    # Plot inset map\n",
    "    axIns = ax.inset_axes([0.61, 0.01, 0.5, 0.5]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "    axIns.set_aspect('equal')\n",
    "    moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    axIns.axis('off')\n",
    "    gdf_SARIn_3_1.plot(ax=axIns, edgecolor='k', facecolor='yellow', alpha=0.25, zorder=3)\n",
    "    gdf_SARIn_3_1_3_6_diff.plot(ax=axIns, edgecolor='k', facecolor='yellow', alpha=0.25, zorder=3)\n",
    "    \n",
    "    # Plot red star to indicate location\n",
    "    axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "        linewidth=1, edgecolor='k', facecolor='r', s=100, zorder=3)\n",
    "    \n",
    "    # Change polar stereographic m to km\n",
    "    km_scale = 1e3\n",
    "    ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax.xaxis.set_major_formatter(ticks_x)\n",
    "    ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax.yaxis.set_major_formatter(ticks_y)  \n",
    "    \n",
    "    # Label axes, set limits, and set title\n",
    "    ax.set_xlabel('x [km]', size=12)\n",
    "    ax.set_ylabel('y [km]', size=12) \n",
    "    ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "    \n",
    "    # Creating custom legend entries\n",
    "    S09_line = plt.Line2D((0, 1), (0, 0), color=S09_outlines_color, linestyle='solid', linewidth=2)\n",
    "    S18_line = plt.Line2D((0, 1), (0, 0), color=stationary_lakes_gdf_color, linestyle='solid', linewidth=2)\n",
    "    CS2_SARIn_patch = mpatches.Patch(edgecolor='k', facecolor='yellow', alpha=0.25)\n",
    "    \n",
    "    # Plot colorbar\n",
    "    axins = inset_axes(ax, width=\"100%\", height=\"100%\", loc='center right',\n",
    "                       bbox_to_anchor=(1.01, 0, 0.02, 1),\n",
    "                       bbox_transform=ax.transAxes,\n",
    "                       borderpad=0)\n",
    "    cbar = fig.colorbar(img, cax=axins)#, label='height change [m quarter$^{-1}$]', orientation='vertical')\n",
    "    cbar.ax.tick_params(labelsize=12)  # Increase font size of colorbar tick labels\n",
    "    cbar.ax.set_ylabel('height change [m quarter$^{-1}$]', fontsize=14)  # Increase font size of colorbar label\n",
    "    \n",
    "    # Set a title for the axes\n",
    "    # title_text = 'Height change from from {} to {}'.format(mid_cyc_dates['cyc_end_dates'][idx+1].astype('datetime64[D]').astype(str), mid_cyc_dates['cyc_start_dates'][idx].astype('datetime64[D]').astype(str))\n",
    "    title_text = 'Height change from {} to {}'.format(mid_cyc_dates['cyc_start_dates'][idx].strftime('%Y-%m-%d'), mid_cyc_dates['cyc_start_dates'][idx+1].strftime('%Y-%m-%d'))\n",
    "    ax.set_title(title_text, fontsize=16, pad=67)\n",
    "    \n",
    "    # Create legend\n",
    "    ax.legend([S09_line, S18_line, CS2_SARIn_patch],\n",
    "        ['Recovery_2 active lake stationary outline', 'Recovery_2 active lake redelineated', 'CryoSat-2 SARIn coverage'], \n",
    "        loc='upper center', bbox_to_anchor=(0.5, 1.28), fontsize=12)\n",
    "    \n",
    "    # Increase tick label font sizes\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "    # Panel - data counts ---------------------------------------------\n",
    "    \n",
    "    # Panel - area multiple search extents ---------------------------------------------\n",
    "    \n",
    "    # Panel - CryoSat-2 era time slice with evolving outlines ---------------------------------------------\n",
    "    \n",
    "    # Panel - ICESat-2 era time slice with evolving outlines (opposite sign) ---------------------------------------------\n",
    "    \n",
    "    # Panel - aggregated evolving outlines ---------------------------------------------\n",
    "    \n",
    "    \n",
    "    # plt.show()\n",
    "    # Save and close plot\n",
    "    plt.savefig(OUTPUT_DIR + '/temp/fig{}.png'.format(idx),\n",
    "        dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4ad7cc-3849-49b6-9822-95314c7c949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panel - area multiple search extents ---------------------------------------------\n",
    "\n",
    "# Select lake to plot\n",
    "lake_gdf = stationary_lakes_gdf[stationary_lakes_gdf['name'] == 'Slessor_23']\n",
    "\n",
    "# Define lake name and polygon\n",
    "lake_name = lake_gdf['name']\n",
    "lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "\n",
    "# Create buffered polygons for various multiples of lake area to find which\n",
    "# best emcompasses the height change signals at previously identified lakes\n",
    "search_extent_poly = multiple_area_buffer(lake_poly, 15)\n",
    "\n",
    "# Establish x_min, x_max, y_min, y_max\n",
    "# x_min, y_min, x_max, y_max = search_extent_poly.iloc[0].geometry.bounds\n",
    "x_min, y_min, x_max, y_max = search_extent_poly.bounds\n",
    "x_buffer = abs(x_max-x_min)*0.1\n",
    "y_buffer = abs(y_max-y_min)*0.1\n",
    "mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "\n",
    "# Create fig, ax\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "# Plot stationary and evolving outlines onto MOA surface imagery\n",
    "moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "ax.imshow(moa_highres_da_subset[0,:,:], cmap=\"gray\", clim=[14000, 17000], extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "\n",
    "# Create lines for legend\n",
    "S09_color = 'paleturquoise'\n",
    "SF18_color  = 'turquoise'\n",
    "stationary_lakes_color = 'darkturquoise'\n",
    "Smith2009 = plt.Line2D((0, 1), (0, 0), color=S09_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "SiegfriedFricker2018 = plt.Line2D((0, 1), (0, 0), color=SF18_color, linestyle=(0, (1, 2)), linewidth=2)\n",
    "stationary_lakes_line = plt.Line2D((0, 1), (0, 0), color=stationary_lakes_color, linestyle=(0, (1, 1)), linewidth=2)\n",
    "search_extent_line = plt.Line2D((0, 1), (0, 0), color='red', linestyle='solid', linewidth=2)\n",
    "\n",
    "# Overlay published active lake outlines for visual comparison and grounding line\n",
    "S09_outlines.boundary.plot(ax=ax, edgecolor=S09_color, linestyle='solid', linewidth=2)\n",
    "SF18_outlines.boundary.plot(ax=ax, edgecolor=SF18_color, linestyle='solid', linewidth=2)\n",
    "stationary_lakes_gdf_postSF18.boundary.plot(ax=ax, edgecolor=stationary_lakes_color, linestyle=(0, (1, 1)), linewidth=2)          \n",
    "\n",
    "# Change polar stereographic m to km\n",
    "km_scale = 1e3\n",
    "ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "ax.xaxis.set_major_formatter(ticks_x)\n",
    "ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "ax.yaxis.set_major_formatter(ticks_y)  \n",
    "\n",
    "# Label axes, set limits\n",
    "ax.set_xlabel('x [km]', size=12)\n",
    "ax.set_ylabel('y [km]', size=12) \n",
    "ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "# Plot inset map\n",
    "axIns = ax.inset_axes([0, -0.02, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "axIns.set_aspect('equal')\n",
    "moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "axIns.axis('off')\n",
    "\n",
    "# Plot red star to indicate location\n",
    "axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "    linewidth=1, edgecolor='k', facecolor='r', s=100, zorder=3)\n",
    "\n",
    "# Plot\n",
    "for i in range(2, 16):\n",
    "    # Create buffered polygon for area multiple search extent\n",
    "    search_extent_buffered_stationary_outline = multiple_area_buffer(lake_poly, i)\n",
    "    \n",
    "    # Plot buffered polygons\n",
    "    gpd.GeoDataFrame(geometry=[search_extent_buffered_stationary_outline]).boundary.plot(ax=ax, edgecolor='red', facecolor='none', linewidth=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a429e-aef0-4e43-be38-5ddce40c6277",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecbfeac-abfe-4601-8e89-91e8ac9f04be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
