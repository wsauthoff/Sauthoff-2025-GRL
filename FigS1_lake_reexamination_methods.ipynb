{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c199d3a-7236-40c4-b1ed-33f946685e07",
   "metadata": {},
   "source": [
    "Notebook does data analysis to re-examine previously identified active subglacial lakes and creates Fig. S1 plotting the lake re-examination methods.\n",
    "\n",
    "Written 2023-07-11 by W. Sauthoff (wsauthoff.github.io)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff75638-a801-40a3-bd59-7d6536dbc054",
   "metadata": {},
   "source": [
    "# Set up computing environment\n",
    "\n",
    "This code runs continental-scale operations on multiple datasets and requires a ~64 GB server or local memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cf437b-64b9-4534-b7a0-4604b1ede61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install earthaccess --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b2a28f-a575-4f1e-b1c9-f601327149aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffcf4c9-ea9f-4c3c-b28a-e5b7c358bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f802c6-6310-465e-9710-7d60a356cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyogrio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47cdb0-00aa-4268-8691-9e44b3bfda2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from aiohttp import ClientResponseError\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "import datetime\n",
    "import earthaccess\n",
    "import fiona\n",
    "import gc\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "import hvplot.pandas\n",
    "from IPython.display import clear_output\n",
    "import logging\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pyogrio\n",
    "from pyproj import CRS, Geod, Transformer\n",
    "import re\n",
    "import rioxarray\n",
    "from rioxarray.exceptions import NoDataInBounds\n",
    "from shapely.geometry import box, MultiPolygon, Point, Polygon\n",
    "from shapely.ops import unary_union\n",
    "from shapely.validation import make_valid\n",
    "import shutil\n",
    "from skimage import measure\n",
    "import time\n",
    "import traceback\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "import xarray as xr\n",
    "\n",
    "# Magic function for plot interactivity\n",
    "%matplotlib widget\n",
    "\n",
    "# Define data directories dependent on home environment\n",
    "if os.getenv('HOME') == '/home/jovyan':\n",
    "    DATA_DIR = '/home/jovyan/data'\n",
    "    OUTPUT_DIR = '/home/jovyan/1_evolving_lakes/output/FigS1_lake_reexamination_methods'\n",
    "\n",
    "# Define constants and coordinate transforms for the geodesic area calculation\n",
    "CRS_LL = 'EPSG:4326' # wgs84 in lon,lat\n",
    "GEOD = CRS(CRS_LL).get_geod() # geod object for calculating geodesic area on defined ellipsoid\n",
    "CRS_XY = 'EPSG:3031' # Antarctic Polar Stereographic in x, y\n",
    "XY_TO_LL = Transformer.from_crs(CRS_XY, CRS_LL, always_xy = True) # make coord transformer\n",
    "geod = Geod(ellps='WGS84') # Create a Geod object for calculating area on the WGS84 ellipsoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4adef9-57d4-4ab4-b066-ba4a4ebb38a5",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d823b8d-85e9-4db8-b4e1-186bb823326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_quadrants_by_coords(input_data, ds_prefix='dataset', x_dim='x', y_dim='y', keep_vars=None):\n",
    "    \"\"\"\n",
    "    Combine quadrant NetCDF files or datasets into a combined dataset using xarray.combine_by_coords.\n",
    "    Overlapping x=0 and/or y=0 rows/columns are dropped depending on quadrant index.\n",
    "    Optionally prune data variables before combining.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_data : str or list\n",
    "        Either:\n",
    "        - str: Directory path containing the quadrant NetCDF files\n",
    "        - list: List of xarray.Dataset objects (must be in A1, A2, A3, A4 Cartesian order)\n",
    "    keep_vars : list, optional\n",
    "        List of variable names to keep in the datasets. If None, all variables are kept.\n",
    "        Coordinate variables (dimensions) are always preserved regardless of this list.\n",
    "    ds_prefix : str\n",
    "        Base name prefix of the quadrant files (only used when input_data is a directory path)\n",
    "    x_dim : str\n",
    "        Name of the x dimension.\n",
    "    y_dim : str\n",
    "        Name of the y dimension.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        Combined dataset with overlaps removed and optionally pruned variables.\n",
    "\n",
    "    # Example usage:\n",
    "    \n",
    "    # Method 1: Using directory path with variable pruning\n",
    "    variables_to_keep = ['x', 'y', 'delta_h']\n",
    "    combined_ds = combine_quadrants_by_coords('/path/to/quadrants', \n",
    "                                             keep_vars=keep_vars,\n",
    "                                             ds_prefix='CryoSat2_SARIn_delta_h')\n",
    "    \n",
    "    # Method 2: Using list of pre-loaded datasets with variable pruning\n",
    "    ds1 = xr.open_dataset('quadrant_A1.nc')\n",
    "    ds2 = xr.open_dataset('quadrant_A2.nc') \n",
    "    ds3 = xr.open_dataset('quadrant_A3.nc')\n",
    "    ds4 = xr.open_dataset('quadrant_A4.nc')\n",
    "    datasets_list = [ds1, ds2, ds3, ds4]  # Must be in A1, A2, A3, A4 order\n",
    "    variables_to_keep = ['delta_h', 'elevation_change']\n",
    "    combined_ds = combine_quadrants_by_coords(datasets_list, keep_vars=keep_vars)\n",
    "    \n",
    "    # Method 3: Without variable pruning (keep all variables)\n",
    "    combined_ds = combine_quadrants_by_coords(datasets_list)\n",
    "    \"\"\"\n",
    "    \n",
    "    def drop_unwanted_variables(dataset, keep_vars):\n",
    "        \"\"\"\n",
    "        Helper function to drop variables not in keep_vars list from a dataset.\n",
    "        Coordinate variables are always preserved.\n",
    "        \"\"\"\n",
    "        if keep_vars is None:\n",
    "            return dataset\n",
    "            \n",
    "        # Get all coordinate variables (dimensions and their coordinates)\n",
    "        coord_vars = set(dataset.coords.keys())\n",
    "        \n",
    "        # Variables to keep = specified variables + coordinate variables\n",
    "        vars_to_preserve = set(keep_vars) | coord_vars\n",
    "        \n",
    "        # Find variables to drop\n",
    "        variables_to_drop = [var for var in dataset.variables if var not in vars_to_preserve]\n",
    "        \n",
    "        return dataset.drop_vars(variables_to_drop, errors='ignore')\n",
    "    \n",
    "    if isinstance(input_data, str):\n",
    "        # Handle directory path - load files\n",
    "        quadrant_dir = input_data\n",
    "        files = sorted(glob.glob(os.path.join(quadrant_dir, f'{ds_prefix}_A*.nc')))\n",
    "        \n",
    "        if not files:\n",
    "            raise ValueError(f\"No quadrant files found in {quadrant_dir} with pattern '{ds_prefix}_A*.nc'\")\n",
    "        \n",
    "        datasets = []\n",
    "        for file in files:\n",
    "            ds = xr.open_dataset(file)\n",
    "            # Prune variables if specified\n",
    "            ds_pruned = drop_unwanted_variables(ds, keep_vars)\n",
    "            datasets.append(ds_pruned)\n",
    "        \n",
    "    elif isinstance(input_data, list):\n",
    "        # Handle list of datasets\n",
    "        if len(input_data) != 4:\n",
    "            raise ValueError(\"List of datasets must contain exactly 4 datasets (A1, A2, A3, A4)\")\n",
    "        \n",
    "        # Validate that all items are xarray datasets and prune variables\n",
    "        datasets = []\n",
    "        for i, ds in enumerate(input_data):\n",
    "            if not isinstance(ds, xr.Dataset):\n",
    "                raise TypeError(f\"Item {i} in the list is not an xarray.Dataset\")\n",
    "            # Prune variables if specified\n",
    "            ds_pruned = drop_unwanted_variables(ds, keep_vars)\n",
    "            datasets.append(ds_pruned)\n",
    "        \n",
    "    else:\n",
    "        raise TypeError(\"input_data must be either a string (directory path) or a list of xarray.Dataset objects\")\n",
    "\n",
    "    print(f\"Found {len(datasets)} quadrant files\")\n",
    "    \n",
    "    # Process datasets to remove overlaps according to quadrant order\n",
    "    processed_datasets = []\n",
    "    for i, ds in enumerate(datasets):\n",
    "        if i == 0:\n",
    "            processed_datasets.append(ds)  # A1: keep all\n",
    "        elif i == 1:\n",
    "            processed_datasets.append(ds.drop_sel({x_dim: 0}, errors='ignore'))  # A2: drop x=0\n",
    "        elif i == 2:\n",
    "            processed_datasets.append(ds.drop_sel({y_dim: 0}, errors='ignore'))  # A3: drop y=0\n",
    "        elif i == 3:\n",
    "            processed_datasets.append(ds.drop_sel({x_dim: 0, y_dim: 0}, errors='ignore'))  # A4: drop x=0 and y=0\n",
    "    \n",
    "    # Merge all together based on coordinates\n",
    "    print(f\"Combining {len(processed_datasets)} quadrants\")\n",
    "\n",
    "    # Use xarray index selecting to occlude the duplicated x=0 vector of data\n",
    "    A12 = xr.concat([processed_datasets[1].isel(x=slice(0,-1)), processed_datasets[0]], dim='x')\n",
    "    A34 = xr.concat([processed_datasets[2].isel(x=slice(0,-1)), processed_datasets[3]], dim='x')\n",
    "    \n",
    "    # Use xarray index selecting to occlude the duplicated y=0 vector of data\n",
    "    combined = xr.concat([A34.isel(y=slice(0,-1)), A12], dim='y')\n",
    "\n",
    "    print(f\"Combined quadrants\")\n",
    "\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c86f410-7d45-4592-8671-e270d6351ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_quarter_year(dt64):\n",
    "    '''\n",
    "    Ensure dt64 is a single datetime64, then convert to datetime.datetime\n",
    "\n",
    "    # Example\n",
    "    dt64 = np.datetime64('2016-11-15T23:15:00.000000')\n",
    "    print(datetime64_to_quarter_decimal_year(dt64))  # Output: 2016.75\n",
    "    '''\n",
    "    dt = dt64.astype('M8[ms]').astype(datetime.datetime)\n",
    "    year = dt.year\n",
    "    start_of_year = datetime.datetime(year, 1, 1)\n",
    "    end_of_year = datetime.datetime(year + 1, 1, 1)\n",
    "    year_length = (end_of_year - start_of_year).total_seconds()\n",
    "    seconds_since_start = (dt - start_of_year).total_seconds()\n",
    "    \n",
    "    fractional_year = seconds_since_start / year_length\n",
    "    quarter_year = round(fractional_year * 4) / 4  # Round to nearest 0.25\n",
    "    \n",
    "    return year + quarter_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b08ae-6a2c-4f57-8a9c-1dd26c80b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_multiple_buffer(ref_polygon, area_multiple, precision=100, exclude_inner=False):\n",
    "    '''\n",
    "    This function takes a polygon and returns a polygon with a buffer such that the area of the buffered polygon\n",
    "    is approximately the specified multiple of the original polygon area. Optionally excludes the inner polygon\n",
    "    to create a ring-like shape.\n",
    "    \n",
    "    Inputs:\n",
    "    * param polygon: Shapely Polygon object\n",
    "    * param area_multiple: the multiple of the original polygon area you wish the buffered polygon to be\n",
    "    * param precision: Precision for the iterative process to find the buffer distance\n",
    "    * param exclude_inner: If True, returns the difference between the buffered area and original polygon\n",
    "    * return: Buffered Polygon or Ring-like Polygon (if exclude_inner=True)\n",
    "    '''\n",
    "    # Ensure we're working with a single geometry, not a Series\n",
    "    if hasattr(ref_polygon, 'iloc'):\n",
    "        ref_polygon = ref_polygon.iloc[0]\n",
    "        \n",
    "    original_area = ref_polygon.area\n",
    "    target_area = area_multiple * original_area\n",
    "    buffer_distance = 0\n",
    "    area_multiple_polygon = ref_polygon\n",
    "    \n",
    "    while area_multiple_polygon.area < target_area:\n",
    "        buffer_distance += precision\n",
    "        area_multiple_polygon = ref_polygon.buffer(buffer_distance)\n",
    "    \n",
    "    if exclude_inner:\n",
    "        # Return the difference between the buffered polygon and the original polygon\n",
    "        return area_multiple_polygon.difference(ref_polygon)\n",
    "    else:\n",
    "        return area_multiple_polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c858ca39-7f32-483a-a037-81ebf05b20d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_gdf_by_folder_contents(gdf, folder_path, exclude=True, prefix=None, suffix=None, suffix_pattern=None, file_extension=None):\n",
    "    '''\n",
    "    Filter the GeoDataFrame based on processed lake names from the folder contents.\n",
    "    \n",
    "    Args:\n",
    "    gdf: GeoDataFrame to be filtered.\n",
    "    folder_path: Path to the directory containing files and/or subdirectories.\n",
    "    exclude: If True, excludes gdf rows where the 'name' is in the folder_path directories or files.\n",
    "             If False, includes only gdf rows where the 'name' is in the folder_path directories or files.\n",
    "    prefix: Optional string to remove from the beginning of filenames.\n",
    "    suffix: Optional string to remove from the end of filenames.\n",
    "    suffix_pattern: Optional regex pattern to remove from the end of filenames.\n",
    "    file_extension: Optional string specifying the file extension to filter (e.g., 'png', 'txt').\n",
    "    \n",
    "    Returns:\n",
    "    GeoDataFrame filtered based on the presence of 'name' in folder_path.\n",
    "\n",
    "    # Example usage:\n",
    "    remaining_lakes = filter_gdf_by_folder_contents(\n",
    "        stationary_outlines_gdf, \n",
    "        folder_path,\n",
    "        # prefix='plot_evolving_outlines_time_series_', \n",
    "        suffix_pattern=r'\\d+\\.\\d+m-level_\\d+x-with',\n",
    "        file_extension='txt'\n",
    "    )\n",
    "    '''\n",
    "    # Return empty GeoDataFrame if input is empty\n",
    "    if gdf is None or gdf.empty:\n",
    "        return gdf\n",
    "\n",
    "    def process_name(name):\n",
    "        '''Helper function to remove prefix and suffix from a name'''\n",
    "        processed_name = name\n",
    "        \n",
    "        # First strip the file extension if it exists\n",
    "        processed_name = os.path.splitext(processed_name)[0]\n",
    "        \n",
    "        if prefix and processed_name.startswith(prefix):\n",
    "            processed_name = processed_name[len(prefix):]\n",
    "            \n",
    "        if suffix_pattern:\n",
    "            processed_name = re.sub(suffix_pattern + '$', '', processed_name)\n",
    "        elif suffix and processed_name.endswith(suffix):\n",
    "            processed_name = processed_name[:-len(suffix)]\n",
    "            \n",
    "        return processed_name.lower().strip()\n",
    "    \n",
    "    # Get all files and filter by extension if specified\n",
    "    all_files = os.listdir(folder_path)\n",
    "    if file_extension:\n",
    "        clean_extension = file_extension.lstrip('.')\n",
    "        all_files = [f for f in all_files if f.lower().endswith(f'.{clean_extension.lower()}')]\n",
    "    \n",
    "    # Process filenames to get lake names\n",
    "    names_in_folder = {\n",
    "        process_name(name)\n",
    "        for name in all_files\n",
    "    }\n",
    "    \n",
    "    # Filter without adding and then dropping a new column\n",
    "    gdf_filtered = gdf[gdf['name'].str.lower().str.strip().apply(\n",
    "        lambda x: (x not in names_in_folder) if exclude else (x in names_in_folder)\n",
    "    )]\n",
    "    \n",
    "    return gdf_filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee59b1ac-1797-4b65-9315-23ee6b871942",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_and_save_optimal_parameters(lake_gdf, within_area_multiples=range(2, 16)):\n",
    "    '''\n",
    "    Find and save optimal levels for each lake at various within evaluation boundaries.\n",
    "    '''\n",
    "    results = find_optimal_parameters(lake_gdf=lake_gdf, within_area_multiples=within_area_multiples)\n",
    "    save_search_results(lake_gdf, results)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "def find_optimal_parameters(lake_gdf, \n",
    "                          within_area_multiples=range(2, 16),\n",
    "                          initial_level=0.01,\n",
    "                          level_increment=0.01,\n",
    "                          within_fraction_target=0.95):\n",
    "    '''\n",
    "    Find optimal search extent and level parameters for a lake.\n",
    "    Processes all within_area_multiples simultaneously for better performance.\n",
    "    '''\n",
    "    # Initialize results DataFrame\n",
    "    results_df = pd.DataFrame(\n",
    "        columns=['within_area_multiple', 'level', 'within_percent', 'dataset_dois'])\n",
    "    \n",
    "    # Track the lowest successful level for each area multiple\n",
    "    best_levels = {multiple: None for multiple in within_area_multiples}\n",
    "    \n",
    "    # Prepare datasets only once\n",
    "    dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(lake_gdf, 30)\n",
    "    \n",
    "    try:\n",
    "        print('Finding optimal levels for all within evaluation boundaries for', lake_gdf['name'].iloc[0])\n",
    "        \n",
    "        # Process all levels\n",
    "        level = initial_level\n",
    "        while level <= 2.0:\n",
    "            # Check if we've found optimal levels for all area multiples\n",
    "            if all(level is not None for level in best_levels.values()):\n",
    "                print('Found optimal levels for all area multiples. Stopping search.')\n",
    "                break\n",
    "                \n",
    "            # Find evolving outlines for the current level (do this only once)\n",
    "            outlines_gdf = find_evolving_outlines(\n",
    "                lake_gdf=lake_gdf,\n",
    "                within_area_multiple=max(within_area_multiples),  # Use the largest area multiple\n",
    "                level=level,\n",
    "                dataset1_masked=dataset1_masked,\n",
    "                dataset2_masked=dataset2_masked, \n",
    "                search_extent_poly=search_extent_poly,\n",
    "                plot=False\n",
    "            )\n",
    "            \n",
    "            # If no outlines found at this level, move to next level\n",
    "            if outlines_gdf is None or outlines_gdf.empty:\n",
    "                print(f'level: {level}, within: no outlines found')\n",
    "                level = round(level + level_increment, 2)\n",
    "                continue\n",
    "            \n",
    "            # Process each area multiple that doesn't have a solution yet\n",
    "            remaining_multiples = [m for m, best_level in best_levels.items() if best_level is None]\n",
    "            \n",
    "            for area_multiple in remaining_multiples:\n",
    "                # Calculate within fraction for this area multiple\n",
    "                within_fraction = calculate_within_fraction(\n",
    "                    evolving_outlines_gdf=outlines_gdf, \n",
    "                    stationary_outline=lake_gdf['geometry'], \n",
    "                    area_multiple=area_multiple\n",
    "                )\n",
    "                \n",
    "                print(f'level: {level}, within_area_multiple: {area_multiple}, within: {round(within_fraction*100)}%')\n",
    "                \n",
    "                # If criteria met and we haven't already found a level for this multiple\n",
    "                if within_fraction >= within_fraction_target and best_levels[area_multiple] is None:\n",
    "                    onlake_outlines, offlake_outlines = extract_intersecting_polygons_recursive(\n",
    "                        outlines_gdf, lake_gdf['geometry'].iloc[0]\n",
    "                    )\n",
    "                    \n",
    "                    if not onlake_outlines.empty:\n",
    "                        # Store this as the best level for this area multiple\n",
    "                        best_levels[area_multiple] = level\n",
    "                        \n",
    "                        # Add result to DataFrame\n",
    "                        dois = [doi for doi in [dataset1_doi, dataset2_doi] if doi is not None]\n",
    "                        results_df.loc[len(results_df)] = {\n",
    "                            'within_area_multiple': area_multiple,\n",
    "                            'level': level,\n",
    "                            'within_percent': round(within_fraction * 100, 1),\n",
    "                            'dataset_dois': ', '.join(dois)\n",
    "                        }\n",
    "                        print(f'Found optimal level {level} for area multiple {area_multiple}')\n",
    "            \n",
    "            # Move to next level\n",
    "            level = round(level + level_increment, 2)\n",
    "                \n",
    "        # Clear output after processing all levels\n",
    "        clear_output(wait=True)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f'Error processing lake {lake_gdf[\"name\"].iloc[0]}: {str(e)}')\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "    finally:\n",
    "        gc.collect()\n",
    "\n",
    "    # Sort the DataFrame\n",
    "    results_df = results_df.sort_values(\n",
    "        by=['level', 'within_area_multiple'], \n",
    "        ascending=[True, True])\n",
    "        \n",
    "    # Apply the remove_higher_duplicates function\n",
    "    results_df = remove_higher_duplicates(results_df)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def calculate_within_fraction(evolving_outlines_gdf, stationary_outline, area_multiple):\n",
    "    '''\n",
    "    Calculate fraction of evolving outlines that fall within the search extent.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    evolving_outlines_gdf : GeoDataFrame\n",
    "        The evolving outlines to analyze\n",
    "    stationary_outline : Geometry\n",
    "        The original lake outline\n",
    "    area_multiple : int\n",
    "        Area multiple for within evaluation boundary\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Fraction of outlines that are within the search extent (0.0 to 1.0)\n",
    "    '''\n",
    "    # Create search extent boundary\n",
    "    within_evaluation_poly = area_multiple_buffer(\n",
    "        ref_polygon=stationary_outline, \n",
    "        area_multiple=area_multiple)\n",
    "    within_evaluation_gdf = gpd.GeoDataFrame(\n",
    "        geometry=gpd.GeoSeries([within_evaluation_poly]), \n",
    "        crs=3031)\n",
    "\n",
    "    # Validate geometries\n",
    "    valid_outlines = evolving_outlines_gdf.loc[\n",
    "        evolving_outlines_gdf.is_valid & ~evolving_outlines_gdf.is_empty].copy()\n",
    "    valid_within_evaluation_poly = within_evaluation_gdf.loc[\n",
    "        within_evaluation_gdf.is_valid & ~within_evaluation_gdf.is_empty].copy()\n",
    "    \n",
    "    if valid_outlines.empty or valid_within_evaluation_poly.empty:\n",
    "        return 0.0\n",
    "\n",
    "    # Convert geometries to ensure they're polygons\n",
    "    valid_outlines.loc[:, 'geometry'] = valid_outlines['geometry'].apply(\n",
    "        lambda geom: geom if isinstance(geom, Polygon) else Polygon())\n",
    "    valid_within_evaluation_poly.loc[:, 'geometry'] = valid_within_evaluation_poly['geometry'].apply(\n",
    "        lambda geom: geom if isinstance(geom, Polygon) else Polygon())\n",
    "\n",
    "    # Perform spatial analysis\n",
    "    within = gpd.sjoin(valid_outlines, within_evaluation_gdf, predicate='within')\n",
    "    overlaps = gpd.sjoin(valid_outlines, within_evaluation_gdf, predicate='overlaps')\n",
    "\n",
    "    # Calculate fraction that are within\n",
    "    total = len(within) + len(overlaps)\n",
    "    if total > 0:\n",
    "        return round(len(within) / total, 2)\n",
    "    return 0.0\n",
    "\n",
    "def remove_higher_duplicates(df):\n",
    "    '''\n",
    "    Remove rows that have duplicate 'level' values at higher 'within_area_multiple' values.\n",
    "    Assumes the dataframe is already sorted by 'level' and 'within_area_multiple'.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame with 'level' and 'within_area_multiple' columns\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with duplicates removed\n",
    "    '''\n",
    "    # Keep track of levels we've seen\n",
    "    seen_levels = set()\n",
    "    # Create a boolean mask for rows to keep\n",
    "    mask = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # If we haven't seen this level before, keep the row\n",
    "        if row['level'] not in seen_levels:\n",
    "            mask.append(True)\n",
    "            seen_levels.add(row['level'])\n",
    "        else:\n",
    "            # If we have seen this level, don't keep the row\n",
    "            mask.append(False)\n",
    "    \n",
    "    # Return filtered dataframe\n",
    "    return df[mask]\n",
    "\n",
    "def save_search_results(lake_gdf, results_df):\n",
    "    '''\n",
    "    Save search results to files. Creates appropriate files for lakes with no results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lake_gdf : GeoDataFrame row\n",
    "        Single lake record containing name\n",
    "    results_df : DataFrame  \n",
    "        Results from find_optimal_parameters, may be empty\n",
    "    '''\n",
    "    os.makedirs(OUTPUT_DIR + '/levels', exist_ok=True)\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        results_df.to_csv(\n",
    "            OUTPUT_DIR + f'/levels/{lake_name}.csv',\n",
    "            index=False\n",
    "        )\n",
    "        print(f'Saved optimal parameters for {lake_name}')\n",
    "    else:\n",
    "        print(f'No outlines found for {lake_name}')\n",
    "        write_no_outlines(OUTPUT_DIR + f'/levels/{lake_name}.txt')\n",
    "        write_no_outlines(f'output/lake_outlines/evolving_outlines/{lake_name}.txt')\n",
    "\n",
    "def write_no_outlines(filepath):\n",
    "    '''Write file indicating no outlines found'''\n",
    "    try:\n",
    "        # Ensure directory exists\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        # Write file\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write('There are no evolving outlines for this lake.')\n",
    "    except Exception as e:\n",
    "        print(f'Error writing no outlines file to {filepath}: {str(e)}')\n",
    "        traceback.print_exc()\n",
    "\n",
    "def prepare_datasets(lake_gdf, area_multiple):\n",
    "    '''\n",
    "    Prepare masked datasets based on lake parameters\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lake_gdf : GeoDataFrame row\n",
    "        Single lake record containing CS2_SARIn_start and geometry\n",
    "    area_multiple : int\n",
    "        Area multiple for buffering\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked))\n",
    "    '''\n",
    "    # Extract CS2_SARIn_start as a single value, not a Series\n",
    "    if isinstance(lake_gdf, pd.Series):\n",
    "        CS2_SARIn_start = lake_gdf['CS2_SARIn_start']\n",
    "    else:\n",
    "        CS2_SARIn_start = lake_gdf.iloc[0]['CS2_SARIn_start']\n",
    "    \n",
    "    # Initialize dataset1\n",
    "    dataset1 = None\n",
    "    dataset1_doi = None\n",
    "    \n",
    "    # Check time period using proper null checking\n",
    "    if pd.isna(CS2_SARIn_start) or str(CS2_SARIn_start) == '<NA>':\n",
    "        dataset1 = None\n",
    "        dataset1_doi = None\n",
    "    elif CS2_SARIn_start == '2013.75':\n",
    "        dataset1 = SARIn_dh.sel(time=slice(np.datetime64('2013-10-01T22:30:00.000000000'), None))\n",
    "        dataset1_doi = dataset1.attrs.get('identifier_product_DOI', 'Unknown')\n",
    "    elif CS2_SARIn_start == '2010.5':\n",
    "        dataset1 = SARIn_dh\n",
    "        dataset1_doi = dataset1.attrs.get('identifier_product_DOI', 'Unknown')\n",
    "    \n",
    "    dataset2 = ATL15_dh\n",
    "    dataset2_doi = dataset2.attrs.get('identifier_product_DOI', 'Unknown')\n",
    "    \n",
    "    # Get geometry properly\n",
    "    geometry = lake_gdf['geometry'] if isinstance(lake_gdf, pd.Series) else lake_gdf.iloc[0]['geometry']\n",
    "    \n",
    "    # Mask datasets\n",
    "    search_extent_poly = area_multiple_buffer(geometry, area_multiple)\n",
    "    x_min, y_min, x_max, y_max = search_extent_poly.bounds\n",
    "    \n",
    "    dataset1_masked, dataset2_masked = mask_datasets(dataset1, dataset2, search_extent_poly, x_min, x_max, y_min, y_max)\n",
    "    \n",
    "    return dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked)\n",
    "    \n",
    "def mask_datasets(dataset1, dataset2, search_extent_poly, x_min, x_max, y_min, y_max):\n",
    "    '''Apply masks to both datasets'''\n",
    "    dataset1_masked = None\n",
    "    if dataset1 is not None:\n",
    "        dataset1_masked = apply_mask_to_dataset(dataset1, search_extent_poly, x_min, x_max, y_min, y_max)\n",
    "    \n",
    "    dataset2_masked = apply_mask_to_dataset(dataset2, search_extent_poly, x_min, x_max, y_min, y_max)\n",
    "    return dataset1_masked, dataset2_masked\n",
    "\n",
    "def apply_mask_to_dataset(dataset, search_extent_poly, x_min, x_max, y_min, y_max):\n",
    "    '''Apply mask to a single dataset'''\n",
    "    dataset_sub = dataset.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "    mask = np.array([[search_extent_poly.contains(Point(x, y)) \n",
    "                     for x in dataset_sub['x'].values] \n",
    "                     for y in dataset_sub['y'].values])\n",
    "    mask_da = xr.DataArray(mask, coords=[dataset_sub.y, dataset_sub.x], dims=['y', 'x'])\n",
    "    return dataset.where(mask_da, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ea53f8-1207-4dbf-980c-c063334bba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_evolving_outlines(lake_gdf, \n",
    "                           within_area_multiple, \n",
    "                           level, \n",
    "                           dataset1_masked, \n",
    "                           dataset2_masked, \n",
    "                           search_extent_poly, \n",
    "                           plot=False): \n",
    "    '''\n",
    "    Create time-variable outlines using skimage contour to generate evolving outlines from surface height anomalies and optionally plot.\n",
    "    If plot=True, create planview dh/dt plots of ice surface height changes with evolving outlines found,\n",
    "    along with a subplot showing data counts used in the gridded dh data.\n",
    "\n",
    "    Inputs:\n",
    "    * lake_gdf: GeoDataFrame containing lake information\n",
    "    * within_area_multiple: Factor used elsewhere to multiply lake area to create a polygon used to calculate the within_fraction\n",
    "    * level: vertical dh in meters to delineate ice surface dh anomaly contour\n",
    "    * dataset1_masked: masked dataset1 to be analyzed\n",
    "    * dataset2_masked: masked dataset2 to be analyzed\n",
    "    * search_extent_poly: buffered polygon that is the extent of masked data available for making outlines\n",
    "    * plot: boolean, if True, create and save plots; default is False for faster production when searching for optimal levels at search extents\n",
    "    using find_and_save_optimal_parameters func\n",
    "    \n",
    "    Outputs: \n",
    "    * geopandas geodataframe of polygons created at each step\n",
    "    * If plot=True, sequence of planview dh visuals with variable ice surface dh contours \n",
    "    plotted to delineate evolving lake boundaries, along with data count subplot.\n",
    "\n",
    "    # Example usage\n",
    "    >>> outlines_gdf = find_evolving_outlines(lake_gdf=lake_gdf, within_area_multiple=2, level=0.1, \n",
    "        dataset1_masked=dataset1_masked, dataset2_masked=dataset2_masked, plot=False)\n",
    "    '''    \n",
    "    # Define lake name and polygon\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "\n",
    "    # Get the time period for this lake\n",
    "    time_period = lake_gdf['CS2_SARIn_start'].iloc[0]\n",
    "\n",
    "    # Establish x_min, x_max, y_min, y_max\n",
    "    x_min, y_min, x_max, y_max = search_extent_poly.bounds\n",
    "    x_buffer, y_buffer = abs(x_max-x_min)*0.05, abs(y_max-y_min)*0.05\n",
    "    \n",
    "    # Create empty lists to store polygons, areas, dh's, dV's and dates\n",
    "    polys = []\n",
    "    areas = []\n",
    "    dhs = []\n",
    "    dVs = []\n",
    "    mid_pt_datetimes = []\n",
    "\n",
    "    # Calculate time steps and diffs for each dataset\n",
    "    if dataset1_masked is not None:\n",
    "        # Get dh values of cycle-to-cycle height change (dh) instead of relative to datum for dataset1\n",
    "        dataset1_dh = dataset1_masked['delta_h'].diff('time')\n",
    "\n",
    "        # Calculate mid-point datetimes\n",
    "        dataset1_mid_pt_times = []\n",
    "        dataset1_datetimes = dataset1_masked['time'].values\n",
    "        for i in range(1, len(dataset1_datetimes)):\n",
    "            mid_pt_days = dataset1_datetimes[i] - dataset1_datetimes[i-1]\n",
    "            mid_pt_date = dataset1_datetimes[i-1] + mid_pt_days\n",
    "            dataset1_mid_pt_times.append(mid_pt_date)\n",
    "        dataset1_mid_pt_times = np.array(dataset1_mid_pt_times)\n",
    "\n",
    "        # Write CRS after diff operation\n",
    "        dataset1_dh.rio.write_crs('epsg:3031', inplace=True)\n",
    "\n",
    "    else:\n",
    "        dataset1_dh = None\n",
    "        dataset1_mid_pt_times = np.array([])\n",
    "\n",
    "    # Get dh values of cycle-to-cycle height change (dh) instead of relative to datum for dataset2\n",
    "    dataset2_dh = dataset2_masked['delta_h'].diff('time')\n",
    "\n",
    "    # Calculate mid-point datetimes\n",
    "    dataset2_mid_pt_times = []\n",
    "    dataset2_datetimes = dataset2_masked['time'].values    \n",
    "    for i in range(1, len(dataset2_datetimes)):\n",
    "        mid_pt_days = dataset2_datetimes[i] - dataset2_datetimes[i-1]\n",
    "        mid_pt_date = dataset2_datetimes[i-1] + mid_pt_days\n",
    "        dataset2_mid_pt_times.append(mid_pt_date)\n",
    "    dataset2_mid_pt_times = np.array(dataset2_mid_pt_times)\n",
    "\n",
    "    # Write CRS after diff operation\n",
    "    dataset2_dh.rio.write_crs('epsg:3031', inplace=True)\n",
    "\n",
    "    # Only proceed with plotting if plot=True\n",
    "    if plot:\n",
    "        # Find magnitude of dh for colorbar mapping across all time slices\n",
    "        height_anom_pos = []\n",
    "        height_anom_neg = []\n",
    "        max_counts = []\n",
    "\n",
    "        # Process dataset1 if available\n",
    "        if dataset1_masked is not None:\n",
    "            for dh_slice in dataset1_dh:\n",
    "                if np.any(~np.isnan(dh_slice)):\n",
    "                    pos = np.nanmax(dh_slice)\n",
    "                    neg = np.nanmin(dh_slice)\n",
    "                    height_anom_pos.append(pos)\n",
    "                    height_anom_neg.append(neg)\n",
    "                    max_counts.append(np.nanmax(dataset1_masked['data_count']))\n",
    "\n",
    "        # Process dataset2\n",
    "        for dh_slice in dataset2_dh:\n",
    "            if np.any(~np.isnan(dh_slice)):\n",
    "                pos = np.nanmax(dh_slice)\n",
    "                neg = np.nanmin(dh_slice)\n",
    "                height_anom_pos.append(pos)\n",
    "                height_anom_neg.append(neg)\n",
    "                max_counts.append(np.nanmax(dataset2_masked['data_count']))\n",
    "\n",
    "        if height_anom_pos:  # Check if we found any valid height anomalies\n",
    "            divnorm = colors.TwoSlopeNorm(vmin=min(height_anom_neg), \n",
    "                                        vcenter=0., \n",
    "                                        vmax=max(height_anom_pos))\n",
    "            countnorm = colors.Normalize(vmin=0, vmax=max(max_counts))\n",
    "        else:\n",
    "            print('No valid height anomalies found for plotting')\n",
    "            return None\n",
    "\n",
    "        # Create plotting elements\n",
    "        stationary_lakes_color = 'darkturquoise'\n",
    "        stationary_line = plt.Line2D([], [], color=stationary_lakes_color, linestyle='solid', linewidth=2)\n",
    "        uplift = plt.Line2D([], [], color='blue', linestyle=(0, (3, 5, 1, 5)), linewidth=2)\n",
    "        subsidence = plt.Line2D([], [], color='red', linestyle=(0, (3, 5, 1, 5, 1, 5)), linewidth=2)\n",
    "        within_area_multiple_line = plt.Line2D([],[], color='dimgray', linestyle='solid', linewidth=2)\n",
    "\n",
    "    def process_timestep(dh, count, mid_pt_time, dataset_name):\n",
    "        if np.any(~np.isnan(dh)):\n",
    "            x_conv = (x_max-x_min)/dh.shape[1]\n",
    "            y_conv = (y_max-y_min)/dh.shape[0]\n",
    "    \n",
    "            # Plot if requested\n",
    "            if plot:\n",
    "                create_and_save_plots(dh, count, mid_pt_time, x_conv, y_conv)\n",
    "    \n",
    "            # Process contours\n",
    "            if np.any(~np.isnan(count)):\n",
    "                # Find positive contours\n",
    "                contours_pos = measure.find_contours(dh.values, level)\n",
    "                process_contours(contours_pos, x_conv, y_conv, dh, mid_pt_time, is_positive=True)\n",
    "    \n",
    "                # Find negative contours\n",
    "                contours_neg = measure.find_contours(dh.values, -level)\n",
    "                process_contours(contours_neg, x_conv, y_conv, dh, mid_pt_time, is_positive=False)\n",
    "    \n",
    "    \n",
    "    def create_and_save_plots(dh, count, mid_pt_time, x_conv, y_conv):\n",
    "        '''Create and save plots for the current timestep'''\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    \n",
    "        # Plot data counts\n",
    "        img1 = ax1.imshow(count, extent=[x_min, x_max, y_min, y_max], \n",
    "            origin='lower', cmap='Greys', norm=countnorm)\n",
    "    \n",
    "        # Plot height change\n",
    "        img2 = ax2.imshow(dh, extent=[x_min, x_max, y_min, y_max], \n",
    "            origin='lower', cmap='coolwarm_r', norm=divnorm)\n",
    "    \n",
    "        # Find and plot contours\n",
    "        contour_pos = measure.find_contours(dh.values, level)\n",
    "        contour_neg = measure.find_contours(dh.values, -level)\n",
    "    \n",
    "        # Plot positive contours\n",
    "        for contour in contour_pos:\n",
    "            if not is_closed_contour(contour):\n",
    "                x = x_min + contour[:,1]*x_conv + (0.5*x_conv)\n",
    "                y = y_min + contour[:,0]*y_conv + (0.5*y_conv)\n",
    "                # Plot unclosed contours\n",
    "                ax1.plot(x, y, color='darkblue', linestyle='dashdot', linewidth=1)\n",
    "                ax2.plot(x, y, color='darkblue', linestyle='dashdot', linewidth=1)\n",
    "            else:\n",
    "                x = x_min + contour[:,1]*x_conv + (0.5*x_conv)\n",
    "                y = y_min + contour[:,0]*y_conv + (0.5*y_conv)\n",
    "                # Plot closed contours\n",
    "                ax1.plot(x, y, color='blue', linestyle=(0, (3, 5, 1, 5)), linewidth=1)\n",
    "                ax2.plot(x, y, color='blue', linestyle=(0, (3, 5, 1, 5)), linewidth=1)\n",
    "    \n",
    "        # Plot negative contours\n",
    "        for contour in contour_neg:\n",
    "            if not is_closed_contour(contour):\n",
    "                x = x_min + contour[:,1]*x_conv + (0.5*x_conv)\n",
    "                y = y_min + contour[:,0]*y_conv + (0.5*y_conv)\n",
    "                # Plot unclosed contours\n",
    "                ax1.plot(x, y, color='darkred', linestyle=(0, (3, 1, 1, 1)), linewidth=1)\n",
    "                ax2.plot(x, y, color='darkred', linestyle=(0, (3, 1, 1, 1)), linewidth=1)\n",
    "            else:\n",
    "                x = x_min + contour[:,1]*x_conv + (0.5*x_conv)\n",
    "                y = y_min + contour[:,0]*y_conv + (0.5*y_conv)\n",
    "                # Plot closed contours\n",
    "                ax1.plot(x, y, color='red', linestyle=(0, (3, 1, 1, 1)), linewidth=1)\n",
    "                ax2.plot(x, y, color='red', linestyle=(0, (3, 1, 1, 1)), linewidth=1)\n",
    "    \n",
    "        # Plot boundaries on both subplots\n",
    "        within_evaluation_poly = area_multiple_buffer(lake_poly, within_area_multiple)\n",
    "        gpd.GeoSeries(within_evaluation_poly).boundary.plot(ax=ax1, color='dimgray')\n",
    "        gpd.GeoSeries(within_evaluation_poly).boundary.plot(ax=ax2, color='dimgray')\n",
    "    \n",
    "        # Common plotting elements\n",
    "        for ax in [ax1, ax2]:\n",
    "            stationary_outlines_gdf.boundary.plot(ax=ax, edgecolor=stationary_lakes_color, linestyle='solid', linewidth=2)          \n",
    "            km_scale = 1e3\n",
    "            ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "            ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "            ax.set_xlabel('x [km]', size=15)\n",
    "            ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "    \n",
    "        # Additional ax1 elements\n",
    "        ax1.set_ylabel('y [km]', size=15)\n",
    "        axIns = ax1.inset_axes([0.01, -0.01, 0.2, 0.2])\n",
    "        axIns.set_aspect('equal')\n",
    "        moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        axIns.axis('off')\n",
    "        axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "            linewidth=1, edgecolor='k', facecolor='r', s=100, zorder=3)\n",
    "    \n",
    "        # Colorbars\n",
    "        divider1 = make_axes_locatable(ax1)\n",
    "        cax1 = divider1.append_axes('right', size='5%', pad=0.2)\n",
    "        fig.colorbar(img1, cax=cax1).set_label('data counts', size=15)\n",
    "    \n",
    "        divider2 = make_axes_locatable(ax2)\n",
    "        cax2 = divider2.append_axes('right', size='5%', pad=0.2)\n",
    "        fig.colorbar(img2, cax=cax2).set_label('height change (dh) [m]', size=15)\n",
    "    \n",
    "        # Legends\n",
    "        for ax, title in zip([ax1, ax2], ['Data counts', 'Height change']):\n",
    "            ax.legend([stationary_line, uplift, subsidence, within_area_multiple_line],\n",
    "                     ['stationary outline',\n",
    "                      f'evolving outline (+ {level} m)',\n",
    "                      f'evolving outline (− {level} m)',\n",
    "                      f'within evaluation line ({int(within_area_multiple)}x)'],\n",
    "                     loc='upper left')\n",
    "            ax.set_title(title)\n",
    "    \n",
    "        # Save plot\n",
    "        fig.suptitle(f'Mid-point date: {pd.Timestamp(mid_pt_time)}', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{OUTPUT_DIR}/find_evolving_outlines/{lake_name}/find_evolving_outlines_{lake_name}_{level}m-level_{within_area_multiple}x-within_{pd.Timestamp(mid_pt_time).strftime(\"%Y-%m-%d\")}.png', \n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "            \n",
    "    def process_contours(contours, x_conv, y_conv, dh, time, is_positive=True):\n",
    "        for contour in contours:\n",
    "            if is_closed_contour(contour):\n",
    "                x = x_min + contour[:,1]*x_conv + (0.5*x_conv)\n",
    "                y = y_min + contour[:,0]*y_conv + (0.5*y_conv)\n",
    "                \n",
    "                # Create and process polygon\n",
    "                poly = Polygon(list(zip(x, y)))\n",
    "                try:\n",
    "                    dhdt_poly = dh.rio.clip([poly])\n",
    "                    lon, lat = XY_TO_LL.transform(x,y)\n",
    "                    poly_area = abs(GEOD.polygon_area_perimeter(lon, lat)[0])\n",
    "                    \n",
    "                    if np.any(~np.isnan(dhdt_poly)):\n",
    "                        poly_dh = np.nanmean(dhdt_poly)\n",
    "                        poly_dV = poly_dh*poly_area\n",
    "                        polys.append(poly)\n",
    "                        areas.append(poly_area)\n",
    "                        dhs.append(poly_dh)\n",
    "                        dVs.append(poly_dV)\n",
    "                        mid_pt_datetimes.append(time)\n",
    "                except NoDataInBounds:\n",
    "                    pass\n",
    "                except Exception as e:\n",
    "                    raise\n",
    "\n",
    "    # Process dataset1 if available\n",
    "    if dataset1_masked is not None:\n",
    "        for i, (dh_slice, mid_pt_time) in enumerate(zip(dataset1_dh, dataset1_mid_pt_times)):\n",
    "            process_timestep(dh_slice, dataset1_masked['data_count'][i], mid_pt_time, 'dataset1')\n",
    "\n",
    "    # Process dataset2\n",
    "    for i, (dh_slice, mid_pt_time) in enumerate(zip(dataset2_dh, dataset2_mid_pt_times)):\n",
    "        process_timestep(dh_slice, dataset2_masked['data_count'][i], mid_pt_time, 'dataset2')\n",
    "\n",
    "    # Return None if no polygons were found\n",
    "    if not polys:\n",
    "        return None\n",
    "\n",
    "    # Create GeoDataFrame if we found any polygons\n",
    "    gdf = gpd.GeoDataFrame({\n",
    "        'within_area_multiple': [within_area_multiple] * len(polys),\n",
    "        'level': [level] * len(polys),\n",
    "        'geometry': polys, \n",
    "        'area (m^2)': areas, \n",
    "        'dh (m)': dhs, \n",
    "        'vol (m^3)': dVs,\n",
    "        'mid_pt_datetime': mid_pt_datetimes\n",
    "    }, crs='EPSG:3031')\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f1491-6ba7-4c8f-8ec0-0174fe1a8c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_closed_contour(contour, tolerance=1.0):\n",
    "    '''\n",
    "    Check if a contour is closed by comparing its start and end points.\n",
    "    \n",
    "    Args:\n",
    "        contour: numpy array of shape (N, 2) containing the contour points\n",
    "        tolerance: maximum distance between start and end points to consider contour closed;\n",
    "        default of 1.0 means start and end points must be within one pixel\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if contour is closed, False otherwise\n",
    "    '''\n",
    "    if len(contour) < 3:\n",
    "        return False\n",
    "        \n",
    "    # Get first and last points\n",
    "    start_point = contour[0]\n",
    "    end_point = contour[-1]\n",
    "    \n",
    "    # Calculate Euclidean distance between start and end points\n",
    "    distance = np.sqrt(np.sum((start_point - end_point) ** 2))\n",
    "    \n",
    "    # Consider contour closed if start and end points are within tolerance\n",
    "    return distance < tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f96c6-7d4a-44d3-a9b5-3578bb4ee54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_duplicate_files(directory_path, keep_extension, delete_extension):\n",
    "    '''\n",
    "    Search a directory for files with matching names but different extensions,\n",
    "    keeping files with one extension and deleting files with another.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to the directory to search\n",
    "        keep_extension (str): File extension to keep (e.g., 'csv')\n",
    "        delete_extension (str): File extension to delete (e.g., 'txt')\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (list of deleted files, list of errors encountered)\n",
    "    \n",
    "    Example usage:\n",
    "        directory = 'path/to/your/directory'\n",
    "        deleted, errors = cleanup_duplicate_files(directory, 'csv', 'txt')\n",
    "        \n",
    "        if deleted:\n",
    "            print('\\nDeleted files:')\n",
    "            for file in deleted:\n",
    "                print(f'- {file}')\n",
    "        \n",
    "        if errors:\n",
    "            print('\\nErrors encountered:')\n",
    "            for error in errors:\n",
    "                print(f'- {error}')\n",
    "    '''\n",
    "    # Setup logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    deleted_files = []\n",
    "    errors = []\n",
    "    \n",
    "    try:\n",
    "        # Convert directory path to Path object\n",
    "        dir_path = Path(directory_path)\n",
    "        \n",
    "        # Validate inputs\n",
    "        if not dir_path.exists():\n",
    "            raise FileNotFoundError(f'Directory {directory_path} does not exist')\n",
    "            \n",
    "        if not isinstance(keep_extension, str) or not isinstance(delete_extension, str):\n",
    "            raise ValueError('File extensions must be strings')\n",
    "            \n",
    "        # Normalize extensions (remove dots if present)\n",
    "        keep_extension = keep_extension.lstrip('.')\n",
    "        delete_extension = delete_extension.lstrip('.')\n",
    "        \n",
    "        if keep_extension == delete_extension:\n",
    "            raise ValueError('Keep and delete extensions must be different')\n",
    "        \n",
    "        # Get all files of both types\n",
    "        files_to_keep = {f.stem: f for f in dir_path.glob(f'*.{keep_extension}')}\n",
    "        files_to_delete = {f.stem: f for f in dir_path.glob(f'*.{delete_extension}')}\n",
    "        \n",
    "        # Find matching files\n",
    "        for filename_stem in files_to_delete.keys():\n",
    "            if filename_stem in files_to_keep:\n",
    "                delete_path = files_to_delete[filename_stem]\n",
    "                try:\n",
    "                    delete_path.unlink()  # Delete the file\n",
    "                    deleted_files.append(str(delete_path))\n",
    "                    logger.info(f'Deleted: {delete_path}')\n",
    "                except Exception as e:\n",
    "                    error_msg = f'Error deleting {delete_path}: {str(e)}'\n",
    "                    errors.append(error_msg)\n",
    "                    logger.error(error_msg)\n",
    "        \n",
    "        # Summary\n",
    "        logger.info(f'Processing complete. Deleted {len(deleted_files)} files. Encountered {len(errors)} errors.')\n",
    "        \n",
    "        return deleted_files, errors\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = f'Fatal error: {str(e)}'\n",
    "        errors.append(error_msg)\n",
    "        logger.error(error_msg)\n",
    "        return deleted_files, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57775bed-cddb-4cdc-bd48-d2df79fead0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_save_evolving_outlines(lake_gdf, row_index=0):\n",
    "    '''\n",
    "    Visualize the evolving outlines for each stationary lake using optimal level and within_area_multiple combination.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    stationary_outlines_gdf : GeoDataFrame\n",
    "        The dataset of stationary lakes\n",
    "    row_index : int, optional (default=0)\n",
    "        Index of the row to use from the sorted levels_df dataframe.\n",
    "        0 gives the smallest level and corresponding within_area_multiple,\n",
    "        -1 gives the largest level and corresponding within_area_multiple found using the find_and_save_optimal_parameters function.\n",
    "    '''\n",
    "    # Store lake name and polygon\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "    print('Visualizing outlines for', lake_name)\n",
    "\n",
    "    try:\n",
    "        # Load levels dataframe\n",
    "        csv_path = OUTPUT_DIR + '/levels/{}.csv'.format(lake_name)\n",
    "        txt_path = OUTPUT_DIR + '/levels/{}.txt'.format(lake_name)\n",
    "\n",
    "        if os.path.exists(csv_path):\n",
    "            levels_df = pd.read_csv(csv_path)\n",
    "        elif os.path.exists(txt_path):\n",
    "            output_path = os.path.join(f'output/lake_outlines/evolving_outlines/{lake_name}.txt')\n",
    "            print(f'Found no levels CSV file but found \"no outlines\" TXT file for {lake_name}. So writing \"no outlines\" TXT file to {output_path}.')\n",
    "            write_no_outlines(output_path)\n",
    "            return\n",
    "        else:\n",
    "            print(f'No levels file found for {lake_name}. Skipping lake.')\n",
    "            return\n",
    "        \n",
    "        # Select row based on provided index\n",
    "        if abs(row_index) >= len(levels_df):\n",
    "            print(f'Warning: row_index {row_index} out of bounds for {lake_name}. Skipping.')\n",
    "            return\n",
    "        else:\n",
    "            selected_row = levels_df.iloc[row_index]\n",
    "\n",
    "        # Print parameters\n",
    "        print(f'Parameters: row_index={row_index}, within_area_multiple={selected_row[\"within_area_multiple\"]}, level={selected_row[\"level\"]}, doi(s)={selected_row[\"dataset_dois\"]}')\n",
    "\n",
    "        # Prepare data sets\n",
    "        dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(lake_gdf, 25)\n",
    "\n",
    "        # Create output folders\n",
    "        os.makedirs(OUTPUT_DIR + f'/find_evolving_outlines/{lake_name}', exist_ok=True)\n",
    "        os.makedirs('output/lake_outlines/evolving_outlines', exist_ok=True)\n",
    "        os.makedirs(OUTPUT_DIR + '/plot_evolving_outlines_time_series', exist_ok=True)\n",
    "        os.makedirs(OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines', exist_ok=True)\n",
    "\n",
    "        # Find evolving outlines\n",
    "        evolving_outlines_gdf = find_evolving_outlines(\n",
    "            lake_gdf=lake_gdf, \n",
    "            within_area_multiple=selected_row['within_area_multiple'], \n",
    "            level=selected_row['level'], \n",
    "            dataset1_masked=dataset1_masked,\n",
    "            dataset2_masked=dataset2_masked,\n",
    "            search_extent_poly=search_extent_poly,\n",
    "            plot=True\n",
    "        )\n",
    "       \n",
    "        if evolving_outlines_gdf is None or evolving_outlines_gdf.empty:\n",
    "            print('No evolving outlines found.')\n",
    "            \n",
    "        try:\n",
    "            onlake_outlines, offlake_outlines = extract_intersecting_polygons_recursive(\n",
    "                evolving_outlines_gdf, \n",
    "                lake_gdf['geometry'].iloc[0]\n",
    "            )\n",
    "            \n",
    "            if onlake_outlines is None or onlake_outlines.empty:\n",
    "                print('No valid filtered outlines found this within_area_multiple and level. Deleting levels CSV and writing \"no outlines\" TXT file.')\n",
    "\n",
    "                # Delete levels CSV file and write 'no outlines' TXT file\n",
    "                os.remove(OUTPUT_DIR + f'/levels/{lake_name}.csv')\n",
    "                write_no_outlines(OUTPUT_DIR + f'/levels/{lake_name}.txt')\n",
    "                write_no_outlines(f'output/lake_outlines/evolving_outlines/{lake_name}.txt')\n",
    "                \n",
    "                # Clean up generated images\n",
    "                img_extension = 'png'\n",
    "                images_folder = os.path.join(OUTPUT_DIR, f'find_evolving_outlines/{lake_name}')\n",
    "                image_files = sorted(glob.glob(os.path.join(images_folder, f'*.{img_extension}')))\n",
    "                for image_file in image_files:\n",
    "                    os.remove(image_file)\n",
    "                try:\n",
    "                    shutil.rmtree(images_folder)\n",
    "                    print(f'Cleaned up folder: {images_folder}')\n",
    "                except Exception as e:\n",
    "                    print(f'Error cleaning up folder for {lake_name}: {str(e)}')\n",
    "                    \n",
    "                return\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f'Error extracting polygons for {lake_name}: {str(e)}')\n",
    "            traceback.print_exc()\n",
    "\n",
    "        # Save results and plot\n",
    "        if not onlake_outlines.empty:\n",
    "            try:\n",
    "                # Add metadata to onlake_outlines\n",
    "                onlake_outlines = onlake_outlines.copy()\n",
    "                onlake_outlines.loc[:, 'within_area_multiple'] = selected_row['within_area_multiple']\n",
    "                onlake_outlines.loc[:, 'level'] = selected_row['level']\n",
    "                onlake_outlines.loc[:, 'row_index'] = row_index\n",
    "        \n",
    "                # Export evolving outlines GeoDataFrame to GeoJSON\n",
    "                filepath = os.path.join(f'output/lake_outlines/evolving_outlines/{lake_name}.geojson')\n",
    "                onlake_outlines.to_file(filename=filepath, driver='GeoJSON')\n",
    "        \n",
    "                if offlake_outlines is not None and not offlake_outlines.empty:\n",
    "                    offlake_outlines = offlake_outlines.copy()\n",
    "                    offlake_outlines.loc[:, 'within_area_multiple'] = selected_row['within_area_multiple'] \n",
    "                    offlake_outlines.loc[:, 'level'] = selected_row['level']\n",
    "                    offlake_outlines.loc[:, 'row_index'] = row_index\n",
    "                \n",
    "                    # Export\n",
    "                    filepath = OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines/{}.geojson'.format(lake_name)\n",
    "                    offlake_outlines.to_file(filename=filepath, driver='GeoJSON')\n",
    "                    print(f'Saved outlines to: {filepath}')\n",
    "                else:\n",
    "                    # Create an empty GeoDataFrame with proper index for offlake_outlines\n",
    "                    # This is the key fix: create a GeoDataFrame with at least one row first to establish the index\n",
    "                    empty_geom = gpd.GeoSeries([None])\n",
    "                    offlake_outlines = gpd.GeoDataFrame(geometry=empty_geom, crs=onlake_outlines.crs)\n",
    "                    # Now add one row with empty/null geometry just to establish the index\n",
    "                    offlake_outlines = offlake_outlines.iloc[0:0]  # Now remove the row but keep the structure\n",
    "                    \n",
    "                    # Now we can safely add metadata columns\n",
    "                    offlake_outlines['within_area_multiple'] = selected_row['within_area_multiple']\n",
    "                    offlake_outlines['level'] = selected_row['level']\n",
    "                    print('starting row_index write on offlake_outlines')\n",
    "                    offlake_outlines['row_index'] = row_index\n",
    "                    \n",
    "                    # Export empty GeoDataFrame\n",
    "                    filepath = OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines/{}.geojson'.format(lake_name)\n",
    "                    offlake_outlines.to_file(filename=filepath, driver='GeoJSON')\n",
    "                    print(f'Saved empty offlake_outlines to: {filepath}')\n",
    "                    \n",
    "                # Convert images to video\n",
    "                try:\n",
    "                    video_from_images(lake_gdf, output_dir=OUTPUT_DIR, \n",
    "                                      row_index=row_index, fps=1, img_extension='png')\n",
    "        \n",
    "                except Exception as e:\n",
    "                    print(f'Error creating video for {lake_name}: {str(e)}')\n",
    "                    traceback.print_exc()\n",
    "                    \n",
    "                # Plot the outlines\n",
    "                try:\n",
    "                    plot_evolving_outlines_time_series(\n",
    "                        lake_gdf=lake_gdf,\n",
    "                        evolving_outlines_gdf=onlake_outlines,\n",
    "                        offlake_outlines_gdf=offlake_outlines\n",
    "                    )\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f'Error creating evolving outlines time series plot for {lake_name}: {str(e)}')\n",
    "                    traceback.print_exc()\n",
    "\n",
    "                # Clear output\n",
    "                clear_output(wait=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'Error saving results for {lake_name}: {str(e)}')\n",
    "                traceback.print_exc()\n",
    "        else:\n",
    "            print(f'No filtered outlines to save for {lake_name}')\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f'Error processing {lake_name}: {str(e)}')\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        cleanup_vars = [\n",
    "            'dataset1_masked', 'dataset2_masked', 'evolving_outlines_gdf', \n",
    "            'onlake_outlines', 'offlake_outlines', 'levels_df', 'selected_row',\n",
    "            'search_extent_poly'\n",
    "        ]\n",
    "        for var in cleanup_vars:\n",
    "            if var in locals():\n",
    "                del locals()[var]\n",
    "        gc.collect()\n",
    "\n",
    "def video_from_images(lake_gdf, output_dir=OUTPUT_DIR, row_index=0, fps=1, img_extension='png', max_retries=3):\n",
    "    '''\n",
    "    Creates a video from still images with additional validation and retry mechanism.\n",
    "    \n",
    "    Parameters:\n",
    "    - lake_gdf: GeoDataFrame containing lake information (expects a 'name' column)\n",
    "    - output_dir: Base directory where the images and video are stored/created\n",
    "    - row_index: row of optimal parameters used to generate evolving outlines\n",
    "    - fps: Frames per second for the output video\n",
    "    - img_extension: Extension of the images to look for in the folder\n",
    "    - max_retries: Maximum number of attempts to create the video\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if video creation was successful, False otherwise\n",
    "    '''\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    print('Making video for', lake_name)\n",
    "    \n",
    "    def validate_images(image_files):\n",
    "        '''Validate that all images are readable and have consistent dimensions'''\n",
    "        if not image_files:\n",
    "            return False, None\n",
    "            \n",
    "        reference_frame = cv2.imread(image_files[0])\n",
    "        if reference_frame is None:\n",
    "            return False, None\n",
    "            \n",
    "        height, width = reference_frame.shape[:2]\n",
    "        \n",
    "        for img_file in image_files:\n",
    "            frame = cv2.imread(img_file)\n",
    "            if frame is None:\n",
    "                print(f'Could not read image: {img_file}')\n",
    "                return False, None\n",
    "            if frame.shape[:2] != (height, width):\n",
    "                print(f'Inconsistent dimensions in {img_file}')\n",
    "                return False, None\n",
    "                \n",
    "        return True, (width, height)\n",
    "\n",
    "    def attempt_video_creation(attempt=1):\n",
    "        try:\n",
    "            # Load levels dataframe\n",
    "            levels_df = pd.read_csv(os.path.join(OUTPUT_DIR, 'levels', f'{lake_name}.csv'))\n",
    "            if levels_df.empty:\n",
    "                print('levels_df empty.')\n",
    "                return False\n",
    "\n",
    "            # Select row based on provided index\n",
    "            if abs(row_index) >= len(levels_df):\n",
    "                print(f'Warning: row_index {row_index} out of bounds for {lake_name}. Using first row.')\n",
    "                selected_row = levels_df.iloc[0]\n",
    "            else:\n",
    "                selected_row = levels_df.iloc[row_index]\n",
    "\n",
    "            # Derive paths\n",
    "            images_folder = os.path.join(OUTPUT_DIR, f'find_evolving_outlines/{lake_name}')\n",
    "            output_video_file = os.path.join(\n",
    "                output_dir,\n",
    "                f'find_evolving_outlines/{lake_name}_{row_index}-idx_{selected_row[\"level\"]}m-level_{selected_row[\"within_area_multiple\"]}x-within.mp4'\n",
    "            )\n",
    "\n",
    "            # Get and sort images\n",
    "            image_files = sorted(glob.glob(os.path.join(images_folder, f'*.{img_extension}')))\n",
    "            \n",
    "            # Validate images\n",
    "            print(f'Validating {len(image_files)} images...')\n",
    "            images_valid, dimensions = validate_images(image_files)\n",
    "            if not images_valid:\n",
    "                print(f'Image validation failed on attempt {attempt}')\n",
    "                return False\n",
    "\n",
    "            # Try different codecs if needed\n",
    "            codecs = ['mp4v', 'avc1', 'H264']\n",
    "            success = False\n",
    "            \n",
    "            for codec in codecs:\n",
    "                try:\n",
    "                    fourcc = cv2.VideoWriter_fourcc(*codec)\n",
    "                    video = cv2.VideoWriter(output_video_file, fourcc, fps, dimensions)\n",
    "                    \n",
    "                    if not video.isOpened():\n",
    "                        print(f'Failed to open video writer with codec {codec}')\n",
    "                        continue\n",
    "                        \n",
    "                    # Write frames\n",
    "                    for image_file in image_files:\n",
    "                        frame = cv2.imread(image_file)\n",
    "                        if frame is not None:\n",
    "                            video.write(frame)\n",
    "                    \n",
    "                    video.release()\n",
    "                    \n",
    "                    # Verify the video was created and is not empty\n",
    "                    if os.path.exists(output_video_file) and os.path.getsize(output_video_file) > 0:\n",
    "                        success = True\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f'Video file empty or not created with codec {codec}')\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f'Error with codec {codec}: {str(e)}')\n",
    "                    continue\n",
    "\n",
    "            if success:\n",
    "                print(f'Video created successfully on attempt {attempt}')\n",
    "                # Clean up images only after successful video creation\n",
    "                for image_file in image_files:\n",
    "                    os.remove(image_file)\n",
    "                try:\n",
    "                    shutil.rmtree(images_folder)\n",
    "                    print(f'Cleaned up folder: {images_folder}')\n",
    "                except Exception as e:\n",
    "                    print(f'Could not delete folder {images_folder}: {e}')\n",
    "                return True\n",
    "            \n",
    "            return False\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error on attempt {attempt}: {str(e)}')\n",
    "            return False\n",
    "\n",
    "    # Main retry loop\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        print(f'\\nAttempt {attempt} of {max_retries}')\n",
    "        if attempt > 1:\n",
    "            print('Waiting 2 seconds before retry...')\n",
    "            time.sleep(2)  # Add delay between attempts\n",
    "            \n",
    "        if attempt_video_creation(attempt):\n",
    "            return True\n",
    "            \n",
    "    print(f'Failed to create video after {max_retries} attempts')\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008328b0-3f7c-4fe0-baa5-a6e787214f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_intersecting_polygons_recursive(gdf, reference_geometry):\n",
    "    '''\n",
    "    Extract and separate intersecting and non-intersecting polygons with topology validation and cleaning.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        Input GeoDataFrame containing polygons to analyze\n",
    "    reference_geometry : Geometry\n",
    "        Reference geometry to check for intersections\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple (GeoDataFrame, GeoDataFrame)\n",
    "        First GeoDataFrame contains intersecting polygons\n",
    "        Second GeoDataFrame contains non-intersecting polygons\n",
    "    '''\n",
    "    import shapely.geometry\n",
    "    from shapely.validation import make_valid\n",
    "    import geopandas as gpd\n",
    "    import pandas as pd\n",
    "    \n",
    "    empty_gdf = gpd.GeoDataFrame(geometry=[], crs=gdf.crs if gdf is not None else None)\n",
    "    \n",
    "    if gdf is None or gdf.empty:\n",
    "        return empty_gdf, empty_gdf\n",
    "        \n",
    "    try:\n",
    "        # Create a copy and clean geometries\n",
    "        gdf_copy = gdf.copy()\n",
    "        \n",
    "        # Clean reference geometry\n",
    "        if not reference_geometry.is_valid:\n",
    "            print('Cleaning reference geometry...')\n",
    "            reference_geometry = make_valid(reference_geometry)\n",
    "            \n",
    "        # Clean all geometries in the GeoDataFrame\n",
    "        gdf_copy['geometry'] = gdf_copy['geometry'].apply(lambda geom: make_valid(geom) if not geom.is_valid else geom)\n",
    "        \n",
    "        # Find directly intersecting polygons\n",
    "        try:\n",
    "            directly_intersecting = gdf_copy.loc[gdf_copy.geometry.intersects(reference_geometry)].copy()\n",
    "        except Exception as e:\n",
    "            print(f'Error in direct intersection: {str(e)}')\n",
    "            return empty_gdf, empty_gdf\n",
    "            \n",
    "        if directly_intersecting.empty:\n",
    "            # If no direct intersections, return empty intersecting and full non-intersecting\n",
    "            return empty_gdf, gdf_copy\n",
    "            \n",
    "        # Initialize already_found with directly intersecting polygons\n",
    "        already_found = directly_intersecting.copy()\n",
    "        already_found_indices = set(already_found.index)\n",
    "        \n",
    "        # Recursive intersection\n",
    "        try:\n",
    "            while True:\n",
    "                # Create union with buffer to handle small topology issues\n",
    "                union_geom = already_found.geometry.union_all()\n",
    "                if not union_geom.is_valid:\n",
    "                    print('Cleaning union geometry...')\n",
    "                    union_geom = make_valid(union_geom)\n",
    "                \n",
    "                # Find new intersecting polygons\n",
    "                new_intersecting = gdf_copy.loc[\n",
    "                    ~gdf_copy.index.isin(already_found_indices) & \n",
    "                    gdf_copy.geometry.intersects(union_geom)\n",
    "                ].copy()\n",
    "                \n",
    "                if new_intersecting.empty:\n",
    "                    break\n",
    "                    \n",
    "                # Add new indices to our set\n",
    "                already_found_indices.update(new_intersecting.index)\n",
    "                \n",
    "                # Combine results\n",
    "                already_found = gpd.GeoDataFrame(\n",
    "                    pd.concat([already_found, new_intersecting], ignore_index=False)\n",
    "                ).copy()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f'Warning: Error in recursive intersection: {str(e)}')\n",
    "            print('Returning directly intersecting polygons only')\n",
    "            non_intersecting = gdf_copy[~gdf_copy.index.isin(directly_intersecting.index)].copy()\n",
    "            return directly_intersecting, non_intersecting\n",
    "        \n",
    "        # Get non-intersecting polygons using the set of indices we've collected\n",
    "        non_intersecting = gdf_copy[~gdf_copy.index.isin(already_found_indices)].copy()\n",
    "        return already_found, non_intersecting\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error in extract_intersecting_polygons_recursive: {str(e)}')\n",
    "        return empty_gdf, empty_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e96a18-e31b-4901-9617-09c2f60b4ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evolving_outlines_time_series(lake_gdf, evolving_outlines_gdf, offlake_outlines_gdf):\n",
    "    '''\n",
    "    Plot evolving outlines time series (on- and off-lake outlines) on ice-surface imagery background.\n",
    "    '''\n",
    "    try:\n",
    "        # Define lake name and polygon\n",
    "        lake_name = lake_gdf['name'].iloc[0]\n",
    "        print(f'Creating evolving outlines time series plot for lake: {lake_name}')\n",
    "        \n",
    "        # Validate inputs\n",
    "        if evolving_outlines_gdf is None or evolving_outlines_gdf.empty:\n",
    "            print(f'No evolving outlines provided for {lake_name}')\n",
    "            return\n",
    "        \n",
    "        # Get parameters using iloc\n",
    "        within_area_multiple = evolving_outlines_gdf['within_area_multiple'].iloc[0]\n",
    "        level = evolving_outlines_gdf['level'].iloc[0]\n",
    "        row_index = evolving_outlines_gdf['row_index'].iloc[0]\n",
    "\n",
    "        print(f'Parameters: row_index={row_index}, within_area_multiple={within_area_multiple}, level={level}')\n",
    "        \n",
    "        stationary_outline = lake_gdf['geometry']\n",
    "        if stationary_outline is None:\n",
    "            print(f'Error: No geometry found for {lake_name}')\n",
    "            return\n",
    "\n",
    "        # Create search extent and within evaluation polygons\n",
    "        search_extent_poly = area_multiple_buffer(\n",
    "            stationary_outline, 25)\n",
    "        within_evaluation_poly = area_multiple_buffer(\n",
    "            stationary_outline, within_area_multiple)\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(5,5))\n",
    "        \n",
    "        # Plot search extent and within evaluation polygons\n",
    "        gpd.GeoDataFrame(geometry=[search_extent_poly]).boundary.plot(\n",
    "            ax=ax, edgecolor='k', facecolor='none', linewidth=1)\n",
    "        gpd.GeoDataFrame(geometry=[within_evaluation_poly]).boundary.plot(\n",
    "            ax=ax, edgecolor='dimgray', facecolor='none', linewidth=1)\n",
    "\n",
    "        # Set up colormap\n",
    "        min_date = pd.to_datetime(cyc_start_datetimes[1])\n",
    "        max_date = pd.to_datetime(cyc_end_datetimes[-1])\n",
    "        date_range = pd.date_range(min_date, max_date, periods=len(cyc_start_datetimes[1:]))\n",
    "        years = date_range.year.unique()\n",
    "        years = pd.to_datetime(years, format='%Y')\n",
    "        n_dates = len(cyc_start_datetimes[1:])\n",
    "        cmap = plt.get_cmap('plasma', n_dates)\n",
    "        norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "        m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        m.set_array(np.linspace(mdates.date2num(min_date), mdates.date2num(max_date), n_dates))\n",
    "        \n",
    "        # Add colorbar\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('bottom', size='2.5%', pad=0.5)\n",
    "        cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "    \n",
    "        # Set ticks for all years but labels only for odd years\n",
    "        tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "        tick_labels = [date.strftime('%Y') if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "        cbar.set_ticks(tick_locations)\n",
    "        cbar.set_ticklabels(tick_labels)\n",
    "        \n",
    "        # Add minor ticks for quarters\n",
    "        cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Quarter intervals only\n",
    "        cbar.set_label('year', size=10, labelpad=10)\n",
    "\n",
    "        # Set plot bounds\n",
    "        x_min, y_min, x_max, y_max = search_extent_poly.bounds\n",
    "        x_buffer = abs(x_max-x_min)*0.05\n",
    "        y_buffer = abs(y_max-y_min)*0.05\n",
    "        ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "        # Plot MOA surface imagery\n",
    "        mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "        mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "        moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "        ax.imshow(moa_highres_da_subset[0,:,:], cmap='gray', clim=[14000, 17000], \n",
    "                  extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "\n",
    "        # Plot stationary lakes\n",
    "        stationary_lake_color = 'darkturquoise'\n",
    "        stationary_outlines_gdf.boundary.plot(\n",
    "            ax=ax, facecolor=stationary_lake_color, linestyle='solid', linewidth=2, alpha=0.25)\n",
    "        stationary_outlines_gdf.boundary.plot(\n",
    "            ax=ax, edgecolor=stationary_lake_color, linestyle='solid', linewidth=2)\n",
    "\n",
    "        # Plot evolving outlines\n",
    "        onlake_lines, offlake_lines = [], []\n",
    "        for idx, dt in enumerate(cyc_start_datetimes[1:]):\n",
    "            x, y = 1, 1\n",
    "            date_num = mdates.date2num(pd.to_datetime(dt))\n",
    "            onlake_line, = ax.plot(x, y, color=cmap(norm(date_num)), linewidth=2)\n",
    "            onlake_lines.append(onlake_line)\n",
    "            offlake_line, = ax.plot(x, y, color=cmap(norm(date_num)), linewidth=2, alpha=0.2)\n",
    "            offlake_lines.append(offlake_line)\n",
    "            \n",
    "            evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['mid_pt_datetime'] == dt]\n",
    "            offlake_outlines_dt = offlake_outlines_gdf[offlake_outlines_gdf['mid_pt_datetime'] == dt]\n",
    "            if not evolving_outlines_dt.empty:\n",
    "                evolving_outlines_dt.boundary.plot(ax=ax, \n",
    "                    color=cmap(norm(date_num)), \n",
    "                    linewidth=0.5)\n",
    "            if not offlake_outlines_dt.empty:\n",
    "                offlake_outlines_dt.boundary.plot(ax=ax, \n",
    "                    color=cmap(norm(date_num)), \n",
    "                    linewidth=0.5, alpha=0.25)\n",
    "\n",
    "        # Format axes\n",
    "        km_scale = 1e3\n",
    "        ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "        ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_xlabel('x [km]', size=10)\n",
    "        ax.set_ylabel('y [km]', size=10)\n",
    "        \n",
    "        # Add legend\n",
    "        stationary_line = plt.Line2D([],[], color=stationary_lake_color, linestyle='solid', linewidth=2)\n",
    "        within_area_multiple_line = plt.Line2D([],[], color='dimgray', linestyle='solid', linewidth=2)\n",
    "        search_extent_line = plt.Line2D([],[], color='black', linestyle='solid', linewidth=2)\n",
    "\n",
    "        ax.legend(\n",
    "            handles=[stationary_line, \n",
    "                     tuple(onlake_lines), \n",
    "                     tuple(offlake_lines),\n",
    "                     within_area_multiple_line, \n",
    "                     search_extent_line],\n",
    "            labels=['stationary outline', \n",
    "                    f'evolving outlines ({level} m)', \n",
    "                    'off-lake evolving outlines', \n",
    "                    f'within evaluation boundary ({int(within_area_multiple)}x)',\n",
    "                    'search extent'],\n",
    "            handlelength=3,\n",
    "            handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "            loc='upper left',\n",
    "            bbox_to_anchor=(0, 1.3))\n",
    "\n",
    "        # Add inset map\n",
    "        axIns = ax.inset_axes([0.02, 0.01, 0.25, 0.25])\n",
    "        axIns.set_aspect('equal')\n",
    "        moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        axIns.scatter(\n",
    "            ((x_max+x_min)/2), ((y_max+y_min)/2),\n",
    "            marker='*', linewidth=0.1, color='k', s=30, zorder=3\n",
    "        )\n",
    "        axIns.axis('off')\n",
    "\n",
    "        # Add title\n",
    "        ax.set_title(f'{lake_name}', size=12, y=1.3)\n",
    "\n",
    "        # Generate output filename and save\n",
    "        output_filename = os.path.join(OUTPUT_DIR, 'plot_evolving_outlines_time_series',\n",
    "            f'{lake_name}_{int(row_index)}-idx_{level}m-level_{int(within_area_multiple)}x-within.png'\n",
    "        )\n",
    "        \n",
    "        plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f'Successfully saved plot to: {output_filename}')\n",
    "        \n",
    "        plt.close()\n",
    "\n",
    "        # Clear intermediate objects to conserve memory\n",
    "        try:\n",
    "            plt.close('all')\n",
    "            del moa_highres_da_subset\n",
    "            del onlake_lines, offlake_lines\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f'Error cleaning up plot resources: {e}')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error in plot_evolving_outlines_time_series function for {lake_name}:')\n",
    "        print(f'Error message: {str(e)}')\n",
    "        print('Error traceback:')\n",
    "        traceback.print_exc()\n",
    "        plt.close('all')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182521e4-b89f-41b7-b1fe-92ec47350fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_group_gdf(stationary_outlines_gdf, lake_group):\n",
    "    '''\n",
    "    Prepare a GeoDataFrame row representing a group of lakes.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    stationary_outlines_gdf : GeoDataFrame\n",
    "        The complete dataset of stationary lakes\n",
    "    lake_group : list\n",
    "        List of lake names to be analyzed together\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    GeoDataFrame\n",
    "        Single-row GeoDataFrame with combined group properties\n",
    "    '''\n",
    "    try:\n",
    "        # Filter GeoDataFrame for lakes in the group\n",
    "        group_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'].isin(lake_group)].copy()\n",
    "        if group_gdf.empty:\n",
    "            print(f'No lakes found for group: {lake_group}')\n",
    "            return None\n",
    "            \n",
    "        print(f'Preparing group geodataframe for lake group: {lake_group}')\n",
    "        \n",
    "        # Create a combined name for the group\n",
    "        group_name = '_'.join(lake_group)\n",
    "        \n",
    "        # Create a union of all lake geometries for the group\n",
    "        group_stationary_outline = group_gdf.geometry.union_all()\n",
    "        if not group_stationary_outline.is_valid:\n",
    "            print('Cleaning group geometry...')\n",
    "            group_stationary_outline = make_valid(group_stationary_outline)\n",
    "        \n",
    "        # Determine the group's time period\n",
    "        group_time_period = determine_group_time_period(group_gdf['CS2_SARIn_start'])\n",
    "        print(f'Group CryoSat-2 SARIn time period determined as: {group_time_period}')\n",
    "        \n",
    "        # Create a GeoDataFrame for the group\n",
    "        group_single_gdf = gpd.GeoDataFrame(\n",
    "            {\n",
    "                'name': [group_name],\n",
    "                'geometry': [group_stationary_outline],\n",
    "                'CS2_SARIn_start': [group_time_period]\n",
    "            },\n",
    "            crs=group_gdf.crs\n",
    "        )\n",
    "        \n",
    "        return group_single_gdf\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error preparing group GeoDataFrame: {str(e)}')\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def determine_group_time_period(time_periods):\n",
    "    '''\n",
    "    Determine the most exclusive time period for a group of lakes.\n",
    "    \n",
    "    Rules:\n",
    "    - If any lake has <NA>, group gets <NA>\n",
    "    - If all lakes have '2010.5', group gets '2010.5'\n",
    "    - If all lakes have either '2013.75' or '2010.5', group gets '2013.75'\n",
    "    - Otherwise, group gets <NA>\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    time_periods : Series\n",
    "        Series of time periods from the group of lakes\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str or pd.NA\n",
    "        The determined time period for the group\n",
    "    '''\n",
    "    # If any lake has NA, group gets NA\n",
    "    if time_periods.isna().any():\n",
    "        return pd.NA\n",
    "        \n",
    "    # Convert to list and remove any NA values\n",
    "    periods = [p for p in time_periods if pd.notna(p)]\n",
    "    \n",
    "    # If all lakes have '2010.5'\n",
    "    if all(p == '2010.5' for p in periods):\n",
    "        return '2010.5'\n",
    "        \n",
    "    # If all lakes have either '2013.75' or '2010.5'\n",
    "    valid_periods = {'2013.75', '2010.5'}\n",
    "    if all(p in valid_periods for p in periods):\n",
    "        return '2013.75'\n",
    "        \n",
    "    # Default to NA for any other case\n",
    "    return pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a661a49-c0a1-41ed-a566-2cb87bef95c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard_lake_outlines(\n",
    "    lake_outlines_to_discard: List[str],\n",
    "    source_dir: str,\n",
    "    dest_dir: str\n",
    ") -> Dict[str, Tuple[bool, str]]:\n",
    "    '''\n",
    "    Move lake outlines from git repo to non-git repo and create indicator files.\n",
    "    Replaces existing files in destination directory.\n",
    "    \n",
    "    Args:\n",
    "        lake_outlines_to_discard: List of lake names to process\n",
    "        output_dir: Path to destination non-git directory\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with lake names as keys and tuples of (success_bool, message) as values\n",
    "    '''\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    \n",
    "    results = {}\n",
    "    for lake in lake_outlines_to_discard:\n",
    "        source_filepath = os.path.join(source_dir, f'{lake}.geojson')\n",
    "        dest_filepath = os.path.join(dest_dir, f'{lake}.geojson')\n",
    "        txt_filepath = os.path.join(source_dir, f'{lake}.txt')\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(source_filepath):\n",
    "                results[lake] = (False, f'Source file does not exist: {source_filepath}')\n",
    "                continue\n",
    "                \n",
    "            # Remove check for existing destination file\n",
    "            # Use copy2 then remove original to ensure atomic operation\n",
    "            shutil.copy2(source_filepath, dest_filepath)\n",
    "            os.remove(source_filepath)\n",
    "            results[lake] = (True, 'Successfully moved and replaced existing file')\n",
    "            \n",
    "            write_no_outlines(txt_filepath)\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[lake] = (False, f'Error: {str(e)}')\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9af54f-6d45-41e5-b906-b19adf1af2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_forward_fill_files(input_dir, output_dir, mid_pt_datetimes):\n",
    "    '''\n",
    "    Generate forward fill or Last Observation Carried Forward (LOCF) versions of lake outline GeoJSON files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dir : str\n",
    "        Directory containing the original lake outline GeoJSON files\n",
    "    output_dir : str\n",
    "        Directory where the forward filled GeoJSON files will be saved\n",
    "    mid_pt_datetimes : list\n",
    "        List of all dh mid point datetimes (as datetime objects or strings) to use for the complete multimission time series\n",
    "    '''\n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Ensure cyc_start_datetimes are datetime objects and sorted\n",
    "    mid_pt_datetimes = [pd.to_datetime(date) for date in mid_pt_datetimes]\n",
    "    mid_pt_datetimes = sorted(mid_pt_datetimes)\n",
    "    \n",
    "    # Get list of all GeoJSON files in input directory\n",
    "    geojson_files = [f for f in os.listdir(input_dir) if f.endswith('.geojson')]\n",
    "    \n",
    "    for file in geojson_files:\n",
    "        lake_name = os.path.splitext(file)[0]\n",
    "        print(f'Processing lake: {lake_name}')\n",
    "        \n",
    "        # Read the original GeoDataFrame\n",
    "        input_file = os.path.join(input_dir, file)\n",
    "        try:\n",
    "            gdf = gpd.read_file(input_file)\n",
    "        except Exception as e:\n",
    "            print(f'Error reading {input_file}: {e}')\n",
    "            continue\n",
    "            \n",
    "        if gdf.empty:\n",
    "            print(f'Empty GeoDataFrame for {lake_name}, skipping.')\n",
    "            continue\n",
    "            \n",
    "        if 'mid_pt_datetime' in gdf.columns:\n",
    "            gdf['mid_pt_datetime'] = pd.to_datetime(gdf['mid_pt_datetime'])\n",
    "            gdf = gdf.sort_values('mid_pt_datetime')\n",
    "            \n",
    "            # Create a new empty GeoDataFrame with the same structure as the input\n",
    "            forward_fill_rows = []\n",
    "            \n",
    "            last_valid_row = None\n",
    "            \n",
    "            # Use the provided mid_pt_datetimes for forward fill processing\n",
    "            for date in mid_pt_datetimes:\n",
    "                # Get rows for the current date\n",
    "                current_date_rows = gdf[gdf['mid_pt_datetime'] == date]\n",
    "                \n",
    "                if not current_date_rows.empty:\n",
    "                    # If we have data for this date, add all rows\n",
    "                    for _, row in current_date_rows.iterrows():\n",
    "                        forward_fill_rows.append(row.copy())\n",
    "                    \n",
    "                    # Update the last valid observation\n",
    "                    last_valid_row = current_date_rows.iloc[-1].copy()\n",
    "                elif last_valid_row is not None:\n",
    "                    # If no data for this date but we have a previous observation, carry it forward\n",
    "                    new_row = last_valid_row.copy()\n",
    "                    new_row['mid_pt_datetime'] = date\n",
    "                    forward_fill_rows.append(new_row)\n",
    "                    print(f'  Carried forward observation from {last_valid_row[\"mid_pt_datetime\"]} to {date}')\n",
    "            \n",
    "            # Create the result GeoDataFrame from the collected rows\n",
    "            if forward_fill_rows:\n",
    "                forward_fill_gdf = gpd.GeoDataFrame(forward_fill_rows, crs=gdf.crs)\n",
    "                \n",
    "                # Save the forward fill GeoDataFrame\n",
    "                output_file = os.path.join(output_dir, file)\n",
    "                forward_fill_gdf.to_file(output_file, driver='GeoJSON')\n",
    "                print(f'  Saved forward fill file to {output_file}')\n",
    "            else:\n",
    "                print(f'  No rows after forward fill for {lake_name}')\n",
    "            # Clear output of each index\n",
    "            clear_output(wait=True)\n",
    "        else:\n",
    "            print(f'  No \"mid_pt_datetime\" column found in {lake_name}, skipping file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4052cf-55d9-491e-803e-bff17de41c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_evolving_outlines_versions(filename, cyc_start_datetimes):\n",
    "    '''\n",
    "    Compare the same GeoJSON filename in two different directories (regular and forward fill versions)\n",
    "    to confirm that forward fill operation occurred correctly.\n",
    "    \n",
    "    Parameters:\n",
    "    filename (str): The name of the GeoJSON file to compare\n",
    "    cyc_start_datetimes (list): List of dates to compare against\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the comparison of dates and associated geometries\n",
    "    '''\n",
    "    import os\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    from datetime import datetime\n",
    "    import re\n",
    "    \n",
    "    # Define the two paths\n",
    "    path1 = f'output/lake_outlines/evolving_outlines/{filename}.geojson'\n",
    "    path2 = f'output/lake_outlines/evolving_outlines/forward_fill/{filename}.geojson'\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not os.path.exists(path1):\n",
    "        raise FileNotFoundError(f'File not found: {path1}')\n",
    "    if not os.path.exists(path2):\n",
    "        raise FileNotFoundError(f'File not found: {path2}')\n",
    "    \n",
    "    # Initialize variables to store dates and geometries\n",
    "    dates1 = []\n",
    "    dates2 = []\n",
    "    geometries1 = []\n",
    "    geometries2 = []\n",
    "    \n",
    "    # Read GeoJSON files using geopandas\n",
    "    try:\n",
    "        gdf1 = gpd.read_file(path1)\n",
    "        gdf2 = gpd.read_file(path2)\n",
    "        \n",
    "        # Store the full GeoDataFrames for later geometry extraction\n",
    "        gdf1_full = gdf1.copy()\n",
    "        gdf2_full = gdf2.copy()\n",
    "    except Exception as e:\n",
    "        # Fallback to manual JSON parsing if geopandas fails\n",
    "        print(f'Falling back to manual JSON parsing: {e}')\n",
    "        try:\n",
    "            with open(path1, 'r') as f1:\n",
    "                data1 = json.load(f1)\n",
    "            with open(path2, 'r') as f2:\n",
    "                data2 = json.load(f2)\n",
    "                \n",
    "            # Extract properties and geometries from features\n",
    "            dates1 = []\n",
    "            dates2 = []\n",
    "            geometries1 = []\n",
    "            geometries2 = []\n",
    "            \n",
    "            # Process file 1\n",
    "            if 'features' in data1:\n",
    "                for feature in data1['features']:\n",
    "                    if 'properties' in feature and 'cyc_start_datetimes' in feature['properties']:\n",
    "                        dates1.append(feature['properties']['cyc_start_datetimes'])\n",
    "                        geometries1.append(feature.get('geometry', None))\n",
    "            elif 'properties' in data1 and 'cyc_start_datetimes' in data1['properties']:\n",
    "                # Handle case when the file is a single feature\n",
    "                dates1.append(data1['properties']['cyc_start_datetimes'])\n",
    "                geometries1.append(data1.get('geometry', None))\n",
    "                \n",
    "            # Process file 2\n",
    "            if 'features' in data2:\n",
    "                for feature in data2['features']:\n",
    "                    if 'properties' in feature and 'cyc_start_datetimes' in feature['properties']:\n",
    "                        dates2.append(feature['properties']['cyc_start_datetimes'])\n",
    "                        geometries2.append(feature.get('geometry', None))\n",
    "            elif 'properties' in data2 and 'cyc_start_datetimes' in data2['properties']:\n",
    "                # Handle case when the file is a single feature\n",
    "                dates2.append(data2['properties']['cyc_start_datetimes'])\n",
    "                geometries2.append(data2.get('geometry', None))\n",
    "        except Exception as e:\n",
    "            raise ValueError(f'Failed to parse GeoJSON file: {e}')\n",
    "    else:\n",
    "        # If geopandas succeeded, extract the mid-point datetime column and geometries\n",
    "        date_col_name = None\n",
    "        \n",
    "        # Determine which column contains the dates\n",
    "        if 'mid_pt_datetime' in gdf1.columns:\n",
    "            date_col_name = 'mid_pt_datetime'\n",
    "        elif 'properties' in gdf1.columns and isinstance(gdf1['properties'].iloc[0], dict):\n",
    "            # Handle case where properties are stored as a dictionary in a column\n",
    "            date_col_name = 'properties'\n",
    "        else:\n",
    "            # Try to find the column in properties\n",
    "            for col in gdf1.columns:\n",
    "                if 'mid_pt_datetime' in col:\n",
    "                    date_col_name = col\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError('Could not find mid_pt_datetime in the first file')\n",
    "        \n",
    "        # Extract dates and geometries from file 1\n",
    "        if date_col_name == 'properties':\n",
    "            # Extract from properties dictionary\n",
    "            dates1_with_idx = []\n",
    "            for idx, prop in enumerate(gdf1['properties']):\n",
    "                if prop and 'mid_pt_datetime' in prop:\n",
    "                    dates1_with_idx.append((idx, prop.get('mid_pt_datetime')))\n",
    "                elif prop and 'cyc_start_datetimes' in prop:\n",
    "                    dates1_with_idx.append((idx, prop.get('cyc_start_datetimes')))\n",
    "            \n",
    "            dates1 = [date for _, date in dates1_with_idx]\n",
    "            geometries1 = [gdf1.iloc[idx]['geometry'] for idx, _ in dates1_with_idx]\n",
    "        else:\n",
    "            # Extract directly from columns\n",
    "            dates1 = gdf1[date_col_name].tolist()\n",
    "            geometries1 = gdf1['geometry'].tolist()\n",
    "        \n",
    "        # Same for file 2\n",
    "        if 'mid_pt_datetime' in gdf2.columns:\n",
    "            date_col_name = 'mid_pt_datetime'\n",
    "        elif 'properties' in gdf2.columns and isinstance(gdf2['properties'].iloc[0], dict):\n",
    "            date_col_name = 'properties'\n",
    "        else:\n",
    "            # Try to find the column in properties\n",
    "            for col in gdf2.columns:\n",
    "                if 'mid_pt_datetime' in col:\n",
    "                    date_col_name = col\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError('Could not find mid_pt_datetime in the second file')\n",
    "        \n",
    "        # Extract dates and geometries from file 2\n",
    "        if date_col_name == 'properties':\n",
    "            # Extract from properties dictionary\n",
    "            dates2_with_idx = []\n",
    "            for idx, prop in enumerate(gdf2['properties']):\n",
    "                if prop and 'mid_pt_datetime' in prop:\n",
    "                    dates2_with_idx.append((idx, prop.get('mid_pt_datetime')))\n",
    "            \n",
    "            dates2 = [date for _, date in dates2_with_idx]\n",
    "            geometries2 = [gdf2.iloc[idx]['geometry'] for idx, _ in dates2_with_idx]\n",
    "        else:\n",
    "            # Extract directly from columns\n",
    "            dates2 = gdf2[date_col_name].tolist()\n",
    "            geometries2 = gdf2['geometry'].tolist()\n",
    "    \n",
    "    # Print debug info\n",
    "    print(f'Found {len(dates1)} dates with evolving outlines in regular file')\n",
    "    print(f'Found {len(dates2)} dates with evolving outlines in forward fill file')\n",
    "    print(f'Number of dates in cyc_start_datetimes: {len(cyc_start_datetimes)}')\n",
    "    \n",
    "    # Function to standardize date formats\n",
    "    def standardize_date(date_str):\n",
    "        if not date_str:\n",
    "            return None\n",
    "            \n",
    "        # Convert to string if it's not already\n",
    "        if not isinstance(date_str, str):\n",
    "            date_str = str(date_str)\n",
    "            \n",
    "        # Remove microseconds if present\n",
    "        date_str = re.sub(r'\\.(\\d+)', '', date_str)\n",
    "        \n",
    "        # Standardize T separator and space\n",
    "        date_str = date_str.replace('T', ' ')\n",
    "        \n",
    "        # Only keep date and time, remove timezone info if present\n",
    "        if '+' in date_str:\n",
    "            date_str = date_str.split('+')[0].strip()\n",
    "        \n",
    "        # Ensure we have seconds\n",
    "        if len(date_str.split(':')) == 2:\n",
    "            date_str += ':00'\n",
    "            \n",
    "        # Try to parse and re-format to ensure consistency\n",
    "        try:\n",
    "            # Parse the date with different formats\n",
    "            for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M']:\n",
    "                try:\n",
    "                    parsed_date = datetime.strptime(date_str, fmt)\n",
    "                    # Return in a standardized format\n",
    "                    return parsed_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            \n",
    "            # If none of the formats worked, return the original string\n",
    "            return date_str\n",
    "        except Exception:\n",
    "            # If parsing fails, return the cleaned string\n",
    "            return date_str\n",
    "    \n",
    "    # Standardize all dates\n",
    "    cyc_start_datetimes_std = [standardize_date(date) for date in cyc_start_datetimes]\n",
    "    dates1_std = [standardize_date(date) for date in dates1]\n",
    "    dates2_std = [standardize_date(date) for date in dates2]\n",
    "    \n",
    "    # Create dictionaries to map standardized dates to their geometries\n",
    "    date1_to_geom = {std_date: geom for std_date, geom in zip(dates1_std, geometries1) if std_date}\n",
    "    date2_to_geom = {std_date: geom for std_date, geom in zip(dates2_std, geometries2) if std_date}\n",
    "    \n",
    "    # Create a set of all unique standardized dates across all sources\n",
    "    all_dates = set(cyc_start_datetimes_std) | set(dates1_std) | set(dates2_std)\n",
    "    all_dates = [date for date in all_dates if date]  # Filter out None values\n",
    "    \n",
    "    # Create comparison DataFrame with both original and standardized dates\n",
    "    comparison = pd.DataFrame({\n",
    "        'Date': sorted(list(all_dates)),\n",
    "        'In cyc_start_datetimes': [date in cyc_start_datetimes_std for date in sorted(list(all_dates))],\n",
    "        'In regular file': [date in dates1_std for date in sorted(list(all_dates))],\n",
    "        'In forward fill file': [date in dates2_std for date in sorted(list(all_dates))],\n",
    "        'Regular file geometry': [date1_to_geom.get(date, None) for date in sorted(list(all_dates))],\n",
    "        'Forward fill file geometry': [date2_to_geom.get(date, None) for date in sorted(list(all_dates))],\n",
    "    })\n",
    "    \n",
    "    # Add original formats for reference\n",
    "    comparison['Dates in cyc_start_datetimes'] = [\n",
    "        next((d for d in cyc_start_datetimes if standardize_date(d) == date), None)\n",
    "        for date in comparison['Date']\n",
    "    ]\n",
    "    comparison['Dates in regular file'] = [\n",
    "        next((d for d in dates1 if standardize_date(d) == date), None) \n",
    "        for date in comparison['Date']\n",
    "    ]\n",
    "    comparison['Dates in forward fill file'] = [\n",
    "        next((d for d in dates2 if standardize_date(d) == date), None) \n",
    "        for date in comparison['Date']\n",
    "    ]\n",
    "    \n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e418c6d-e384-4788-8482-1a786f3c9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolving_outlines_geom_calc(stationary_outline_gdf, dataset1, dataset2, forward_fill=False): \n",
    "    '''\n",
    "    Create dataframe of active area, dh, dV calculations for evolving compared to stationary outlines\n",
    "    Area measurements rounded to nearest whole number due to 1-km grid resolution\n",
    "    Height measurements rounded to 2 decimal places to match 9 cm measurement precision\n",
    "    Volume measurements rounded to whole numbers due to combined uncertainties\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    stationary_outline_gdf : GeoDataFrame\n",
    "        Contains stationary lake outline information\n",
    "    dataset1 : xarray.Dataset\n",
    "        First dataset with delta_h variable\n",
    "    dataset2 : xarray.Dataset\n",
    "        Second dataset with delta_h variable\n",
    "    forward_fill : bool, default=False\n",
    "        If True, use Last Observation Carried Forward (LOCF) methodology by reading \n",
    "        from forward-filled geojson files\n",
    "    '''\n",
    "    # Define lake name and polygon and buffered polygon to use as off-lake region\n",
    "    lake_name = stationary_outline_gdf['name'].iloc[0]\n",
    "    print(f'Processing lake with evolving_outlines_geom_calc func: {lake_name}')\n",
    "\n",
    "    # Open evolving outlines geodataframe\n",
    "    try:\n",
    "        # Choose the appropriate path based on forward_fill parameter\n",
    "        if forward_fill:\n",
    "            outlines_path = os.path.join(os.getcwd(), \n",
    "                'output/lake_outlines/evolving_outlines/forward_fill/{}.geojson'.format(lake_name))\n",
    "        else:\n",
    "            outlines_path = os.path.join(os.getcwd(), \n",
    "                'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name))\n",
    "            \n",
    "        evolving_outlines_gdf = gpd.read_file(outlines_path)\n",
    "    except fiona.errors.DriverError:\n",
    "        print(f'File for {lake_name} not found. Skipping...')\n",
    "        return  \n",
    "\n",
    "    # Ensure there are outlines in outines_gdf\n",
    "    if evolving_outlines_gdf.empty:\n",
    "        print('There are no evolving outlines in geodataframe.')\n",
    "        return  \n",
    "\n",
    "    # Define region of interest for slicing from evolving and stationary outlines\n",
    "    evolving_union = evolving_outlines_gdf.union_all()\n",
    "    evolving_union_region = area_multiple_buffer(evolving_union, 2, exclude_inner=True)\n",
    "    x_min, y_min, x_max, y_max = evolving_union_region.bounds\n",
    "\n",
    "    # Create empty lists to store metrics\n",
    "    mid_pt_datetimes = []\n",
    "    evolving_outlines_areas = []\n",
    "    evolving_outlines_dhs = []\n",
    "    evolving_region_dhs = []\n",
    "    evolving_outlines_dhs_corr = []\n",
    "    evolving_outlines_dVs_corr = []\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore', category=RuntimeWarning)\n",
    "\n",
    "        # Process dataset1 if available\n",
    "        if dataset1 is not None:\n",
    "            # Create masks and calculations for dataset1\n",
    "            dataset1_ROI_subset = dataset1.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "            dataset1_ROI_da = dataset1_ROI_subset['delta_h']\n",
    "            \n",
    "            dataset1_evolving_region_mask = np.array([[evolving_union_region.contains(Point(x, y)) \n",
    "                                                      for x in dataset1_ROI_da.x.values] \n",
    "                                                      for y in dataset1_ROI_da.y.values])\n",
    "            dataset1_evolving_region_masked = dataset1_ROI_da.where(xr.DataArray(dataset1_evolving_region_mask, \n",
    "                                                            coords=[dataset1_ROI_da.y, dataset1_ROI_da.x], \n",
    "                                                            dims=['y', 'x']))\n",
    "            \n",
    "            dataset1_ROI_dh = dataset1_ROI_da.diff('time')\n",
    "            dataset1_evolving_region_dh = dataset1_evolving_region_masked.diff('time')\n",
    "\n",
    "            # Calculate mid-point datetimes\n",
    "            dataset1_mid_pt_datetimes = []\n",
    "            dataset1_datetimes = dataset1_ROI_da['time'].values\n",
    "            for i in range(1, len(dataset1_datetimes)):\n",
    "                mid_pt_days = dataset1_datetimes[i] - dataset1_datetimes[i-1]\n",
    "                mid_pt_datetime = dataset1_datetimes[i-1] + mid_pt_days\n",
    "                dataset1_mid_pt_datetimes.append(mid_pt_datetime)\n",
    "            dataset1_mid_pt_datetimes = np.array(dataset1_mid_pt_datetimes)\n",
    "\n",
    "            for i, mid_pt_datetime in enumerate(dataset1_mid_pt_datetimes):\n",
    "                # Get the subset for this timestep\n",
    "                timestep_subset_evolving_outlines_gdf = evolving_outlines_gdf[\n",
    "                    evolving_outlines_gdf['mid_pt_datetime'] == mid_pt_datetime]\n",
    "                \n",
    "                evolving_outlines_geom_calc_process_timestep(\n",
    "                    evolving_region_dh_slice=dataset1_evolving_region_dh.isel(time=i),\n",
    "                    timestep_subset_evolving_outlines_gdf=timestep_subset_evolving_outlines_gdf,\n",
    "                    ROI_dh_slice=dataset1_ROI_dh.isel(time=i),\n",
    "                    timestep=mid_pt_datetime,\n",
    "                    mid_pt_datetimes=mid_pt_datetimes,\n",
    "                    evolving_outlines_areas=evolving_outlines_areas,\n",
    "                    evolving_outlines_dhs=evolving_outlines_dhs,\n",
    "                    evolving_region_dhs=evolving_region_dhs,\n",
    "                    evolving_outlines_dhs_corr=evolving_outlines_dhs_corr,\n",
    "                    evolving_outlines_dVs_corr=evolving_outlines_dVs_corr\n",
    "                )\n",
    "\n",
    "        # Process dataset2\n",
    "        dataset2_ROI_subset = dataset2.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "        dataset2_ROI_da = dataset2_ROI_subset['delta_h']\n",
    "        \n",
    "        dataset2_evolving_region_mask = np.array([[evolving_union_region.contains(Point(x, y)) \n",
    "                                                  for x in dataset2_ROI_da.x.values] \n",
    "                                                  for y in dataset2_ROI_da.y.values])\n",
    "        dataset2_evolving_region_masked = dataset2_ROI_da.where(xr.DataArray(dataset2_evolving_region_mask, \n",
    "                                                        coords=[dataset2_ROI_da.y, dataset2_ROI_da.x], \n",
    "                                                        dims=['y', 'x']))\n",
    "\n",
    "        dataset2_ROI_dh = dataset2_ROI_da.diff('time')\n",
    "        dataset2_evolving_region_dh = dataset2_evolving_region_masked.diff('time')\n",
    "\n",
    "        # Calculate mid-point datetimes\n",
    "        dataset2_mid_pt_datetimes = []\n",
    "        dataset2_datetimes = dataset2_ROI_da['time'].values\n",
    "        for i in range(1, len(dataset2_datetimes)):\n",
    "            mid_pt_days = dataset2_datetimes[i] - dataset2_datetimes[i-1]\n",
    "            mid_pt_datetime = dataset2_datetimes[i-1] + mid_pt_days\n",
    "            dataset2_mid_pt_datetimes.append(mid_pt_datetime)\n",
    "        dataset2_mid_pt_datetimes = np.array(dataset2_mid_pt_datetimes)\n",
    "\n",
    "        for i, mid_pt_datetime in enumerate(dataset2_mid_pt_datetimes):\n",
    "            # Get the subset for this timestep\n",
    "            timestep_subset_evolving_outlines_gdf = evolving_outlines_gdf[\n",
    "                evolving_outlines_gdf['mid_pt_datetime'] == mid_pt_datetime]\n",
    "            \n",
    "            evolving_outlines_geom_calc_process_timestep(\n",
    "                evolving_region_dh_slice=dataset2_evolving_region_dh.isel(time=i),\n",
    "                timestep_subset_evolving_outlines_gdf=timestep_subset_evolving_outlines_gdf,\n",
    "                ROI_dh_slice=dataset2_ROI_dh.isel(time=i),\n",
    "                timestep=mid_pt_datetime,\n",
    "                mid_pt_datetimes=mid_pt_datetimes,\n",
    "                evolving_outlines_areas=evolving_outlines_areas,\n",
    "                evolving_outlines_dhs=evolving_outlines_dhs,\n",
    "                evolving_region_dhs=evolving_region_dhs,\n",
    "                evolving_outlines_dhs_corr=evolving_outlines_dhs_corr,\n",
    "                evolving_outlines_dVs_corr=evolving_outlines_dVs_corr\n",
    "            )\n",
    "\n",
    "    # Create dataframe if we have data\n",
    "    if len(mid_pt_datetimes) > 0:\n",
    "        d = {\n",
    "            'mid_pt_datetime': mid_pt_datetimes,\n",
    "            'evolving_outlines_area (m^2)': [round(x, 0) for x in evolving_outlines_areas],  # Round to whole numbers\n",
    "            'evolving_outlines_dh (m)': [round(x, 2) for x in evolving_outlines_dhs],  # 2 decimals for height\n",
    "            'evolving_outlines_region_dh (m)': [round(x, 2) for x in evolving_region_dhs],  # 2 decimals for height\n",
    "            'evolving_outlines_dh_corr (m)': [round(x, 2) for x in evolving_outlines_dhs_corr],  # 2 decimals for height\n",
    "            'evolving_outlines_dV_corr (m^3)': [round(x, 0) for x in evolving_outlines_dVs_corr],  # Round to whole numbers\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(d)\n",
    "        df = df.fillna(0.0)\n",
    "        \n",
    "        # Convert area and volume columns to integer type since we're using whole numbers\n",
    "        df['evolving_outlines_area (m^2)'] = df['evolving_outlines_area (m^2)'].astype(int)\n",
    "        df['evolving_outlines_dV_corr (m^3)'] = df['evolving_outlines_dV_corr (m^3)'].astype(int)\n",
    "\n",
    "        # Define the appropriate output path based on forward_fill parameter\n",
    "        if forward_fill:\n",
    "            output_path = f'output/geometric_calcs/evolving_outlines_geom_calc/forward_fill'\n",
    "        else:\n",
    "            output_path = f'output/geometric_calcs/evolving_outlines_geom_calc/'\n",
    "\n",
    "        output_file = os.path.join(output_path, f'{lake_name}.csv')\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f'Results saved to: {output_file}')\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(f'No data processed for {lake_name}')\n",
    "        return None\n",
    "\n",
    "def evolving_outlines_geom_calc_process_timestep(evolving_region_dh_slice,\n",
    "                     timestep_subset_evolving_outlines_gdf,\n",
    "                     ROI_dh_slice,\n",
    "                     timestep,\n",
    "                     mid_pt_datetimes,\n",
    "                     evolving_outlines_areas,\n",
    "                     evolving_outlines_dhs,\n",
    "                     evolving_region_dhs,\n",
    "                     evolving_outlines_dhs_corr,\n",
    "                     evolving_outlines_dVs_corr):\n",
    "    '''\n",
    "    Process a single timestep of lake dh data and calculate various metrics.\n",
    "    dh measurements rounded to two decimal places (due to ~9 cm precision)\n",
    "    Area measurements rounded to whole numbers (due to 1-km grid resolution)\n",
    "    dV measurements rounded to whole numbers (due to combined uncertainty)\n",
    "    ''' \n",
    "    # Initialize evolving outlines variables with default values of 0\n",
    "    evolving_outlines_area = 0\n",
    "    evolving_outlines_dh = 0.0\n",
    "    evolving_outlines_dh_corr = 0.0\n",
    "    evolving_outlines_dV_corr = 0\n",
    "\n",
    "    # Calculate metrics for evolving union region\n",
    "    evolving_region_dh = round(float(np.nanmean(evolving_region_dh_slice)), 2)  # 2 decimals for height\n",
    "\n",
    "    if not timestep_subset_evolving_outlines_gdf.empty:\n",
    "        # Calculate metrics for evolving outlines\n",
    "        evolving_outlines_area = round(float(timestep_subset_evolving_outlines_gdf['area (m^2)'].sum()), 0)\n",
    "        union_timestep_subset_evolving_outlines = timestep_subset_evolving_outlines_gdf['geometry'].union_all()\n",
    "        union_timestep_subset_evolving_outlines_mask = np.array([[union_timestep_subset_evolving_outlines.contains(Point(x, y)) \n",
    "                                                                for x in ROI_dh_slice['x'].values] \n",
    "                                                                for y in ROI_dh_slice['y'].values])\n",
    "        union_timestep_subset_evolving_outlines_masked = ROI_dh_slice.where(xr.DataArray(union_timestep_subset_evolving_outlines_mask, \n",
    "                                                                 coords=[ROI_dh_slice.y, ROI_dh_slice.x], \n",
    "                                                                 dims=['y', 'x']))\n",
    "\n",
    "        evolving_outlines_dh = round(float(np.nanmean(union_timestep_subset_evolving_outlines_masked)), 2)  # 2 decimals\n",
    "        evolving_outlines_dh_corr = round(evolving_outlines_dh - evolving_region_dh, 2)  # 2 decimals\n",
    "        evolving_outlines_dV_corr = round(evolving_outlines_dh_corr * evolving_outlines_area, 0)  # Whole numbers\n",
    "    \n",
    "    # Append all metrics to lists\n",
    "    mid_pt_datetimes.append(timestep)\n",
    "    evolving_outlines_areas.append(evolving_outlines_area)\n",
    "    evolving_outlines_dhs.append(evolving_outlines_dh)\n",
    "    evolving_region_dhs.append(evolving_region_dh)\n",
    "    evolving_outlines_dhs_corr.append(evolving_outlines_dh_corr)\n",
    "    evolving_outlines_dVs_corr.append(evolving_outlines_dV_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862cd63c-acff-4cf7-b1d6-91786a521ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationary_outline_geom_calc(stationary_outline_gdf, dataset1, dataset2, sub_dir): \n",
    "    '''\n",
    "    Create dataframe of active area, dh, dV calculations for evolving compared to stationary outlines\n",
    "    Area measurements rounded to nearest whole number due to 1-km grid resolution\n",
    "    Height measurements rounded to two decimal places to match ~9 cm measurement precision\n",
    "    Volume measurements rounded to whole numbers due to combined uncertainties\n",
    "    '''\n",
    "    # Define lake name and polygon and buffered polygon to use as off-lake region\n",
    "    lake_name = stationary_outline_gdf['name'].iloc[0]\n",
    "    print(f'Processing lake with stationary_outline_geom_calc func: {lake_name}')\n",
    "    stationary_outline = stationary_outline_gdf['geometry'].iloc[0]\n",
    "    stationary_region = area_multiple_buffer(stationary_outline, 2, exclude_inner=True)\n",
    "    \n",
    "    # Create empty lists to store metrics\n",
    "    mid_pt_datetimes = []\n",
    "    stationary_outline_dhs = []\n",
    "    stationary_region_dhs = []\n",
    "    stationary_outline_dhs_corr = []\n",
    "    stationary_dVs_corr = []\n",
    "\n",
    "    # Get bounds for data slicing\n",
    "    x_min, y_min, x_max, y_max = stationary_region.bounds\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore', category=RuntimeWarning)\n",
    "        \n",
    "        # Process dataset1 if available\n",
    "        if dataset1 is not None:\n",
    "            # Prepare dataset1 masks and calculations\n",
    "            dataset1_ROI_subset = dataset1.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "            dataset1_ROI_da = dataset1_ROI_subset['delta_h']\n",
    "            \n",
    "            # Create masks\n",
    "            dataset1_stationary_mask = np.array([[stationary_outline.contains(Point(x, y)) \n",
    "                                                for x in dataset1_ROI_da.x.values] \n",
    "                                                for y in dataset1_ROI_da.y.values])\n",
    "            dataset1_stationary_masked = dataset1_ROI_da.where(xr.DataArray(dataset1_stationary_mask, \n",
    "                                                            coords=[dataset1_ROI_da.y, dataset1_ROI_da.x],\n",
    "                                                            dims=['y', 'x']))\n",
    "            \n",
    "            dataset1_stationary_region_mask = np.array([[stationary_region.contains(Point(x, y)) \n",
    "                                                        for x in dataset1_ROI_da.x.values] \n",
    "                                                        for y in dataset1_ROI_da.y.values])\n",
    "            dataset1_stationary_region_masked = dataset1_ROI_da.where(xr.DataArray(dataset1_stationary_region_mask, \n",
    "                                                            coords=[dataset1_ROI_da.y, dataset1_ROI_da.x], \n",
    "                                                            dims=['y', 'x']))\n",
    "\n",
    "            # Calculate dh differences\n",
    "            dataset1_ROI_dh = dataset1_ROI_da.diff('time')\n",
    "            dataset1_stationary_dh = dataset1_stationary_masked.diff('time')\n",
    "            dataset1_stationary_region_dh = dataset1_stationary_region_masked.diff('time')\n",
    "\n",
    "            # Calculate mid-point datetimes\n",
    "            dataset1_mid_pt_datetimes = []\n",
    "            dataset1_datetimes = dataset1_ROI_da['time'].values\n",
    "            for i in range(1, len(dataset1_datetimes)):\n",
    "                mid_pt_days = dataset1_datetimes[i] - dataset1_datetimes[i-1]\n",
    "                mid_pt_datetime = dataset1_datetimes[i-1] + mid_pt_days\n",
    "                dataset1_mid_pt_datetimes.append(mid_pt_datetime)\n",
    "            dataset1_mid_pt_datetimes = np.array(dataset1_mid_pt_datetimes)\n",
    "            \n",
    "            # Process timesteps for dataset1\n",
    "            for i, mid_pt_datetime in enumerate(dataset1_mid_pt_datetimes):\n",
    "                stationary_outline_geom_calc_process_timestep(\n",
    "                    ROI_dh_slice=dataset1_ROI_dh.isel(time=i),\n",
    "                    stationary_dh_slice=dataset1_stationary_dh.isel(time=i),\n",
    "                    stationary_region_dh_slice=dataset1_stationary_region_dh.isel(time=i),\n",
    "                    stationary_outline_gdf=stationary_outline_gdf,\n",
    "                    timestep=mid_pt_datetime,\n",
    "                    mid_pt_datetimes=mid_pt_datetimes,\n",
    "                    stationary_outline_dhs=stationary_outline_dhs,\n",
    "                    stationary_region_dhs=stationary_region_dhs,\n",
    "                    stationary_outline_dhs_corr=stationary_outline_dhs_corr,\n",
    "                    stationary_dVs_corr=stationary_dVs_corr\n",
    "                )\n",
    "        \n",
    "        # Process dataset2\n",
    "        # Prepare dataset2 masks and calculations\n",
    "        dataset2_ROI_subset = dataset2.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "        dataset2_ROI_da = dataset2_ROI_subset['delta_h']\n",
    "        \n",
    "        # Create masks\n",
    "        dataset2_stationary_mask = np.array([[stationary_outline.contains(Point(x, y)) \n",
    "                                            for x in dataset2_ROI_da.x.values] \n",
    "                                            for y in dataset2_ROI_da.y.values])\n",
    "        dataset2_stationary_masked = dataset2_ROI_da.where(xr.DataArray(dataset2_stationary_mask,\n",
    "                                                        coords=[dataset2_ROI_da.y, dataset2_ROI_da.x],\n",
    "                                                        dims=['y', 'x']))\n",
    "        \n",
    "        dataset2_stationary_region_mask = np.array([[stationary_region.contains(Point(x, y)) \n",
    "                                                    for x in dataset2_ROI_da.x.values] \n",
    "                                                    for y in dataset2_ROI_da.y.values])\n",
    "        dataset2_stationary_region_masked = dataset2_ROI_da.where(xr.DataArray(dataset2_stationary_region_mask, \n",
    "                                                        coords=[dataset2_ROI_da.y, dataset2_ROI_da.x], \n",
    "                                                        dims=['y', 'x']))\n",
    "\n",
    "        # Calculate dh differences\n",
    "        dataset2_ROI_dh = dataset2_ROI_da.diff('time')\n",
    "        dataset2_stationary_dh = dataset2_stationary_masked.diff('time')\n",
    "        dataset2_stationary_region_dh = dataset2_stationary_region_masked.diff('time')\n",
    "\n",
    "        # Calculate mid-point datetimes\n",
    "        dataset2_mid_pt_datetimes = []\n",
    "        dataset2_datetimes = dataset2_ROI_da['time'].values\n",
    "        for i in range(1, len(dataset2_datetimes)):\n",
    "            mid_pt_days = dataset2_datetimes[i] - dataset2_datetimes[i-1]\n",
    "            mid_pt_datetime = dataset2_datetimes[i-1] + mid_pt_days\n",
    "            dataset2_mid_pt_datetimes.append(mid_pt_datetime)\n",
    "        dataset2_mid_pt_datetimes = np.array(dataset2_mid_pt_datetimes)\n",
    "\n",
    "        # Process timesteps for dataset2\n",
    "        for i, mid_pt_datetime in enumerate(dataset2_mid_pt_datetimes):\n",
    "            stationary_outline_geom_calc_process_timestep(\n",
    "                ROI_dh_slice=dataset2_ROI_dh.isel(time=i),\n",
    "                stationary_dh_slice=dataset2_stationary_dh.isel(time=i),\n",
    "                stationary_region_dh_slice=dataset2_stationary_region_dh.isel(time=i),\n",
    "                stationary_outline_gdf=stationary_outline_gdf,\n",
    "                timestep=mid_pt_datetime,\n",
    "                mid_pt_datetimes=mid_pt_datetimes,\n",
    "                stationary_outline_dhs=stationary_outline_dhs,\n",
    "                stationary_region_dhs=stationary_region_dhs,\n",
    "                stationary_outline_dhs_corr=stationary_outline_dhs_corr,\n",
    "                stationary_dVs_corr=stationary_dVs_corr\n",
    "            )\n",
    "\n",
    "    # Create dataframe if we have data\n",
    "    if len(mid_pt_datetimes) > 0:\n",
    "  \n",
    "        d = {\n",
    "            'mid_pt_datetime': mid_pt_datetimes,\n",
    "            'stationary_outline_area (m^2)': [stationary_outline_gdf['area (m^2)'].iloc[0]] * len(mid_pt_datetimes),\n",
    "            'stationary_outline_dh (m)': [round(x, 2) for x in stationary_outline_dhs],\n",
    "            'stationary_outline_region_dh (m)': [round(x, 2) for x in stationary_region_dhs],\n",
    "            'stationary_outline_dh_corr (m)': [round(x, 2) for x in stationary_outline_dhs_corr],\n",
    "            'stationary_outline_dV_corr (m^3)': [round(x, 0) for x in stationary_dVs_corr]\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(d)\n",
    "        df = df.fillna(0.0)\n",
    "        \n",
    "        # Convert area and volume columns to integer type\n",
    "        df['stationary_outline_area (m^2)'] = df['stationary_outline_area (m^2)'].astype(int)\n",
    "        df['stationary_outline_dV_corr (m^3)'] = df['stationary_outline_dV_corr (m^3)'].astype(int)\n",
    "        \n",
    "        output_path = f'output/geometric_calcs/stationary_outline_geom_calc/'\n",
    "        output_file = os.path.join(output_path, sub_dir, f'{lake_name}.csv')\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f'Results saved to: {output_file}')\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(f'No data processed for {lake_name}')\n",
    "        return None\n",
    "\n",
    "def stationary_outline_geom_calc_process_timestep(*,  # Force kwarg use\n",
    "                    ROI_dh_slice,\n",
    "                    stationary_dh_slice, \n",
    "                    stationary_region_dh_slice, \n",
    "                    stationary_outline_gdf,\n",
    "                    timestep,\n",
    "                    mid_pt_datetimes,\n",
    "                    stationary_outline_dhs,\n",
    "                    stationary_region_dhs,\n",
    "                    stationary_outline_dhs_corr,\n",
    "                    stationary_dVs_corr):\n",
    "    '''\n",
    "    Process a single timestep of lake height data and calculate various metrics.\n",
    "    Height measurements rounded to 2 decimal places (9cm precision)\n",
    "    Area measurements rounded to nearest 1,000,000 m² (1 km grid resolution)\n",
    "    Volume measurements rounded to whole numbers (combined uncertainty)\n",
    "    '''    \n",
    "    # Calculate metrics for stationary outline\n",
    "    stationary_outline_dh = round(float(np.nanmean(stationary_dh_slice)), 2)  # 2 decimals for height\n",
    "    stationary_region_dh = round(float(np.nanmean(stationary_region_dh_slice)), 2)  # 2 decimals for height\n",
    "    stationary_outline_dh_corr = round(stationary_outline_dh - stationary_region_dh, 2)  # 2 decimals for height\n",
    "    \n",
    "    # Round area to nearest 1,000,000 m² before volume calculation\n",
    "    stationary_dV_corr = round(stationary_outline_dh_corr * stationary_outline_gdf['area (m^2)'].iloc[0], 0)  # Whole numbers for volume\n",
    "    \n",
    "    # Append all metrics to lists\n",
    "    mid_pt_datetimes.append(timestep)\n",
    "    stationary_outline_dhs.append(stationary_outline_dh)\n",
    "    stationary_region_dhs.append(stationary_region_dh)\n",
    "    stationary_outline_dhs_corr.append(stationary_outline_dh_corr)\n",
    "    stationary_dVs_corr.append(stationary_dV_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e184bf11-21e2-4afe-9a6b-1a5db1df1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_geodesic_area(poly):\n",
    "    '''\n",
    "    Calculate geodesic area of polygon or multipolygon. Polygon or multipolygon must extracted from geodataframe\n",
    "    that has CRS EPSG:4326.\n",
    "    '''\n",
    "    # Ensure geom exists and geom is valid\n",
    "    if poly is None or not poly.is_valid:\n",
    "        return None\n",
    "\n",
    "    # Calculate geodesic area and return it\n",
    "    if isinstance(poly, Polygon):\n",
    "        return abs(geod.polygon_area_perimeter(poly.exterior.coords.xy[0], poly.exterior.coords.xy[1])[0])\n",
    "    elif isinstance(poly, MultiPolygon):\n",
    "        total_area = 0\n",
    "        for part in poly.geoms:\n",
    "            total_area += abs(geod.polygon_area_perimeter(part.exterior.coords.xy[0], part.exterior.coords.xy[1])[0])\n",
    "        return total_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4f9c03-cdc3-4862-a43f-0a6086ea4e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evolving_and_stationary_comparison(lake_gdf, forward_fill=False):\n",
    "    '''\n",
    "    Plot and compare the evolving outlines and stationary outline of a lake along with active area, dh, and dV.\n",
    "\n",
    "    This function generates plot for a given lake, showing the differences between the evolving \n",
    "    and stationary outlines over time. It includes visualizations of the outlines on a map, as well as plots for \n",
    "    active area, cumulative height change, and cumulative volume displacement. The results are saved as a PNG file.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    lake_gdf : GeoDataFrame\n",
    "        A GeoDataFrame containing a single lake's data with attributes such as 'name' and 'geometry'.\n",
    "        The GeoDataFrame should have a single row corresponding to the lake.\n",
    "    forward_fill : bool, default=False\n",
    "        If True, use Last Observation Carried Forward (LOCF) methodology by reading \n",
    "        from forward-filled geojson files\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None: The results are saved as PNG files in the OUTPUT_DIR + '/plot_evolving_and_stationary_comparison/' directory \n",
    "    with filenames corresponding to the lake names.\n",
    "\n",
    "    Example:\n",
    "    >>> lake_gdf = gpd.read_file('path_to_lake.geojson')\n",
    "    >>> plot_evolving_and_stationary_comparison(lake_gdf, forward_fill=False)\n",
    "    '''\n",
    "    # First check if lake_gdf is valid and has data\n",
    "    if lake_gdf is None or lake_gdf.empty:\n",
    "        print('Empty lake_gdf provided. Skipping...')\n",
    "        return\n",
    "    \n",
    "    # Define lake name and polygon and buffered polygon to use as off-lake region\n",
    "    lake_name = lake_gdf['name'].values[0]\n",
    "    lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "    # print(f'Processing lake: {lake_name}')\n",
    "    \n",
    "    # Open evolving outline(s) and evolving outlines and geometric calculation comparison geodataframes for plotting\n",
    "    # Modify file paths based on forward_fill parameter\n",
    "    if forward_fill:\n",
    "        evolving_outlines_path = f'output/lake_outlines/evolving_outlines/forward_fill/{lake_name}.geojson'\n",
    "        offlake_outlines_path = f'{OUTPUT_DIR}/find_evolving_outlines/offlake_outlines/{lake_name}.geojson'\n",
    "        evolving_geom_calcs_path = f'output/geometric_calcs/evolving_outlines_geom_calc/forward_fill/{lake_name}.csv'\n",
    "        evolving_union_geom_calcs_path = f'output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/{lake_name}.csv'\n",
    "        stationary_geom_calcs_path = f'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes/{lake_name}.csv'\n",
    "    else:\n",
    "        evolving_outlines_path = f'output/lake_outlines/evolving_outlines/{lake_name}.geojson'\n",
    "        offlake_outlines_path = f'{OUTPUT_DIR}/find_evolving_outlines/offlake_outlines/{lake_name}.geojson'\n",
    "        evolving_geom_calcs_path = f'output/geometric_calcs/evolving_outlines_geom_calc/{lake_name}.csv'\n",
    "        evolving_union_geom_calcs_path = f'output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/{lake_name}.csv'\n",
    "        stationary_geom_calcs_path = f'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes/{lake_name}.csv'\n",
    "\n",
    "    try:\n",
    "        # Read files using the modified paths\n",
    "        evolving_outlines_gdf = gpd.read_file(evolving_outlines_path)       \n",
    "        offlake_outlines_gdf = gpd.read_file(offlake_outlines_path)\n",
    "        evolving_geom_calcs_df = pd.read_csv(evolving_geom_calcs_path)\n",
    "        evolving_union_geom_calcs_df = pd.read_csv(evolving_union_geom_calcs_path)\n",
    "        stationary_geom_calcs_df = pd.read_csv(stationary_geom_calcs_path)\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f'Skipping lake {lake_name}: Missing required file. {str(e)}')\n",
    "        return\n",
    "\n",
    "    # By default make has_offlake_outlines Boolean True, unless offlake_outlines_gdf is empty\n",
    "    has_offlake_outlines = True\n",
    "    if offlake_outlines_gdf.empty:\n",
    "        has_offlake_outlines = False\n",
    "    \n",
    "    # Convert of strings to datetime\n",
    "    evolving_geom_calcs_df['mid_pt_datetime'] = pd.to_datetime(evolving_geom_calcs_df['mid_pt_datetime'])\n",
    "\n",
    "    # Fig setup\n",
    "    nows, ncols = 1, 4\n",
    "    fig = plt.figure(figsize=(16, 5))\n",
    "    \n",
    "    # Create GridSpec to control subplot sizes\n",
    "    gs = fig.add_gridspec(nows, ncols, width_ratios=[1, 1, 1, 1])\n",
    "    ax = [fig.add_subplot(gs[0, i]) for i in range(ncols)]\n",
    "    \n",
    "    # Define colors and linestyles that will be reused and create lines for legend\n",
    "    stationary_color  = 'darkturquoise'\n",
    "    stationary_line = plt.Line2D([], [], color=stationary_color, linestyle='solid', linewidth=1)\n",
    "\n",
    "    # Set up colormap\n",
    "    cmap = plt.get_cmap('plasma', len(mid_pt_datetimes)-1)\n",
    "    \n",
    "    # Norm to time variable\n",
    "    norm = plt.Normalize(mdates.date2num(mid_pt_datetimes[0]), \n",
    "                         mdates.date2num(mid_pt_datetimes[-1]))\n",
    "\n",
    "    \n",
    "    # Panel - evolving outlines ------------------------------------------------------\n",
    "\n",
    "    # Create buffered polygon for the area multiple within evaluation boundary\n",
    "    within_eval_poly = area_multiple_buffer(lake_gdf['geometry'].iloc[0], evolving_outlines_gdf['within_area_multiple'][0])    \n",
    "\n",
    "    # Combine stationary outline(s) with evolving outlines in unary union to plot all within bounds of plot\n",
    "    evolving_stationary_outlines_union = evolving_stationary_outlines_union_evolving_lakes_gdf[evolving_stationary_outlines_union_evolving_lakes_gdf['name'] == lake_name]['geometry'].iloc[0]\n",
    "    all_poly_union = unary_union([within_eval_poly, evolving_stationary_outlines_union])\n",
    "    x_min, y_min, x_max, y_max = all_poly_union.bounds\n",
    "    del all_poly_union\n",
    "\n",
    "    buffer_frac = 0.05\n",
    "    x_buffer = abs(x_max-x_min)*buffer_frac\n",
    "    y_buffer = abs(y_max-y_min)*buffer_frac\n",
    "    mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "    mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "    \n",
    "    # Plot stationary and evolving outlines onto MOA surface imagery\n",
    "    moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "    ax[0].imshow(moa_highres_da_subset[0,:,:], cmap='gray', clim=[14000, 17000], extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "    \n",
    "    # Plot evolving outlines\n",
    "    onlake_lines, offlake_lines = [], []\n",
    "    x, y = 1, 1\n",
    "    for idx, dt in enumerate(mid_pt_datetimes):\n",
    "        onlake_line, = ax[0].plot(x, y, color=cmap(norm(mdates.date2num(mid_pt_datetimes[idx]))), linewidth=2)\n",
    "        onlake_lines.append(onlake_line)\n",
    "        evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['mid_pt_datetime'] == dt]\n",
    "        if not evolving_outlines_dt.empty:\n",
    "            evolving_outlines_dt.boundary.plot(ax=ax[0], \n",
    "                color=cmap(norm(mdates.date2num(mid_pt_datetimes[idx]))),\n",
    "                linewidth=1)\n",
    "    \n",
    "    # Plot offlake outlines if available\n",
    "    if has_offlake_outlines:\n",
    "        for idx, dt in enumerate(mid_pt_datetimes):\n",
    "            offlake_line, = ax[0].plot(x, y, color=cmap(norm(mdates.date2num(mid_pt_datetimes[idx]))), linewidth=2, alpha=0.2)\n",
    "            offlake_lines.append(offlake_line)\n",
    "            offlake_outlines_dt = offlake_outlines_gdf[offlake_outlines_gdf['mid_pt_datetime'] == dt]\n",
    "            if not offlake_outlines_dt.empty:\n",
    "                offlake_outlines_dt.boundary.plot(ax=ax[0], \n",
    "                    color=cmap(norm(mdates.date2num(mid_pt_datetimes[idx]))),\n",
    "                    linewidth=1, alpha=0.25)\n",
    "                \n",
    "    # Plot within evaluation polygon\n",
    "    gpd.GeoDataFrame(geometry=[within_eval_poly]).boundary.plot(ax=ax[0], edgecolor='darkgray', facecolor='none', linewidth=1)\n",
    "\n",
    "    # Create evolving outlines unary union and plot\n",
    "    evolving_union_gdf = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]\n",
    "    evolving_union_gdf.boundary.plot(ax=ax[0], color='k', linestyle='dotted', linewidth=1)\n",
    "\n",
    "    # Plot stationary outline\n",
    "    stationary_outlines_gdf.boundary.plot(ax=ax[0], color=stationary_color, linewidth=1, zorder=0)\n",
    "        \n",
    "    # Plot inset map\n",
    "    axIns = ax[0].inset_axes([0.05, 0, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "    axIns.set_aspect('equal')\n",
    "    moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    axIns.axis('off')\n",
    "    # Plot star to indicate location\n",
    "    axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "        linewidth=0.1, color='k', s=30, zorder=3)\n",
    "\n",
    "    # Create stationary region and evolving outlines region and plot\n",
    "    stationary_region = area_multiple_buffer(lake_poly, 2)\n",
    "    stationary_region = stationary_region.difference(lake_poly)\n",
    "    evolving_union_region = area_multiple_buffer(evolving_union_gdf['geometry'], 2)\n",
    "    evolving_union_region = evolving_union_region.difference(evolving_union_gdf['geometry'].iloc[0])\n",
    "    gpd.GeoDataFrame(geometry=[stationary_region], crs='3031').plot(ax=ax[0], color=stationary_color, linewidth=2, alpha=0.2)\n",
    "    gpd.GeoDataFrame(geometry=[evolving_union_region], crs='3031').plot(ax=ax[0], color='k', linewidth=2, alpha=0.2)\n",
    "\n",
    "    # Set up colormap\n",
    "    min_date = pd.to_datetime(cyc_start_datetimes[1])\n",
    "    max_date = pd.to_datetime(cyc_end_datetimes[-1])\n",
    "    date_range = pd.date_range(min_date, max_date, periods=len(mid_pt_datetimes))\n",
    "    years = date_range.year.unique()\n",
    "    years = pd.to_datetime(years, format='%Y')\n",
    "    cmap = plt.get_cmap('plasma', len(mid_pt_datetimes[1:]))\n",
    "    norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "    m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    m.set_array(np.array([mdates.date2num(date) for date in mid_pt_datetimes]))\n",
    "\n",
    "    # Add colorbar\n",
    "    divider = make_axes_locatable(ax[0])\n",
    "    cax = divider.append_axes('bottom', size='2.5%', pad=0.5)\n",
    "    cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "\n",
    "    # Set ticks for all years but labels only for odd years\n",
    "    tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "    tick_labels = [date.strftime('%Y') if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "    cbar.set_ticks(tick_locations)\n",
    "    cbar.set_ticklabels(tick_labels)\n",
    "    \n",
    "    # Add minor ticks for quarters\n",
    "    cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Quarter year intervals only\n",
    "    cbar.set_label('year', size=10, labelpad=10)\n",
    "\n",
    "    # Change polar stereographic m to km\n",
    "    km_scale = 1e3\n",
    "    ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax[0].xaxis.set_major_formatter(ticks_x)\n",
    "    ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax[0].yaxis.set_major_formatter(ticks_y)\n",
    "    \n",
    "    # Set axes limit, title, and axis label\n",
    "    ax[0].set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "    ax[0].set_xlabel('x [km]')\n",
    "    ax[0].set_ylabel('y [km]')\n",
    "\n",
    "    \n",
    "    # Panel - Active area ---------------------------------------------\n",
    "\n",
    "    # Plot horizontal zero line for reference\n",
    "    ax[1].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "    \n",
    "    # Plot stationary outline and evolving outlines unary union areas\n",
    "    ax[1].axhline(np.divide(lake_gdf['area (m^2)'].iloc[0], 1e6), \n",
    "        color=stationary_color, linestyle='solid', linewidth=2)\n",
    "    ax[1].axhline(np.divide(evolving_union_gdf['area (m^2)'].iloc[0], 1e6), \n",
    "        color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "    # Plot evolving outlines time series using LineCollection from points/segments to plot multi-colored line\n",
    "    x = mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime'])\n",
    "    y = np.divide(evolving_geom_calcs_df['evolving_outlines_area (m^2)'], 1e6)\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "    lc.set_array(x)  # Set the values used for colormapping, using matplotlib dates for colors\n",
    "    lc.set_linewidth(2)\n",
    "    line = ax[1].add_collection(lc)\n",
    "    scatter = ax[1].scatter(x, y, c=x, cmap=cmap, s=9, norm=norm, zorder=2)\n",
    "\n",
    "    # Panel C - Cumulative dh/dt -------------------------------------------------------\n",
    "    # Plot horizontal zero line for reference\n",
    "    ax[2].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "\n",
    "    # Plot stationary outlines off-lake region dh\n",
    "    ax[2].plot(mdates.date2num(stationary_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.cumsum(stationary_geom_calcs_df['stationary_outline_region_dh (m)']).astype(float), \n",
    "        color='lightgray', linestyle='solid', linewidth=1)\n",
    "\n",
    "    # Plot evolving outlines off-lake region dh\n",
    "    ax[2].plot(mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.cumsum(evolving_geom_calcs_df['evolving_outlines_region_dh (m)']), \n",
    "        color='dimgray', linestyle='solid', linewidth=1)\n",
    "\n",
    "    # Plot stationary outline time series (uncorrected)\n",
    "    ax[2].plot(mdates.date2num(stationary_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.cumsum(stationary_geom_calcs_df['stationary_outline_dh (m)']), \n",
    "        color=stationary_color, linestyle='solid', linewidth=1)\n",
    "    ax[2].scatter(mdates.date2num(stationary_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.cumsum(stationary_geom_calcs_df['stationary_outline_dh (m)']),\n",
    "        color=stationary_color, linestyle='solid', linewidth=1, s=2.5)\n",
    "    \n",
    "    # Plot stationary outline time series\n",
    "    ax[2].plot(mdates.date2num(stationary_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.cumsum(stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "        color=stationary_color, linestyle='solid', linewidth=1)\n",
    "\n",
    "    # Plot evolving outlines time series (uncorrected) using multi-colored LineCollection from points/segments\n",
    "    y = np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh (m)'])\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "    lc.set_array(x)\n",
    "    lc.set_linewidth(1)\n",
    "    line = ax[2].add_collection(lc)\n",
    "    scatter = ax[2].scatter(x, y, c=x, cmap=cmap, norm=norm, s=4.5)\n",
    "    \n",
    "    # Plot evolving outlines time series (corrected) using LineCollection from points/segments to plot multi-colored line\n",
    "    y = np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'])\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "    lc.set_array(x)\n",
    "    lc.set_linewidth(2)\n",
    "    line = ax[2].add_collection(lc)\n",
    "    scatter = ax[2].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9, zorder=2)\n",
    "\n",
    "    # Plot evolving outlines union outline time series\n",
    "    ax[2].plot(mdates.date2num(evolving_union_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.cumsum(evolving_union_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "        color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "    # Plot bias\n",
    "    ax[2].plot(x, np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)']-\n",
    "        stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "        color='red', linestyle='solid', linewidth=1)\n",
    "    \n",
    "    \n",
    "    # Panel D - Cumulative dV/dt --------------------------------------------------\n",
    "    # Plot horizontal line at zero for reference\n",
    "    ax[3].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "    \n",
    "    # Plot stationary outline time series\n",
    "    ax[3].plot(mdates.date2num(stationary_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.divide(np.cumsum(stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "        color=stationary_color, linestyle='solid', linewidth=1)\n",
    "    \n",
    "    # Plot multi-colored line and scatter for data points\n",
    "    y = np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "    \n",
    "    # Plot evolving outlines time series using LineCollection from points/segments to plot multi-colored line\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "    lc.set_array(x)\n",
    "    lc.set_linewidth(2)\n",
    "    line = ax[3].add_collection(lc)\n",
    "    scatter = ax[3].scatter(x, y, c=x, cmap=cmap, norm=norm, s=10, zorder=2)\n",
    "\n",
    "    # Plot evolving outlines union outline time series\n",
    "    ax[3].plot(mdates.date2num(evolving_union_geom_calcs_df['mid_pt_datetime']), \n",
    "        np.divide(np.cumsum(evolving_union_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "        color='k', linestyle='dashed', linewidth=1)\n",
    "    \n",
    "    # Plot bias\n",
    "    ax[3].plot(x, np.divide(np.cumsum(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)']-\n",
    "                            stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "                            color='red', linestyle='solid', linewidth=1)\n",
    "\n",
    "    for i in range(1, ncols):\n",
    "        # Set x-axis limits\n",
    "        ax[i].set_xlim(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "        \n",
    "        # Set ticks for all years but labels only for even years (to match colorbar)\n",
    "        tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "        tick_labels = [date.strftime('%Y') if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "        ax[i].set_xticks(tick_locations)\n",
    "        ax[i].set_xticklabels(tick_labels)\n",
    "        \n",
    "        # Add minor ticks for quarters\n",
    "        ax[i].xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))\n",
    "        \n",
    "        ax[i].set_xlabel('year')\n",
    "    \n",
    "    # Add legends\n",
    "    evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=1)\n",
    "    within_eval_line = plt.Line2D([], [], color='darkgray', linestyle='solid', linewidth=1)\n",
    "    stationary_region_patch = mpatches.Patch(color=stationary_color, alpha=0.2)\n",
    "    evolving_union_region_patch = mpatches.Patch(color='k', alpha=0.2)\n",
    "    \n",
    "    legend_elements = [tuple(onlake_lines),\n",
    "                      evolving_union_line, \n",
    "                      stationary_line, \n",
    "                      within_eval_line,\n",
    "                      stationary_region_patch,\n",
    "                      evolving_union_region_patch]\n",
    "                      \n",
    "    legend_labels = [f'evolving outlines ({evolving_outlines_gdf.level[0]} m)',\n",
    "                   'evolving outlines union',\n",
    "                   'stationary outline',\n",
    "                   f'within evaluation line ({int(evolving_outlines_gdf.within_area_multiple[0])}x)',\n",
    "                   'stationary region',\n",
    "                   'evolving union region']\n",
    "    \n",
    "    if has_offlake_outlines:\n",
    "        legend_elements.insert(1, tuple(offlake_lines))\n",
    "        legend_labels.insert(1, 'off-lake evolving outlines')\n",
    "    \n",
    "    legend = ax[0].legend(legend_elements, legend_labels, \n",
    "        handlelength=3,\n",
    "        handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "        loc='upper left',\n",
    "        bbox_to_anchor=(0, 1.47))\n",
    "\n",
    "    legend = ax[1].legend([tuple(onlake_lines), \n",
    "                           evolving_union_line, \n",
    "                           stationary_line],\n",
    "        ['evolving outlines', \n",
    "         'evolving outlines union', \n",
    "         'stationary outline'], \n",
    "        handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "        fontsize='small', loc='upper left',\n",
    "        bbox_to_anchor=(0, 1.15))\n",
    "\n",
    "    evolving_region = plt.Line2D([], [], color='dimgray', linestyle='solid', linewidth=1)\n",
    "    stationary_region = plt.Line2D([], [], color='lightgray', linestyle='solid', linewidth=1)\n",
    "    bias = plt.Line2D([], [], color='red', linestyle='solid', linewidth=1)\n",
    "\n",
    "    legend = ax[2].legend(\n",
    "        [evolving_region,\n",
    "         stationary_region,\n",
    "         tuple(onlake_lines),\n",
    "         evolving_union_line,\n",
    "         stationary_line,  \n",
    "         bias],\n",
    "        ['evolving outlines region',\n",
    "         'stationary outline region',\n",
    "         'evolving outlines',\n",
    "         'evolving outlines union',\n",
    "         'stationary outline', \n",
    "         'bias (evolving - stationary)'],\n",
    "         handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "         fontsize='small', loc='upper left',\n",
    "         bbox_to_anchor=(0, 1.25))\n",
    "\n",
    "    legend = ax[3].legend([tuple(onlake_lines), \n",
    "                           stationary_line,\n",
    "                           evolving_union_line,\n",
    "                           bias],\n",
    "        ['evolving outlines', \n",
    "         'stationary outline',\n",
    "         'evolving outlines union',\n",
    "         'bias (evolving - stationary)'], \n",
    "        handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "        fontsize='small', loc='upper left',\n",
    "        bbox_to_anchor=(0, 1.15))\n",
    "\n",
    "    # Set titles\n",
    "    ax[0].set_title(f'{lake_name}', size=12, y=1.47)\n",
    "    ax[1].set_title('active area [km$^2$]', size=12, y=1.15)\n",
    "    ax[2].set_title('cumulative dh [m]', size=12, y=1.25)\n",
    "    ax[3].set_title('cumulative dV [km$^3$]', size=12, y=1.15)\n",
    "\n",
    "    # Modify the output path for saving the figure\n",
    "    if forward_fill:\n",
    "        output_path = os.path.join(OUTPUT_DIR, 'plot_evolving_and_stationary_comparison', 'forward_fill', f'{lake_name}.png')\n",
    "    else:\n",
    "        output_path = os.path.join(OUTPUT_DIR, 'plot_evolving_and_stationary_comparison', f'{lake_name}.png')\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f'Figure saved to: {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca67058-ce5f-419f-a127-81cc275a3695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evolving_and_stationary_comparison_sequential(lake_gdf, forward_fill=False):\n",
    "    '''\n",
    "    Plot and compare the evolving outlines and stationary outline of a lake along with active area, dh, and dV,\n",
    "    creating separate plots for each time step showing the progression of changes.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    lake_gdf (GeoDataFrame): A GeoDataFrame containing a single lake's data with attributes.\n",
    "    forward_fill (bool, default=False): \n",
    "        If True, use Last Observation Carried Forward (LOCF) methodology \n",
    "        by reading from forward-filled geojson files\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None: Results saved as PNG files in OUTPUT_DIR + '/plot_evolving_and_stationary_comparison_sequential/'\n",
    "    '''\n",
    "    # Define lake name and polygon\n",
    "    lake_name = lake_gdf['name'].values[0]\n",
    "    lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "    # print(f'Processing lake: {lake_name}')\n",
    "    \n",
    "    # Make output directory if it doesn't yet exist\n",
    "    if forward_fill:\n",
    "        sequential_dir = os.path.join(OUTPUT_DIR, f'plot_evolving_and_stationary_comparison_sequential/forward_fill/{lake_name}')\n",
    "    else:\n",
    "        sequential_dir = os.path.join(OUTPUT_DIR, f'plot_evolving_and_stationary_comparison_sequential/{lake_name}')\n",
    "    os.makedirs(sequential_dir, exist_ok=True)\n",
    "\n",
    "    # First check if lake_gdf is valid and has data\n",
    "    if lake_gdf is None or lake_gdf.empty:\n",
    "        print('Empty lake_gdf provided. Skipping...')\n",
    "        return\n",
    "    \n",
    "    # Open required files\n",
    "    try:\n",
    "        # Modify file paths based on forward_fill parameter\n",
    "        if forward_fill:\n",
    "            evolving_outlines_path = f'output/lake_outlines/evolving_outlines/forward_fill/{lake_name}.geojson'\n",
    "            offlake_outlines_path = os.path.join(OUTPUT_DIR + f'/find_evolving_outlines/offlake_outlines/{lake_name}.geojson')\n",
    "            evolving_geom_calcs_path = f'output/geometric_calcs/evolving_outlines_geom_calc/forward_fill/{lake_name}.csv'\n",
    "            evolving_union_geom_calcs_path = f'output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/{lake_name}.csv'\n",
    "            stationary_geom_calcs_path = f'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes/{lake_name}.csv'\n",
    "        else:\n",
    "            evolving_outlines_path = f'output/lake_outlines/evolving_outlines/{lake_name}.geojson'\n",
    "            offlake_outlines_path = os.path.join(OUTPUT_DIR + f'/find_evolving_outlines/offlake_outlines/{lake_name}.geojson')\n",
    "            evolving_geom_calcs_path = f'output/geometric_calcs/evolving_outlines_geom_calc/{lake_name}.csv'\n",
    "            evolving_union_geom_calcs_path = f'output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/{lake_name}.csv'\n",
    "            stationary_geom_calcs_path = f'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes/{lake_name}.csv'\n",
    "\n",
    "        # Read files using the paths\n",
    "        evolving_outlines_gdf = gpd.read_file(evolving_outlines_path)\n",
    "        offlake_outlines_gdf = gpd.read_file(offlake_outlines_path)\n",
    "        evolving_geom_calcs_df = pd.read_csv(evolving_geom_calcs_path)\n",
    "        evolving_union_geom_calcs_df = pd.read_csv(evolving_union_geom_calcs_path)\n",
    "        stationary_geom_calcs_df = pd.read_csv(stationary_geom_calcs_path)\n",
    "\n",
    "    except (fiona.errors.DriverError, pyogrio.errors.DataSourceError, FileNotFoundError) as e:\n",
    "        print(f'Error loading files for {lake_name}: {str(e)}. Skipping...')\n",
    "        return\n",
    "\n",
    "    # By default make has_offlake_outlines Boolean True, unless offlake_outlines_gdf is empty\n",
    "    has_offlake_outlines = True\n",
    "    if offlake_outlines_gdf.empty:\n",
    "        has_offlake_outlines = False\n",
    "    \n",
    "    # Convert strings to datetime\n",
    "    evolving_geom_calcs_df['mid_pt_datetime'] = pd.to_datetime(evolving_geom_calcs_df['mid_pt_datetime'])\n",
    "\n",
    "    # Get evolving outlines union\n",
    "    evolving_union_gdf = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]\n",
    "\n",
    "    # Define colors and setup\n",
    "    stationary_color = 'darkturquoise'\n",
    "    cmap = plt.get_cmap('plasma', len(mid_pt_datetimes)-1)\n",
    "    norm = plt.Normalize(mdates.date2num(mid_pt_datetimes[0]), \n",
    "                        mdates.date2num(mid_pt_datetimes[-1]))\n",
    "\n",
    "    # Prepare datasets\n",
    "    dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(lake_gdf, 30)\n",
    "\n",
    "    # Get plot bounds\n",
    "    within_eval_poly = area_multiple_buffer(lake_gdf['geometry'].iloc[0], evolving_outlines_gdf['within_area_multiple'][0])\n",
    "    evolving_stationary_outlines_union = evolving_stationary_outlines_union_evolving_lakes_gdf[evolving_stationary_outlines_union_evolving_lakes_gdf['name'] == lake_name]['geometry'].iloc[0]\n",
    "    all_poly_union = unary_union([search_extent_poly, within_eval_poly, evolving_stationary_outlines_union])\n",
    "    x_min, y_min, x_max, y_max = all_poly_union.bounds\n",
    "\n",
    "    # Calculate time steps and diffs for each dataset\n",
    "    if dataset1_masked is not None:\n",
    "        # Get dh values of cycle-to-cycle height change (dh) instead of relative to datum for dataset1\n",
    "        dataset1_dh = dataset1_masked['delta_h'].diff('time')\n",
    "\n",
    "        # Calculate mid-point datetimes\n",
    "        dataset1_mid_pt_times = []\n",
    "        dataset1_datetimes = dataset1_masked['time'].values\n",
    "        for i in range(1, len(dataset1_datetimes)):\n",
    "            mid_pt_days = dataset1_datetimes[i] - dataset1_datetimes[i-1]\n",
    "            mid_pt_date = dataset1_datetimes[i-1] + mid_pt_days\n",
    "            dataset1_mid_pt_times.append(mid_pt_date)\n",
    "        dataset1_mid_pt_times = np.array(dataset1_mid_pt_times)\n",
    "\n",
    "        # Write CRS after diff operation\n",
    "        dataset1_dh.rio.write_crs('epsg:3031', inplace=True)\n",
    "\n",
    "    else:\n",
    "        dataset1_dh = None\n",
    "        dataset1_mid_pt_times = np.array([])\n",
    "\n",
    "    # Get dh values of cycle-to-cycle height change (dh) instead of relative to datum for dataset2\n",
    "    dataset2_dh = dataset2_masked['delta_h'].diff('time')\n",
    "\n",
    "    # Calculate mid-point datetimes\n",
    "    dataset2_mid_pt_times = []\n",
    "    dataset2_datetimes = dataset2_masked['time'].values    \n",
    "    for i in range(1, len(dataset2_datetimes)):\n",
    "        mid_pt_days = dataset2_datetimes[i] - dataset2_datetimes[i-1]\n",
    "        mid_pt_date = dataset2_datetimes[i-1] + mid_pt_days\n",
    "        dataset2_mid_pt_times.append(mid_pt_date)\n",
    "    dataset2_mid_pt_times = np.array(dataset2_mid_pt_times)\n",
    "\n",
    "    # Find magnitude of dh for colorbar mapping\n",
    "    height_anom_pos = []\n",
    "    height_anom_neg = []\n",
    "\n",
    "    # Process both datasets for height anomalies\n",
    "    if dataset1_masked is not None:\n",
    "        for dh_slice in dataset1_dh:\n",
    "            if np.any(~np.isnan(dh_slice)):\n",
    "                height_anom_pos.append(np.nanmax(dh_slice))\n",
    "                height_anom_neg.append(np.nanmin(dh_slice))\n",
    "\n",
    "    for dh_slice in dataset2_dh:\n",
    "        if np.any(~np.isnan(dh_slice)):\n",
    "            height_anom_pos.append(np.nanmax(dh_slice))\n",
    "            height_anom_neg.append(np.nanmin(dh_slice))\n",
    "\n",
    "    if not height_anom_pos:\n",
    "        print('No valid height anomalies found for plotting')\n",
    "        return None\n",
    "\n",
    "    # Create color normalization for height changes\n",
    "    divnorm = colors.TwoSlopeNorm(vmin=min(height_anom_neg), \n",
    "                                vcenter=0., \n",
    "                                vmax=max(height_anom_pos))\n",
    "\n",
    "    # Create on- and off-lake line segments and solid lines that will be used in legends \n",
    "    fig, ax = plt.subplots()\n",
    "    onlake_lines, offlake_lines = [], []\n",
    "    for idx, dt in enumerate(mid_pt_datetimes):\n",
    "        x, y = 1, 1\n",
    "        onlake_line, = ax.plot(x, y, color=cmap(norm(mdates.date2num(mid_pt_datetimes[idx]))), linewidth=2)\n",
    "        onlake_lines.append(onlake_line)\n",
    "        offlake_line, = ax.plot(x, y, color=cmap(norm(mdates.date2num(mid_pt_datetimes[idx]))), linewidth=2, alpha=0.2)\n",
    "        offlake_lines.append(offlake_line)\n",
    "\n",
    "    stationary_color  = 'darkturquoise'\n",
    "    stationary_line = plt.Line2D([], [], color=stationary_color, linestyle='solid', linewidth=1)\n",
    "    within_eval_line = plt.Line2D([], [], color='darkgray', linestyle='solid', linewidth=1)\n",
    "    stationary_region_patch = mpatches.Patch(color=stationary_color, alpha=0.2)\n",
    "    evolving_union_region_patch = mpatches.Patch(color='k', alpha=0.2)\n",
    "\n",
    "    # Calculate y-axis limits for cumulative dh plot (Panel C)\n",
    "    dh_data = pd.concat([\n",
    "        pd.Series(np.cumsum(stationary_geom_calcs_df['stationary_outline_region_dh (m)'])),\n",
    "        pd.Series(np.cumsum(evolving_geom_calcs_df['evolving_outlines_region_dh (m)'])),\n",
    "        pd.Series(np.cumsum(stationary_geom_calcs_df['stationary_outline_dh_corr (m)'])),\n",
    "        pd.Series(np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)']))\n",
    "    ])\n",
    "    dh_min, dh_max = dh_data.min(), dh_data.max()\n",
    "    dh_range = dh_max - dh_min\n",
    "\n",
    "    # Calculate y-axis limits for cumulative dV plot (Panel D)\n",
    "    dv_data = pd.concat([\n",
    "        pd.Series(np.cumsum(stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)'] / 1e9)),\n",
    "        pd.Series(np.cumsum(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'] / 1e9)),\n",
    "        pd.Series(np.cumsum(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'] / 1e9 - \n",
    "                            stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)'] / 1e9)),\n",
    "        pd.Series(np.cumsum(evolving_union_geom_calcs_df['stationary_outline_dV_corr (m^3)'] / 1e9))\n",
    "    ])\n",
    "    dv_min, dv_max = dv_data.min(), dv_data.max()\n",
    "    dv_range = dv_max - dv_min\n",
    "\n",
    "    # Iterate through each date to create sequential plots\n",
    "    for date_idx, current_date in enumerate(evolving_geom_calcs_df['mid_pt_datetime']):\n",
    "        gc.collect()  # Garbage collection\n",
    "        \n",
    "        current_date_pd = pd.Timestamp(current_date)\n",
    "        print(f'Creating plot for date: {current_date_pd}')\n",
    "        \n",
    "        # Create figure\n",
    "        fig = plt.figure(figsize=(16, 5))\n",
    "        nows, ncols = 1, 4\n",
    "        gs = fig.add_gridspec(nows, ncols, \n",
    "                              width_ratios=[1.2, 0.8, 0.8, 0.8],  # Make first panel wider\n",
    "                              wspace=0.4)  # Add horizontal spacing between subplots\n",
    "        ax = [fig.add_subplot(gs[0, i]) for i in range(ncols)]\n",
    "        \n",
    "        # Filter data up to current date\n",
    "        current_mask = evolving_geom_calcs_df['mid_pt_datetime'] <= current_date\n",
    "        current_evolving_geom_calcs = evolving_geom_calcs_df[current_mask]\n",
    "        current_evolving_union_geom_calcs = evolving_union_geom_calcs_df[current_mask]\n",
    "        current_stationary_geom_calcs = stationary_geom_calcs_df[current_mask]\n",
    "\n",
    "        # Panel A - dh and evolving outlines\n",
    "        # Find corresponding dh slice for the current date\n",
    "        current_dh = None\n",
    "        if dataset1_masked is not None and current_date in dataset1_mid_pt_times:\n",
    "            idx = np.where(dataset1_mid_pt_times == current_date)[0][0]\n",
    "            current_dh = dataset1_dh[idx]\n",
    "        elif current_date in dataset2_mid_pt_times:\n",
    "            idx = np.where(dataset2_mid_pt_times == current_date)[0][0]\n",
    "            current_dh = dataset2_dh[idx]\n",
    "\n",
    "        if current_dh is not None:\n",
    "            # Plot height change\n",
    "            img = ax[0].imshow(current_dh, extent=[x_min, x_max, y_min, y_max],\n",
    "                origin='lower', cmap='coolwarm_r', norm=divnorm)\n",
    "\n",
    "        # Create stationary region and evolving outlines region and plot\n",
    "        stationary_region = area_multiple_buffer(lake_poly, 2)\n",
    "        stationary_region = stationary_region.difference(lake_poly)\n",
    "        evolving_union_region = area_multiple_buffer(evolving_union_gdf['geometry'], 2)\n",
    "        evolving_union_region = evolving_union_region.difference(evolving_union_gdf['geometry'].iloc[0])\n",
    "        gpd.GeoDataFrame(geometry=[stationary_region], crs='3031').plot(ax=ax[0], color=stationary_color, linewidth=2, alpha=0.2)\n",
    "        gpd.GeoDataFrame(geometry=[evolving_union_region], crs='3031').plot(ax=ax[0], color='k', linewidth=2, alpha=0.2)\n",
    "\n",
    "        # Add colorbars\n",
    "        # First create the divider\n",
    "        divider = make_axes_locatable(ax[0])\n",
    "        \n",
    "        # Create the vertical colorbar axes (for dh)\n",
    "        cax_vertical = divider.append_axes('right', size='5%', pad=0.05)\n",
    "\n",
    "        # Add dh colorbar\n",
    "        plt.colorbar(img, cax=cax_vertical, label='height change (dh) [m]')\n",
    "        \n",
    "        # Set up colormap\n",
    "        min_date = pd.to_datetime(cyc_start_datetimes[1])\n",
    "        max_date = pd.to_datetime(cyc_end_datetimes[-1])\n",
    "        date_range = pd.date_range(min_date, max_date, periods=len(mid_pt_datetimes))\n",
    "        years = date_range.year.unique()\n",
    "        years = pd.to_datetime(years, format='%Y')\n",
    "        cmap = plt.get_cmap('plasma', len(mid_pt_datetimes[1:]))\n",
    "        norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "        m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        m.set_array(np.array([mdates.date2num(date) for date in mid_pt_datetimes[1:]]))\n",
    "    \n",
    "        # Add colorbar\n",
    "        divider = make_axes_locatable(ax[0])\n",
    "        cax = divider.append_axes('bottom', size='2.5%', pad=0.5)\n",
    "        cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "    \n",
    "        # Set ticks for all years but labels only for odd years\n",
    "        tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "        tick_labels = [date.strftime('%Y') if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "        cbar.set_ticks(tick_locations)\n",
    "        cbar.set_ticklabels(tick_labels)\n",
    "        \n",
    "        # Add minor ticks for quarters\n",
    "        cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Quarter intervals only\n",
    "        cbar.set_label('year', size=10, labelpad=10)\n",
    "        \n",
    "        # Add inset map\n",
    "        axIns = ax[0].inset_axes([0.05, 0, 0.3, 0.3])\n",
    "        axIns.set_aspect('equal')\n",
    "        moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "        axIns.axis('off')\n",
    "        axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', linewidth=0.1, color='k', s=30, zorder=3)\n",
    "        \n",
    "        # Change polar stereographic m to km\n",
    "        km_scale = 1e3\n",
    "        ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "        ax[0].xaxis.set_major_formatter(ticks_x)\n",
    "        ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "        ax[0].yaxis.set_major_formatter(ticks_y)\n",
    "\n",
    "        # Set axes limit, title, and axis label\n",
    "        ax[0].set(xlim=(x_min, x_max), ylim=(y_min, y_max))\n",
    "        ax[0].set_xlabel('x [km]')\n",
    "        ax[0].set_ylabel('y [km]')\n",
    "\n",
    "        # FIXME duplicated from earlier\n",
    "        # Store line segments for multi-colored line in legend\n",
    "        onlake_lines, offlake_lines = [], []\n",
    "        for idx, dt in enumerate(mid_pt_datetimes):\n",
    "            x, y = 1, 1\n",
    "            onlake_line, = ax[0].plot(x, y, color=cmap(norm(mdates.date2num(mid_pt_datetimes[idx]))), linewidth=2)\n",
    "            onlake_lines.append(onlake_line)\n",
    "            # offlake_line, = ax[0].plot(x, y, color=cmap(norm(mdates.date2num(mid_pt_datetimes[idx]))), linewidth=2, alpha=0.2)\n",
    "            # offlake_lines.append(offlake_line)\n",
    "\n",
    "        # Plot evolving outlines up to current date\n",
    "        for idx, dt in enumerate(evolving_geom_calcs_df['mid_pt_datetime'][:date_idx + 1]):\n",
    "\n",
    "            evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['mid_pt_datetime'] == dt]\n",
    "            # offlake_outlines_dt = offlake_outlines_gdf[offlake_outlines_gdf['mid_pt_datetime'] == dt]\n",
    "            \n",
    "            if not evolving_outlines_dt.empty:\n",
    "                evolving_outlines_dt.boundary.plot(ax=ax[0], \n",
    "                    color=cmap(norm(mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime'].iloc[[idx]]))),\n",
    "                    linewidth=1)\n",
    "\n",
    "        # Plot offlake outlines if available\n",
    "        if has_offlake_outlines:\n",
    "            for idx, dt in enumerate(evolving_geom_calcs_df['mid_pt_datetime'][:date_idx + 1]):\n",
    "                offlake_line, = ax[0].plot(x, y, color=cmap(norm(mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime'].iloc[[idx]]))), linewidth=2, alpha=0.2)\n",
    "                offlake_lines.append(offlake_line)\n",
    "                offlake_outlines_dt = offlake_outlines_gdf[offlake_outlines_gdf['mid_pt_datetime'] == dt]\n",
    "                if not offlake_outlines_dt.empty:\n",
    "                    offlake_outlines_dt.boundary.plot(ax=ax[0], \n",
    "                        color=cmap(norm(mdates.date2num(mid_pt_datetimes[idx]))),\n",
    "                        linewidth=1, alpha=0.25)\n",
    "\n",
    "        # Add other map elements (evaluation boundary, stationary outline, etc.)\n",
    "        gpd.GeoDataFrame(geometry=[within_eval_poly]).boundary.plot(ax=ax[0], edgecolor='darkgray', facecolor='none', linewidth=1)\n",
    "        evolving_union_gdf.boundary.plot(ax=ax[0], color='k', linestyle='dotted', linewidth=1)\n",
    "        stationary_outlines_gdf.boundary.plot(ax=ax[0], color=stationary_color, linewidth=1, zorder=0)\n",
    "\n",
    "\n",
    "        # Panel B - Active area\n",
    "\n",
    "        ax[1].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "        ax[1].axhline(np.divide(lake_gdf['area (m^2)'].iloc[0], 1e6), \n",
    "                      color=stationary_color, linestyle='solid', linewidth=2)\n",
    "        ax[1].axhline(np.divide(evolving_union_gdf['area (m^2)'].iloc[0], 1e6), \n",
    "                      color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "        # Plot multi-colored evolving outlines time series\n",
    "        x = mdates.date2num(current_evolving_geom_calcs['mid_pt_datetime'])\n",
    "        y = np.divide(current_evolving_geom_calcs['evolving_outlines_area (m^2)'], 1e6)\n",
    "        \n",
    "        # Plot multi-colored evolving outlines time series using LineCollection from points/segments\n",
    "        points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "        segments = np.concatenate([points[:-1], points[1:]], axis=1) if len(points) > 1 else []\n",
    "        if len(segments) > 0:\n",
    "            lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "            lc.set_array(x)\n",
    "            lc.set_linewidth(2)\n",
    "            ax[1].add_collection(lc)\n",
    "        ax[1].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9, zorder=2)\n",
    "\n",
    "\n",
    "        # Panel C - Cumulative dh/dt\n",
    "        ax[2].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "        \n",
    "        # Plot cumulative values up to current date\n",
    "        ax[2].plot(x, np.cumsum(current_stationary_geom_calcs['stationary_outline_region_dh (m)']).astype(float), \n",
    "            color='lightgray', linestyle='solid', linewidth=1)\n",
    "        ax[2].plot(x, np.cumsum(current_evolving_geom_calcs['evolving_outlines_region_dh (m)']), \n",
    "            color='dimgray', linestyle='solid', linewidth=1)\n",
    "        ax[2].plot(x, np.cumsum(current_stationary_geom_calcs['stationary_outline_dh_corr (m)']), \n",
    "            color=stationary_color, linestyle='solid', linewidth=1)\n",
    "        ax[2].plot(x, np.cumsum(current_evolving_union_geom_calcs['stationary_outline_dh_corr (m)']), \n",
    "            color='k', linestyle='dashed', linewidth=1)\n",
    "        ax[2].plot(x, np.cumsum(current_evolving_geom_calcs['evolving_outlines_dh_corr (m)']-\n",
    "            current_stationary_geom_calcs['stationary_outline_dh_corr (m)']), \n",
    "            color='red', linestyle='solid', linewidth=1)\n",
    "        \n",
    "        # Plot multi-colored evolving outlines time series using LineCollection from points/segments\n",
    "        y = np.cumsum(current_evolving_geom_calcs['evolving_outlines_dh_corr (m)'])\n",
    "        points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "        segments = np.concatenate([points[:-1], points[1:]], axis=1) if len(points) > 1 else []\n",
    "        if len(segments) > 0:\n",
    "            lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "            lc.set_array(x)\n",
    "            lc.set_linewidth(2)\n",
    "            ax[2].add_collection(lc)\n",
    "        ax[2].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9, zorder=2)\n",
    "\n",
    "        # Set y-axis limits\n",
    "        ax[2].set_ylim(dh_min - 0.1 * dh_range, dh_max + 0.1 * dh_range)\n",
    "\n",
    "\n",
    "        # Panel D - Cumulative dV/dt\n",
    "        ax[3].axhline(0, color='k', linestyle='dashed', linewidth=0.75)\n",
    "        \n",
    "        ax[3].plot(x, np.divide(np.cumsum(current_stationary_geom_calcs['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "            color=stationary_color, linestyle='solid', linewidth=1)\n",
    "        ax[3].plot(x, np.divide(np.cumsum(current_evolving_union_geom_calcs['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "            color='k', linestyle='dashed', linewidth=1)\n",
    "        ax[3].plot(x, np.divide(np.cumsum(current_evolving_geom_calcs['evolving_outlines_dV_corr (m^3)']-\n",
    "            current_stationary_geom_calcs['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "            color='red', linestyle='solid', linewidth=1)\n",
    "\n",
    "        # Plot multi-colored evolving outlines time series using LineCollection from points/segments\n",
    "        y = np.cumsum(np.divide(current_evolving_geom_calcs['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "        points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "        segments = np.concatenate([points[:-1], points[1:]], axis=1) if len(points) > 1 else []\n",
    "        if len(segments) > 0:\n",
    "            lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "            lc.set_array(x)\n",
    "            lc.set_linewidth(2)\n",
    "            ax[3].add_collection(lc)\n",
    "        ax[3].scatter(x, y, c=x, cmap=cmap, norm=norm, s=10, zorder=2)\n",
    "\n",
    "        # Add legends\n",
    "        evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=1)\n",
    "        within_eval_line = plt.Line2D([], [], color='darkgray', linestyle='solid', linewidth=1)\n",
    "        stationary_region_patch = mpatches.Patch(color=stationary_color, alpha=0.2)\n",
    "        evolving_union_region_patch = mpatches.Patch(color='k', alpha=0.2)\n",
    "      \n",
    "        legend_elements = [tuple(onlake_lines),\n",
    "                          evolving_union_line, \n",
    "                          stationary_line, \n",
    "                          within_eval_line,\n",
    "                          stationary_region_patch,\n",
    "                          evolving_union_region_patch]\n",
    "                          \n",
    "        legend_labels = [f'evolving outlines ({evolving_outlines_gdf.level[0]} m)',\n",
    "                       'evolving outlines union',\n",
    "                       'stationary outline',\n",
    "                       f'within evaluation line ({int(evolving_outlines_gdf.within_area_multiple[0])}x)',\n",
    "                       'stationary region',\n",
    "                       'evolving union region']\n",
    "        \n",
    "        if has_offlake_outlines:\n",
    "            legend_elements.insert(1, tuple(offlake_lines))\n",
    "            legend_labels.insert(1, 'off-lake evolving outlines')\n",
    "        \n",
    "        legend = ax[0].legend(legend_elements, legend_labels, \n",
    "            handlelength=3,\n",
    "            handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "            loc='upper left',\n",
    "            bbox_to_anchor=(0, 1.5))\n",
    "        \n",
    "        legend = ax[1].legend([tuple(onlake_lines), \n",
    "                               evolving_union_line, \n",
    "                               stationary_line],\n",
    "            ['evolving outlines', \n",
    "             'evolving outlines union', \n",
    "             'stationary outline'], \n",
    "            handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "            fontsize='small', loc='upper left',\n",
    "            bbox_to_anchor=(0, 1.15))\n",
    "\n",
    "        evolving_region = plt.Line2D([], [], color='dimgray', linestyle='solid', linewidth=1)\n",
    "        stationary_region = plt.Line2D([], [], color='lightgray', linestyle='solid', linewidth=1)\n",
    "        evolving_union_line = plt.Line2D([], [], color='k', linestyle='dashed', linewidth=1)\n",
    "        bias = plt.Line2D([], [], color='red', linestyle='solid', linewidth=1)\n",
    "        legend = ax[2].legend(\n",
    "            [evolving_region,\n",
    "             stationary_region,\n",
    "             tuple(onlake_lines),\n",
    "             evolving_union_line,\n",
    "             stationary_line,  \n",
    "             bias],\n",
    "            ['evolving outlines region',\n",
    "             'stationary outline region',\n",
    "             'evolving outlines',\n",
    "             'evolving outlines union',\n",
    "             'stationary outline', \n",
    "             'bias (evolving - stationary)'],\n",
    "             handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "             fontsize='small', loc='upper left',\n",
    "             bbox_to_anchor=(0, 1.22))\n",
    "\n",
    "        legend = ax[3].legend([tuple(onlake_lines), \n",
    "                               evolving_union_line, \n",
    "                               stationary_line, \n",
    "                               bias],\n",
    "            ['evolving outlines',\n",
    "             'evolving outlines union',\n",
    "             'stationary outline',\n",
    "             'bias (evolving - stationary)'], \n",
    "            handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "            fontsize='small', loc='upper left',\n",
    "            bbox_to_anchor=(0, 1.15))\n",
    "\n",
    "        # Set y-axis limits\n",
    "        ax[3].set_ylim(dv_min - 0.1 * dv_range, dv_max + 0.1 * dv_range)\n",
    "\n",
    "        for i in range(1, ncols):\n",
    "            # Set x-axis limits\n",
    "            ax[i].set_xlim(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "            \n",
    "            # Set ticks for all years but labels only for even years (to match colorbar)\n",
    "            tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "            tick_labels = [date.strftime('%Y') if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "            ax[i].set_xticks(tick_locations)\n",
    "            ax[i].set_xticklabels(tick_labels)\n",
    "            \n",
    "            # Add minor ticks for quarters\n",
    "            ax[i].xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))\n",
    "            \n",
    "            ax[i].set_xlabel('year')\n",
    "\n",
    "        # Set titles\n",
    "        ax[0].set_title(f'{lake_name}\\ndh mid-point date: {current_date_pd.strftime(\"%Y-%m-%d\")}', size=12, y=1.5)\n",
    "        ax[1].set_title('active area [km$^2$]', size=12, y=1.15)\n",
    "        ax[2].set_title('cumulative dh [m]', size=12, y=1.22)\n",
    "        ax[3].set_title('cumulative dV [km$^3$]', size=12, y=1.15)\n",
    "\n",
    "        # Save and close\n",
    "        try:\n",
    "            plt.savefig(os.path.join(sequential_dir, f'{lake_name}_{current_date_pd.strftime(\"%Y%m%d\")}.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        except Exception as e:\n",
    "            print(f'Error saving plot for {current_date_pd}: {str(e)}')\n",
    "\n",
    "        # Clean up to conserve memory\n",
    "        current_evolving_geom_calcs = None\n",
    "        current_stationary_geom_calcs = None\n",
    "        evolving_outlines_dt = None\n",
    "        offlake_outlines_dt = None\n",
    "        del current_dh, fig\n",
    "        if 'img' in locals():  # Only delete img if it exists\n",
    "            del img\n",
    "\n",
    "        # Explicitly clear the figure, close, and clear any reference to it\n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "\n",
    "        # Force garbage collection\n",
    "        gc.collect() \n",
    "        \n",
    "    # Modify video creation to use the new forward_fill output path\n",
    "    try:\n",
    "        video_from_images_2(lake_gdf, output_dir=OUTPUT_DIR, \n",
    "            fps=1, img_extension='png', forward_fill=forward_fill)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error creating video for {lake_name}: {str(e)}')\n",
    "        traceback.print_exc()\n",
    "\n",
    "def video_from_images_2(lake_gdf, output_dir=OUTPUT_DIR, fps=1, img_extension='png', forward_fill=False):\n",
    "    '''\n",
    "    Creates a video from still images stored in a folder based on the lake_gdf input, then deletes the images.\n",
    "    \n",
    "    Parameters:\n",
    "    - lake_gdf: GeoDataFrame containing lake information (expects a 'name' column).\n",
    "    - output_dir: Base directory where the images and video are stored/created.\n",
    "    - fps: Frames per second for the output video.\n",
    "    - img_extension: Extension of the images to look for in the folder.\n",
    "    - forward_fill: Boolean flag to use forward fill directories\n",
    "\n",
    "    # Example usage\n",
    "    video_from_images(lake_gdf, OUTPUT_DIR, fps=0.5, img_extension='png', forward_fill=False)\n",
    "    '''\n",
    "    lake_name = lake_gdf['name'].iloc[0]\n",
    "    print('Making video for', lake_name)\n",
    "\n",
    "    # Derive paths based on lake_gdf and forward_fill\n",
    "    if forward_fill:\n",
    "        images_folder = os.path.join(output_dir, \n",
    "            f'plot_evolving_and_stationary_comparison_sequential/forward_fill/{lake_name}')\n",
    "        output_video_file = os.path.join(output_dir, \n",
    "            f'plot_evolving_and_stationary_comparison_sequential/forward_fill/{lake_name}.mp4')\n",
    "    else:\n",
    "        images_folder = os.path.join(output_dir, \n",
    "            f'plot_evolving_and_stationary_comparison_sequential/{lake_name}')\n",
    "        output_video_file = os.path.join(output_dir, \n",
    "            f'plot_evolving_and_stationary_comparison_sequential/{lake_name}.mp4')\n",
    "    \n",
    "    # Get all images in the folder with the specified extension\n",
    "    image_files = glob.glob(os.path.join(images_folder, f'*.{img_extension}'))\n",
    "    if not image_files:\n",
    "        print(f'No images found in {images_folder} with extension {img_extension}')\n",
    "        return\n",
    "    \n",
    "    # Read the first image to determine the video size\n",
    "    frame = cv2.imread(image_files[0])\n",
    "    if frame is None:\n",
    "        print(f'Could not read the image {image_files[0]}')\n",
    "        return\n",
    "    height, width, layers = frame.shape\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(output_video_file, fourcc, fps, (width, height))\n",
    "\n",
    "    for image_file in sorted(image_files):\n",
    "        frame = cv2.imread(image_file)\n",
    "        if frame is not None:\n",
    "            video.write(frame)\n",
    "            # Clear the frame from memory\n",
    "            frame = None\n",
    "            gc.collect()\n",
    "\n",
    "    # Release the VideoWriter object\n",
    "    video.release()\n",
    "    video = None\n",
    "    gc.collect()\n",
    "    print(f'Video file {output_video_file} created successfully.')\n",
    "\n",
    "    # Delete the images in the directory\n",
    "    for image_file in image_files:\n",
    "        os.remove(image_file)\n",
    "    print(f'Deleted {len(image_files)} image(s) from {images_folder}')\n",
    "\n",
    "    # Force delete the folder and its contents\n",
    "    try:\n",
    "        shutil.rmtree(images_folder)\n",
    "        print(f'Deleted folder and all contents: {images_folder}')\n",
    "    except Exception as e:\n",
    "        print(f'Could not delete folder {images_folder}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27227c0-08f3-49ac-b03a-ea27195fd984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_evolving_union(lake_ps, evolving_outlines_gdf, incl_stationary=True):\n",
    "    '''\n",
    "    Find the union of evolving outlines and optionally the corresponding stationary\n",
    "    outline and return as a GeoSeries.\n",
    "    \n",
    "    Args:\n",
    "    lake_ps: Pandas series of lake row from stationary lakes geodataframe\n",
    "    evolving_outlines_gdf: GeoDataFrame containing evolving outlines\n",
    "    incl_stationary: Boolean indicating whether to include stationary outline\n",
    "    Returns:\n",
    "    GeoSeries containing the union of the outlines, or None if an error occurs.\n",
    "    '''\n",
    "    lake_name = lake_ps['name']\n",
    "        \n",
    "    try:\n",
    "        if incl_stationary:\n",
    "            # Create a temporary GeoDataFrame for union\n",
    "            temp_gdf = gpd.GeoDataFrame(\n",
    "                geometry=[lake_ps['geometry']] + evolving_outlines_gdf['geometry'].tolist(),\n",
    "                crs=evolving_outlines_gdf.crs\n",
    "            )\n",
    "        else: \n",
    "            temp_gdf = evolving_outlines_gdf[['geometry']].copy()\n",
    "            \n",
    "        # Use union_all() method to find the union of outlines\n",
    "        outlines_union = temp_gdf.geometry.union_all()\n",
    "        \n",
    "        # Create a new GeoSeries with the lake name as index\n",
    "        result = gpd.GeoSeries([outlines_union], index=[lake_name], crs=evolving_outlines_gdf.crs)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error creating union for {lake_name}: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28864789-2af0-4268-86ef-82594074f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_continental_sums(geom_calc_folder):\n",
    "    '''\n",
    "    Sum geometric variables for all lakes in specified folder to create continentally integrated estimates of active area, dh, dV.\n",
    "    Values are rounded to maintain appropriate significant figures:\n",
    "    - dh measurements rounded to two decimal places (due to ~9 cm precision)\n",
    "    - Area measurements rounded to whole numbers (due to 1-km grid resolution)\n",
    "    - dV measurements rounded to whole numbers (due to combined uncertainty)\n",
    "    \n",
    "    Args:\n",
    "        geom_calc_folder (str): folder of evolving outlines geojson being summed\n",
    "    '''\n",
    "    # Define directory\n",
    "    directory = os.path.join('output/geometric_calcs', geom_calc_folder)\n",
    "    print(f'\\nProcessing folder: {geom_calc_folder}')\n",
    "    \n",
    "    # Initialize lists for different lake categories based on satellite coverage\n",
    "    dfs_superset_IS2_lakes = []\n",
    "    dfs_subset_noCS2_IS2_lakes = []\n",
    "    dfs_subset_CS2_IS2_lakes_SARInPreExpansion = []\n",
    "    dfs_subset_CS2_IS2_lakes_SARInPostExpansion = []\n",
    "    \n",
    "    # Process each CSV file\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            lake_name = os.path.splitext(file)[0]\n",
    "            lake_row = reexamined_stationary_outlines_gdf[reexamined_stationary_outlines_gdf['name'] == lake_name]\n",
    "            \n",
    "            if not lake_row.empty:\n",
    "                dfs_superset_IS2_lakes.append(df)\n",
    "                \n",
    "                SARIn_date = lake_row['CS2_SARIn_start'].values[0]\n",
    "                if SARIn_date == '<NA>':\n",
    "                    dfs_subset_noCS2_IS2_lakes.append(df)\n",
    "                if SARIn_date in ['2010.5']:\n",
    "                    dfs_subset_CS2_IS2_lakes_SARInPreExpansion.append(df)\n",
    "                if SARIn_date in ['2010.5', '2013.75']:\n",
    "                    dfs_subset_CS2_IS2_lakes_SARInPostExpansion.append(df)\n",
    "    \n",
    "    # Function to apply rounding based on column names\n",
    "    def apply_precision(df):\n",
    "        # Round values according to specified precision\n",
    "        for col in df.columns:\n",
    "            if col == 'mid_pt_datetime':\n",
    "                continue\n",
    "            # Area measurements to whole numbers\n",
    "            elif 'evolving_outlines_area (m^2)' in col \\\n",
    "                or 'stationary_outline_area (m^2)' in col:\n",
    "                df[col] = df[col].round(0).astype(int)\n",
    "            # dh measurements to two decimal places\n",
    "            elif 'evolving_outlines_dh (m)' in col \\\n",
    "                or 'evolving_outlines_region_dh (m)' in col \\\n",
    "                or 'evolving_outlines_dh_corr (m)' in col \\\n",
    "                or 'stationary_outline_dh (m)' in col \\\n",
    "                or 'stationary_outline_region_dh (m)' in col \\\n",
    "                or 'stationary_outline_dh_corr (m)' in col:\n",
    "                df[col] = df[col].round(2)\n",
    "            elif 'evolving_outlines_dV_corr (m^3)' in col \\\n",
    "                or 'evolving_outlines_dV_corr (m^3)' in col \\\n",
    "                or 'stationary_outline_dV_corr (m^3)' in col \\\n",
    "                or 'stationary_outline_dV_corr (m^3)' in col:\n",
    "                # dV measurements to whole numbers\n",
    "                df[col] = df[col].round(0).astype(int)\n",
    "        return df\n",
    "    \n",
    "    # Process and save each subset\n",
    "    for subset_name, dfs_list in [\n",
    "        ('subset_noCS2_IS2_lakes', dfs_subset_noCS2_IS2_lakes),\n",
    "        ('subset_CS2_IS2_lakes_SARInPreExpansion', dfs_subset_CS2_IS2_lakes_SARInPreExpansion),\n",
    "        ('subset_CS2_IS2_lakes_SARInPostExpansion', dfs_subset_CS2_IS2_lakes_SARInPostExpansion)\n",
    "    ]:\n",
    "        if dfs_list:\n",
    "            df_concat = pd.concat(dfs_list, ignore_index=True)\n",
    "            df_sum = df_concat.groupby('mid_pt_datetime').sum().reset_index()\n",
    "            \n",
    "            # Apply rounding according to precision requirements\n",
    "            df_sum = apply_precision(df_sum)\n",
    "            \n",
    "            output_path = os.path.join(directory, f'{subset_name}_sum.csv')\n",
    "            df_sum.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Process superset data\n",
    "    if dfs_superset_IS2_lakes:\n",
    "        superset_IS2_lakes = pd.concat(dfs_superset_IS2_lakes, ignore_index=True)\n",
    "        superset_IS2_lakes_sum = superset_IS2_lakes.groupby('mid_pt_datetime').sum().reset_index()\n",
    "        superset_IS2_lakes_sum['mid_pt_datetime'] = pd.to_datetime(superset_IS2_lakes_sum['mid_pt_datetime'])\n",
    "        threshold = pd.Timestamp('2019-01-01 06:00:00')\n",
    "        superset_IS2_lakes_sum = superset_IS2_lakes_sum[\n",
    "            superset_IS2_lakes_sum['mid_pt_datetime'] >= threshold\n",
    "        ].reset_index(drop=True)\n",
    "        \n",
    "        # Apply rounding according to precision requirements\n",
    "        superset_IS2_lakes_sum = apply_precision(superset_IS2_lakes_sum)\n",
    "        \n",
    "        output_path = os.path.join(directory, 'superset_IS2_lakes_sum.csv')\n",
    "        superset_IS2_lakes_sum.to_csv(output_path, index=False)\n",
    "    print(f'Successfully processed all data in {geom_calc_folder}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8476ed5e-f7d9-43a2-8e21-e11ffbf0bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def even_year_formatter(x, pos):\n",
    "    '''\n",
    "    Custom formatter that only labels even years\n",
    "    '''\n",
    "    date = mdates.num2date(x)\n",
    "    if date.year % 2 == 0:\n",
    "        return date.strftime('%Y')\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5e0e1c-b26e-4ee3-87eb-ceea82bc5d73",
   "metadata": {},
   "source": [
    "# Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d1542f-fb79-4965-a13f-3c84a3fa69fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import subglacial lake outlines \n",
    "stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/stationary_outlines_gdf.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1720b14b-ed5a-4870-b3f9-e9429a07e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CryoSat-2 SARIn mode mask\n",
    "# See 0_preprocess_data.ipynb for data source and pre-processing steps\n",
    "gdf_SARIn_3_1 = gpd.read_file('output/CryoSat2_SARIn_mode_masks/gdf_SARIn_3_1.geojson')\n",
    "gdf_SARIn_3_1_3_6_diff= gpd.read_file('output/CryoSat2_SARIn_mode_masks/gdf_SARIn_3_1_3_6_diff.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c97b1e-bbd7-4f54-a26d-21d2aa3c9c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import MODIS MOA 2014 coastline and grounding line\n",
    "# https://doi.org/10.5067/RNF17BP824UM\n",
    "\n",
    "shp = DATA_DIR + '/moa2014_coastline_v01.shp' \n",
    "moa_2014_coastline = gpd.read_file(shp)\n",
    "shp = DATA_DIR + '/moa2014_grounding_line_v01.shp' \n",
    "moa_2014_groundingline = gpd.read_file(shp)\n",
    "\n",
    "# Import MODIS Mosaic of Antarctica (MOA) surface imagery\n",
    "# https://doi.org/10.5067/RNF17BP824UM\n",
    "\n",
    "moa_highres = DATA_DIR + '/moa125_2014_hp1_v01.tif'\n",
    "moa_highres_da = rioxarray.open_rasterio(moa_highres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e972c7-99cd-4b3d-a00e-4ec6e8e063b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Smith and Sauthoff, 2025 (CryoSat-2 SARIn Height Change and Reference DEM for Antarctica)\n",
    "# https://doi.org/10.5281/zenodo.14963551\n",
    "\n",
    "# Specify the variables to keep\n",
    "keep_vars = ['time', 'y', 'x', 'delta_h', 'data_count']\n",
    "\n",
    "# Combine quadrants into one data set\n",
    "SARIn_dh = combine_quadrants_by_coords(DATA_DIR, ds_prefix='CryoSat2_SARIn_delta_h', keep_vars=keep_vars)\n",
    "\n",
    "# View data set\n",
    "SARIn_dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a222315-65b6-41c9-b947-173104035140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View time data variable to select time step to slice data to conserve memory\n",
    "SARIn_dh['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14361c53-5f4d-4674-9cd3-8b998c37fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove time slices that occur during the ICESat-2 era that will not be used \n",
    "# to conserve memory when loaded for data analysis\n",
    "\n",
    "# end_date includes one quarter of overlapping data with ICESat-2 time series\n",
    "# to allow for cyc-to-cyc differencing to remove datum from delta_h to create cycle-to-cycle dh\n",
    "end_date = '2019-01-01T00:00:00.000000000'\n",
    "\n",
    "SARIn_dh = SARIn_dh.sel(time=slice(None, end_date))\n",
    "\n",
    "# Preview temporally subset data set's time variable\n",
    "SARIn_dh['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5dd4c7-cb4d-4e98-a6e5-2396278c2647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Authenticate with Earthdata Login\n",
    "auth = earthaccess.login()\n",
    "\n",
    "# Find ICESat-2 ATL15 v004 data granules\n",
    "# https://doi.org/10.5067/ATLAS/ATL15.004\n",
    "results = earthaccess.search_data(\n",
    "    doi='10.5067/ATLAS/ATL15.004',\n",
    "    bounding_box=(-180, -90, 180, -60),  # (lower_left_lon, lower_left_lat , upper_right_lon, upper_right_lat))\n",
    "    cloud_hosted=True,\n",
    ")\n",
    "\n",
    "# Open data granules as s3 files to stream\n",
    "files = earthaccess.open(results)\n",
    "\n",
    "# View files list\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ffb0a5-9651-474e-b47b-02340c93c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to 1-km resolution data sets\n",
    "filtered_files = [f for f in files if '01km' in str(f)]\n",
    "\n",
    "# Delete intermediary objects for memory conservation\n",
    "del results, files\n",
    "\n",
    "# Sort alphabetically by the data set file name\n",
    "filtered_files.sort(key=lambda x: str(x).split('/')[-1])\n",
    "\n",
    "# Display filtered list\n",
    "filtered_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae55ad9-3130-429d-af7c-bab068eaa9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to open dataset safely with retries\n",
    "def safe_open_and_filter(file, group='delta_h', retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            print(f'Opening file (attempt {attempt + 1})')\n",
    "            ds = xr.open_dataset(file, group=group)\n",
    "\n",
    "            return ds\n",
    "\n",
    "            # Clear output\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Attempt {attempt+1} for file {file} failed: {e}')\n",
    "            if attempt < retries - 1:\n",
    "                print('Retrying...')\n",
    "            else:\n",
    "                print(f'Failed to open file: {file} after {retries} attempts.')\n",
    "                raise e\n",
    "\n",
    "# Dynamically open, retry, and filter datasets\n",
    "ATL15_datasets = []\n",
    "for i, file in enumerate(filtered_files):\n",
    "    print(f'Processing dataset {i+1}/{len(filtered_files)}: {file}')\n",
    "    ATL15_datasets.append(safe_open_and_filter(file))\n",
    "    print(f'Finished processed data')\n",
    "\n",
    "# Assign ATL15_datasets to variables dynamically\n",
    "for i, ds in enumerate(ATL15_datasets):\n",
    "    dataset_name = f'ATL15_A{i+1}'\n",
    "    globals()[dataset_name] = ds\n",
    "    print(f'Dataset assigned to variable: {dataset_name}')\n",
    "\n",
    "# Check datasets (optional)\n",
    "print('All datasets processed and assigned.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e289f451-c347-4516-a939-9394ea17ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the variables to keep\n",
    "keep_vars = ['time', 'y', 'x', 'delta_h', 'data_count']\n",
    "\n",
    "# Combine quadrants into one data set\n",
    "ATL15_dh = combine_quadrants_by_coords([ATL15_A1, ATL15_A2, ATL15_A3, ATL15_A4], keep_vars=keep_vars)\n",
    "\n",
    "# View data set\n",
    "ATL15_dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b88e5-6957-4f63-b9d9-70591d18af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add datasets attributes\n",
    "ATL15_dh.attrs['identifier_product_DOI'] = '10.5067/ATLAS/ATL15.004'\n",
    "ATL15_dh.attrs['shortName'] = 'ATL15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0bae96-66b2-4c2e-8c49-5623c3fde835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scripps Grounding Line\n",
    "# https://doi.pangaea.de/10.1594/PANGAEA.819147\n",
    "Scripps_gl = gpd.read_file(DATA_DIR + \n",
    "    '/boundaries/Depoorter2013/Antarctica_masks/scripps_antarctica_polygons_v1.shp')\n",
    "\n",
    "# Isolate only land ice\n",
    "Scripps_landice = Scripps_gl[Scripps_gl['Id_text'] == 'Grounded ice or land']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df19fcb6-5ea0-4d70-946d-53e960a9bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip altimetry datasets to grounding line to limit analysis to below grounded ice\n",
    "SARIn_dh.rio.write_crs(3031, inplace=True)\n",
    "SARIn_dh = SARIn_dh.rio.clip(Scripps_landice.geometry.values, Scripps_landice.crs, drop=False)\n",
    "ATL15_dh.rio.write_crs(3031, inplace=True)\n",
    "ATL15_dh = ATL15_dh.rio.clip(Scripps_landice.geometry.values, Scripps_landice.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955c5399-d3c1-47b0-90df-ba9ecaa637a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cyc_dates\n",
    "cyc_dates = pd.read_csv('output/cycle_dates.csv', parse_dates=['cyc_start_datetimes', 'cyc_end_datetimes'])\n",
    "\n",
    "# View dates\n",
    "cyc_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad65559-91e2-4419-86ca-66b37ac09980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store the cyc_dates columns as a np array with datetime64[ns] data type\n",
    "cyc_start_datetimes = [np.datetime64(ts) for ts in cyc_dates['cyc_start_datetimes']]\n",
    "cyc_end_datetimes = [np.datetime64(ts) for ts in cyc_dates['cyc_end_datetimes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa355c-fb15-40c5-8207-46749801d19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of height change (dh) mid-point datetimes\n",
    "\n",
    "# Use full datasets of each altimetry mission to get all possible dh mid points\n",
    "dataset1 = SARIn_dh\n",
    "dataset2 = ATL15_dh\n",
    "\n",
    "# Calculate mid-point datetimes for dataset1\n",
    "dataset1_mid_pt_times = []\n",
    "dataset1_datetimes = dataset1['time'].values\n",
    "for i in range(1, len(dataset1_datetimes)):\n",
    "    mid_pt_days = dataset1_datetimes[i] - dataset1_datetimes[i-1]\n",
    "    mid_pt_date = dataset1_datetimes[i-1] + mid_pt_days\n",
    "    dataset1_mid_pt_times.append(mid_pt_date)\n",
    "dataset1_mid_pt_times = np.array(dataset1_mid_pt_times)\n",
    "\n",
    "# Calculate mid-point datetimes\n",
    "dataset2_mid_pt_times = []\n",
    "dataset2_datetimes = dataset2['time'].values    \n",
    "for i in range(1, len(dataset2_datetimes)):\n",
    "    mid_pt_days = dataset2_datetimes[i] - dataset2_datetimes[i-1]\n",
    "    mid_pt_date = dataset2_datetimes[i-1] + mid_pt_days\n",
    "    dataset2_mid_pt_times.append(mid_pt_date)\n",
    "dataset2_mid_pt_times = np.array(dataset2_mid_pt_times)\n",
    "\n",
    "# Combine\n",
    "mid_pt_datetimes = []\n",
    "mid_pt_datetimes = np.concatenate([dataset1_mid_pt_times, dataset2_mid_pt_times])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596962e3-e1e3-47c9-b19e-62d368182982",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analysis of previously identified lakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd247dc3-6600-41e4-9b3b-c03b75aa04bf",
   "metadata": {},
   "source": [
    "## Find and save optimal parameters; visualize and save evolving outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da53083f-49f3-43cf-9cbc-b494c7194820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find evolving outlines at optimal levels at various within_evaluation boundaries for each lake\n",
    "\n",
    "# Get list of remaining lakes left to process that don't have an output file in the folder_path\n",
    "folder_path = OUTPUT_DIR + '/levels'\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "remaining_lakes = filter_gdf_by_folder_contents(stationary_outlines_gdf, folder_path)\n",
    "\n",
    "if remaining_lakes.empty:\n",
    "    print('All lakes processed.')\n",
    "else:\n",
    "    for i in range(len(remaining_lakes)):\n",
    "        print(len(remaining_lakes), 'lake(s) remain.')\n",
    "        \n",
    "        # Process the lake\n",
    "        lake_gdf = remaining_lakes.iloc[0:1]\n",
    "        find_and_save_optimal_parameters(lake_gdf)\n",
    "        visualize_and_save_evolving_outlines(lake_gdf)\n",
    "\n",
    "        # Recheck which lakes still need processing\n",
    "        remaining_lakes = filter_gdf_by_folder_contents(stationary_outlines_gdf, folder_path)\n",
    "        \n",
    "        if remaining_lakes.empty:\n",
    "            print('All lakes processed.')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fef049-b6b2-489c-a1c0-2119aef45a12",
   "metadata": {},
   "source": [
    "## Extended analysis at lakes with no evolving outlines found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2fc45-e536-4969-8082-d579f3c21892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry lakes with no evolving outlines found at greater within_area_multiples\n",
    "\n",
    "# Find evolving outlines at various search extents and levels for each lake\n",
    "\n",
    "# Get list of remaining lakes left to process that don't have an output file in the folder_path\n",
    "folder_path = OUTPUT_DIR + '/levels'\n",
    "remaining_lakes = filter_gdf_by_folder_contents(stationary_outlines_gdf, folder_path, exclude=False, file_extension='txt')\n",
    "\n",
    "if remaining_lakes.empty:\n",
    "    print('All lakes processed.')\n",
    "else:\n",
    "    for i in range(len(remaining_lakes)):\n",
    "        print(len(remaining_lakes), 'lake(s) remain.')\n",
    "        \n",
    "        # Process the lake\n",
    "        lake_gdf = remaining_lakes.iloc[i:i+1]\n",
    "        find_and_save_optimal_parameters(lake_gdf, within_area_multiples=range(16, 21))\n",
    "        visualize_and_save_evolving_outlines(lake_gdf)\n",
    "\n",
    "        if remaining_lakes.empty:\n",
    "            print('All lakes processed.')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bb5a49-3397-4460-9a78-503fb52994a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any 'no outlines' TXT files when there is a levels CSV file in two directories\n",
    "cleanup_duplicate_files(directory_path=OUTPUT_DIR + '/levels',\n",
    "    keep_extension='csv', delete_extension='txt')\n",
    "\n",
    "cleanup_duplicate_files(directory_path='output/lake_outlines/evolving_outlines/',\n",
    "    keep_extension='geojson', delete_extension='txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6177f68b-afdb-40c7-9ee2-358008ab9b27",
   "metadata": {},
   "source": [
    "## Ensure outlines were visualized and saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71468bef-565c-4ee1-bd71-281c249e8b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all lakes were processed using the visualize_and_save_evolving_outlines func\n",
    "\n",
    "# First set of lakes: lakes that don't have a txt or geojson file generated\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "remaining_lakes_set1 = filter_gdf_by_folder_contents(stationary_outlines_gdf, folder_path, exclude=True)\n",
    "\n",
    "# Second set of lakes: lakes with evolving outlines saved as geojson but without mp4 visualizations\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "remaining_lakes_set2 = filter_gdf_by_folder_contents(stationary_outlines_gdf, folder_path, exclude=False, file_extension='.geojson')\n",
    "\n",
    "# Further filter the second set to include only lakes without mp4 visualizations\n",
    "folder_path = OUTPUT_DIR + '/find_evolving_outlines'\n",
    "remaining_lakes_set2 = filter_gdf_by_folder_contents(\n",
    "    remaining_lakes_set2, \n",
    "    folder_path,\n",
    "    exclude=True,\n",
    "    suffix_pattern=r'_\\d+-idx_\\d+\\.\\d+m-level_\\d+x-within',\n",
    "    file_extension='mp4'\n",
    ")\n",
    "\n",
    "# Second set of lakes: lakes with evolving outlines saved as geojson but without png visualizations\n",
    "# Do this only on lake with evolving outlines saved as geojson file in evolving_outlines directory\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "remaining_lakes = filter_gdf_by_folder_contents(stationary_outlines_gdf, folder_path, exclude=False, file_extension='.geojson')\n",
    "\n",
    "# Visualization png's saved to plot_evolving_outlines_time_series directory\n",
    "folder_path = OUTPUT_DIR + '/plot_evolving_outlines_time_series'\n",
    "remaining_lakes = filter_gdf_by_folder_contents(\n",
    "    remaining_lakes, \n",
    "    folder_path,\n",
    "    suffix_pattern=r'_\\d+-idx_\\d+\\.\\d+m-level_\\d+x-within',\n",
    "    file_extension='png'\n",
    ")\n",
    "\n",
    "# Combine both sets of lakes (no need to drop duplicates)\n",
    "remaining_lakes = pd.concat([remaining_lakes_set1, remaining_lakes_set2], ignore_index=True)\n",
    "\n",
    "# Reprocess remaining lakes\n",
    "if remaining_lakes.empty:\n",
    "    print('All lakes processed.')\n",
    "    \n",
    "else:\n",
    "    total_lakes = len(remaining_lakes)\n",
    "    processed_lakes = 0\n",
    "\n",
    "    try:\n",
    "        for i in range(len(remaining_lakes)):\n",
    "            remaining_count = total_lakes - processed_lakes\n",
    "            print(f'{remaining_count} lakes remain.')\n",
    "        \n",
    "            # Process the lake\n",
    "            lake_gdf = remaining_lakes.iloc[i:i+1]\n",
    "            visualize_and_save_evolving_outlines(lake_gdf)\n",
    "\n",
    "            # Increment processing counter\n",
    "            processed_lakes += 1\n",
    "\n",
    "            # Clear output of each index\n",
    "            clear_output(wait=True)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f'Error processing lake at index {i}: {str(e)}')\n",
    "            \n",
    "    print('All lakes processed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d9f9d-2b9b-497e-bc61-6d9a13894d3b",
   "metadata": {},
   "source": [
    "# Lake groups\n",
    "\n",
    "From reviewing the evolving outlines, we observed lakes that have neighbor lake and appear to interact with that neighbor, so we analyze those lake groupings as lake systems where two or more lakes are analyzed together see if perhaps the lakes should be considered as one lake or remain as separate lakes. Additionally The upper Thwaites lakes are close neighbors we attempted to group them to see if a more optimal level could be obtained when analyzed simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2c9c41-c404-45a6-bf71-ed8ffb6303d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define lake groups\n",
    "lake_groups = [\n",
    "    ['Mac1', 'Mac2'],\n",
    "    ['Site_B', 'Site_C'],\n",
    "    ['Slessor_4', 'Slessor_5'],\n",
    "    ['Thw_70', 'Thw_124', 'Thw_142', 'Thw_170'],\n",
    "    ['Thw_124', 'Thw_142', 'Thw_170'],\n",
    "    ['Thw_124', 'Thw_142'],\n",
    "    ['Thw_142', 'Thw_170']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2978f041-1e33-44ed-b579-db0bbd74545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each group\n",
    "for lake_group in lake_groups:\n",
    "\n",
    "    # Create combined lake group GeoDataFrame row\n",
    "    group_single_gdf = prepare_group_gdf(stationary_outlines_gdf, lake_group)\n",
    "    \n",
    "    # First find search extents and levels for the group\n",
    "    find_and_save_optimal_parameters(group_single_gdf, within_area_multiples=range(5, 16))\n",
    "        \n",
    "    # Then finalize the evolving outlines using these parameters\n",
    "    visualize_and_save_evolving_outlines(group_single_gdf)\n",
    "\n",
    "del lake_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde5fc9-103b-4a4e-886d-06a44f0e891c",
   "metadata": {},
   "source": [
    "After reviewing the results of the lake groups analysis, I found:\n",
    "* Mac1_Mac2 have distinct activity but there is an interesting dh expression between the lakes during the CryoSat-2 era that may be indicative of lake migration; however, analyzing as a group does not add new information as the interesting feature was found over Mac2 in the individual lake analysis and the level found analyzing as a lake system (0.4 m) was higher than analyzing separately: Mac1 (0.30 m) and Mac2 (0.36 m).\n",
    "* Site_B_Site_C have two dh espressions that both overlap with locations of the two lakes; it's unclear if either of the dh expressions belong to one of the lakes or the other because the two dh expressions are nearly centered between the two lakes with some lateral offset. This is an improvement from analyzing the two lakes separately where Site C had a lowest optimal level of 1.27 m and Site B had no evolving outlines found compared to analyzed as lake group had a lowest optimal level of 1.2 m.\n",
    "* Slessor_4_Slessor_5 have unconvincing evidence of being one lake system: there is one time slice where there dh expression covering both lakes, 2019-07-02 to 2019-10-02, but several other time slices have dh expressions of opposite sign.\n",
    "* Thw_70_Thw_124_Thw_142_Thw_170 does not improve analysis from individual lakes because there is no overlapping activity and the level (1.37 m) is higher than two out of four of the lakes analyzed individually.\n",
    "* Thw_124_Thw_142_Thw_170 has some potential as the there overlapping activity over the three lakes and a lower level (0.53 m) than two of the three lakes; however, there are many off-lake outlines identified, so we will plot the second most optimal level of this lake grouping.\n",
    "* Thw_124_Thw_142 has some potential in identifying overlapping outlines shared between these lakes; however, Thw_170 is included in the found outlines, so it makes sense to use that grouping instead.\n",
    "* Thw_142_Thw_170 was not useful because of the high level (1.86 m) found to be most optimal for this grouping.\n",
    "\n",
    "Based on this, we will delete the evolving outlines generated and saved to geojson files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaedcc3-0194-43f9-8a13-3063da885840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Site_B_Site_C to Site_BC to follow naming convention used for at past lake unions,\n",
    "# Lake_78 and Slessor_23\n",
    "\n",
    "old_names = ['output/lake_outlines/evolving_outlines/Site_B_Site_C.geojson', \n",
    "             OUTPUT_DIR + '/levels/Site_B_Site_C.csv',\n",
    "             OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines/Site_B_Site_C.geojson']\n",
    "new_names = ['output/lake_outlines/evolving_outlines/Site_BC.geojson',\n",
    "            OUTPUT_DIR + '/levels/Site_BC.csv',\n",
    "            OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines/Site_BC.geojson']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1cc6d-780f-432b-83a7-99e695501fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through both lists simultaneously\n",
    "for old_name, new_name in zip(old_names, new_names):\n",
    "    # Try to rename file\n",
    "    if os.path.exists(old_name):\n",
    "        os.rename(old_name, new_name)\n",
    "        print(f'Successfully renamed {old_name} to {new_name}')\n",
    "    else:\n",
    "        print(f'File not found: {old_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1def7d8-0fb3-4630-a6fc-06b075dd41a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_outputs_to_delete = [\n",
    "    'Mac1_Mac2',\n",
    "    'Site_B', 'Site_C',\n",
    "    'Slessor_4_Slessor_5',\n",
    "    'Thw_70_Thw_124_Thw_142_Thw_170',\n",
    "    'Thw_142_Thw_170'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5727487d-b1fa-4a1a-bc92-dfd4b1b4ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in lakes_outputs_to_delete:\n",
    "    dir = OUTPUT_DIR + '/levels/'\n",
    "    file_path = os.path.join(dir, f'{filename}.csv')\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f'Deleted: {file_path}')\n",
    "    else:\n",
    "        print(f'Not found: {file_path}')\n",
    "\n",
    "    dir = 'output/lake_outlines/evolving_outlines/'\n",
    "    for ext in ('.geojson', '.txt'):\n",
    "        file_path = os.path.join(dir, f'{filename}{ext}')\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f'Deleted: {file_path}')\n",
    "        else:\n",
    "            print(f'Not found: {file_path}')\n",
    "\n",
    "del dir, lakes_outputs_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39f9066-a790-451e-a701-68284a467bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write no outlines txt file to two lakes that will be replaced by the combination lake\n",
    "dir = 'output/lake_outlines/evolving_outlines/'\n",
    "\n",
    "for lake in ['Site_B', 'Site_C']:\n",
    "    write_no_outlines(dir + lake)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253a34c5-5457-4a21-b6ac-179a9ccb2325",
   "metadata": {},
   "source": [
    "Next I address lakes that have evolving outlines that appear flawed because of the number of off-lake outlines that make it appear that the lowest level selected using the algorithm perhaps was too low. We address this by selecting the next most optimal level/within_area_multiple combination contained in the levels csv file for these lakes in the first row (instead of the default zeroth row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a01d67-6acc-4c00-a449-4cbdffacb083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lake groups\n",
    "lake_groups_analyze_row_1 = [\n",
    "    ['Thw_124', 'Thw_142', 'Thw_170'],\n",
    "    ['Thw_124', 'Thw_142'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c07ad-d580-4c10-a600-1ee4fe92b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lake_group in lake_groups_analyze_row_1:\n",
    "\n",
    "    # Create combined lake group GeoDataFrame row\n",
    "    group_single_gdf = prepare_group_gdf(stationary_outlines_gdf, lake_group)\n",
    "    \n",
    "    # Visualize second level stored in first row\n",
    "    visualize_and_save_evolving_outlines(group_single_gdf, row_index=1)\n",
    "\n",
    "del lake_groups_analyze_row_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4697db-30cc-4438-9237-653d15f00c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lake groups\n",
    "lake_groups_analyze_row_2 = [\n",
    "    ['Thw_124', 'Thw_142', 'Thw_170'],\n",
    "    ['Thw_124', 'Thw_142']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd29d3d3-8648-4eb7-9936-6a1e3129ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lake_group in lake_groups_analyze_row_2:\n",
    "\n",
    "    # Create combined lake group GeoDataFrame row\n",
    "    group_single_gdf = prepare_group_gdf(stationary_outlines_gdf, lake_group)\n",
    "    \n",
    "    # Visualize second level stored in first row\n",
    "    visualize_and_save_evolving_outlines(group_single_gdf, row_index=2)\n",
    "\n",
    "del lake_groups_analyze_row_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb897137-e826-49d5-a436-7c11ef13ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lake groups\n",
    "lake_groups_analyze_row_3 = [\n",
    "    ['Thw_124', 'Thw_142', 'Thw_170'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db9a661-5115-4354-8d0c-195ca9b85dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lake_group in lake_groups_analyze_row_3:\n",
    "\n",
    "    # Create combined lake group GeoDataFrame row\n",
    "    group_single_gdf = prepare_group_gdf(stationary_outlines_gdf, lake_group)\n",
    "    \n",
    "    # Visualize second level stored in first row\n",
    "    visualize_and_save_evolving_outlines(group_single_gdf, row_index=3)\n",
    "\n",
    "del lake_groups_analyze_row_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ea689b-3ab5-4fb7-a0ea-8f125f41ac90",
   "metadata": {},
   "source": [
    "The various levels of Thw_124_Thw_142_Thw_170 did not provide better outlines than analyzing the lakes individually, but instead Thw_124_Thw_142 is able to capture the lobing activity of Thw_142 and more of Thw_124's activity at a lower level than when each lake is analyzed separately. However, their activity appears spatial distinct, so we will:\n",
    "1) use evolving outlines generated in the analysis of lake group, Thw_124_Thw_142, for the individual lake evolving outlines for each respective lake by separating them spatially.\n",
    "2) delete the Thw_124_Thw_142_Thw_170 geojson file as it did not prove useful analyzing these three lakes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef041aa-ac94-4636-a8f1-cfca74ea6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "evolving_outlines_path = os.path.join('output/lake_outlines/evolving_outlines/Thw_124_Thw_142.geojson')\n",
    "\n",
    "# Read the evolving outlines\n",
    "Thw_124_Thw_142_evolving_outlines_gdf = gpd.read_file(evolving_outlines_path)\n",
    "\n",
    "# Get the stationary outlines for each lake\n",
    "Thw_124_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == 'Thw_124']\n",
    "Thw_142_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == 'Thw_142']\n",
    "\n",
    "if Thw_124_gdf.empty or Thw_142_gdf.empty:\n",
    "    raise ValueError(f'Could not find one or both lakes in stationary_outlines_gdf: {Thw_124_gdf['name']}, {Thw_142_gdf['name']}')\n",
    "\n",
    "# Extract outlines that intersect with each lake\n",
    "Thw_124_outlines, Thw_124_offlake_outlines = extract_intersecting_polygons_recursive(Thw_124_Thw_142_evolving_outlines_gdf, Thw_124_gdf.geometry.iloc[0])\n",
    "Thw_142_outlines, Thw_142_offlake_outlines = extract_intersecting_polygons_recursive(Thw_124_Thw_142_evolving_outlines_gdf, Thw_142_gdf.geometry.iloc[0])\n",
    "\n",
    "if Thw_124_outlines is not None and not Thw_124_outlines.empty:\n",
    "    lake_name = 'Thw_124'\n",
    "    Thw_124_outlines.to_file(f'output/lake_outlines/evolving_outlines/{lake_name}.geojson', driver='GeoJSON')\n",
    "    print(f'Saved outlines for {lake_name}')\n",
    "\n",
    "if Thw_142_outlines is not None and not Thw_142_outlines.empty:\n",
    "    lake_name = 'Thw_142'\n",
    "    Thw_142_outlines.to_file(f'output/lake_outlines/evolving_outlines/{lake_name}.geojson', driver='GeoJSON')\n",
    "    print(f'Saved outlines for {lake_name}')\n",
    "\n",
    "del evolving_outlines_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd132a0-c1a5-4c6a-9c30-002940f74f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick visualization to ensure outlines were split properly\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Get bounds for plot extent\n",
    "total_bounds = Thw_124_Thw_142_evolving_outlines_gdf.total_bounds\n",
    "x_min, y_min, x_max, y_max = total_bounds\n",
    "buffer_factor = 0.2\n",
    "x_buffer = (x_max - x_min) * buffer_factor\n",
    "y_buffer = (y_max - y_min) * buffer_factor\n",
    "\n",
    "# Plot MOA background\n",
    "mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "moa_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "ax.imshow(moa_subset[0,:,:], cmap='gray', clim=[14000, 17000], \n",
    "         extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "\n",
    "# Plot the outlines\n",
    "Thw_124_outlines_imported = gpd.read_file('output/lake_outlines/evolving_outlines/Thw_124.geojson')\n",
    "Thw_142_outlines_imported = gpd.read_file('output/lake_outlines/evolving_outlines/Thw_142.geojson')\n",
    "Thw_124_outlines_imported.boundary.plot(ax=ax, color='red', linewidth=1, label='Thw_124 evolving')\n",
    "Thw_142_outlines_imported.boundary.plot(ax=ax, color='blue', linewidth=1, label='Thw_142 evolving')\n",
    "    \n",
    "# Plot stationary outlines\n",
    "Thw_124_gdf.boundary.plot(ax=ax, color='darkred', linestyle='--', linewidth=2, label=f'{Thw_124_gdf['name'].iloc[0]} stationary')\n",
    "Thw_142_gdf.boundary.plot(ax=ax, color='darkblue', linestyle='--', linewidth=2, label=f'{Thw_142_gdf['name'].iloc[0]} stationary')\n",
    "\n",
    "# Add inset map\n",
    "axins = ax.inset_axes([0.05, 0.05, 0.3, 0.3])\n",
    "axins.set_aspect('equal')\n",
    "moa_2014_coastline.plot(ax=axins, color='gray', edgecolor='k', linewidth=0.1)\n",
    "moa_2014_groundingline.plot(ax=axins, color='ghostwhite', edgecolor='k', linewidth=0.1)\n",
    "center_x = (x_min + x_max) / 2\n",
    "center_y = (y_min + y_max) / 2\n",
    "axins.scatter(center_x, center_y, c='red', marker='*', s=100, zorder=5)\n",
    "axins.axis('off')\n",
    "\n",
    "# Format main plot\n",
    "km_scale = 1e3\n",
    "ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale)))\n",
    "ax.set_xlabel('x [km]')\n",
    "ax.set_ylabel('y [km]')\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2)\n",
    "ax.set_title('Split Evolving Lake Outlines')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfa50b-245a-433c-bfce-ccfcc9d9296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot evolving outlines time series for Thw_124 and Thw_142\n",
    "Thw_124_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == 'Thw_124']\n",
    "Thw_142_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == 'Thw_142']\n",
    "plot_evolving_outlines_time_series(Thw_124_gdf, Thw_124_outlines, Thw_124_offlake_outlines)\n",
    "plot_evolving_outlines_time_series(Thw_142_gdf, Thw_142_outlines, Thw_142_offlake_outlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8e019-df5f-45b7-9f98-36bd7f39a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del Thw_124_gdf, Thw_124_outlines, Thw_124_offlake_outlines\n",
    "del Thw_142_gdf, Thw_142_outlines, Thw_142_offlake_outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c1b497-8580-48e0-9bc1-c9958cfc6c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_outputs_to_delete = [\n",
    "    'Thw_124_Thw_142_Thw_170',\n",
    "    'Thw_124_Thw_142'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d8f9dc-1458-41d6-8ddb-a7876954a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in lakes_outputs_to_delete:\n",
    "    dir = OUTPUT_DIR + '/levels/'\n",
    "    file_path = os.path.join(dir, f'{filename}.csv')\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f'Deleted: {file_path}')\n",
    "    else:\n",
    "        print(f'Not found: {file_path}')\n",
    "\n",
    "    dir = 'output/lake_outlines/evolving_outlines/'\n",
    "    for ext in ('.geojson', '.txt'):\n",
    "        file_path = os.path.join(dir, f'{filename}{ext}')\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f'Deleted: {file_path}')\n",
    "        else:\n",
    "            print(f'Not found: {file_path}')\n",
    "\n",
    "del dir, lakes_outputs_to_delete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d89cea-4589-4838-a74f-ab12eeab3fa7",
   "metadata": {},
   "source": [
    "# Revise stationary_outlines_gdf\n",
    "\n",
    "We will revise stationary_outlines_gdf to not have Site_B and Site_C as individual lakes, but instead have Site_BC as a combined lake group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc837b4d-ea77-402a-8e68-ff7672c6ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of stationary_outlines_gdf\n",
    "reexamined_stationary_outlines_gdf = stationary_outlines_gdf.copy(deep=True)\n",
    "\n",
    "# Create combined Site_B_Site_C row\n",
    "site_bc_row = prepare_group_gdf(reexamined_stationary_outlines_gdf, ['Site_B', 'Site_C'])\n",
    "\n",
    "# Copy the citation to new Site_B_Site_C row if Sites B and C have the same citation\n",
    "if (reexamined_stationary_outlines_gdf[reexamined_stationary_outlines_gdf['name'] == 'Site_B']['cite'].iloc[0] == \n",
    "    reexamined_stationary_outlines_gdf[reexamined_stationary_outlines_gdf['name'] == 'Site_C']['cite'].iloc[0]):\n",
    "    site_bc_row['cite'] = (reexamined_stationary_outlines_gdf[reexamined_stationary_outlines_gdf['name'] == 'Site_B']\n",
    "                          ['cite'].iloc[0])\n",
    "\n",
    "# Drop individual lakes we are replacing\n",
    "reexamined_stationary_outlines_gdf = reexamined_stationary_outlines_gdf.drop(\n",
    "    reexamined_stationary_outlines_gdf[reexamined_stationary_outlines_gdf['name'].isin(['Site_B', 'Site_C'])].index)\n",
    "\n",
    "# Get evolving outlines and calculate area for Site_BC\n",
    "try:\n",
    "    lake_name = 'Site_BC'\n",
    "    \n",
    "    evolving_outlines_gdf = gpd.read_file('output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name))\n",
    "    \n",
    "    # Get evolving outlines union\n",
    "    evolving_union_gs = find_evolving_union(site_bc_row.iloc[0], evolving_outlines_gdf, incl_stationary=False)\n",
    "    \n",
    "    if evolving_union_gs is not None:\n",
    "        # Create temporary GeoDataFrame with the union\n",
    "        temp_gdf = gpd.GeoDataFrame(\n",
    "            index=[0], \n",
    "            crs='EPSG:3031', \n",
    "            geometry=[evolving_union_gs.iloc[0]])\n",
    "        \n",
    "        # Convert to 4326 for area calculation\n",
    "        temp_gdf = temp_gdf.to_crs('4326')\n",
    "        \n",
    "        # Calculate area\n",
    "        area = calculate_geodesic_area(temp_gdf['geometry'].iloc[0])\n",
    "        \n",
    "        # Update site_bc_row with calculated area and geometry\n",
    "        site_bc_row['area (m^2)'] = area if area is not None else None\n",
    "        site_bc_row['geometry'] = evolving_union_gs.iloc[0]\n",
    "\n",
    "\n",
    "    # Rename to follow combination naming convention used for at passed lakes Lake_78 and Slessor_23\n",
    "    site_bc_row['name'] = 'Site_BC'\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f'Error processing {lake_name}: {str(e)}')\n",
    "\n",
    "# Ensure that new entry isn't already in inventory before adding to avoid duplicate entry\n",
    "gdf_diff = site_bc_row[~site_bc_row['name'].isin(reexamined_stationary_outlines_gdf['name'])]\n",
    "\n",
    "# Add the new row to stationary_outlines_gdf\n",
    "reexamined_stationary_outlines_gdf = pd.concat([reexamined_stationary_outlines_gdf, gdf_diff], ignore_index=True)\n",
    "\n",
    "# Sort the GeoDataFrame alphabetically by the 'name' column; reset the index after sorting\n",
    "reexamined_stationary_outlines_gdf = reexamined_stationary_outlines_gdf.sort_values('name').reset_index(drop=True)\n",
    "\n",
    "# Print processing confirmation\n",
    "print(f'\\nProcessed {lake_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e71d1a-29a5-4349-92a7-40ac75485980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View new row to ensure worked properly\n",
    "reexamined_stationary_outlines_gdf[reexamined_stationary_outlines_gdf['name'] == 'Site_BC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3db4ae-6a82-479e-bca7-dbbede52230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Lacked sufficient data density in altimetry datasets data_counts to include:\n",
    "# Scambos and others, 2011 (within Livingstone and others, 2022 inventory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b428502e-ed81-480f-9a62-90f9b9a560ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "reexamined_stationary_outlines_gdf.to_file(\n",
    "    'output/lake_outlines/stationary_outlines/reexamined_stationary_outlines_gdf.geojson',\n",
    "    driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a146a886-92a4-4525-8e8e-6278af3caea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "reexamined_stationary_outlines_gdf = gpd.read_file(\n",
    "    'output/lake_outlines/stationary_outlines/reexamined_stationary_outlines_gdf.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d725d06-64e9-4c64-b3ee-24502c1cc4b4",
   "metadata": {},
   "source": [
    "# Review evolving outlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc765b3-b2cc-4c81-bd06-ebc0cd663c7d",
   "metadata": {},
   "source": [
    "In your `OUTPUT_DIR/FigS1_lake_reexamination_methods/plot_evolving_outlines_time_series` folder. There you will see the time series of evolving outlines plotted in aggregate for each lake. Some lakes will have very few evolving outlines that don't appear much different from the off-lake outlines generated. We additionally looking at the data_counts, dh, and evolving outline video for each time lake in the `find_evolving_outlines` folder for each lake.\n",
    "\n",
    "In these cases we cannot be certain the evolving outlines are indicative of lake behavior or just background height anomalies. So we delete these evolving outlines geojson files and conclude there were no evolving outlines found for these lakes.\n",
    "\n",
    "Some deletions are due to a lake's evolving outlines being those of a close neighbor, (e.g., Mac5 evolving outlines were those of Mac4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fb876c-1051-46eb-b063-4dff4de58cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete outlines of lake that had incomplete data coverage (no SARIn and only partial ATL15)\n",
    "! rm ./output/lake_outlines/evolving_outlines/Crane_Glacier.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db4fb7-676b-4c87-8644-0a2792ab0fe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lake_outlines_to_discard = [\n",
    "    'Bindschadler_1',\n",
    "    'Bindschadler_3',\n",
    "    'Bindschadler_5',\n",
    "    'Bindschadler_6',\n",
    "    'Byrd_s1',\n",
    "    'Byrd_s7',\n",
    "    'David_s4',\n",
    "    'David_s5',\n",
    "    'EAP_3',\n",
    "    'EAP_5',\n",
    "    'EAP_7',\n",
    "    'EAP_8',\n",
    "    'EAP_9',\n",
    "    'Foundation_2',\n",
    "    'Foundation_4',\n",
    "    'Foundation_9',\n",
    "    'Foundation_14',\n",
    "    'Institute_W1',\n",
    "    'JG_D2_a',\n",
    "    'Kamb_1',\n",
    "    'Kamb_2',\n",
    "    'Kamb_3',\n",
    "    'Kamb_4',\n",
    "    'Kamb_7',\n",
    "    'Kamb_9',\n",
    "    'Kamb_11',\n",
    "    'L1',\n",
    "    'LennoxKing_1',\n",
    "    'Mac5',\n",
    "    'Mac6',\n",
    "    'Raymond_1',\n",
    "    'Rec10',\n",
    "    'Slessor_5',\n",
    "    'Slessor_6',\n",
    "    'Slessor_7',\n",
    "    'TL122',\n",
    "    'U1',\n",
    "    'Whillans_8',\n",
    "    'Wilkes_1',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811cb148-3575-4e49-a2ad-4aab0592b6d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Discard outlines by moving out of git evolving_outlines repo and into non-git discarded_outlines repo\n",
    "discard_lake_outlines(\n",
    "    lake_outlines_to_discard=lake_outlines_to_discard,\n",
    "    source_dir='output/lake_outlines/evolving_outlines',\n",
    "    dest_dir=OUTPUT_DIR + '/find_evolving_outlines/discarded_outlines'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e05fcb-1dcf-41ae-bc12-f65c3912f31a",
   "metadata": {},
   "source": [
    "Similar to the lake groups, we try the next highest level for evolving outlines that appear flawed because of the number of off-lake outlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639cd0a4-5fce-43e3-9d5e-060a07c51d81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakes_analyze_idx_1 = [\n",
    "    'Byrd_s9',\n",
    "    'Byrd_s11',\n",
    "    'ConwaySubglacialLake',\n",
    "    'David_1',\n",
    "    'EngelhardtSubglacialLake',\n",
    "    'Foundation_N1',\n",
    "    'Foundation_N3',\n",
    "    'KT3',\n",
    "    'Lake78',\n",
    "    'Mac1',\n",
    "    'Nimrod_2',\n",
    "    'R1',\n",
    "    'Rec1',\n",
    "    'Rec2',\n",
    "    'Rec6',\n",
    "    'Slessor_4',\n",
    "    'Slessor_23',\n",
    "    'Thw_170',\n",
    "    'UpperSubglacialLakeConway',\n",
    "    'UpperEngelhardtSubglacialLake'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6927714-3424-40a8-98e7-0e8681b1f085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for lake in lakes_analyze_idx_1:\n",
    "    # Process the lake at the next highest level\n",
    "    lake_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == lake]\n",
    "    visualize_and_save_evolving_outlines(lake_gdf, row_index=1)\n",
    "\n",
    "del lakes_analyze_idx_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a9fd6e-9147-401a-ba2b-2275872c9d1f",
   "metadata": {},
   "source": [
    "We assess these results (in `OUTPUT_DIR/FigS1_lake_reexamination_methods/plot_evolving_outlines_time_series` folder) and decide if 1) lakes analyzed at next highest dh level are satisfactory, or 2) outlines that can be deleted because they are too similar to background off-lake activity or 2) lakes that need to be visualized at the next level because they are too similar to background off-lake activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fa1921-4f69-46a8-afbd-02a9423b8107",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_analyze_idx_2 = [\n",
    "    'Byrd_s9',\n",
    "    'ConwaySubglacialLake',\n",
    "    'David_1',\n",
    "    'EngelhardtSubglacialLake',\n",
    "    'Foundation_N1',\n",
    "    'KT3',\n",
    "    'Lake78',\n",
    "    'Nimrod_2',\n",
    "    'R1',\n",
    "    'Rec2',\n",
    "    'Rec6',\n",
    "    'Slessor_4',\n",
    "    'Slessor_23',\n",
    "    'Thw_170',\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5edc11-5dae-4269-a938-c41a24fd4968",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lake in lakes_analyze_idx_2:\n",
    "    # Process the lake at all other levels\n",
    "    lake_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == lake]\n",
    "    visualize_and_save_evolving_outlines(lake_gdf, row_index=2)\n",
    "    \n",
    "del lakes_analyze_idx_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb22059f-5f46-4815-a04e-f24898a545bc",
   "metadata": {},
   "source": [
    "We assess these results (in `OUTPUT_DIR/FigS1_lake_reexamination_methods/plot_evolving_outlines_time_series` folder) and decide if 1) lakes analyzed at next highest dh level are satisfactory, or 2) outlines that can be deleted because they are too similar to background off-lake activity or 2) lakes that need to be visualized at the next level because they are too similar to background off-lake activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b124741c-3323-4b76-a4d4-a8b9ec34a4e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lake_outlines_to_discard = [\n",
    "    'David_1',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf38a47-b021-4719-b296-69567f2e59f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Discard outlines by moving out of git repo and into non-git repo\n",
    "discard_lake_outlines(\n",
    "    lake_outlines_to_discard=lake_outlines_to_discard,\n",
    "    source_dir='output/lake_outlines/evolving_outlines',\n",
    "    dest_dir=OUTPUT_DIR + '/find_evolving_outlines/discarded_outlines'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdbc72d-f295-44f6-9392-ac8782df1ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_analyze_idx_3 = [\n",
    "    'Byrd_s9',\n",
    "    'ConwaySubglacialLake',\n",
    "    'Foundation_N1',\n",
    "    'R1',\n",
    "    'Rec2',\n",
    "    'Rec6',\n",
    "    'Slessor_4',\n",
    "    'Thw_170'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50beaef3-0a7e-46a8-bca7-1eddb2623a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lake in lakes_analyze_idx_3:\n",
    "    # Process the lake at all other levels\n",
    "    lake_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == lake]\n",
    "    visualize_and_save_evolving_outlines(lake_gdf, row_index=3)\n",
    "    \n",
    "del lakes_analyze_idx_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f6bca-c64d-43df-91d3-51460230218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_analyze_idx_4 = [\n",
    "    'ConwaySubglacialLake',\n",
    "    'Rec2',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b234e9f3-ae03-45b7-8d58-aa3ec6bc9ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lake in lakes_analyze_idx_4:\n",
    "    # Process the lake at all other levels\n",
    "    lake_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == lake]\n",
    "    visualize_and_save_evolving_outlines(lake_gdf, row_index=4)\n",
    "    \n",
    "del lakes_analyze_idx_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f9fdd-63c7-4bbd-8fe3-60fa9ffba6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_analyze_idx_5 = [\n",
    "    'ConwaySubglacialLake',\n",
    "    'Rec2',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36661c41-6b4d-4aea-9864-4b8f6bf1abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lake in lakes_analyze_idx_5:\n",
    "    # Process the lake at all other levels\n",
    "    lake_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'] == lake]\n",
    "    visualize_and_save_evolving_outlines(lake_gdf, row_index=5)\n",
    "\n",
    "del lakes_analyze_idx_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c239c59-5973-4bf0-afa0-31f90f24e054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there are no lakes with both geojson and txt file for evolving outlines\n",
    "dir = OUTPUT_DIR + '/levels'\n",
    "file_dict = defaultdict(list)\n",
    "\n",
    "# Get all files in directory\n",
    "for file_path in Path(dir).glob('**/*'):\n",
    "    if file_path.is_file():\n",
    "        # Get base name without extension\n",
    "        base_name = file_path.stem\n",
    "        # Add full filename to list under base name\n",
    "        file_dict[base_name].append(file_path.name)\n",
    "\n",
    "# Filter to only files with duplicates\n",
    "duplicates = {k: v for k, v in file_dict.items() if len(v) > 1}\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1912df48-84f8-489b-bf30-e190a856d389",
   "metadata": {},
   "source": [
    "# Union outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca8bc5-fd0c-494e-aa96-68a57e6e2b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import geodataframe needed for this step\n",
    "reexamined_stationary_outlines_gdf = gpd.read_file(\n",
    "    'output/lake_outlines/stationary_outlines/reexamined_stationary_outlines_gdf.geojson')\n",
    "\n",
    "# Create two new lakes geodataframe that are the union of \n",
    "# 1) the evolving outlines for each lake found to have evolving outlines\n",
    "# 2) the evolving outlines and the stationary outline for lakes with activity (found to have evolving outlines)\n",
    "# 3) the evolving outlines and the stationary outline for all lakes\n",
    "\n",
    "# List all folders in target directory\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "\n",
    "# Filter stationary_outlines_gdf to only include lakes that have evolving outlines found at them\n",
    "stationary_outlines_gdf_evolving_lakes = filter_gdf_by_folder_contents(reexamined_stationary_outlines_gdf, folder_path, file_extension='geojson', exclude=False)\n",
    "\n",
    "# Remove Site A, B, C, LSLM, and LSLC because their outlines were perfect circles created using their \n",
    "# point locations and approx. areas so should not be part of the union\n",
    "exclude_list = ['Site_A', 'LowerConwaySubglacialLake', 'LowerMercerSubglacialLake']\n",
    "stationary_outlines_gdf_all_lakes = reexamined_stationary_outlines_gdf[~reexamined_stationary_outlines_gdf['name'].isin(exclude_list)]\n",
    "\n",
    "# Create initial GDFs\n",
    "evolving_outlines_union_gdf = stationary_outlines_gdf_evolving_lakes.copy(deep=True)\n",
    "\n",
    "# First process lakes with evolving outlines\n",
    "for idx, row in stationary_outlines_gdf_evolving_lakes.iterrows():\n",
    "    try:\n",
    "        lake_ps = stationary_outlines_gdf_evolving_lakes.loc[idx]\n",
    "        lake_name = lake_ps['name']\n",
    "        \n",
    "        try:\n",
    "            evolving_outlines_gdf = gpd.read_file('output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name))\n",
    "        except Exception as e:\n",
    "            print(f'Skipping because evolving outlines geojson file not found for {lake_name}.')\n",
    "            continue\n",
    "            \n",
    "        # Process evolving outlines\n",
    "        evolving_union_gs = find_evolving_union(lake_ps, evolving_outlines_gdf, incl_stationary=False)\n",
    "\n",
    "        if evolving_union_gs is None:\n",
    "            print(f'Skipping {lake_name}: Could not create union of outlines')\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Store polygon from geoseries in geodataframes with CRS\n",
    "            evolving_outlines_union_gdf_idx = gpd.GeoDataFrame(\n",
    "                index=[0], crs='EPSG:3031', geometry=[evolving_union_gs.iloc[0]])\n",
    "            \n",
    "            # Convert GeoDataFrames to EPSG:4326 CRS for geodesic area calculation\n",
    "            evolving_outlines_union_gdf_idx = evolving_outlines_union_gdf_idx.to_crs('4326')\n",
    "            \n",
    "            # Update geometries in union GDFs using the correct index\n",
    "            evolving_outlines_union_gdf.loc[idx, 'geometry'] = evolving_union_gs.iloc[0]\n",
    "            \n",
    "            # Calculate and store areas\n",
    "            area = calculate_geodesic_area(evolving_outlines_union_gdf_idx['geometry'].iloc[0])\n",
    "            \n",
    "            if area is not None:\n",
    "                evolving_outlines_union_gdf.loc[idx, 'area (m^2)'] = area\n",
    "                \n",
    "        except (IndexError, AttributeError) as e:\n",
    "            print(f'Error processing geometries for {lake_name}: {str(e)}')\n",
    "            continue\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f'Unexpected error processing {lake_name}: {str(e)}')\n",
    "        continue\n",
    "\n",
    "# Then process remaining lakes for all_lakes GDF\n",
    "remaining_lakes = set(stationary_outlines_gdf_all_lakes['name']) - set(stationary_outlines_gdf_evolving_lakes['name'])\n",
    "for lake_name in remaining_lakes:\n",
    "    try:\n",
    "        # Get the lake's data using boolean indexing\n",
    "        mask = evolving_stationary_outlines_union_all_lakes_gdf['name'] == lake_name\n",
    "        if not mask.any():\n",
    "            print(f'Lake {lake_name} not found in all_lakes GDF')\n",
    "            continue\n",
    "            \n",
    "        lake_geom = evolving_stationary_outlines_union_all_lakes_gdf.loc[mask, 'geometry'].iloc[0]\n",
    "        \n",
    "        # Convert to 4326 for area calculation\n",
    "        temp_gdf = gpd.GeoDataFrame(\n",
    "            index=[0], crs='EPSG:3031', geometry=[lake_geom])\n",
    "        temp_gdf = temp_gdf.to_crs('4326')\n",
    "        area = calculate_geodesic_area(temp_gdf['geometry'].iloc[0])\n",
    "        \n",
    "        if area is not None:\n",
    "            evolving_stationary_outlines_union_all_lakes_gdf.loc[mask, 'area (m^2)'] = area\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f'Error processing non-evolving lake {lake_name}: {str(e)}')\n",
    "        continue\n",
    "\n",
    "# Delete intermediary GDFs\n",
    "del stationary_outlines_gdf_evolving_lakes, stationary_outlines_gdf_all_lakes\n",
    "\n",
    "# Make additional_lakes_gdf for Site A, LSLM, and LSLC whose stationary outlines were removed \n",
    "# because they were approximations using point location and reported area instead of an actual outline\n",
    "initial_lakes = ['Site_A', 'LowerConwaySubglacialLake', 'LowerEngelhardtSubglacialLake', 'LowerMercerSubglacialLake']\n",
    "additional_lakes_gdf = stationary_outlines_gdf[stationary_outlines_gdf['name'].isin(initial_lakes)].copy()\n",
    "\n",
    "# Add these additional lakes as rows to the union gdf's\n",
    "for idx, row in additional_lakes_gdf.iterrows():\n",
    "    try:\n",
    "        lake_ps = additional_lakes_gdf.loc[idx]\n",
    "        lake_name = lake_ps['name']\n",
    "        \n",
    "        try:\n",
    "            evolving_outlines_gdf = gpd.read_file('output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name))\n",
    "        except Exception as e:\n",
    "            print(f'Skipping because evolving outlines geojson file not found for {lake_name}.')\n",
    "            continue\n",
    "           \n",
    "        # Only get evolving outlines union since there's no stationary outline\n",
    "        evolving_union_gs = find_evolving_union(lake_ps, evolving_outlines_gdf, incl_stationary=False)\n",
    "        \n",
    "        if evolving_union_gs is None:\n",
    "            print(f'Skipping {lake_name}: Could not create union of outlines')\n",
    "            continue\n",
    "           \n",
    "        try:\n",
    "            # Create temporary GeoDataFrame with the union\n",
    "            temp_gdf = gpd.GeoDataFrame(\n",
    "                index=[0], \n",
    "                crs='EPSG:3031', \n",
    "                geometry=[evolving_union_gs.iloc[0]])\n",
    "            \n",
    "            # Convert to 4326 for area calculation\n",
    "            temp_gdf = temp_gdf.to_crs('4326')\n",
    "            \n",
    "            # Calculate area\n",
    "            area = calculate_geodesic_area(temp_gdf['geometry'].iloc[0])\n",
    "            \n",
    "            # Create new row from the current lake's data\n",
    "            new_row_gdf = gpd.GeoDataFrame([{\n",
    "                'name': lake_name,\n",
    "                'area (m^2)': area if area is not None else None,\n",
    "                'cite': lake_ps['cite'],\n",
    "                'CS2_SARIn_start': lake_ps['CS2_SARIn_start'],\n",
    "                'geometry': evolving_union_gs.iloc[0]\n",
    "            }], crs='EPSG:3031')\n",
    "\n",
    "            # For evolving_outlines_union_gdf\n",
    "            if not lake_name in evolving_outlines_union_gdf['name'].values:\n",
    "                evolving_outlines_union_gdf = pd.concat([\n",
    "                    evolving_outlines_union_gdf, \n",
    "                    new_row_gdf\n",
    "                ], ignore_index=True)\n",
    "            else:\n",
    "                print(f'Skipping {lake_name} for evolving_outlines_union_gdf - already exists')\n",
    "           \n",
    "        except (IndexError, AttributeError) as e:\n",
    "            print(f'Error processing geometries for {lake_name}: {str(e)}')\n",
    "            continue\n",
    "           \n",
    "    except Exception as e:\n",
    "        print(f'Unexpected error processing {lake_name}: {str(e)}')\n",
    "        continue\n",
    "\n",
    "# Sort the GeoDataFrame alphabetically by the 'name' column; Reset the index after sorting; Reproject GeoDataFrame to EPSG:3031\n",
    "evolving_outlines_union_gdf = evolving_outlines_union_gdf.sort_values('name').reset_index(drop=True).set_crs('3031')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4cc8f3-bf12-4ec1-a5a9-2b0252ae1be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and report on invalid or NA geometries\n",
    "for gdf_name, gdf in [\n",
    "    ('evolving_outlines_union_gdf', evolving_outlines_union_gdf)\n",
    "]:\n",
    "    # Count NA geometries\n",
    "    na_count = gdf.geometry.isna().sum()\n",
    "    \n",
    "    # Count invalid geometries (that aren't NA)\n",
    "    invalid_count = (~gdf.geometry.is_valid & gdf.geometry.notna()).sum()\n",
    "    \n",
    "    # Report results\n",
    "    if na_count > 0 or invalid_count > 0:\n",
    "        print(f'Found issues in {gdf_name}:')\n",
    "        if na_count > 0:\n",
    "            print(f'  - {na_count} NA geometries')\n",
    "        if invalid_count > 0:\n",
    "            print(f'  - {invalid_count} invalid geometries')\n",
    "            \n",
    "            # Optional: Show indices of invalid geometries for debugging\n",
    "            invalid_indices = gdf[~gdf.geometry.is_valid & gdf.geometry.notna()].index.tolist()\n",
    "            print(f'  - Invalid geometry indices: {invalid_indices}')\n",
    "    else:\n",
    "        print(f'No issues found in {gdf_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d9d33b-b0c0-4abc-90e2-0141c34cd589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print lengths of GDFs to ensure everything worked properly\n",
    "print('Length of geodataframes for all lakes:')\n",
    "print(len(reexamined_stationary_outlines_gdf))\n",
    "\n",
    "# List all folders in target directory\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "\n",
    "# Filter stationary_outlines_gdf to only include lakes that have evolving outlines found at them\n",
    "print('Length of geodataframes for lakes that were found to have evolving outlines:')\n",
    "print(len(evolving_outlines_union_gdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b45f49-fb85-4b81-9e5b-ef03adbf466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot each boundary with a label for the legend\n",
    "stationary_outlines_gdf.boundary.plot(ax=ax, color='blue', label='Stationary outlines')\n",
    "evolving_outlines_union_gdf.boundary.plot(ax=ax, color='red', label='Evolving outlines union')\n",
    "evolving_stationary_outlines_union_evolving_lakes_gdf.boundary.plot(ax=ax, color='purple', label='Evolving-stationary union at evolving lakes')\n",
    "evolving_stationary_outlines_union_all_lakes_gdf.boundary.plot(ax=ax, color='k', linestyle='dashed', label='Evolving-stationary union at all lakes')\n",
    "Scripps_landice.boundary.plot(ax=ax, lw=0.5, label='Scripps Land Ice')\n",
    "\n",
    "# Add the legend above the plot and outside the plotting area\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5512d2-a3ea-4bb3-80fb-3c1fe8e3aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "evolving_outlines_union_gdf.to_file('output/lake_outlines/stationary_outlines/evolving_outlines_union_gdf.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d35df-ec1a-4713-9a9b-9e3c3d0c32ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import to ensure saved properly\n",
    "evolving_outlines_union_gdf = gpd.read_file(\n",
    "    'output/lake_outlines/stationary_outlines/evolving_outlines_union_gdf.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ae52fc-aa5a-4fb8-9028-f5000bc2bae5",
   "metadata": {},
   "source": [
    "# Forward fill\n",
    "To prepare for geometric calculations, we will make copies of the evolving outlines geojson files that foward fill  time steps with no evolving outlines with the most recently observed evolving outline when available. Some lakes will have early time steps with no observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029e9dd3-bcab-48fc-9c54-7c417e3dbd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_forward_fill_files(input_dir = 'output/lake_outlines/evolving_outlines', \n",
    "                    output_dir = 'output/lake_outlines/evolving_outlines/forward_fill', \n",
    "                    mid_pt_datetimes=mid_pt_datetimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b05235-02cd-4432-8c78-06d3b33b93f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot check a lake to ensure forward fill is ocurring properly\n",
    "filename = 'MercerSubglacialLake'\n",
    "compare_evolving_outlines_versions(filename, mid_pt_datetimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38187942-2948-4996-a955-18782ee8a52a",
   "metadata": {},
   "source": [
    "# Geometric calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfcbcb3-cec0-4bb4-9e07-5e918fdcdaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load geodataframes needed for these geometric calculations\n",
    "reexamined_stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/reexamined_stationary_outlines_gdf.geojson')\n",
    "evolving_outlines_union_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/evolving_outlines_union_gdf.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd862020-3685-4bf3-9f99-ad008c0326fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories before processing any lakes\n",
    "os.makedirs('output/geometric_calcs/evolving_outlines_geom_calc', exist_ok=True)\n",
    "os.makedirs('output/geometric_calcs/evolving_outlines_geom_calc/forward_fill', exist_ok=True)\n",
    "os.makedirs('output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes', exist_ok=True)\n",
    "os.makedirs('output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c966f417-1d75-41c5-8cef-f7e50a3f49c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process geometric calculations at lakes where evolving outlines were found\n",
    "\n",
    "# Get list of remaining lakes left to process based on\n",
    "\n",
    "# 1) Filter out lakes that have evolving outlines\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "evolving_lakes_gdf = filter_gdf_by_folder_contents(reexamined_stationary_outlines_gdf, \n",
    "    folder_path, exclude=True, file_extension='txt')\n",
    "\n",
    "# 2) Filter out lakes that have already been processed in each directory\n",
    "folder_paths = [\n",
    "    'output/geometric_calcs/evolving_outlines_geom_calc/',\n",
    "    'output/geometric_calcs/evolving_outlines_geom_calc/forward_fill',\n",
    "    'output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes/',\n",
    "]\n",
    "\n",
    "# Create a list to store filtered GeoDataFrames\n",
    "filtered_gdfs = []\n",
    "\n",
    "# Filter the original GeoDataFrame for each directory and add to list\n",
    "for folder_path in folder_paths:\n",
    "    filtered_gdf = filter_gdf_by_folder_contents(evolving_lakes_gdf, folder_path)\n",
    "    if not filtered_gdf.empty:\n",
    "        filtered_gdfs.append(filtered_gdf)\n",
    "\n",
    "# Concatenate all filtered GeoDataFrames and drop duplicates\n",
    "if filtered_gdfs:\n",
    "    remaining_lakes = pd.concat(filtered_gdfs, ignore_index=True)\n",
    "    remaining_lakes = remaining_lakes.drop_duplicates(subset='name').reset_index(drop=True)\n",
    "    remaining_lakes = remaining_lakes.sort_values(by='name')\n",
    "else:\n",
    "    remaining_lakes = gpd.GeoDataFrame(columns=evolving_lakes_gdf.columns)\n",
    "\n",
    "print(f'{len(remaining_lakes)} lake(s) to process.')\n",
    "\n",
    "# Process geometric calculations on lakes\n",
    "total_lakes = len(remaining_lakes)\n",
    "processed_lakes = 0\n",
    "\n",
    "for i in remaining_lakes.index:\n",
    "    remaining_count = total_lakes - processed_lakes\n",
    "    print(f'{remaining_count} lakes remain.')\n",
    "    \n",
    "    try:\n",
    "        # Isolate lake from remaining_lakes as geodataframe\n",
    "        stationary_outline_gdf = remaining_lakes.loc[[i]]\n",
    "        lake_name = stationary_outline_gdf['name'].iloc[0]\n",
    "        \n",
    "        if stationary_outline_gdf.empty:\n",
    "            print('Skipping empty lake entry')\n",
    "            continue\n",
    "        \n",
    "        # Prepare data sets\n",
    "        dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(stationary_outline_gdf, 30)\n",
    "    \n",
    "        # Check if lake has evolving outlines\n",
    "        has_evolving_outlines = os.path.exists(os.path.join(\n",
    "            os.getcwd(), 'output/lake_outlines/evolving_outlines', f'{stationary_outline_gdf['name'].iloc[0]}.geojson'))\n",
    "\n",
    "        # Add check for non-evolving outlines - raise an exception to halt the entire process\n",
    "        if not has_evolving_outlines:\n",
    "            raise Exception(f'CRITICAL ERROR: Lake {lake_name} does not have evolving outlines but should based on initial filtering. Processing halted to investigate the issue.')\n",
    "\n",
    "        if has_evolving_outlines:\n",
    "            # Calculate active area, dh, and dV at lakes with evolving outlines\n",
    "            evolving_outlines_geom_calc(stationary_outline_gdf=stationary_outline_gdf,\n",
    "                dataset1=dataset1_masked, dataset2=dataset2_masked)\n",
    "\n",
    "            # At lakes with evolving outlines with forward fill\n",
    "            evolving_outlines_geom_calc(stationary_outline_gdf=stationary_outline_gdf,\n",
    "                dataset1=dataset1_masked, dataset2=dataset2_masked, forward_fill=True)\n",
    "            \n",
    "            # At evolving outlines union (at evolving lakes only)\n",
    "            evolving_union_gdf = evolving_outlines_union_gdf[\n",
    "                evolving_outlines_union_gdf['name'] == lake_name]\n",
    "\n",
    "            if not evolving_union_gdf.empty:\n",
    "                stationary_outline_geom_calc(stationary_outline_gdf=evolving_union_gdf,\n",
    "                    dataset1=dataset1_masked, dataset2=dataset2_masked, sub_dir='evolving_union_at_evolving_lakes')\n",
    "            else:\n",
    "                print(f'No evolving union outline found for lake: {lake_name}')\n",
    "            \n",
    "        # Increment processing counter\n",
    "        processed_lakes += 1\n",
    "        \n",
    "        # Clear output of each index\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error processing lake '{lake_name}' at index {i}: {str(e)}')\n",
    "        continue\n",
    "\n",
    "if processed_lakes == total_lakes:\n",
    "    print('All remaining lakes have been processed.')\n",
    "else:\n",
    "    print(f'Processing complete. {total_lakes - processed_lakes} lakes were skipped due to errors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222fc432-78dc-4a64-9655-a3a67e155748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process geometric calculations at all previously identified lakes using stationary outlines\n",
    "\n",
    "# Get list of remaining lakes left to process based on\n",
    "\n",
    "# Filter out lakes that have already been processed in each directory\n",
    "folder_paths = [\n",
    "    'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes/',\n",
    "]\n",
    "\n",
    "# Create a list to store filtered GeoDataFrames\n",
    "filtered_gdfs = []\n",
    "\n",
    "# Filter the original GeoDataFrame for each directory and add to list\n",
    "for folder_path in folder_paths:\n",
    "    filtered_gdf = filter_gdf_by_folder_contents(reexamined_stationary_outlines_gdf, folder_path)\n",
    "    if not filtered_gdf.empty:\n",
    "        filtered_gdfs.append(filtered_gdf)\n",
    "\n",
    "# Concatenate all filtered GeoDataFrames and drop duplicates\n",
    "if filtered_gdfs:\n",
    "    remaining_lakes = pd.concat(filtered_gdfs, ignore_index=True)\n",
    "    remaining_lakes = remaining_lakes.drop_duplicates(subset='name').reset_index(drop=True)\n",
    "else:\n",
    "    remaining_lakes = gpd.GeoDataFrame(columns=reexamined_stationary_outlines_gdf.columns)\n",
    "\n",
    "print(f'{len(remaining_lakes)} lake(s) to process.')\n",
    "\n",
    "# Process geometric calculations on lakes\n",
    "total_lakes = len(remaining_lakes)\n",
    "processed_lakes = 0\n",
    "\n",
    "for i in remaining_lakes.index:\n",
    "    remaining_count = total_lakes - processed_lakes\n",
    "    print(f'{remaining_count} lakes remain.')\n",
    "    \n",
    "    try:\n",
    "        # Isolate lake from remaining_lakes as geodataframe\n",
    "        stationary_outline_gdf = remaining_lakes.loc[[i]]\n",
    "        lake_name = stationary_outline_gdf['name'].iloc[0]\n",
    "        \n",
    "        if stationary_outline_gdf.empty:\n",
    "            print('Skipping empty lake entry')\n",
    "            continue\n",
    "        \n",
    "        # Prepare data sets\n",
    "        dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(stationary_outline_gdf, 30)\n",
    "\n",
    "        # Calculate active area, dh, and dV at stationary outlines (at all lakes)\n",
    "        stationary_outline_geom_calc(stationary_outline_gdf=stationary_outline_gdf,\n",
    "           dataset1=dataset1_masked, dataset2=dataset2_masked, sub_dir='stationary_outlines_at_all_lakes')\n",
    "            \n",
    "        # Increment processing counter\n",
    "        processed_lakes += 1\n",
    "        \n",
    "        # Clear output of each index\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error processing lake '{lake_name}' at index {i}: {str(e)}')\n",
    "        continue\n",
    "\n",
    "if processed_lakes == total_lakes:\n",
    "    print('All remaining lakes have been processed.')\n",
    "else:\n",
    "    print(f'Processing complete. {total_lakes - processed_lakes} lakes were skipped due to errors.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e438f2-91c7-42c3-9d8b-b4b967af55f0",
   "metadata": {},
   "source": [
    "# Visualizations of geometric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57d98dd-1fd0-429e-8974-075ad32242ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate visualizations of geometric data\n",
    "\n",
    "# Load geodataframes needed\n",
    "reexamined_stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/reexamined_stationary_outlines_gdf.geojson')\n",
    "evolving_outlines_union_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/evolving_outlines_union_gdf.geojson')\n",
    "\n",
    "# Get list of remaining lakes left to process based on\n",
    "# 1) lakes that have evolving outlines\n",
    "folder_path = 'output/lake_outlines/evolving_outlines'\n",
    "remaining_lakes = filter_gdf_by_folder_contents(reexamined_stationary_outlines_gdf, \n",
    "    folder_path, exclude=True, file_extension='txt')\n",
    "\n",
    "# 2) Check which lakes need processing in any of the four output folders\n",
    "comparison_folder = OUTPUT_DIR + '/plot_evolving_and_stationary_comparison'\n",
    "sequential_folder = OUTPUT_DIR + '/plot_evolving_and_stationary_comparison_sequential'\n",
    "comparison_ff_folder = OUTPUT_DIR + '/plot_evolving_and_stationary_comparison/forward_fill'\n",
    "sequential_ff_folder = OUTPUT_DIR + '/plot_evolving_and_stationary_comparison_sequential/forward_fill'\n",
    "\n",
    "# Make sure all output directories exist\n",
    "os.makedirs(comparison_folder, exist_ok=True)\n",
    "os.makedirs(sequential_folder, exist_ok=True)\n",
    "os.makedirs(comparison_ff_folder, exist_ok=True)\n",
    "os.makedirs(sequential_ff_folder, exist_ok=True)\n",
    "\n",
    "# Reset index to avoid indexing issues\n",
    "remaining_lakes = remaining_lakes.reset_index(drop=True)\n",
    "\n",
    "# Dictionary to store lakes that failed processing and their error messages\n",
    "failed_lakes = {}\n",
    "\n",
    "if remaining_lakes.empty:\n",
    "    print('All lakes processed.')\n",
    "else:\n",
    "    while not remaining_lakes.empty:\n",
    "        print(f'{len(remaining_lakes)} lake(s) remain.')\n",
    "        \n",
    "        # Always get the first lake to process\n",
    "        lake_gdf = remaining_lakes.iloc[[0]]  # Always take the first row\n",
    "        lake_name = lake_gdf['name'].iloc[0]\n",
    "        \n",
    "        if lake_gdf.empty:\n",
    "            print(f'Warning: Empty lake_gdf for {lake_name}. Skipping...')\n",
    "            # Remove lake from remaining_lakes to avoid infinite loop\n",
    "            remaining_lakes = remaining_lakes.iloc[1:].reset_index(drop=True)\n",
    "            failed_lakes[lake_name] = 'Empty lake_gdf'\n",
    "            continue\n",
    "        \n",
    "        # Check if the lake needs processing in each folder\n",
    "        needs_comparison = not lake_name in [\n",
    "            os.path.splitext(f)[0] for f in os.listdir(comparison_folder)\n",
    "            if os.path.isfile(os.path.join(comparison_folder, f))\n",
    "        ]\n",
    "        \n",
    "        needs_sequential = not lake_name in [\n",
    "            os.path.splitext(f)[0] for f in os.listdir(sequential_folder) \n",
    "            if f.endswith('.mp4')\n",
    "        ]\n",
    "        \n",
    "        needs_comparison_ff = not lake_name in [\n",
    "            os.path.splitext(f)[0] for f in os.listdir(comparison_ff_folder)\n",
    "            if os.path.isfile(os.path.join(comparison_ff_folder, f))\n",
    "        ]\n",
    "        \n",
    "        needs_sequential_ff = not lake_name in [\n",
    "            os.path.splitext(f)[0] for f in os.listdir(sequential_ff_folder) \n",
    "            if f.endswith('.mp4')\n",
    "        ]\n",
    "        \n",
    "        lake_processed = False\n",
    "        error_message = ''\n",
    "        \n",
    "        try:\n",
    "            # Process standard comparison if needed\n",
    "            if needs_comparison:\n",
    "                print(f'Processing {lake_name} with plot_evolving_and_stationary_comparison (forward_fill=False)')\n",
    "                plot_evolving_and_stationary_comparison(lake_gdf)\n",
    "                \n",
    "            # Process forward fill comparison if needed\n",
    "            if needs_comparison_ff:\n",
    "                print(f'Processing {lake_name} with plot_evolving_and_stationary_comparison (forward_fill=True)')\n",
    "                plot_evolving_and_stationary_comparison(lake_gdf, forward_fill=True)\n",
    "            \n",
    "            # Process standard sequential if needed\n",
    "            if needs_sequential:\n",
    "                print(f'Processing {lake_name} with plot_evolving_and_stationary_comparison_sequential (forward_fill=False)')\n",
    "                plot_evolving_and_stationary_comparison_sequential(lake_gdf)\n",
    "                \n",
    "            # Process forward fill sequential if needed\n",
    "            if needs_sequential_ff:\n",
    "                print(f'Processing {lake_name} with plot_evolving_and_stationary_comparison_sequential (forward_fill=True)')\n",
    "                plot_evolving_and_stationary_comparison_sequential(lake_gdf, forward_fill=True)\n",
    "                \n",
    "            lake_processed = True\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_message = str(e)\n",
    "            print(f'Error processing lake {lake_name}: {error_message}')\n",
    "            failed_lakes[lake_name] = error_message\n",
    "        \n",
    "        # Always remove the current lake from remaining_lakes to avoid infinite loop\n",
    "        remaining_lakes = remaining_lakes.iloc[1:].reset_index(drop=True)\n",
    "        \n",
    "        # Clear output of each index\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        if remaining_lakes.empty:\n",
    "            print('All lakes processed.')\n",
    "            break\n",
    "            \n",
    "    # Print summary of failed lakes\n",
    "    if failed_lakes:\n",
    "        print('\\n--- PROCESSING SUMMARY ---')\n",
    "        print(f'Failed to process {len(failed_lakes)} lakes:')\n",
    "        for lake, error in failed_lakes.items():\n",
    "            print(f'Lake {lake}: {error}')\n",
    "    else:\n",
    "        print('All lakes processed successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e55cf8a-3eea-4bf5-8daa-3f199633f87b",
   "metadata": {},
   "source": [
    "# Final check that all lakes have been processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92df904-9b3d-4471-8bf0-d12206e32e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load geodataframe needed\n",
    "reexamined_stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/reexamined_stationary_outlines_gdf.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff17b42-ea5b-4e56-b44a-0daf34850c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: 'output/geometric_calcs' directories will have four more csv files than expected after if continental summation \n",
    "# (next section) is run before this celll because of continental sums files generate four csv files\n",
    "# so if this code is rerun after the 'Continental summations' section, the counts will be higher than expected\n",
    "\n",
    "# Ensure all lakes were reanalyzed\n",
    "print('Analysis done on all previously identified lakes')\n",
    "\n",
    "print(len(stationary_outlines_gdf), 'lakes reanalyzed')\n",
    "\n",
    "print(len(reexamined_stationary_outlines_gdf), \n",
    "    'lakes analyzed in reexamined inventory due to Site_B and Site_C being combined into Site_BC and Crane_Glacier not analyzed because insufficient data coverage')\n",
    "\n",
    "dir = OUTPUT_DIR + '/levels'\n",
    "print(len([f for f in os.listdir(dir) if os.path.isfile(os.path.join(dir, f))]), \n",
    "      'lakes analyzed using find_and_save_optimal_paraters func')\n",
    "\n",
    "# Breakdown of lakes where evolving outlines were found vs. not\n",
    "print(len([f for f in os.listdir('output/lake_outlines/evolving_outlines') if f.endswith('.geojson')]),\n",
    "      'lakes found to have evolving outlines')\n",
    "print(len([f for f in os.listdir('output/lake_outlines/evolving_outlines') if f.endswith('.txt')]),\n",
    "      'lakes found to have no evolving outlines')\n",
    "print(len([f for f in os.listdir('output/lake_outlines/evolving_outlines') if f.endswith('.geojson')]) +\n",
    "      len([f for f in os.listdir('output/lake_outlines/evolving_outlines') if f.endswith('.txt')]), \n",
    "      'sum of lakes found with and without evolving outlines')\n",
    "\n",
    "dir = 'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.csv')]),\n",
    "      'lakes analyzed using stationary_outline_geom_calc/stationary_outlines_at_all_lakes')\n",
    "\n",
    "dir = 'output/geometric_calcs/stationary_outline_geom_calc/evolving_stationary_union_at_all_lakes'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.csv')]),\n",
    "      'lakes analyzed using stationary_outline_geom_calc/evolving_stationary_union_at_all_lakes')\n",
    "\n",
    "# Analysis done on evolving lakes\n",
    "print('\\nAnalysis done only on previously identified lakes found to have evolving outlines')\n",
    "\n",
    "dir = 'output/geometric_calcs/evolving_outlines_geom_calc'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.csv') and not Path(f).stem.endswith('_sum')]),\n",
    "      'lakes analyzed using evolving_outlines_geom_calc')\n",
    "\n",
    "dir = 'output/geometric_calcs/evolving_outlines_geom_calc'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.csv') and not Path(f).stem.endswith('_sum')]),\n",
    "      'lakes analyzed using evolving_outlines_geom_calc/forward_fill')\n",
    "\n",
    "dir = 'output/geometric_calcs/stationary_outline_geom_calc/evolving_union_at_evolving_lakes'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.csv') and not Path(f).stem.endswith('_sum')]),\n",
    "      'lakes analyzed using stationary_outline_geom_calc/evolving_union_at_evolving_lakes')\n",
    "\n",
    "dir = 'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.csv') and not Path(f).stem.endswith('_sum')]),\n",
    "      'lakes analyzed using stationary_outline_geom_calc/stationary_outlines_at_evolving_lakes')\n",
    "\n",
    "dir = 'output/geometric_calcs/stationary_outline_geom_calc/evolving_stationary_union_at_evolving_lakes'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.csv') and not Path(f).stem.endswith('_sum')]),\n",
    "      'lakes analyzed using stationary_outline_geom_calc/evolving_stationary_union_at_evolving_lakes')\n",
    "\n",
    "dir = OUTPUT_DIR + '/plot_evolving_and_stationary_comparison'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.png')]),\n",
    "      'lakes analyzed using plot_evolving_and_stationary_comparison')\n",
    "\n",
    "dir = OUTPUT_DIR + '/plot_evolving_and_stationary_comparison'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.png')]),\n",
    "      'lakes analyzed using plot_evolving_and_stationary_comparison/forward_fill')\n",
    "\n",
    "dir = OUTPUT_DIR + '/plot_evolving_and_stationary_comparison_sequential'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.mp4')]),\n",
    "      'lakes analyzed using plot_evolving_and_stationary_comparison_sequential')\n",
    "\n",
    "dir = OUTPUT_DIR + '/plot_evolving_and_stationary_comparison_sequential'\n",
    "print(len([f for f in os.listdir(dir) if f.endswith('.mp4')]),\n",
    "      'lakes analyzed using plot_evolving_and_stationary_comparison_sequential/forward_fill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5be2863-8295-476c-8929-1f809318ca49",
   "metadata": {},
   "source": [
    "# Continental summations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e265cdf-dfb8-407b-80d0-4d9d12d94bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load geodataframe needed\n",
    "reexamined_stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/reexamined_stationary_outlines_gdf.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ba2ece-c90f-4879-9835-e7f6df453e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of comparison types\n",
    "geom_calc_folders = [\n",
    "    'evolving_outlines_geom_calc',\n",
    "    'evolving_outlines_geom_calc/forward_fill',\n",
    "    'stationary_outline_geom_calc/evolving_union_at_evolving_lakes',\n",
    "    'stationary_outline_geom_calc/stationary_outlines_at_all_lakes',\n",
    "]\n",
    "\n",
    "# Process each comparison type\n",
    "for geom_calc_folder in geom_calc_folders:\n",
    "    process_continental_sums(geom_calc_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0d9ed4-5dc6-45a6-8af0-e51da56cae4d",
   "metadata": {},
   "source": [
    "# Fig. S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277eb44-f225-4553-ad0c-62452e3b3eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.family': 'DejaVu Sans',\n",
    "    'font.size': 9,\n",
    "    'axes.labelsize': 10,\n",
    "    'axes.titlesize': 9,\n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 9,\n",
    "    'legend.fontsize': 9,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e971a37-5db7-46e3-9b7f-a9ee01739403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets necessary for plotting\n",
    "reexamined_stationary_outlines_gdf = gpd.read_file('output/lake_outlines/stationary_outlines/reexamined_stationary_outlines_gdf.geojson')\n",
    "evolving_outlines_union_gdf = gpd.read_file('output/lake_outlines/evolving_outlines_union_gdf.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15678ca9-d0e5-4ae7-946f-a337e514e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make output directory for figures\n",
    "os.makedirs(OUTPUT_DIR + '/figures', exist_ok=True)\n",
    "\n",
    "# Select lake\n",
    "lake_gdf = reexamined_stationary_outlines_gdf[reexamined_stationary_outlines_gdf['name'] == 'Institute_E1']\n",
    "\n",
    "# Define lake name and polygon\n",
    "lake_name = lake_gdf['name'].iloc[0]\n",
    "lake_poly = lake_gdf['geometry'].iloc[0]\n",
    "\n",
    "# Load evolving outlines as geodataframe\n",
    "try:\n",
    "    onlake_outlines_gdf = gpd.read_file(os.path.join(\n",
    "        os.getcwd(), 'output/lake_outlines/evolving_outlines/{}.geojson'.format(lake_name)))\n",
    "except fiona.errors.DriverError:\n",
    "    print(f'File for {lake_name} not found.')\n",
    "\n",
    "# Load off-lake evolving outlines as geodataframe\n",
    "try:\n",
    "    offlake_outlines_gdf = gpd.read_file(OUTPUT_DIR + '/find_evolving_outlines/offlake_outlines/{}.geojson'.format(lake_name))\n",
    "except fiona.errors.DriverError:\n",
    "    print(f'File for {lake_name} not found.')\n",
    "\n",
    "# Ensure the result is a GeoDataFrame with proper geometry\n",
    "evolving_outlines_gdf = gpd.GeoDataFrame(\n",
    "    pd.concat([onlake_outlines_gdf, offlake_outlines_gdf], ignore_index=True),\n",
    "    geometry='geometry', crs=onlake_outlines_gdf.crs)\n",
    "\n",
    "# Load evolving outlines union\n",
    "evolving_stationary_union_gdf = evolving_stationary_outlines_union_evolving_lakes_gdf[evolving_stationary_outlines_union_evolving_lakes_gdf['name'] == lake_name]\n",
    "evolving_union_gdf = evolving_outlines_union_gdf[evolving_outlines_union_gdf['name'] == lake_name]\n",
    "\n",
    "# Load evolving outlines search parameters\n",
    "row_index=evolving_outlines_gdf['row_index'][0]\n",
    "within_area_multiple=evolving_outlines_gdf['within_area_multiple'][0]\n",
    "level=evolving_outlines_gdf['level'][0]\n",
    "\n",
    "# Attempt to open the geometric calculations CSV files\n",
    "try:\n",
    "    evolving_geom_calcs_df = pd.read_csv(os.path.join(\n",
    "        os.getcwd(), 'output/geometric_calcs/evolving_outlines_geom_calc/forward_fill/{}.csv'.format(lake_name)))\n",
    "    stationary_geom_calcs_df = pd.read_csv(os.path.join(\n",
    "        os.getcwd(), \n",
    "        'output/geometric_calcs/stationary_outline_geom_calc/stationary_outlines_at_all_lakes/{}.csv'.format(lake_name)))\n",
    "except FileNotFoundError:\n",
    "    print(f'CSV files for {lake_name} not found.')\n",
    "\n",
    "# Convert of strings to datetime\n",
    "evolving_geom_calcs_df['mid_pt_datetime'] = pd.to_datetime(evolving_geom_calcs_df['mid_pt_datetime'])\n",
    "stationary_geom_calcs_df['mid_pt_datetime'] = pd.to_datetime(stationary_geom_calcs_df['mid_pt_datetime'])\n",
    "\n",
    "# Prepare datasets - using larger buffer for initial masking\n",
    "dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(lake_gdf, 25)\n",
    "\n",
    "dataset1_dh = dataset1_masked['delta_h'].diff('time')\n",
    "dataset2_dh = dataset2_masked['delta_h'].diff('time')\n",
    "\n",
    "# Select time step to show for CryoSat-2 (CS2) and ICESat-2 (IS2) eras\n",
    "CS2_i = 12\n",
    "IS2_i = 13\n",
    "\n",
    "# Isolate dh time steps\n",
    "CS2_timestep = dataset1_masked['time'][CS2_i].values\n",
    "IS2_timestep = dataset2_masked['time'][IS2_i].values\n",
    "\n",
    "# Convert date_list to numpy array\n",
    "CS2_cyc_start_datetimes_npa = np.array(cyc_start_datetimes[1:], dtype='datetime64')\n",
    "IS2_cyc_start_datetimes_npa = np.array(cyc_start_datetimes[1:], dtype='datetime64')\n",
    "\n",
    "# Find matching cyc_dates index\n",
    "CS2_cyc_dates_idx = np.where(CS2_cyc_start_datetimes_npa == CS2_timestep)[0][0]\n",
    "IS2_cyc_dates_idx = np.where(IS2_cyc_start_datetimes_npa == IS2_timestep)[0][0]\n",
    "\n",
    "# Initialize empty lists for height anomalies\n",
    "height_anom_pos = []\n",
    "height_anom_neg = []\n",
    "\n",
    "# Get height anomalies for CryoSat-2 timestep (i = 20)\n",
    "if dataset1_masked is not None:\n",
    "    if np.any(~np.isnan(dataset1_dh[CS2_i])):\n",
    "        height_anom_pos.append(np.nanmax(dataset1_dh[CS2_i]))\n",
    "        height_anom_neg.append(np.nanmin(dataset1_dh[CS2_i]))\n",
    "\n",
    "# Get height anomalies for ICESat-2 timestep (i = 12)\n",
    "if np.any(~np.isnan(dataset2_dh[IS2_i])):\n",
    "    height_anom_pos.append(np.nanmax(dataset2_dh[IS2_i]))\n",
    "    height_anom_neg.append(np.nanmin(dataset2_dh[IS2_i]))\n",
    "\n",
    "# Find max height anomalies across both time slices\n",
    "max_height_anom_pos = max(height_anom_pos)\n",
    "max_height_anom_neg = min(height_anom_neg)\n",
    "max_anom = max([max_height_anom_pos, abs(max_height_anom_neg)])\n",
    "del height_anom_pos, height_anom_neg\n",
    "\n",
    "# Create the diverging normalization for the colormap\n",
    "divnorm = colors.TwoSlopeNorm(vmin=-max_anom, vcenter=0., vmax=max_anom)\n",
    "del max_height_anom_pos, max_height_anom_neg\n",
    "\n",
    "# Establish x_min, x_max, y_min, y_max\n",
    "ROI_poly = area_multiple_buffer(lake_poly, 25)\n",
    "x_min, y_min, x_max, y_max = ROI_poly.bounds\n",
    "x_buffer, y_buffer = abs(x_max-x_min)*0.02, abs(y_max-y_min)*0.02\n",
    "\n",
    "# Subsetting dataset\n",
    "dataset1 = SARIn_dh\n",
    "dataset2 = ATL15_dh\n",
    "\n",
    "# Prepare datasets\n",
    "dataset1_doi, dataset2_doi, search_extent_poly, (dataset1_masked, dataset2_masked) = prepare_datasets(lake_gdf, 25)\n",
    "\n",
    "# Create figure with GridSpec - modified to include timeline panel\n",
    "fig = plt.figure(figsize=(10,16))  # Increased height to accommodate new panel\n",
    "gs = fig.add_gridspec(4, 2, height_ratios=[0.25, 1, 1, 1], hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Create remaining axes array excluding timeline row\n",
    "axs = np.array([[fig.add_subplot(gs[i,j]) for j in range(2)] for i in range(1,4)])\n",
    "\n",
    "\n",
    "# Panel - Satellite era timeline\n",
    "\n",
    "# Add timeline panel spanning both columns\n",
    "timeline_ax = fig.add_subplot(gs[0, :])\n",
    "\n",
    "# Create timeline visualization\n",
    "timeline_ax.set_xlim(cyc_start_datetimes[0], cyc_end_datetimes[-1])\n",
    "timeline_ax.xaxis.set_major_locator(mdates.YearLocator(base=1))\n",
    "timeline_ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "timeline_ax.xaxis.set_minor_locator(mdates.MonthLocator([1, 4, 7, 10]))\n",
    "\n",
    "# Plot colored bars for different satellite eras\n",
    "for i, date in enumerate(cyc_dates['cyc_start_datetimes']):\n",
    "    dataset = cyc_dates['dataset'][i]   \n",
    "    color = 'mediumpurple' if dataset == 'SARIn_dh' else 'lightgreen'\n",
    "    label = 'CryoSat-2' if dataset == 'SARIn_dh' else 'ICESat-2'\n",
    "    timeline_ax.axvspan(cyc_dates['cyc_start_datetimes'][i], cyc_dates['cyc_end_datetimes'][i], \n",
    "                      color=color, alpha=0.3, label=label)\n",
    "\n",
    "# Get the dates for our dh time periods shown in panels b and c\n",
    "# For CryoSat-2\n",
    "cs2_start_date = cyc_start_datetimes[CS2_cyc_dates_idx]\n",
    "cs2_end_date = cyc_end_datetimes[CS2_cyc_dates_idx+1]\n",
    "\n",
    "# For ICESat-2\n",
    "is2_start_date = cyc_start_datetimes[IS2_cyc_dates_idx]\n",
    "is2_end_date = cyc_end_datetimes[IS2_cyc_dates_idx+1]\n",
    "\n",
    "# Add vertical spans for the timesteps shown in top panels\n",
    "# For CryoSat-2\n",
    "timeline_ax.axvspan(mdates.date2num(cs2_start_date), \n",
    "                              mdates.date2num(cs2_end_date),\n",
    "                              color='gray', alpha=0.3)\n",
    "\n",
    "# For ICESat-2\n",
    "timeline_ax.axvspan(mdates.date2num(is2_start_date),\n",
    "                              mdates.date2num(is2_end_date),\n",
    "                              color='gray', alpha=0.3)\n",
    "# Create patches for legend\n",
    "cs2_patch = mpatches.Patch(color='mediumpurple', alpha=0.3, label='CryoSat-2')\n",
    "is2_patch = mpatches.Patch(color='lightgreen', alpha=0.3, label='ICESat-2')\n",
    "timestep_patch = mpatches.Patch(color='gray', alpha=0.3, label='displayed dh')\n",
    "\n",
    "# Add legend with all patches\n",
    "timeline_ax.legend(handles=[cs2_patch, is2_patch, timestep_patch], \n",
    "                  loc='upper right')\n",
    "\n",
    "# Customize timeline appearance\n",
    "# timeline_ax.legend(loc='upper right')\n",
    "timeline_ax.set_title('Multi-mission satellite timeline')\n",
    "\n",
    "# Remove y-axis ticks and labels\n",
    "timeline_ax.set_yticks([])\n",
    "\n",
    "\n",
    "# Panel - CryoSat-2 dh time step with evolving outlines and area multiple within evaluation lines\n",
    "\n",
    "# Plot dh time step (subtract 1 to account for differencing removes first time step in dataset1_dh)\n",
    "img = axs[0,0].imshow(dataset1_dh[CS2_i-1], cmap='RdBu', norm=divnorm, \n",
    "                      origin='lower', extent=[x_min, x_max, y_min, y_max])\n",
    "\n",
    "dt = cyc_start_datetimes[1:][CS2_cyc_dates_idx]\n",
    "evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['mid_pt_datetime'] == dt]\n",
    "\n",
    "if not evolving_outlines_gdf.empty:\n",
    "    # Split into positive and negative dh values\n",
    "    positive_outlines = evolving_outlines_dt[evolving_outlines_dt['dh (m)'] > 0]\n",
    "    negative_outlines = evolving_outlines_dt[evolving_outlines_dt['dh (m)'] < 0]\n",
    "    \n",
    "    # Plot positive dh in blue\n",
    "    if not positive_outlines.empty:\n",
    "        positive_outlines.boundary.plot(ax=axs[0,0], color='blue', linewidth=1)\n",
    "    \n",
    "    # Plot negative dh in red\n",
    "    if not negative_outlines.empty:\n",
    "        negative_outlines.boundary.plot(ax=axs[0,0], color='red', linewidth=1)\n",
    "\n",
    "# Plot inset map\n",
    "axIns = axs[0,0].inset_axes([0.02, 0, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "axIns.set_aspect('equal')\n",
    "moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1)\n",
    "moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1)\n",
    "axIns.axis('off')\n",
    "# Plot red star to indicate location\n",
    "axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "    linewidth=1, color='k', s=75)\n",
    "\n",
    "# Set a title for the axes\n",
    "title_text = f'Height change from \\n{date_to_quarter_year(cyc_start_datetimes[CS2_cyc_dates_idx])} to {date_to_quarter_year(cyc_end_datetimes[CS2_cyc_dates_idx+1])}'\n",
    "axs[0,0].set_title(title_text, y=1)\n",
    "\n",
    "# Create lines for legend\n",
    "stationary_color = 'darkturquoise'\n",
    "stationary_line = plt.Line2D([], [], color=stationary_color, linestyle='solid', linewidth=2)\n",
    "within_eval_lines = plt.Line2D([], [], color='gray', linestyle='dotted', linewidth=2)\n",
    "optimal_within_eval_line = plt.Line2D([], [], color='gray', linestyle='solid', linewidth=2)\n",
    "\n",
    "# Plot legend\n",
    "legend = axs[0,0].legend([stationary_line, \n",
    "                          within_eval_lines,\n",
    "                          optimal_within_eval_line], \n",
    "    ['stationary outline', \n",
    "     'within evaluation boundaries',\n",
    "     'optimal within boundary'],\n",
    "    handlelength=3,\n",
    "    handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper center')\n",
    "\n",
    "\n",
    "# Panel - ICESat-2 dh time step with evolving outlines and area multiple within evaluation lines  \n",
    "\n",
    "# Plot dh time step (subtract 1 to account for differencing removes first time step in dataset2_dh)\n",
    "img = axs[0,1].imshow(dataset2_dh[IS2_i-1], cmap='RdBu', norm=divnorm, \n",
    "                      origin='lower', extent=[x_min, x_max, y_min, y_max])\n",
    "\n",
    "dt = cyc_start_datetimes[1:][IS2_cyc_dates_idx]\n",
    "evolving_outlines_dt = evolving_outlines_gdf[evolving_outlines_gdf['mid_pt_datetime'] == dt]\n",
    "\n",
    "if not evolving_outlines_gdf.empty:\n",
    "    # Split into positive and negative dh values\n",
    "    positive_outlines = evolving_outlines_dt[evolving_outlines_dt['dh (m)'] > 0]\n",
    "    negative_outlines = evolving_outlines_dt[evolving_outlines_dt['dh (m)'] < 0]\n",
    "    \n",
    "    # Plot positive dh in blue\n",
    "    if not positive_outlines.empty:\n",
    "        positive_outlines.boundary.plot(ax=axs[0,1], color='blue', linewidth=1)\n",
    "    \n",
    "    # Plot negative dh in red\n",
    "    if not negative_outlines.empty:\n",
    "        negative_outlines.boundary.plot(ax=axs[0,1], color='red', linewidth=1)\n",
    "\n",
    "# Add colorbar space to both axes for consistent sizing\n",
    "for ax in [axs[0,0], axs[0,1]]:\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "    if ax == axs[0,1]:  # Only add the actual colorbar to the second plot\n",
    "        cb = fig.colorbar(img, cax=cax)\n",
    "        cb.set_label('dh [m quarter$^{-1}$]')\n",
    "    else:\n",
    "        # Hide the empty axis for the first plot\n",
    "        cax.set_visible(False)\n",
    "\n",
    "# Set a title for the axes\n",
    "title_text = f'Height change from \\n{date_to_quarter_year(cyc_start_datetimes[IS2_cyc_dates_idx])} to {date_to_quarter_year(cyc_end_datetimes[IS2_cyc_dates_idx+1])}'\n",
    "axs[0,1].set_title(title_text, y=1)\n",
    "\n",
    "for ax in [axs[0,0], axs[0,1]]:\n",
    "    ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "axs[0,1].sharey(axs[0,0])\n",
    "\n",
    "# Create lines for legend\n",
    "pos_dh_anom = plt.Line2D([], [], color='blue', linestyle='solid', linewidth=2)\n",
    "neg_dh_anom = plt.Line2D([], [], color='red', linestyle='solid', linewidth=2)\n",
    "\n",
    "# Plot legend\n",
    "legend = axs[0,1].legend([pos_dh_anom, \n",
    "                          neg_dh_anom], \n",
    "    [f'pos. dh anomaly (>+{level} m)', \n",
    "     f'neg. dh anomaly (<−{level} m)'],\n",
    "    handlelength=3,\n",
    "    handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper center')\n",
    "\n",
    "\n",
    "# Panel - Evolving outlines time series plot ---------------------------------------------\n",
    "\n",
    "# Set up colormap\n",
    "cmap = plt.get_cmap('plasma', len(cyc_start_datetimes)-1)\n",
    "\n",
    "# Norm to time variable\n",
    "norm = plt.Normalize(mdates.date2num(cyc_dates['cyc_start_datetimes'].iloc[1]), \n",
    "                     mdates.date2num(cyc_dates['cyc_start_datetimes'].iloc[-1]))\n",
    "\n",
    "# Zoom in slightly to bounds of optimal within evaluation boundary\n",
    "# Establish x_min, x_max, y_min, y_max\n",
    "optimal_within_eval_poly = area_multiple_buffer(lake_poly, 20)\n",
    "x_min, y_min, x_max, y_max = optimal_within_eval_poly.bounds\n",
    "x_buffer, y_buffer = abs(x_max-x_min)*0.01, abs(y_max-y_min)*0.01\n",
    "\n",
    "# Plot MOA surface imagery\n",
    "mask_x = (moa_highres_da.x >= x_min-x_buffer) & (moa_highres_da.x <= x_max+x_buffer)\n",
    "mask_y = (moa_highres_da.y >= y_min-y_buffer) & (moa_highres_da.y <= y_max+y_buffer)\n",
    "moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)\n",
    "axs[1,0].imshow(moa_highres_da_subset[0,:,:], cmap='gray', clim=[14000, 17000], \n",
    "          extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer])\n",
    "\n",
    "# Plot evolving outlines\n",
    "onlake_lines, offlake_lines = [], []\n",
    "for i, dt in enumerate(cyc_start_datetimes[1:]):\n",
    "    x, y = 1, 1\n",
    "    onlake_line, = axs[1,0].plot(x, y, color=cmap(norm(mdates.date2num(mid_pt_datetimes[i]))), linewidth=2)\n",
    "    onlake_lines.append(onlake_line)\n",
    "    offlake_line, = axs[1,0].plot(x, y, color=cmap(norm(date_to_quarter_year(mid_pt_datetimes[i]))), linewidth=2, alpha=0.2)\n",
    "    offlake_lines.append(offlake_line)\n",
    "    \n",
    "    onlake_outlines_dt = onlake_outlines_gdf[onlake_outlines_gdf['mid_pt_datetime'] == dt]\n",
    "    offlake_outlines_dt = offlake_outlines_gdf[offlake_outlines_gdf['mid_pt_datetime'] == dt]\n",
    "    \n",
    "    if not onlake_outlines_dt.empty:\n",
    "        onlake_outlines_dt.boundary.plot(ax=axs[1,0], \n",
    "            color=cmap(norm(mdates.date2num(mid_pt_datetimes[i]))), \n",
    "            linewidth=1)\n",
    "    if not offlake_outlines_dt.empty:\n",
    "        offlake_outlines_dt.boundary.plot(ax=axs[1,0], \n",
    "            color=cmap(norm(mdates.date2num(mid_pt_datetimes[i]))), \n",
    "            linewidth=1, alpha=0.5)\n",
    "\n",
    "# Plot evolving outlines unary union\n",
    "evolving_union_gdf.boundary.plot(ax=axs[1,0], color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "# Create stationary region and evolving outlines region and plot\n",
    "stationary_region = area_multiple_buffer(lake_poly, 2)\n",
    "stationary_region = stationary_region.difference(lake_poly)\n",
    "evolving_union_region = area_multiple_buffer(evolving_union_gdf['geometry'], 2)\n",
    "evolving_union_region = evolving_union_region.difference(evolving_union_gdf['geometry'].iloc[0])\n",
    "gpd.GeoDataFrame(geometry=[stationary_region], crs='3031').plot(ax=axs[1,0], color=stationary_color, linewidth=2, alpha=0.2)\n",
    "gpd.GeoDataFrame(geometry=[evolving_union_region], crs='3031').plot(ax=axs[1,0], color='k', linewidth=2, alpha=0.2)\n",
    "\n",
    "# Set up colormap\n",
    "min_date = pd.to_datetime(cyc_start_datetimes[1])\n",
    "max_date = pd.to_datetime(cyc_end_datetimes[-1])\n",
    "date_range = pd.date_range(min_date, max_date, periods=len(mid_pt_datetimes))\n",
    "years = date_range.year.unique()\n",
    "years = pd.to_datetime(years, format='%Y')\n",
    "n_dates = len(mid_pt_datetimes[1:])\n",
    "cmap = plt.get_cmap('plasma', n_dates)\n",
    "norm = plt.Normalize(mdates.date2num(min_date), mdates.date2num(max_date))\n",
    "m = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "m.set_array(np.linspace(mdates.date2num(min_date), mdates.date2num(max_date), n_dates))\n",
    "\n",
    "# Add colorbar\n",
    "divider = make_axes_locatable(axs[1,0])\n",
    "cax = divider.append_axes('bottom', size='3%', pad=0.5)\n",
    "cbar = fig.colorbar(m, cax=cax, orientation='horizontal')\n",
    "\n",
    "# Set ticks for all years but labels only for odd years\n",
    "tick_locations = [mdates.date2num(date) for date in years[1:]]\n",
    "tick_labels = [f''{date.strftime('%y')}' if date.year % 2 == 0 else '' for date in years[1:]]\n",
    "cbar.set_ticks(tick_locations)\n",
    "cbar.set_ticklabels(tick_labels)\n",
    "\n",
    "# Add minor ticks for quarters\n",
    "cbar.ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))  # Quarter intervals only\n",
    "cbar.set_label('year', size=10, labelpad=5)\n",
    "\n",
    "axs[1,0].set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "    \n",
    "# Emphasize zeroth row_index within evaluation line selected for particular lake\n",
    "gpd.GeoSeries(area_multiple_buffer(lake_poly, within_area_multiple)).boundary.plot(ax=ax, linestyle='solid', color='gray')\n",
    "\n",
    "# Plot legend\n",
    "evolving_union_line = plt.Line2D([], [], color='k', linestyle='dotted', linewidth=2)\n",
    "stationary_region_patch = mpatches.Patch(color=stationary_color, alpha=0.2)\n",
    "evolving_union_region_patch = mpatches.Patch(color='k', alpha=0.2)\n",
    "\n",
    "legend = axs[1,0].legend([stationary_region_patch,\n",
    "                       evolving_union_region_patch,\n",
    "                       tuple(onlake_lines),\n",
    "                       evolving_union_line], \n",
    "    ['stationary region',\n",
    "     'evolving union region',\n",
    "     'evolving outlines',\n",
    "     'evolving outlines union'],\n",
    "    handlelength=3,\n",
    "    handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper right')\n",
    "\n",
    "\n",
    "# Panel - active area ---------------------------------------------\n",
    "    \n",
    "# Plot horizontal zero line for reference\n",
    "axs[1,1].axhline(0, color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "# Plot stationary outline and evolving outlines unary union areas\n",
    "axs[1,1].axhline(np.divide(lake_gdf['area (m^2)'], 1e6).values, \n",
    "                 color=stationary_color, linestyle='solid', linewidth=2)\n",
    "axs[1,1].axhline(np.divide(evolving_union_gdf['area (m^2)'], 1e6).values, \n",
    "                 color='k', linestyle='dotted', linewidth=2)\n",
    "\n",
    "# Plot multi-colored line and scatter for data points\n",
    "x = mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime'])\n",
    "y = np.divide(evolving_geom_calcs_df['evolving_outlines_area (m^2)'], 1e6)\n",
    "\n",
    "# Plot evolving outlines time series using LineCollection from points/segments to plot multi-colored line\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(x)  # Set the values used for colormapping, using matplotlib dates for colors\n",
    "lc.set_linewidth(2)\n",
    "line = axs[1,1].add_collection(lc)\n",
    "scatter = axs[1,1].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "# Add legend\n",
    "legend = axs[1,1].legend([stationary_line, \n",
    "                          evolving_union_line, \n",
    "                          tuple(onlake_lines)], \n",
    "    ['stationary outline',\n",
    "     'evolving outlines union',\n",
    "     'evolving outlines'],\n",
    "     handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "     loc='upper center')\n",
    "\n",
    "\n",
    "# Panel - dh/dt -------------------------------------------------------\n",
    "\n",
    "# Plot horizontal zero line for reference\n",
    "axs[2,0].axhline(0, color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "# Plot stationary outlines off-lake region dh\n",
    "axs[2,0].plot(mdates.date2num(stationary_geom_calcs_df['mid_pt_datetime']), \n",
    "    np.cumsum(stationary_geom_calcs_df['stationary_outline_region_dh (m)']),\n",
    "    color='lightgray', linestyle='solid', linewidth=2)\n",
    "axs[2,0].scatter(mdates.date2num(stationary_geom_calcs_df['mid_pt_datetime']), \n",
    "    np.cumsum(stationary_geom_calcs_df['stationary_outline_region_dh (m)']),\n",
    "    color='lightgray', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# Plot evolving outlines off-lake region dh\n",
    "axs[2,0].plot(mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime']), \n",
    "    np.cumsum(evolving_geom_calcs_df['evolving_outlines_region_dh (m)']), \n",
    "    color='dimgray', linestyle='solid', linewidth=2)\n",
    "axs[2,0].scatter(mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime']), \n",
    "    np.cumsum(evolving_geom_calcs_df['evolving_outlines_region_dh (m)']),\n",
    "    color='dimgray', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# Plot stationary outline time series (uncorrected)\n",
    "axs[2,0].plot(mdates.date2num(stationary_geom_calcs_df['mid_pt_datetime']), \n",
    "    np.cumsum(stationary_geom_calcs_df['stationary_outline_dh (m)']), \n",
    "    color=stationary_color, linestyle='solid', linewidth=1)\n",
    "axs[2,0].scatter(mdates.date2num(stationary_geom_calcs_df['mid_pt_datetime']), \n",
    "    np.cumsum(stationary_geom_calcs_df['stationary_outline_dh (m)']),\n",
    "    color=stationary_color, linestyle='solid', linewidth=1, s=2.5)\n",
    "\n",
    "# Plot stationary outline time series\n",
    "axs[2,0].plot(mdates.date2num(stationary_geom_calcs_df['mid_pt_datetime']), \n",
    "    np.cumsum(stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "    color=stationary_color, linestyle='solid', linewidth=2)\n",
    "axs[2,0].scatter(mdates.date2num(stationary_geom_calcs_df['mid_pt_datetime']), \n",
    "    np.cumsum(stationary_geom_calcs_df['stationary_outline_dh_corr (m)']),\n",
    "    color=stationary_color, linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# Plot evolving outlines time series using multi-colored LineCollection from points/segments (uncorrected)\n",
    "x = mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime'])\n",
    "y = np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh (m)'])\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(1)\n",
    "line = axs[2,0].add_collection(lc)\n",
    "scatter = axs[2,0].scatter(x, y, c=x, cmap=cmap, norm=norm, s=4.5)\n",
    "\n",
    "# Plot evolving outlines time series using multi-colored LineCollection from points/segments\n",
    "x = mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime'])\n",
    "y = np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'])\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(2)\n",
    "line = axs[2,0].add_collection(lc)\n",
    "scatter = axs[2,0].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "# Plot bias\n",
    "axs[2,0].plot(mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime']),\n",
    "    np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'] -\n",
    "              stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "    color='red', linestyle='solid', linewidth=2)\n",
    "axs[2,0].scatter(mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime']),\n",
    "    np.cumsum(evolving_geom_calcs_df['evolving_outlines_dh_corr (m)'] -\n",
    "              stationary_geom_calcs_df['stationary_outline_dh_corr (m)']), \n",
    "    color='red', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# Add legend\n",
    "evolving_region = plt.Line2D([], [], color='dimgray', linestyle='solid', linewidth=2)\n",
    "stationary_region = plt.Line2D([], [], color='lightgray', linestyle='solid', linewidth=2)\n",
    "bias = plt.Line2D([], [], color='red', linestyle='solid', linewidth=2)\n",
    "legend = axs[2,0].legend(\n",
    "    [evolving_region,\n",
    "     stationary_region,\n",
    "     tuple(onlake_lines),\n",
    "     stationary_line,  \n",
    "     bias],\n",
    "    ['evolving outlines regional',\n",
    "     'stationary outline regional',\n",
    "     'evolving outlines',\n",
    "     'stationary outline', \n",
    "     'bias (evolving − stationary)'],\n",
    "     handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "     loc='upper center')\n",
    "\n",
    "\n",
    "# Panel - dV/dt --------------------------------------------------\n",
    "\n",
    "# Plot horizontal line at zero for reference\n",
    "axs[2,1].axhline(0, color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "# Plot stationary outline time series\n",
    "axs[2,1].plot(mdates.date2num(stationary_geom_calcs_df['mid_pt_datetime']), \n",
    "    np.divide(np.cumsum(stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "    color=stationary_color, linestyle='solid', linewidth=2)\n",
    "axs[2,1].scatter(mdates.date2num(stationary_geom_calcs_df['mid_pt_datetime']), \n",
    "    np.divide(np.cumsum(stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)']), 1e9), \n",
    "    color=stationary_color, linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# Plot multi-colored line and scatter for data points\n",
    "x = mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime'])\n",
    "y = np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'], 1e9))\n",
    "\n",
    "# Plot evolving outlines time series using LineCollection from points/segments to plot multi-colored line\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "lc = LineCollection(segments, cmap=cmap, norm=norm, linestyle='solid')\n",
    "lc.set_array(x)\n",
    "lc.set_linewidth(2)\n",
    "line = axs[2,1].add_collection(lc)\n",
    "scatter = axs[2,1].scatter(x, y, c=x, cmap=cmap, norm=norm, s=9)\n",
    "\n",
    "# Plot bias\n",
    "axs[2,1].plot(mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime']),\n",
    "    np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "                        stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)'], 1e9)), \n",
    "    color='red', linestyle='solid', linewidth=2)\n",
    "axs[2,1].scatter(mdates.date2num(evolving_geom_calcs_df['mid_pt_datetime']),\n",
    "    np.cumsum(np.divide(evolving_geom_calcs_df['evolving_outlines_dV_corr (m^3)'] -\n",
    "                        stationary_geom_calcs_df['stationary_outline_dV_corr (m^3)'], 1e9)), \n",
    "    color='red', linestyle='solid', linewidth=2, s=5)\n",
    "\n",
    "# Add legend\n",
    "legend = axs[2,1].legend([tuple(onlake_lines), stationary_line, bias],\n",
    "    ['evolving outlines', \n",
    "     'stationary outline',\n",
    "     'bias (evolving − stationary)'], \n",
    "    handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper center')\n",
    "\n",
    "# Adjust y-axis limits to avoid legend/data overlap\n",
    "axs[1,1].set_ylim(-25, 625)\n",
    "axs[2,0].set_ylim(-0.5, 9)\n",
    "axs[2,1].set_ylim(-0.4, 0.4)\n",
    "\n",
    "# Add vertical spans to time series plots\n",
    "for ax in [axs[1,1], axs[2,0], axs[2,1]]:\n",
    "    # Add vertical spans for both timesteps\n",
    "    ax.axvspan(mdates.date2num(cs2_start_date), \n",
    "                         mdates.date2num(cs2_end_date),\n",
    "                         color='gray', alpha=0.1)\n",
    "    \n",
    "    ax.axvspan(mdates.date2num(is2_start_date),\n",
    "                         mdates.date2num(is2_end_date),\n",
    "                         color='gray', alpha=0.1)\n",
    "\n",
    "# Label the timeline panel\n",
    "timeline_ax.text(0.01, 0.98, 'a', transform=timeline_ax.transAxes, \n",
    "                fontsize=20, va='top', ha='left')\n",
    "\n",
    "# Plot elements common to more than one ax object\n",
    "for i in range(axs.shape[0]):\n",
    "    for j in range(axs.shape[1]):\n",
    "        # Set common tick size for all axes\n",
    "        axs[i,j].tick_params(axis='both')\n",
    "        \n",
    "        # Add subplot labels (a, b, c, etc.)\n",
    "        axs[i,j].text(0.02, 0.98, chr(97 + i*2 + j + 1), transform=axs[i,j].transAxes, \n",
    "                     fontsize=16, va='top', ha='left')\n",
    "\n",
    "        # Special formatting for map plots (top row and [1,0])\n",
    "        if (i == 0) or (i == 1 and j == 0):\n",
    "            # Set x and y labels for map plots\n",
    "            axs[i,j].set_xlabel('x [km]')\n",
    "            if j == 0:  # Only for first column\n",
    "                axs[i,j].set_ylabel('y [km]')\n",
    "\n",
    "            # Add map-specific elements\n",
    "            reexamined_stationary_outlines_gdf.boundary.plot(ax=axs[i,j], \n",
    "                edgecolor=stationary_color, linestyle='solid', linewidth=2, zorder=0)\n",
    "            \n",
    "            # Convert meters to kilometers\n",
    "            km_scale = 1e3\n",
    "            ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "            ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "            axs[i,j].xaxis.set_major_formatter(ticks_x)\n",
    "            axs[i,j].yaxis.set_major_formatter(ticks_y)\n",
    "\n",
    "            # Add evaluation lines\n",
    "            for within_area_multiple_i in range(2,20):\n",
    "                gpd.GeoSeries(area_multiple_buffer(lake_poly, within_area_multiple_i)).boundary.plot(\n",
    "                    ax=axs[i,j], linestyle='dotted', color='gray')\n",
    "            \n",
    "            # Add emphasized evaluation line\n",
    "            gpd.GeoSeries(area_multiple_buffer(lake_poly, within_area_multiple)).boundary.plot(\n",
    "                ax=axs[i,j], linestyle='solid', color='gray')\n",
    "\n",
    "        # Format time series plots\n",
    "        else:\n",
    "            axs[i,j].set_xlabel('year')\n",
    "            \n",
    "            # Set y labels for time series plots\n",
    "            if j == 0:  # First column\n",
    "                if i == 2:\n",
    "                    axs[i,j].set_ylabel('cumulative $dh$ [m]')\n",
    "            elif j == 1:  # Second column\n",
    "                if i == 1:\n",
    "                    axs[i,j].set_ylabel('active area [km$^2$]')\n",
    "                elif i == 2:\n",
    "                    axs[i,j].set_ylabel('cumulative $dV$ [km$^3$]')\n",
    "            \n",
    "            # Add date formatting for specific time series plots\n",
    "            if (i == 1 and j == 1) or (i == 2):  # axs[1,1], axs[2,0], and axs[2,1]\n",
    "                axs[i,j].xaxis.set_major_formatter(ticker.FuncFormatter(even_year_formatter))\n",
    "                axs[i,j].xaxis.set_major_locator(mdates.YearLocator())\n",
    "                axs[i,j].xaxis.set_minor_locator(mdates.MonthLocator([1, 4, 7, 10]))  # Quarter year ticks (Jan, Apr, Jul, Oct)\n",
    "                axs[i,j].set_xlim(cyc_start_datetimes[1], cyc_end_datetimes[-1])\n",
    "\n",
    "# Save and preview plot\n",
    "plt.savefig(OUTPUT_DIR + '/FigS1_lake_reexamination_methods.jpg',\n",
    "    dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c404262a-4294-4c03-a7c3-b71bcc8f6edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
