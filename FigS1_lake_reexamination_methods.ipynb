{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a180750-d435-44ea-94f0-c96b9dbea099",
   "metadata": {
    "user_expressions": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Make function to plot aggregated evolving contours and store files\n",
    "# Modify find contours function or use if statement to achieve less than 10% of contours intersecting with buffer\n",
    "# Import/read ATL11 (try h5coro)\n",
    "# Add ATL11 overlay\n",
    "# Add something along the lines of code below to illustrate lake area/outline:\n",
    "# # Plot polygons in the GeoDataFrame\n",
    "# gdf.plot(ax=ax, color='lightblue', edgecolor='black', linewidth=1, label='Polygon Area')\n",
    "\n",
    "# # Optionally, if you want to plot just the boundaries with a different style\n",
    "# gdf.boundary.plot(ax=ax, color='red', linewidth=2, label='Polygon Boundary')\n",
    "\n",
    "# In find_outlines_and_plot add steps to:\n",
    "# add off-lake dh/dt column; perhaps use unary_union of evolving outlines polys in each timestep\n",
    "\n",
    "# lake_name='KT1'    \n",
    "# outline = SiegfriedFricker2018_outlines[(SiegfriedFricker2018_outlines['name'] == lake_name)]\n",
    "# region = outline['geometry'].buffer(10000)\n",
    "\n",
    "# # Clip ATL15 to lake outline and region\n",
    "# ATL15_dh_sub.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "# ATL15_outline = ATL15_dh_sub.rio.clip(outline.geometry.values, outline.crs)\n",
    "# ATL15_region = ATL15_dh_sub.rio.clip(region.geometry.values, region.crs)\n",
    "# ATL15_region = ATL15_dh_sub.rio.clip(outline.geometry.values, outline.crs, invert=True)\n",
    "\n",
    "# Simplify subsetting code:\n",
    "# CS2_dh_sub = CS2_dh.sel(y=slice(y_max, y_min), x=slice(x_min, x_max))\n",
    "# ATL15_dh_sub = ATL15_dh.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca6a6e-114b-437a-babe-8dfa175f4299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to Fig. S1 of Sauthoff and others, 2024\n",
    "# This code runs continental-scale operations on multiple datasets and\n",
    "# requires a 128 GB server or local memory\n",
    "#\n",
    "# Written 2023-07-11 by W. Sauthoff (wsauthoff.github.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff75638-a801-40a3-bd59-7d6536dbc054",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Set up computing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b2a28f-a575-4f1e-b1c9-f601327149aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47cdb0-00aa-4268-8691-9e44b3bfda2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import datetime\n",
    "import earthaccess\n",
    "import gc\n",
    "import geopandas as gpd\n",
    "from IPython.display import clear_output\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path\n",
    "from pyproj import CRS, Transformer\n",
    "import rioxarray\n",
    "from rioxarray.exceptions import NoDataInBounds\n",
    "from shapely.geometry import box, Polygon\n",
    "from shapely.ops import unary_union\n",
    "from skimage import measure\n",
    "import time\n",
    "import xarray as xr\n",
    "\n",
    "# Magic functions\n",
    "%matplotlib widget\n",
    "# %matplotlib inline\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "def play_sound():\n",
    "    display(Audio(url=\"http://codeskulptor-demos.commondatastorage.googleapis.com/pang/pop.mp3\", autoplay=True))\n",
    "\n",
    "# Define data directories dependent on home environment\n",
    "# Replace with your directory file paths\n",
    "if os.getenv('HOME') == '/home/jovyan':\n",
    "    DATA_DIR = '/home/jovyan/data'\n",
    "    SCRIPT_DIR = '/home/jovyan/repos_my/script_dir'\n",
    "    OUTPUT_DIR = '/home/jovyan/1_outlines_candidates/output/FigS1_lake_reexamination_methods.ipynb'\n",
    "\n",
    "# Define constants and coordinate transforms for the geodesic area calculation\n",
    "CRS_LL = \"EPSG:4326\" # wgs84 in lon,lat\n",
    "GEOD = CRS(CRS_LL).get_geod() # geod object for calculating geodesic area on defined ellipsoid\n",
    "CRS_XY = \"EPSG:3031\" # Antarctic Polar Stereographic in x, y\n",
    "XY_TO_LL = Transformer.from_crs(CRS_XY, CRS_LL, always_xy = True) # make coord transformer\n",
    "\n",
    "# Change default font to increase font size\n",
    "plt.rcParams.update({'font.size': 8})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4adef9-57d4-4ab4-b066-ba4a4ebb38a5",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "e074c049-cbfc-41a5-8d42-35f57f5cdffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def read_field(group, field):\n",
    "#     \"\"\"\n",
    "#     generic field-reading function\n",
    "#     \"\"\"\n",
    "#     data=np.array(group[field])\n",
    "#     bad=(data==group[field].attrs['_FillValue'])\n",
    "#     data[bad]=np.NaN\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "649ccc68-98f0-4c40-969e-d20c4d37de06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def read_ATL11(filename, pair):\n",
    "#     \"\"\"\n",
    "#     ATL11 reader\n",
    "#     \"\"\"\n",
    "#     with h5py.File(filename,'r') as h5f:\n",
    "#         longitude=read_field(h5f[pair],'longitude')\n",
    "#         latitude=read_field(h5f[pair],'latitude')\n",
    "#         h_corr=read_field(h5f[pair],'h_corr')\n",
    "#         h_corr_sigma=read_field(h5f[pair],'h_corr_sigma')\n",
    "#         h_corr_sigma_s=read_field(h5f[pair],'h_corr_sigma_systematic')\n",
    "#         quality=np.array(h5f[pair]['quality_summary'])\n",
    "#     for col in range(h_corr.shape[1]):\n",
    "#         h_corr[quality==1]=np.NaN\n",
    "#     # return the values\n",
    "#     return longitude, latitude, h_corr, np.sqrt(h_corr_sigma**2+h_corr_sigma_s**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "a4f894df-e7b5-42e7-9469-523ab310517b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def datetime64_to_fractional_year(date):\n",
    "    \"\"\"\n",
    "    Convert a numpy.datetime64 object into a fractional year, rounded to 0, .25, .5, or .75.\n",
    "    \"\"\"\n",
    "    year = date.astype('datetime64[Y]').astype(int) + 1970\n",
    "    start_of_year = np.datetime64(f'{year}-01-01')\n",
    "    start_of_next_year = np.datetime64(f'{year + 1}-01-01')\n",
    "    year_length = (start_of_next_year - start_of_year).astype('timedelta64[D]').astype(int)\n",
    "    day_of_year = (date - start_of_year).astype('timedelta64[D]').astype(int)\n",
    "    fractional_year = year + day_of_year / year_length\n",
    "\n",
    "    # Round to nearest quarter\n",
    "    rounded_fractional_year = round(fractional_year * 4) / 4\n",
    "    return rounded_fractional_year\n",
    "\n",
    "# # Example usage\n",
    "# date = np.datetime64('2024-01-05')\n",
    "# fractional_year = datetime64_to_fractional_year(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "323680f2-75c7-4398-93c9-7c4ecbf76d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def muliple_area_buffer(polygon, area_multiple, precision=100):\n",
    "    \"\"\"\n",
    "    This function takes a polygon and returns a polygon with a buffer such that the area of the buffered polygon\n",
    "    is approximately the specified multiple the area of the original polygon.\n",
    "\n",
    "    :param polygon: Shapely Polygon object\n",
    "    :param area_multiple: the multiple of the original polygon area you wish the buffered polygon to be\n",
    "    :param precision: Precision for the iterative process to find the buffer distance\n",
    "    :return: Buffered Polygon\n",
    "    \"\"\"\n",
    "    original_area = polygon.area\n",
    "    target_area = area_multiple * original_area\n",
    "    buffer_distance = 0\n",
    "    buffered_polygon = polygon\n",
    "\n",
    "    while True:\n",
    "        buffered_polygon = polygon.buffer(buffer_distance)\n",
    "        if buffered_polygon.area >= target_area:\n",
    "            break\n",
    "        buffer_distance += precision\n",
    "    \n",
    "    # Convert to geodataframe\n",
    "    buffered_polygon_gdf = gpd.GeoDataFrame({'geometry': [buffered_polygon]})\n",
    "    \n",
    "    # Set CRS\n",
    "    buffered_polygon_gdf.geometry.crs = 'EPSG:3031'\n",
    "\n",
    "    return buffered_polygon_gdf\n",
    "\n",
    "# # Example usage\n",
    "# # Define a simple square polygon\n",
    "# square = Polygon([(0, 0), (1, 0), (1, 1), (0, 1)])\n",
    "\n",
    "# # Apply the function to find the buffered polygon area and bounds\n",
    "# buffered_poly = double_area_buffer(square)\n",
    "# buffered_poly.area, buffered_poly.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0c6ac957-3f76-4167-b794-ef6704a7d63b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_folder(directory):\n",
    "    # Check if the directory already exists\n",
    "    if not os.path.exists(directory):\n",
    "        # If it doesn't exist, create a new directory\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Folder '{directory}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Folder '{directory}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "333fd087-aed3-42ee-ba7a-1f7fc88c43a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_data_counts(lake_gdf, dataset1, dataset2): \n",
    "    '''\n",
    "    Create planview plots of counts going into gridded ice-surface height change (dh/dt) products\n",
    "    \n",
    "    Inputs:\n",
    "    * lake_gdf: geodataframe of lake to be analyzed\n",
    "    * dataset1: dataset1 to be analyzed\n",
    "    * dataset2: dataset2 to be analyzed in conjunction with dataset1; currently configured to \n",
    "    splice CryoSat-2 and ICESat-2 eras\n",
    "    \n",
    "    Outputs:\n",
    "    * Sequence of planview data count visuals of lake around surrounding area\n",
    "    '''\n",
    "    # Define lake name and polygon\n",
    "    lake_name = lake_gdf.name.values[0]\n",
    "    lake_poly = lake_gdf.iloc[0].geometry\n",
    "    \n",
    "    # # Create buffered polygons for various multiples of lake area to find which\n",
    "    # # best emcompasses the height change signals at previously identified lakes\n",
    "    buffered_poly_2x = muliple_area_buffer(lake_poly, 2)\n",
    "    # buffered_poly_3x = muliple_area_buffer(lake_poly, 3)\n",
    "    # buffered_poly_4x = muliple_area_buffer(lake_poly, 4)\n",
    "    # buffered_poly_5x = muliple_area_buffer(lake_poly, 5)\n",
    "    \n",
    "    # Establish x_min, x_max, y_min, y_max\n",
    "    x_min, y_min, x_max, y_max = buffered_poly_2x.iloc[0].geometry.bounds\n",
    "    x_buffer = abs(x_max-x_min)*0.5\n",
    "    y_buffer = abs(y_max-y_min)*0.5\n",
    "\n",
    "    # Subsetting datasets\n",
    "    mask_x = (dataset1.x >= x_min - x_buffer) & (dataset1.x <= x_max + x_buffer)\n",
    "    mask_y = (dataset1.y >= y_min - y_buffer) & (dataset1.y <= y_max + y_buffer)\n",
    "    dataset1_subset = dataset1.where(mask_x & mask_y, drop=True)\n",
    "    mask_x = (dataset2.x >= x_min - x_buffer) & (dataset2.x <= x_max + x_buffer)\n",
    "    mask_y = (dataset2.y >= y_min - y_buffer) & (dataset2.y <= y_max + y_buffer)\n",
    "    dataset2_subset = dataset2.where(mask_x & mask_y, drop=True)\n",
    "    \n",
    "    # Make output folders\n",
    "    create_folder(OUTPUT_DIR + '/{}'.format('plot_data_counts'))\n",
    "    create_folder(OUTPUT_DIR + '/plot_data_counts/{}'.format(lake_name))\n",
    "    print('Now creating/saving plots...')\n",
    "\n",
    "    # Create lines for legend\n",
    "    S09_color = 'paleturquoise'\n",
    "    SF18_color  = 'turquoise'\n",
    "    lake_locations_notSF18_color = 'darkturquoise'\n",
    "    Smith2009 = plt.Line2D((0, 1), (0, 0), color=S09_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "    SiegfriedFricker2018 = plt.Line2D((0, 1), (0, 0), color=SF18_color, linestyle=(0, (1, 2)), linewidth=2)\n",
    "    lakes_notSF18 = plt.Line2D((0, 1), (0, 0), color=lakes_notSF18_color, linestyle=(0, (1, 1)), linewidth=2)\n",
    "\n",
    "    # Calculate cycle-to-cycle dHeight at each cycle of the spliced data sets\n",
    "    for idx in range(len(midcyc_dates)):\n",
    "        # For midcyc_dates indexes <= 32, use CryoSat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the CryoSat-2 era before ICESat-2 launch (2010-08-17 to 2018-08-17)\n",
    "        if idx <= 32:\n",
    "            count_subset = dataset1_subset['data_count'][idx,:,:]\n",
    "            count_subset.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "        # For midcyc_dates indexes > 32, use ICESat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the ICESat-2 era (2018-11-16 to most recently available data)\n",
    "        elif idx > 32:\n",
    "            # Subtract 33 from idx to start over at index zero with new dataset\n",
    "            count_subset = dataset2_subset['data_count'][(idx-33),:,:]\n",
    "            count_subset.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "            \n",
    "        # Only plot arrays that have data in them; some time slices have no data because no CryoSat-2 SARIn coverage\n",
    "        # Returns True if there is at least one non-NaN value in data_stacked, and False if all values are NaN\n",
    "        if np.any(~np.isnan(count_subset)):\n",
    "            # Create fig, ax\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "            # Plot figure\n",
    "            img = ax.imshow(count_subset, extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer], \n",
    "                origin='lower', cmap='viridis')\n",
    "\n",
    "            # Plot buffered polygons\n",
    "            buffered_poly_2x.boundary.plot(ax=ax, edgecolor='k', facecolor='none', linewidth=0.5)\n",
    "            # buffered_poly_3x.boundary.plot(ax=ax, edgecolor='k', facecolor='none', linewidth=0.5)\n",
    "            # buffered_poly_4x.boundary.plot(ax=ax, edgecolor='k', facecolor='none', linewidth=0.5)\n",
    "            # buffered_poly_5x.boundary.plot(ax=ax, edgecolor='k', facecolor='none', linewidth=0.5)\n",
    "            \n",
    "            # Overlay published active lake outlines for visual comparison and grounding line\n",
    "            # S09_outlines.boundary.plot(ax=ax, facecolor=S09_color, linestyle=(0, (1, 10)), linewidth=2, alpha=0.25)\n",
    "            SF18_outlines_S09only.boundary.plot(ax=ax, edgecolor=S09_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "            # SF18_outlines_SF18only.boundary.plot(ax=ax, facecolor=SF18_color, linestyle=(0, (1, 3)), linewidth=2, alpha=0.25)\n",
    "            SF18_outlines_SF18only.boundary.plot(ax=ax, edgecolor=SF18_color, linestyle=(0, (1, 2)), linewidth=2)\n",
    "            # Sauthoff2023_S23outlines.boundary.plot(ax=ax, facecolor=S23_color, linestyle=(0, (1, 1)), linewidth=2, alpha=0.25)\n",
    "            lakes_notSF18.boundary.plot(ax=ax, edgecolor=lakes_notSF18_color, linestyle=(0, (1, 1)), linewidth=2)          \n",
    "            \n",
    "            # Change polar stereographic m to km\n",
    "            km_scale = 1e3\n",
    "            ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "            ax.xaxis.set_major_formatter(ticks_x)\n",
    "            ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "            ax.yaxis.set_major_formatter(ticks_y)  \n",
    "\n",
    "            # Label axes, set limits, and set title\n",
    "            ax.set_xlabel('x [km]', size=15)\n",
    "            ax.set_ylabel('y [km]', size=15) \n",
    "            ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "#             # Plot inset map\n",
    "#             axIns = ax.inset_axes([0.01, 0.01, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "#             axIns.set_aspect('equal')\n",
    "#             moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "#             moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "#             axIns.axis('off')\n",
    "\n",
    "#             # Plot red star to indicate location\n",
    "#             axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "#                 linewidth=1, edgecolor='k', facecolor='r', s=100, zorder=3)\n",
    "\n",
    "            # Add colorbar \n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "            fig.colorbar(img, cax=cax).set_label('data counts', size=15)\n",
    "\n",
    "            # Add legend\n",
    "            ax.legend([Smith2009, \n",
    "                       SiegfriedFricker2018, \n",
    "                       lakes_notSF18\n",
    "                      ],\n",
    "                ['static outline (S09)', \n",
    "                 'static outline (SF18)', \n",
    "                 'other'\n",
    "                ], \n",
    "                loc='upper center')\n",
    "            \n",
    "            # Set a title for the axes\n",
    "            ax.set_title('Data counts from from {} to {}'.format(cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)))\n",
    "            \n",
    "            # Save and close fig\n",
    "            plt.savefig(OUTPUT_DIR + \n",
    "                '/plot_data_counts/{}/plot_data_counts_{}_{}-{}.png'\n",
    "                .format(lake_name, lake_name, cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "    print('Complete')\n",
    "        \n",
    "# # Example usage\n",
    "# plot_data_counts(lake_gdf=lake_gdf, dataset1=CS2_Smith2017, dataset2=ATL15_dh): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "9970b6cb-b3b0-494c-b6c6-290679b5194d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_height_changes(lake_gdf, dataset1, dataset2): \n",
    "    '''\n",
    "    Create planview plots of ice surface height changes (dh/dt)\n",
    "    \n",
    "    Inputs:\n",
    "    * lake_gdf: geodataframe of lake to be analyzed\n",
    "    * dataset1: dataset1 to be analyzed\n",
    "    * dataset2: dataset2 to be analyzed in conjunction with dataset1; currently configured to \n",
    "    splice CryoSat-2 and ICESat-2 eras\n",
    "    \n",
    "    Outputs: \n",
    "    * Sequence of planview delta height visuals of lake around surrounding area\n",
    "    '''\n",
    "    # Define lake name and polygon\n",
    "    lake_name = lake_gdf.name.values[0]\n",
    "    lake_poly = lake_gdf.iloc[0].geometry\n",
    "    \n",
    "    # Create buffered polygons for various multiples of lake area to find which\n",
    "    # best emcompasses the height change signals at previously identified lakes\n",
    "    buffered_poly = muliple_area_buffer(lake_poly, 10)\n",
    "\n",
    "    # Establish x_min, x_max, y_min, y_max\n",
    "    x_min, y_min, x_max, y_max = buffered_poly.iloc[0].geometry.bounds\n",
    "    x_buffer = abs(x_max-x_min)*0.2\n",
    "    y_buffer = abs(y_max-y_min)*0.2\n",
    "    \n",
    "    # Subsetting datasets\n",
    "    mask_x = (dataset1.x >= x_min - x_buffer) & (dataset1.x <= x_max + x_buffer)\n",
    "    mask_y = (dataset1.y >= y_min - y_buffer) & (dataset1.y <= y_max + y_buffer)\n",
    "    dataset1_subset = dataset1.where(mask_x & mask_y, drop=True)\n",
    "    mask_x = (dataset2.x >= x_min - x_buffer) & (dataset2.x <= x_max + x_buffer)\n",
    "    mask_y = (dataset2.y >= y_min - y_buffer) & (dataset2.y <= y_max + y_buffer)\n",
    "    dataset2_subset = dataset2.where(mask_x & mask_y, drop=True)\n",
    "    \n",
    "    # Find magnitude of ice surface deformation in bounding box to create appropriate color map scale\n",
    "    # Create empty lists to store data\n",
    "    height_anom_pos = []\n",
    "    height_anom_neg = []\n",
    "    for idx in range(len(midcyc_dates)):\n",
    "        if idx <= 32:\n",
    "            dhdt_subset = dataset1_subset.delta_h[idx+1,:,:]-dataset1_subset.delta_h[idx,:,:]\n",
    "        elif idx > 32:\n",
    "            # Subtract 33 from idx to start over at index zero with new dataset\n",
    "            dhdt_subset = dataset2_subset.delta_h[(idx-33)+1,:,:]-dataset2_subset.delta_h[(idx-33),:,:]\n",
    "        if np.any(~np.isnan(dhdt_subset)):       \n",
    "            pos = np.nanmax(dhdt_subset)\n",
    "            neg = np.nanmin(dhdt_subset)\n",
    "            height_anom_pos += [pos]\n",
    "            height_anom_neg += [neg]\n",
    "    \n",
    "    # Store max pos/neg height anomalies from all cycles to create colorbar bounds later\n",
    "    max_height_anom_pos = max(height_anom_pos)\n",
    "    max_height_anom_neg = min(height_anom_neg)\n",
    "    \n",
    "    # Establish diverging colorbar\n",
    "    divnorm=colors.TwoSlopeNorm(vmin=max_height_anom_neg, vcenter=0., vmax=max_height_anom_pos)\n",
    "   \n",
    "    # Make output folders\n",
    "    create_folder(OUTPUT_DIR + '/{}'.format('plot_height_changes'))\n",
    "    create_folder(OUTPUT_DIR + '/plot_height_changes/{}'.format(lake_name))\n",
    "    print('Now creating/saving plots...')\n",
    "\n",
    "    # Create lines for legend\n",
    "    S09_color = 'paleturquoise'\n",
    "    SF18_color  = 'turquoise'\n",
    "    lake_locations_notSF18_color = 'darkturquoise'\n",
    "    S09_line = plt.Line2D((0, 1), (0, 0), color=S09_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "    SF18_line = plt.Line2D((0, 1), (0, 0), color=SF18_color, linestyle=(0, (1, 2)), linewidth=2)\n",
    "    lake_locations_notSF18_line = plt.Line2D((0, 1), (0, 0), color=lakes_notSF18_color, linestyle=(0, (1, 1)), linewidth=2)\n",
    "\n",
    "    # Calculate cycle-to-cycle dHeight at each cycle of the spliced data sets\n",
    "    for idx in range(len(midcyc_dates)):\n",
    "        # For midcyc_dates indexes <= 32, use CryoSat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the CryoSat-2 era before ICESat-2 launch (2010-08-17 to 2018-08-17)\n",
    "        if idx <= 32:\n",
    "            dhdt_subset = dataset1_subset.delta_h[idx+1,:,:]-dataset1_subset.delta_h[idx,:,:]\n",
    "            # dhdt_subset.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "        # For midcyc_dates indexes > 32, use ICESat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the ICESat-2 era (2018-11-16 to most recently available data)\n",
    "        elif idx > 32:\n",
    "            # Subtract 33 from idx to reset idx to zero for new dataset\n",
    "            dhdt_subset = dataset2_subset.delta_h[(idx-33)+1,:,:]-dataset2_subset.delta_h[(idx-33),:,:]\n",
    "            # dhdt_subset.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "        # Only plot arrays that have data in them; some time slices have no data because no CryoSat-2 SARIn coverage\n",
    "        # Returns True if there is at least one non-NaN value in data_stacked, and False if all values are NaN\n",
    "        if np.any(~np.isnan(dhdt_subset)):\n",
    "            # Create fig, ax\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "            # Plot figure\n",
    "            img = ax.imshow(dhdt_subset, extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer],\n",
    "                origin='lower', cmap='coolwarm_r', \n",
    "                norm=divnorm)\n",
    "\n",
    "            # Plot buffered polygons\n",
    "            buffered_poly.boundary.plot(ax=ax, edgecolor='k', facecolor='none', linewidth=0.5)\n",
    "            \n",
    "            # Overlay published active lake outlines for visual comparison and grounding line\n",
    "            SF18_outlines_S09only.boundary.plot(ax=ax, edgecolor=S09_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "            SF18_outlines_SF18only.boundary.plot(ax=ax, edgecolor=SF18_color, linestyle=(0, (1, 2)), linewidth=2)\n",
    "            lake_locations_notSF18.boundary.plot(ax=ax, edgecolor=lake_locations_notSF18_color, linestyle=(0, (1, 1)), linewidth=2)          \n",
    "            \n",
    "            # Change polar stereographic m to km\n",
    "            km_scale = 1e3\n",
    "            ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "            ax.xaxis.set_major_formatter(ticks_x)\n",
    "            ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "            ax.yaxis.set_major_formatter(ticks_y)  \n",
    "\n",
    "            # Label axes, set limits, and set title\n",
    "            ax.set_xlabel('x [km]', size=15)\n",
    "            ax.set_ylabel('y [km]', size=15) \n",
    "            ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "#             # Plot inset map\n",
    "#             axIns = ax.inset_axes([0.01, 0.01, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "#             axIns.set_aspect('equal')\n",
    "#             moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "#             moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "#             axIns.axis('off')\n",
    "\n",
    "#             # Plot red star to indicate location\n",
    "#             axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "#                 linewidth=1, edgecolor='k', facecolor='r', s=100, zorder=3)\n",
    "\n",
    "            # Add colorbar \n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "            fig.colorbar(img, cax=cax).set_label('height change (dh) [m]', size=15)\n",
    "\n",
    "            # Add legend\n",
    "            ax.legend([S09_line, \n",
    "                       SF18_line, \n",
    "                       lake_locations_notSF18_line\n",
    "                      ],\n",
    "                ['static outline (S09)', \n",
    "                 'static outline (SF18)', \n",
    "                 'other'\n",
    "                ], \n",
    "                loc='upper center')\n",
    "            \n",
    "            # Set a title for the axes\n",
    "            ax.set_title('Height change from from {} to {}'.format(cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)))\n",
    "            \n",
    "            # Save and close fig\n",
    "            plt.savefig(OUTPUT_DIR + \n",
    "                '/plot_height_changes/{}/plot_height_changes+{}_{}-{}.png'\n",
    "                .format(lake_name, lake_name, cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "    print('Complete')\n",
    "        \n",
    "# # Example usage\n",
    "# plot_height_changes(lake_gdf=lake_gdf, dataset1=CS2_Smith2017, dataset2=ATL15_dh): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "1ee88d99-c7c7-454f-8e5c-1fd7d8e021fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_evolving_outlines(lake_gdf, area_multiple_search_extent, level, dataset1, dataset2, plot=False): \n",
    "    '''\n",
    "    Create planview dh/dt plots of ice surface height changes \n",
    "    Create time-variable outlines using skimage contour to plot evolving outlines as polygons.\n",
    "    \n",
    "    Inputs:\n",
    "    * ROI_name: str of the region of interest for using in file name saving\n",
    "    * ROI_poly: list of polygon(s) that define evolving outline search extent\n",
    "    * level: vertical distance in meters to delineate ice surface deformation contour\n",
    "    * dataset1: dataset1 to be analyzed\n",
    "    * dataset2: dataset2 to be analyzed in conjunction with dataset1; currently configured to \n",
    "    splice CryoSat-2 and ICESat-2 eras\n",
    "    \n",
    "    Outputs: \n",
    "    * Sequence of planview delta height visuals of ICESat-2 ATL15 with variable ice surface \n",
    "    deformation contours plotted to delineate evolving lake boundaries.\n",
    "    * geopandas geodataframe of polygons created at each step (would need to modify to collect all polygons \n",
    "    at all time steps)\n",
    "    '''\n",
    "    # Define lake name and polygon\n",
    "    lake_name = lake_gdf.name.values[0]\n",
    "    lake_poly = lake_gdf.iloc[0].geometry\n",
    "    buffered_gdf = muliple_area_buffer(lake_poly, area_multiple_search_extent+1)\n",
    "    \n",
    "    # Clipping datasets\n",
    "    if dataset1 != 'none':\n",
    "        dataset1_clipped = dataset1.rio.clip(buffered_gdf.geometry, dataset1.rio.crs)\n",
    "    dataset2_clipped = dataset2.rio.clip(buffered_gdf.geometry, dataset2.rio.crs)\n",
    "    \n",
    "    # Establish x_min, x_max, y_min, y_max\n",
    "    x_min, y_min, x_max, y_max = buffered_gdf.iloc[0].geometry.bounds\n",
    "    x_buffer = abs(x_max-x_min)*0.2\n",
    "    y_buffer = abs(y_max-y_min)*0.2\n",
    "   \n",
    "    # Only proceed with plotting if plot=True\n",
    "    if plot:\n",
    "        # Find magnitude of ice surface deformation in bounding box to create appropriate color map scale\n",
    "        # Create empty lists to store data\n",
    "        height_anom_pos = []\n",
    "        height_anom_neg = []\n",
    "        for idx in range(len(midcyc_dates)):\n",
    "            if idx <= 32:\n",
    "                dhdt_subset = dataset1_subset.delta_h[idx+1,:,:]-dataset1_subset.delta_h[idx,:,:]\n",
    "            elif idx > 32:\n",
    "                # Subtract 33 from idx to start over at index zero with new dataset\n",
    "                dhdt_subset = dataset2_subset.delta_h[(idx-33)+1,:,:]-dataset2_subset.delta_h[(idx-33),:,:]\n",
    "            if np.any(~np.isnan(dhdt_subset)):       \n",
    "                pos = np.nanmax(dhdt_subset)\n",
    "                neg = np.nanmin(dhdt_subset)\n",
    "                height_anom_pos += [pos]\n",
    "                height_anom_neg += [neg]\n",
    "\n",
    "        # Store max pos/neg height anomalies from all cycles to create colorbar bounds later\n",
    "        max_height_anom_pos = max(height_anom_pos)\n",
    "        max_height_anom_neg = min(height_anom_neg)\n",
    "\n",
    "        # Establish diverging colorbar\n",
    "        divnorm=colors.TwoSlopeNorm(vmin=max_height_anom_neg, vcenter=0., vmax=max_height_anom_pos)\n",
    "\n",
    "        \n",
    "        # Create lines for legend\n",
    "        S09_color = 'cyan'\n",
    "        SF18_color  = 'darkcyan'\n",
    "        lakes_notSF18_color = 'deepskyblue'\n",
    "        S09_line = plt.Line2D((0, 1), (0, 0), color=S09_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "        SF18_line = plt.Line2D((0, 1), (0, 0), color=SF18_color, linestyle=(0, (1, 2)), linewidth=2)\n",
    "        lake_locations_notSF18_line = plt.Line2D((0, 1), (0, 0), color=lakes_notSF18_color, linestyle=(0, (1, 1)), linewidth=2)\n",
    "        uplift = plt.Line2D((0, 1), (0, 0), color='mediumblue', linestyle=(0, (3, 5, 1, 5)), linewidth=2)\n",
    "        subsidence = plt.Line2D((0, 1), (0, 0), color='maroon', linestyle=(0, (3, 5, 1, 5, 1, 5)), linewidth=2)\n",
    "        search_extent = plt.Line2D((0, 1), (0, 0), color='red', linestyle='solid', linewidth=2)\n",
    "    \n",
    "    # Create empty list to store polygons, areas, dh's, dvol's and dates\n",
    "    polys = []\n",
    "    areas = []\n",
    "    dhs = []\n",
    "    dvols =[]\n",
    "    midcyc_datetimes = []\n",
    "\n",
    "    # Calculate cycle-to-cycle dHeight at each cycle of the spliced data sets\n",
    "    for idx in range(len(midcyc_dates)-1):  # Less one because cycle-to-cycle differencing reduces length by one\n",
    "        # For midcyc_dates indexes <= 32, use CryoSat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the CryoSat-2 era before ICESat-2 launch (2010-08-17 to 2018-08-17)\n",
    "        if idx <= 32:\n",
    "            if dataset1 != 'none':\n",
    "                dhdt_clipped = dataset1_clipped.delta_h[idx+1,:,:]-dataset1_clipped.delta_h[idx,:,:]\n",
    "                dhdt_clipped.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "            else:\n",
    "                continue\n",
    "        # For midcyc_dates indexes > 32, use ICESat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the ICESat-2 era (2018-11-16 to most recently available data)\n",
    "        elif idx > 32:\n",
    "            # Subtract 33 from idx to start over at index zero with new dataset\n",
    "            dhdt_clipped = dataset2_clipped.delta_h[(idx-33)+1,:,:]-dataset2_clipped.delta_h[(idx-33),:,:]\n",
    "            dhdt_clipped.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "        # Create mapping conversion factor to map array location to polar stereographic x,y\n",
    "        x_conv = (x_max-x_min)/dhdt_clipped.shape[1]\n",
    "        y_conv = (y_max-y_min)/dhdt_clipped.shape[0]\n",
    "\n",
    "        # Only plot arrays that have data in them; some time slices have no data because no CryoSat-2 SARIn coverage\n",
    "        # Returns True if there is at least one non-NaN value in data_stacked, and False if all values are NaN\n",
    "        if np.any(~np.isnan(dhdt_clipped)):\n",
    "            if plot:\n",
    "                # Create fig, ax\n",
    "                fig, ax = plt.subplots()\n",
    "\n",
    "                # Plot figure\n",
    "                img = ax.imshow(dhdt_clipped, extent=[x_min, x_max, y_min, y_max], \n",
    "                    origin='lower', cmap='coolwarm_r', \n",
    "                    norm=divnorm)\n",
    "\n",
    "                # Plot the boundary of the evolving outline search extent\n",
    "                gpd.GeoSeries(buffered_gdf.iloc[0].geometry).boundary.plot(ax=ax, color='red')\n",
    "\n",
    "            # Create empty lists to store contours \n",
    "            contours_pos = []\n",
    "            contours_neg = []\n",
    "\n",
    "            # Create contours at the positive and negative threshold \n",
    "            contour_pos = measure.find_contours(dhdt_clipped.values, level)\n",
    "            # If at least one contour, add to list of contours\n",
    "            if len(contour_pos) > 0: \n",
    "                contours_pos += [contour_pos]\n",
    "            contour_neg = measure.find_contours(dhdt_clipped.values, -level)\n",
    "            if len(contour_neg) > 0: \n",
    "                contours_neg += [contour_neg]\n",
    "\n",
    "            # Plot contours and make into polygons\n",
    "            for i in range(len(contours_pos)): \n",
    "                for j in range(len(contours_pos[i])):\n",
    "                    x = x_min+contours_pos[i][j][:,1]*x_conv\n",
    "                    y = y_min+contours_pos[i][j][:,0]*y_conv\n",
    "                    if plot:\n",
    "                        ax.plot(x, y, color='mediumblue', linestyle='dashdot', linewidth=1, label=level)\n",
    "\n",
    "                    # Make polygons from evolving outlines and store to list\n",
    "                    if len(contours_pos[i][j][:,1]) > 2: \n",
    "                        poly = Polygon(list(zip(x, y)))\n",
    "                        try:\n",
    "                            dhdt_poly = dhdt_clipped.rio.clip([poly])\n",
    "                            lon, lat = XY_TO_LL.transform(x,y)\n",
    "                            poly_area = abs(GEOD.polygon_area_perimeter(lon, lat)[0])\n",
    "                            if np.any(~np.isnan(dhdt_poly)):\n",
    "                                poly_dh = np.nanmean(dhdt_poly)\n",
    "                                poly_dvol = poly_dh*poly_area\n",
    "                                polys += [poly]\n",
    "                                areas += [poly_area]\n",
    "                                dhs += [poly_dh]\n",
    "                                dvols += [poly_dvol]\n",
    "                                midcyc_datetimes += [midcyc_dates[idx]]\n",
    "                        except NoDataInBounds:\n",
    "                            continue  # Skip to the next iteration\n",
    "                        except Exception as e:\n",
    "                            # Handle any other exceptions\n",
    "                            raise\n",
    "\n",
    "            for i in range(len(contours_neg)): \n",
    "                for j in range(len(contours_neg[i])):\n",
    "                    x = x_min+contours_neg[i][j][:,1]*x_conv\n",
    "                    y = y_min+contours_neg[i][j][:,0]*y_conv\n",
    "                    if plot:\n",
    "                        ax.plot(x, y, color='maroon', linestyle=(0, (3, 1, 1, 1)), linewidth=1, label=-level)\n",
    "\n",
    "                    if len(contours_neg[i][j][:,1]) > 2: \n",
    "                        poly = Polygon(list(zip(x, y)))\n",
    "                        try:\n",
    "                            dhdt_poly = dhdt_clipped.rio.clip([poly])\n",
    "                            lon, lat = XY_TO_LL.transform(x,y)\n",
    "                            poly_area = abs(GEOD.polygon_area_perimeter(lon, lat)[0])\n",
    "                            if np.any(~np.isnan(dhdt_poly)):\n",
    "                                poly_dh = np.nanmean(dhdt_poly)\n",
    "                                poly_dvol = poly_dh*poly_area\n",
    "                                polys += [poly]\n",
    "                                areas += [poly_area]\n",
    "                                dhs += [poly_dh]\n",
    "                                dvols += [poly_dvol]\n",
    "                                midcyc_datetimes += [midcyc_dates[idx]]\n",
    "                        except NoDataInBounds:\n",
    "                            continue  # Skip to the next iteration\n",
    "                        except Exception as e:\n",
    "                            # Handle any other exceptions\n",
    "                            raise\n",
    "\n",
    "            if plot:\n",
    "                # Overlay published active lake outlines for visual comparison and grounding line\n",
    "                S09_color = 'paleturquoise'\n",
    "                SF18_color  = 'turquoise'\n",
    "                lake_locations_notSF18_color = 'darkturquoise'\n",
    "                S09_outlines.boundary.plot(ax=ax, edgecolor=S09_color, linestyle=(0, (1, 10)), linewidth=2)\n",
    "                SF18_outlines_SF18only.boundary.plot(ax=ax, edgecolor=SF18_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "                lake_locations_notSF18.boundary.plot(ax=ax, edgecolor=lake_locations_notSF18_color, linestyle=(0, (1, 1)), linewidth=2)\n",
    "\n",
    "                # Change polar stereographic m to km\n",
    "                km_scale = 1e3\n",
    "                ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "                ax.xaxis.set_major_formatter(ticks_x)\n",
    "                ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "                ax.yaxis.set_major_formatter(ticks_y)  \n",
    "\n",
    "                # Label axes, set limits, and set title\n",
    "                ax.set_xlabel('x [km]', size=15)\n",
    "                ax.set_ylabel('y [km]', size=15) \n",
    "                ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "                # Plot inset map\n",
    "                axIns = ax.inset_axes([-0.01, 0.01, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "                axIns.set_aspect('equal')\n",
    "                moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "                moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "                axIns.axis('off')\n",
    "\n",
    "                # # Plot black rectangle to indicate location\n",
    "                # rect = Rectangle((x_min, y_min), (x_max-x_min), (y_max-y_min), fill=False, linewidth=2, color='k', zorder=3)\n",
    "                # axIns.add_artist(rect)\n",
    "\n",
    "                # Plot red star to indicate location\n",
    "                axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "                    linewidth=1, edgecolor='k', facecolor='r', s=100, zorder=3)\n",
    "\n",
    "                # Add colorbar \n",
    "                divider = make_axes_locatable(ax)\n",
    "                cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "                fig.colorbar(img, cax=cax).set_label('height change (dh) [m]', size=15)\n",
    "\n",
    "                # Add legend\n",
    "                ax.legend([S09_line, \n",
    "                           SF18_line, \n",
    "                           lake_locations_notSF18_line,\n",
    "                           uplift, \n",
    "                           subsidence,\n",
    "                           search_extent],\n",
    "                    ['static outline (S09)', \n",
    "                     'static outline (SF18)', \n",
    "                     'other',\n",
    "                     ('+ '+str(level)+' m uplift evolving outline'), \n",
    "                     ('â€“ '+str(level)+' m subsidence evolving outline'),\n",
    "                     'search extent'], \n",
    "                    loc='upper left')\n",
    "\n",
    "                # Set a title for the axes\n",
    "                ax.set_title('Evolving outlines and height change \\nfrom from {} to {}'.format(cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)))\n",
    "\n",
    "                # Save and close fig\n",
    "                plt.savefig(OUTPUT_DIR + \n",
    "                    '/find_evolving_outlines/{}/find_evolving_outlines_{}_{}x-search-extent_{}m-level_{}-{}.png'\n",
    "                    .format(lake_name, lake_name, area_multiple_search_extent, level, cyc_start_dates[idx].astype('datetime64[D]').astype(str), cyc_end_dates[idx].astype('datetime64[D]').astype(str)), dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "    \n",
    "    # Store polygons in geopandas geodataframe for further analysis\n",
    "    area_multiple_search_extents = [area_multiple_search_extent for _ in range(len(polys))]\n",
    "    levels = [level for _ in range(len(polys))]\n",
    "    d = {'area_multiple_search_extent': area_multiple_search_extents,\n",
    "         'level': levels,\n",
    "         'geometry': polys, \n",
    "         'area (m^2)': areas, \n",
    "         'dh (m)': dhs, \n",
    "         'vol (m^3)': dvols,\n",
    "         'midcyc_datetime': midcyc_datetimes}\n",
    "    gdf = gpd.GeoDataFrame(d, crs=\"EPSG:3031\")\n",
    "\n",
    "    return gdf\n",
    "\n",
    "# # Example usage\n",
    "# outlines_gdf = find_evolving_outlines(lake_gdf=lake_gdf, area_multiple_search_extent=2, level=0.1, dataset1=dataset1, dataset2=dataset2, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "f798a765-1842-451d-aab0-c63c94a5f9ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_intersections(gdf):\n",
    "    num_polygons = len(gdf)\n",
    "    \n",
    "    for i in range(num_polygons):\n",
    "        for j in range(i+1, num_polygons):\n",
    "            if gdf.iloc[i].geometry.intersects(gdf.iloc[j].geometry):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "ff943efc-e148-4c68-b7d4-61a4542cffbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_intersecting_polygons(outlines_gdf, polygon):\n",
    "    \"\"\"\n",
    "    Extract polygons from a GeoDataFrame that intersect with a specified polygon,\n",
    "    as well as polygons that intersect with those intersected polygons.\n",
    "\n",
    "    Parameters:\n",
    "    outlines_gdf (geopandas.GeoDataFrame): A GeoDataFrame containing polygon geometries (here evolving outlines).\n",
    "    polygon (shapely.Polygon): A shapely polygon\n",
    "\n",
    "    Returns:\n",
    "    geopandas.GeoDataFrame: A GeoDataFrame containing the polygons that either\n",
    "                            intersect directly with the input polygon or intersect with\n",
    "                            polygons that intersect with the initial polygon.\n",
    "    \"\"\"\n",
    "    # Step 0: Deal with invalid geometries\n",
    "    # Raise the input polygon if it's invalid\n",
    "    if not polygon.is_valid:\n",
    "        raise ValueError(\"Input polygon is invalid and will be excluded.\")\n",
    "\n",
    "    # Exclude invalid geometries in the GeoDataFrame before processing\n",
    "    valid_gdf = outlines_gdf[outlines_gdf['geometry'].is_valid]\n",
    "    \n",
    "    # Step 1: Find polygons that intersect with the given polygon\n",
    "    directly_intersecting = valid_gdf[valid_gdf.intersects(polygon)]\n",
    "\n",
    "    # Step 2: Find polygons that intersect with the polygons found in Step 1\n",
    "    # We use 'unary_union' to merge all polygons found in Step 1 into a single geometry\n",
    "    # Then we select polygons from the original GeoDataFrame that intersect with this merged geometry\n",
    "    indirectly_intersecting = valid_gdf[valid_gdf.intersects(directly_intersecting.unary_union)]\n",
    "\n",
    "    # Step 3: Find polygons that intersect with the polygons found in Step 1\n",
    "    # We use 'unary_union' to merge all polygons found in Step 1 into a single geometry\n",
    "    # Then we select polygons from the original GeoDataFrame that intersect with this merged geometry\n",
    "    second_indirectly_intersecting = valid_gdf[valid_gdf.intersects(indirectly_intersecting.unary_union)]\n",
    "\n",
    "    # Combine both directly and indirectly intersecting polygons to ensure no duplicates\n",
    "    result_gdf = gpd.GeoDataFrame(pd.concat([directly_intersecting, indirectly_intersecting, second_indirectly_intersecting], ignore_index=True).drop_duplicates(subset='geometry'))\n",
    "\n",
    "    return result_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "289e6700-cfc7-49c9-9643-925794b574d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_evolving_outlines(lake_gdf, outlines_gdf):\n",
    "    '''\n",
    "    Func to plot evolving outlines in aggregate\n",
    "    '''\n",
    "    # Create fig, ax\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    \n",
    "    # Define lake name and polygon\n",
    "    lake_name = lake_gdf.name.values[0]\n",
    "    lake_poly = lake_gdf.iloc[0].geometry\n",
    "    \n",
    "    # Create buffered polygons for various multiples of lake area to find which\n",
    "    # best emcompasses the height change signals at previously identified lakes\n",
    "    search_extent_buffered_lake_poly = muliple_area_buffer(lake_poly, int(outlines_gdf['area_multiple_search_extent'][0]))\n",
    "    \n",
    "    # Plot buffered polygons\n",
    "    search_extent_buffered_lake_poly.boundary.plot(ax=ax, edgecolor='red', facecolor='none', linewidth=1)\n",
    "\n",
    "    # Set colormap and normalize to date values\n",
    "    cmap = plt.get_cmap('plasma', len(midcyc_dates)-1)\n",
    "    norm = plt.Normalize(datetime64_to_fractional_year(midcyc_dates[0]), datetime64_to_fractional_year(midcyc_dates[-1]))\n",
    "    m = plt.cm.ScalarMappable(cmap=cmap)\n",
    "    m.set_array(np.array([datetime64_to_fractional_year(date) for date in midcyc_dates[0:-1]]))\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"bottom\", size=\"2.5%\", pad=0.5)\n",
    "    fig.colorbar(m, cax=cax, orientation='horizontal').set_label('evolving outline year', size=10)\n",
    "\n",
    "    # Overlay published active lake outlines for visual comparison and grounding line\n",
    "    S09_color = 'paleturquoise'\n",
    "    SF18_color  = 'turquoise'\n",
    "    lake_locations_notSF18_color = 'darkturquoise'\n",
    "    S09_outlines.boundary.plot(ax=ax, facecolor=S09_color, linestyle=(0, (1, 3)), linewidth=2, alpha=0.25)\n",
    "    S09_outlines.boundary.plot(ax=ax, edgecolor=S09_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "    SF18_outlines_SF18only.boundary.plot(ax=ax, facecolor=SF18_color, linestyle=(0, (1, 2)), linewidth=2, alpha=0.25)\n",
    "    SF18_outlines_SF18only.boundary.plot(ax=ax, edgecolor=SF18_color, linestyle=(0, (1, 2)), linewidth=2)\n",
    "    lake_locations_notSF18.boundary.plot(ax=ax, facecolor=lake_locations_notSF18_color, linestyle=(0, (1, 1)), linewidth=2, alpha=0.25)\n",
    "    lake_locations_notSF18.boundary.plot(ax=ax, edgecolor=lake_locations_notSF18_color, linestyle=(0, (1, 1)), linewidth=2)\n",
    "\n",
    "    # Use for loop to plot each outline in the geopandas dataframe and color by date\n",
    "    lines = []  # list of lines to be used for the legend\n",
    "    for idx, dt in enumerate(midcyc_dates):\n",
    "        x = 1; y = 1\n",
    "        line, = ax.plot(x, y, color=cmap(norm(datetime64_to_fractional_year(midcyc_dates[idx]))), linewidth=2)\n",
    "        lines.append(line)\n",
    "        # Filter GeoDataFrame for the current date\n",
    "        current_outlines = outlines_gdf[outlines_gdf['midcyc_datetime'] == dt]\n",
    "        # Check if the filtered GeoSeries is empty\n",
    "        if not current_outlines.empty:\n",
    "            current_outlines.boundary.plot(ax=ax, \n",
    "                color=cmap(norm(datetime64_to_fractional_year(midcyc_dates[idx]))), linewidth=1)\n",
    "\n",
    "    # Change polar stereographic m to km for cleaner-looking axes labels\n",
    "    km_scale = 1e3\n",
    "    ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax.xaxis.set_major_formatter(ticks_x)\n",
    "    ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    ax.yaxis.set_major_formatter(ticks_y)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Label axes and set limits\n",
    "    ax.set_xlabel('x [km]', size=10)\n",
    "    ax.set_ylabel('y [km]', size=10)\n",
    "    # Extract x_min, y_min, x_max, y_max from the total_bounds attribute\n",
    "    x_min, y_min, x_max, y_max = search_extent_buffered_lake_poly.total_bounds\n",
    "    x_buffer = abs(x_max-x_min)*0.2\n",
    "    y_buffer = abs(y_max-y_min)*0.2\n",
    "    ax.set(xlim=(x_min-x_buffer, x_max+x_buffer), ylim=(y_min-y_buffer, y_max+y_buffer))\n",
    "\n",
    "    # Create lines for legend\n",
    "    S09_color = 'paleturquoise'\n",
    "    SF18_color  = 'turquoise'\n",
    "    lake_locations_notSF18_color = 'darkturquoise'\n",
    "    S09_line = plt.Line2D((0, 1), (0, 0), color=S09_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "    SF18_line = plt.Line2D((0, 1), (0, 0), color=SF18_color, linestyle=(0, (1, 2)), linewidth=2)\n",
    "    lake_locations_notSF18_line = plt.Line2D((0, 1), (0, 0), color=lakes_notSF18_color, linestyle=(0, (1, 1)), linewidth=2)\n",
    "    search_extent = plt.Line2D((0, 1), (0, 0), color='red', linestyle='solid', linewidth=2)\n",
    "    \n",
    "    # Create legend\n",
    "    ax.legend(handles=[S09_line, \n",
    "        SF18_line,\n",
    "        lake_locations_notSF18_line,\n",
    "        tuple(lines),\n",
    "        search_extent], \n",
    "        labels=['static outline (Smith and others, 2009)',\n",
    "            'static outline (Siegfried & Fricker, 2018)', \n",
    "            'other',\n",
    "            'evolving outline (this study)',\n",
    "            'search extent'], \n",
    "              handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "             loc='upper left', bbox_to_anchor=(0, 1.35))\n",
    "\n",
    "    # Plot inset map to show location \n",
    "    axIns = ax.inset_axes([0.02, 0.01, 0.25, 0.25]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "    axIns.set_aspect('equal')\n",
    "    moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "    axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', linewidth=1, edgecolor='k', facecolor='r', s=50, zorder=3)\n",
    "    # rect = Rectangle((x_min, y_min), (x_max-x_min), (y_max-y_min), fill=False, linewidth=2, color='k', zorder=3)\n",
    "    # axIns.add_artist(rect) \n",
    "    axIns.axis('off')\n",
    "\n",
    "    plt.savefig(OUTPUT_DIR + '/plot_evolving_outlines/plot_evolving_outlines_{}_{}x-search-extent_{}m-level.png'\n",
    "        .format(lake_name, outlines_gdf['area_multiple_search_extent'][0], outlines_gdf['level'][0]),\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "09a64b8c-09ac-424f-a6ed-225e5ff60791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def remove_outlier_polygon(gdf, axis='x', max_or_min='max'):\n",
    "#     '''\n",
    "#     Func to remove outline outliers by removing a polygon\n",
    "#     with the most extreme x or y coordinates\n",
    "    \n",
    "#     Inputs\n",
    "#     * geopandas geodataframe\n",
    "#     * axis indicates whether the x or y axis will \n",
    "#     have its extreme polygon removed\n",
    "#     * max_or_min indicates whether the max or min centroid axis\n",
    "#     value will be removed\n",
    "    \n",
    "#     Ouputs\n",
    "#     * geopandas geodataframe with one outline removed\n",
    "#     '''\n",
    "    \n",
    "#     # Check if the GeoDataFrame is empty\n",
    "#     if gdf.empty:\n",
    "#         print(\"GeoDataFrame is empty. Nothing to remove.\")\n",
    "#         return gdf\n",
    "    \n",
    "#     # Choose the axis and max or min for extreme value\n",
    "#     if axis not in ['x', 'y']:\n",
    "#         raise ValueError(\"Invalid axis. Use 'x' or 'y'.\")\n",
    "#     if max_or_min not in ['max', 'min']:\n",
    "#         raise ValueError(\"Invalid max_or_min. Use 'max' or 'min'.\")\n",
    "\n",
    "#     # Calculate the centroid and extreme value\n",
    "#     if axis == 'x':\n",
    "#         if max_or_min == 'max':\n",
    "#             idx_to_remove = gdf.geometry.bounds['maxx'].idxmax()\n",
    "#         else:\n",
    "#             idx_to_remove = gdf.geometry.bounds['minx'].idxmin()\n",
    "#     else:\n",
    "#         if max_or_min == 'max':\n",
    "#             idx_to_remove = gdf.geometry.bounds['maxy'].idxmax()\n",
    "#         else:\n",
    "#             idx_to_remove = gdf.geometry.bounds['miny'].idxmin()\n",
    "\n",
    "#     # Remove the polygon with the specified polygon index to remove\n",
    "#     gdf_filtered = gdf.drop(idx_to_remove)\n",
    "\n",
    "#     print(f\"Removed polygon with extreme {max_or_min} {axis}-value at index {idx_to_remove}.\")\n",
    "\n",
    "#     return gdf_filtered\n",
    "\n",
    "# # Example usage:\n",
    "# # Assuming 'gdf' is your GeoDataFrame\n",
    "# # new_gdf = remove_extreme_polygon(gdf, axis='x', positive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "3fe4e5ef-5e6c-446f-89f3-1e8d765ce5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_evolving_static_outlines(lake_gdf, outlines_gdf, dataset1, dataset2): \n",
    "    '''\n",
    "    Create geodataframe of static outline geometric variables\n",
    "    '''\n",
    "    # Define lake name and polygon and buffered polygon to use as off-lake region\n",
    "    lake_name = lake_gdf.name.values[0]\n",
    "    static_outline = lake_gdf.iloc[0].geometry\n",
    "    static_outline_buffered = muliple_area_buffer(static_outline, 2)\n",
    "    evolving_outlines_unary_union = outlines_gdf.unary_union\n",
    "    evolving_outlines_unary_union_buffered = muliple_area_buffer(evolving_outlines_unary_union, 2)\n",
    "\n",
    "    # Establish x_min, x_max, y_min, y_max\n",
    "    x_min, y_min, x_max, y_max = lake_poly.bounds\n",
    "    x_buffer = abs(x_max-x_min)*0.2\n",
    "    y_buffer = abs(y_max-y_min)*0.2\n",
    "\n",
    "    # Clipping datasets\n",
    "    if dataset1 != 'none':\n",
    "        dataset1_static_outline_clipped = dataset1.rio.clip([static_outline], dataset1.rio.crs)\n",
    "        dataset1_static_region_clipped = dataset1.rio.clip([static_outline_buffered.geometry[0]], dataset1.rio.crs)\n",
    "        dataset1_static_region_clipped = dataset1.rio.clip([static_outline], dataset1.rio.crs, invert=True)\n",
    "        dataset1_evolving_region_clipped = dataset1.rio.clip([lake_poly_buffered.geometry[0]], dataset1.rio.crs)\n",
    "        dataset1_evolving_region_clipped = dataset1.rio.clip([lake_poly], dataset1.rio.crs, invert=True)\n",
    "    dataset2_static_outline_clipped = dataset1.rio.clip([static_outline], dataset1.rio.crs)\n",
    "    dataset2_static_region_clipped = dataset1.rio.clip([static_outline_buffered.geometry[0]], dataset1.rio.crs)\n",
    "    dataset2_static_region_clipped = dataset1.rio.clip([static_outline], dataset1.rio.crs, invert=True)\n",
    "    dataset2_evolving_region_clipped = dataset1.rio.clip([lake_poly_buffered.geometry[0]], dataset1.rio.crs)\n",
    "    dataset2_evolving_region_clipped = dataset1.rio.clip([lake_poly], dataset1.rio.crs, invert=True)\n",
    "\n",
    "    # Create empty list to store polygons, areas, perimeters and dates\n",
    "    static_outline_dhs = []\n",
    "    static_region_dhs = []\n",
    "    static_outline_dhs_corr = []\n",
    "    static_outline_dvols_corr = []\n",
    "    evolving_outlines_dhs = []\n",
    "    evolving_regions_dhs = []\n",
    "    evolving_outlines_dhs_corr = []\n",
    "    evolving_outlines_dvols_corr = []\n",
    "    midcyc_datetimes = []\n",
    "\n",
    "    # Calculate cycle-to-cycle dHeight at each cycle of the spliced data sets\n",
    "    # for idx in range(len(midcyc_dates)-1):  \n",
    "    for idx, dt in enumerate(cyc_dates['midcyc_dates'], 0):\n",
    "\n",
    "        # For midcyc_dates indexes <= 32, use CryoSat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the CryoSat-2 era before ICESat-2 launch (2010-08-17 to 2018-08-17)\n",
    "        if idx <= 32:\n",
    "            if dataset1 != 'none':\n",
    "                dh_static_outline_clipped = dataset1_static_outline_clipped.delta_h[idx+1,:,:]-dataset1_static_outline_clipped.delta_h[idx,:,:]\n",
    "                dh_static_region_clipped = dataset1_static_region_clipped.delta_h[idx+1,:,:]-dataset1_static_region_clipped.delta_h[idx,:,:]\n",
    "                dh_evolving_region_clipped = dataset1_evolving_region_clipped.delta_h[idx+1,:,:]-dataset1_evolving_region_clipped.delta_h[idx,:,:]\n",
    "                # dhdt_clipped.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "                # Filter rows that match the current time slice\n",
    "                outlines_gdf_dt_sub = outlines_gdf[outlines_gdf['midcyc_datetime'] == dt]\n",
    "                dataset1_evolving_outlines_clipped = dataset1.rio.clip(outlines_gdf_dt_sub.geometry, dataset1.rio.crs)\n",
    "                dh_evolving_outline_clipped = dataset1_evolving_outlines_clipped.delta_h[idx+1,:,:]-dataset1_evolving_outlines_clipped.delta_h[idx,:,:]\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "        # For midcyc_dates indexes > 32, use ICESat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the ICESat-2 ATL06 v003 (2018-11-16 to 2023-04-02)\n",
    "        elif idx > 32 & idx <=49:  # Less one because cycle-to-cycle differencing reduces length by one\n",
    "            # Subtract 33 from idx to start over at index zero with new dataset\n",
    "            dh_static_outline_clipped = dataset2_outline_clipped.delta_h[(idx-33)+1,:,:]-dataset2_outline_clipped.delta_h[(idx-33),:,:]\n",
    "            dh_static_region_clipped = dataset2_region_clipped.delta_h[(idx-33)+1,:,:]-dataset2_region_clipped.delta_h[(idx-33),:,:]\n",
    "            dh_evolving_region_clipped = dataset2_evolving_region_clipped.delta_h[idx+1,:,:]-dataset2_evolving_region_clipped.delta_h[idx,:,:]\n",
    "            # dhdt_clipped.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "            # Filter rows that match the current time slice\n",
    "            outlines_gdf_dt_sub = outlines_gdf[outlines_gdf['midcyc_datetime'] == dt]\n",
    "            dataset1_evolving_outlines_clipped = dataset2.rio.clip(outlines_gdf_dt_sub.geometry, dataset2.rio.crs)\n",
    "            dh_evolving_outline_clipped = dataset2_evolving_outlines_clipped.delta_h[idx+1,:,:]-dataset2_evolving_outlines_clipped.delta_h[idx,:,:]\n",
    "        \n",
    "        \n",
    "        # This if statement will skip time slices when delta_h_outline_clipped is all nan's, such as lakes without CryoSat-2 SARIn coverage\n",
    "        if np.any(~np.isnan(delta_h_outline_clipped)):\n",
    "            # Store static outline dh and dvol            \n",
    "            static_outline_dh = np.nanmean(dh_static_outline_clipped)\n",
    "            static_region_dh = np.nanmean(dh_static_region_clipped)\n",
    "            static_outline_dh_corr = static_outline_dh - static_region_dh\n",
    "            static_outline_dvol = static_outline_dh_corr*lake_poly.area\n",
    "            static_outline_dhs += [static_outline_dh]\n",
    "            static_region_dhs += [static_region_dh]\n",
    "            static_outline_dhs_corr += [static_outline_dh_corr]\n",
    "            static_outline_dvols += [static_outline_dvol]\n",
    "            evolving_outlines_dh = np.nanmean(dh_evolving_outlines_clipped)\n",
    "            evolving_regions_dh = np.nanmean(dh_evolving_regions_clipped)\n",
    "            evolving_outlines_dh_corr = evolving_outline_dh - evolving_region_dh\n",
    "            evolving_outlines_dvol = evolving_outline_dh_corr*sum(outlines_gdf_dt_sub.geometry.area)\n",
    "            evolving_outlines_dhs += [evolving_outlines_dh]\n",
    "            evolving_regions_dhs += [evolving_regions_dh]\n",
    "            evolving_outlines_dhs_corr += [evolving_outlines_dh_corr]\n",
    "            evolving_outlines_dvols += [evolving_outlines_dvol]\n",
    "            midcyc_datetimes += [midcyc_dates[idx]]\n",
    "\n",
    "    # Store polygons in geopandas geodataframe for further analysis\n",
    "    d = {'static_outline_dh (m)': static_outline_dhs, \n",
    "         'static_region_dh (m)': static_region_dhs,\n",
    "         'static_outline_dh_corr (m)': static_outline_dhs_corr,\n",
    "         'static_outline_dvol (m^3)': static_outline_dvols,\n",
    "         'evolving_outlines_dh (m)': evolving_outlines_dhs,\n",
    "         'evolving_regions_dh (m)': evolving_regions_dhs,\n",
    "         'evolving_outlines_dvol (m^3)': evolving_outlines_dvols,\n",
    "         'evolving_regions_dvol (m^3)': evolving_regions_dvols,\n",
    "         'midcyc_datetime': midcyc_datetimes}\n",
    "    df = pd.DataFrame(d)\n",
    "\n",
    "    # Export evolving outlines GeoDataFrame to GeoJSON\n",
    "    df.to_csv('outlines/compare_evolving_static_outlines/{}.csv'.format(lake_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5e0e1c-b26e-4ee3-87eb-ceea82bc5d73",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "48d1542f-fb79-4965-a13f-3c84a3fa69fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smith and others, 2009, J. Glac., doi:10.3189/002214309789470879\n",
      "97\n",
      "Fricker & Scambos, 2009, J. Glac., doi:10.3189/002214309788608813\n",
      "7\n",
      "McMillan and others, 2013, GRL, doi:10.1002/grl.50689\n",
      "1\n",
      "Kim and others, 2016, TC, doi:10.5194/tc-10-2971-2016\n",
      "3\n",
      "Carter and others, 2013, J. Glac., doi:10.3189/2013JoG13J085\n",
      "1\n",
      "Fricker and others, 2010, J. Glac., doi:10.3189/002214310791968557\n",
      "8\n",
      "Fricker and others, 2014, J. Glac., doi:10.3189/2014JoG14J063\n",
      "9\n",
      "Siegfried & Fricker, 2018, Ann. Glac., doi:10.1017/aog.2017.36\n",
      "1\n",
      "Smith and others, 2017, TC, doi:10.5194/tc-11-451-2017\n",
      "4\n",
      "Gray et al. (2005); Smith et al. (2009)\n",
      "2\n",
      "Wingham et al. (2006); Surveyed by ICECAP in 2008/09 (Wright et al., 2012)\n",
      "1\n",
      "Wingham et al. (2006)\n",
      "4\n",
      "Fricker et al. (2007), Blankenship et al. (2009), Siegfried et al., (2014), Siegfried & Fricker (2018)\n",
      "1\n",
      "Fricker et al. (2007); Siegfried et al. (2014); Siegfried & Fricker (2018)\n",
      "1\n",
      "Lake Engelhardt (Fricker et al., 2007); Carter et al., (2013); Siegfried & Fricker (2018)\n",
      "1\n",
      "Fricker et al. (2007); Siegfried & Fricker (2018)\n",
      "2\n",
      "Lake Whillans, (Fricker et al., 2007); Geophysical observations made at low stand (Christianson et al., 2012 and Horgan et al. 2012); Carter et al., (2013); Siegfried et al. (2014); Siegfried & Fricker (2018).\n",
      "1\n",
      "Lake Conway (Fricker et al., 2007); Carter et al., (2013); Siegfried et al. (2014); Siegfried & Fricker (2018)\n",
      "1\n",
      "Stearns et al., (2008)\n",
      "2\n",
      "Smith et al. (2009)\n",
      "114\n",
      "Smith et al. (2009); (Welch et al., 2009)\n",
      "1\n",
      "Smith et al. (2009); McMillan et al., (2013); Flament et al. (2014); Li et al., (2020)\n",
      "1\n",
      "Smith et al. (2009); Siegfried & Fricker (2018)\n",
      "9\n",
      "Smith et al. (2009). Kim et al. (2016); Siegfried & Fricker (2018)\n",
      "1\n",
      "Smith et al. (2009). Kim et al. (2016)\n",
      "3\n",
      "Smith et al. (2009); Kim et al. (2016) \n",
      "1\n",
      "Smith et al. (2009), Mac1 in Fricker et al. (2010); Siegfried & Fricker (2018)\n",
      "1\n",
      "Smith et al. (2009), Mac2 in Fricker et al. (2010); Siegfried & Fricker (2018)\n",
      "1\n",
      "Smith et al. (2009); Wright et al., (2012); Siegfried & Fricker (2018)\n",
      "1\n",
      "Smith et al. (2009); Carter et al., (2013); Siegfried & Fricker (2018)\n",
      "3\n",
      "Fricker et al. (2010); Siegfried & Fricker (2018).\n",
      "1\n",
      "Fricker et al. (2010), part of MacAyeal3 in Smith et al. (2009)\n",
      "2\n",
      "N. Young (personal comm.)\n",
      "3\n",
      "Kim et al. (2016); Siegfried & Fricker (2018)\n",
      "3\n",
      "Smith et al. (2017); Hoffman et al. (2020); Malczyk et al. (2020)\n",
      "2\n",
      "Smith et al. (2017); Hoffman et al. (2020); Malczyk et al. (2020). \n",
      "1\n",
      "Smith et al. (2017) and Hoffman et al. (2020). \n",
      "2\n",
      "Hoffman et al. (2020).\n",
      "4\n",
      "                S09                       SF18\n",
      "0    Bindschadler_1             Bindschadler_1\n",
      "1    Bindschadler_1                       Mac7\n",
      "2    Bindschadler_2             Bindschadler_2\n",
      "3    Bindschadler_2                       Mac8\n",
      "4    Bindschadler_3             Bindschadler_3\n",
      "5    Bindschadler_4             Bindschadler_4\n",
      "6    Bindschadler_5             Bindschadler_5\n",
      "7    Bindschadler_6             Bindschadler_6\n",
      "8            Byrd_1                     Byrd_1\n",
      "9            Byrd_2                     Byrd_2\n",
      "10         Byrd_s10                   Byrd_s10\n",
      "11         Byrd_s11                   Byrd_s11\n",
      "12         Byrd_s12                   Byrd_s12\n",
      "13         Byrd_s13                   Byrd_s13\n",
      "14         Byrd_s14                   Byrd_s14\n",
      "15         Byrd_s15                   Byrd_s15\n",
      "16          Byrd_s1                    Byrd_s1\n",
      "17          Byrd_s2                    Byrd_s2\n",
      "18          Byrd_s3                    Byrd_s3\n",
      "19          Byrd_s4                    Byrd_s4\n",
      "20          Byrd_s5                    Byrd_s5\n",
      "21          Byrd_s6                    Byrd_s6\n",
      "22          Byrd_s7                    Byrd_s7\n",
      "23          Byrd_s8                    Byrd_s8\n",
      "24          Byrd_s9                    Byrd_s9\n",
      "25          David_1                    David_1\n",
      "26         David_s1                   David_s1\n",
      "27         David_s2                   David_s2\n",
      "28         David_s3                   David_s3\n",
      "29         David_s4                   David_s4\n",
      "30         David_s5                   David_s5\n",
      "31            EAP_1                      EAP_1\n",
      "32            EAP_2                      EAP_2\n",
      "33            EAP_3                      EAP_3\n",
      "34            EAP_4                      EAP_4\n",
      "35            EAP_5                      EAP_5\n",
      "36            EAP_6                      EAP_6\n",
      "37            EAP_7                      EAP_7\n",
      "38            EAP_8                      EAP_8\n",
      "39            EAP_9                      EAP_9\n",
      "40    Foundation_10              Foundation_10\n",
      "41    Foundation_11              Foundation_11\n",
      "42    Foundation_12              Foundation_12\n",
      "43    Foundation_13              Foundation_13\n",
      "44    Foundation_14              Foundation_14\n",
      "45    Foundation_15              Foundation_15\n",
      "46    Foundation_16              Foundation_16\n",
      "47     Foundation_1               Foundation_1\n",
      "48     Foundation_2               Foundation_2\n",
      "49     Foundation_3               Foundation_3\n",
      "50     Foundation_4               Foundation_4\n",
      "51     Foundation_5               Foundation_5\n",
      "52     Foundation_6               Foundation_6\n",
      "53     Foundation_7               Foundation_7\n",
      "54     Foundation_8               Foundation_8\n",
      "55     Foundation_9               Foundation_9\n",
      "56    Foundation_N1              Foundation_N1\n",
      "57    Foundation_N2              Foundation_N2\n",
      "58    Foundation_N3              Foundation_N3\n",
      "59     InstituteE_1               Institute_E1\n",
      "60     InstituteE_2               Institute_E2\n",
      "61     InstituteW_1               Institute_W1\n",
      "62     InstituteW_2               Institute_W2\n",
      "63      KambTrunk_1                        KT1\n",
      "64          Kamb_10                    Kamb_10\n",
      "65          Kamb_11                    Kamb_11\n",
      "66          Kamb_12                    Kamb_12\n",
      "67           Kamb_1                     Kamb_1\n",
      "68           Kamb_2                     Kamb_2\n",
      "69           Kamb_3                     Kamb_3\n",
      "70           Kamb_4                     Kamb_4\n",
      "71           Kamb_5                     Kamb_5\n",
      "72           Kamb_6                     Kamb_6\n",
      "73           Kamb_7                     Kamb_7\n",
      "74           Kamb_8                     Kamb_8\n",
      "75           Kamb_9                     Kamb_9\n",
      "76        Lambert_1                  Lambert_1\n",
      "77     LennoxKing_1               LennoxKing_1\n",
      "78       Macayeal_1                       Mac1\n",
      "79       Macayeal_2                       Mac2\n",
      "80       Macayeal_3                       Mac4\n",
      "81       Macayeal_3                       Mac5\n",
      "82       Macayeal_4                       Mac6\n",
      "83         Mercer_1                     Lake78\n",
      "84         Mercer_2       MercerSubglacialLake\n",
      "85          Mertz_1                    Mertz_1\n",
      "86         Mulock_1                   Mulock_1\n",
      "87         Nimrod_1                   Nimrod_1\n",
      "88         Nimrod_2                   Nimrod_2\n",
      "89         Ninnis_1                   Ninnis_1\n",
      "90         Ninnis_2                   Ninnis_2\n",
      "91        Raymond_1                  Raymond_1\n",
      "92      Recovery_10                       Rec9\n",
      "93      Recovery_11                      Rec10\n",
      "94       Recovery_1                       Rec1\n",
      "95       Recovery_2                       Rec1\n",
      "96       Recovery_3                       Rec2\n",
      "97       Recovery_4                       Rec3\n",
      "98       Recovery_5                       Rec4\n",
      "99       Recovery_6                       Rec5\n",
      "100      Recovery_7                       Rec6\n",
      "101      Recovery_9                       Rec8\n",
      "102       Rutford_1                  Rutford_1\n",
      "103       Slessor_1                  Slessor_1\n",
      "104       Slessor_2                 Slessor_23\n",
      "105       Slessor_3                 Slessor_23\n",
      "106       Slessor_4                  Slessor_4\n",
      "107       Slessor_5                  Slessor_5\n",
      "108       Slessor_6                  Slessor_6\n",
      "109       Slessor_7                  Slessor_7\n",
      "110        Totten_1                   Totten_1\n",
      "111        Totten_2                   Totten_2\n",
      "112         Cook_E2                    Cook_E2\n",
      "113         Cook_E1                    Cook_E1\n",
      "114        Vostok_1                   Vostok_1\n",
      "115      Whillans_1   EngelhardtSubglacialLake\n",
      "116     Whillans_2a                     Lake12\n",
      "117     Whillans_2b                     Lake10\n",
      "118      Whillans_3     WhillansSubglacialLake\n",
      "119      Whillans_4       ConwaySubglacialLake\n",
      "120      Whillans_5  UpperSubglacialLakeConway\n",
      "121      Whillans_6                 Whillans_6\n",
      "122      Whillans_7                 Whillans_7\n",
      "123      Whillans_8                 Whillans_8\n",
      "124        Wilkes_1                   Wilkes_1\n",
      "125        Wilkes_2                   Wilkes_2\n",
      "                S09                       SF18\n",
      "1    Bindschadler_1                       Mac7\n",
      "3    Bindschadler_2                       Mac8\n",
      "59     InstituteE_1               Institute_E1\n",
      "60     InstituteE_2               Institute_E2\n",
      "61     InstituteW_1               Institute_W1\n",
      "62     InstituteW_2               Institute_W2\n",
      "63      KambTrunk_1                        KT1\n",
      "78       Macayeal_1                       Mac1\n",
      "79       Macayeal_2                       Mac2\n",
      "80       Macayeal_3                       Mac4\n",
      "81       Macayeal_3                       Mac5\n",
      "82       Macayeal_4                       Mac6\n",
      "83         Mercer_1                     Lake78\n",
      "84         Mercer_2       MercerSubglacialLake\n",
      "92      Recovery_10                       Rec9\n",
      "93      Recovery_11                      Rec10\n",
      "94       Recovery_1                       Rec1\n",
      "95       Recovery_2                       Rec1\n",
      "96       Recovery_3                       Rec2\n",
      "97       Recovery_4                       Rec3\n",
      "98       Recovery_5                       Rec4\n",
      "99       Recovery_6                       Rec5\n",
      "100      Recovery_7                       Rec6\n",
      "101      Recovery_9                       Rec8\n",
      "104       Slessor_2                 Slessor_23\n",
      "105       Slessor_3                 Slessor_23\n",
      "115      Whillans_1   EngelhardtSubglacialLake\n",
      "116     Whillans_2a                     Lake12\n",
      "117     Whillans_2b                     Lake10\n",
      "118      Whillans_3     WhillansSubglacialLake\n",
      "119      Whillans_4       ConwaySubglacialLake\n",
      "120      Whillans_5  UpperSubglacialLakeConway\n"
     ]
    }
   ],
   "source": [
    "# Import subglacial lake outlines \n",
    "# Combine paths\n",
    "# file_path = os.path.join(os.getcwd(), '0_lake_locations.py')\n",
    "exec(open('0_lake_locations.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c97b1e-bbd7-4f54-a26d-21d2aa3c9c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODIS MOA 2014 coastline and grounding line\n",
    "# https://nsidc.org/data/nsidc-0730/versions/1\n",
    "shp = DATA_DIR + '/boundaries/MODIS_MOA/2014/moa2014_coastline_v01.shp' \n",
    "moa_2014_coastline = gpd.read_file(shp)\n",
    "shp = DATA_DIR + '/boundaries/MODIS_MOA/2014/moa2014_grounding_line_v01.shp' \n",
    "moa_2014_groundingline = gpd.read_file(shp, crs=3031)\n",
    "# moa_2014_groundingline['geometry'] = moa_2014_groundingline.buffer(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d43f854-e9c7-4251-a30d-522a248383b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Smith and others, 2017, TC method CryoSat-2 SARIn height and dheight data (closed source aquired from Ben Smith)\n",
    "CS2_Smith2017 = xr.open_dataset(DATA_DIR + '/altimetry/CryoSat2/CS2_SARIn_Smith2017method/mos_2010.5_2019.0_relative_to_ATL14.nc')\n",
    "CS2_Smith2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3248a5a-3a1a-4855-a3a7-97e2050e1fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CS2_Smith2017_count = CS2_Smith2017.where(CS2_Smith2017['count'] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f6efce-7341-400e-a660-2e42bbb24d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with ATL11 read in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b17078-d831-4b2d-a16d-214e5c79b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log into NASA Earthdata to search for datasets\n",
    "earthaccess.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f33130e-0262-4ba9-a696-7f9bda24bf11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Find ICESat-2 ATL11 r003 data granules\n",
    "# results = earthaccess.search_data(\n",
    "#     doi='10.5067/ATLAS/ATL11.006',\n",
    "#     # short_name='ATL15',\n",
    "#     # version='003',\n",
    "#     bounding_box=(180, -90, -180, -60),  # (lower_left_lon, lower_left_lat , upper_right_lon, upper_right_lat))\n",
    "#     cloud_hosted=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc8217a-507e-4cbe-9e62-207236db2639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Open data granules as s3 files to stream\n",
    "# files = earthaccess.open(results)\n",
    "# # files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e74d8-78d9-44f9-8fe1-651cae14789a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fae3032-84d5-4123-a67c-4daf2cf4643d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import datatree as dt\n",
    "\n",
    "# # Open the HDF5 file\n",
    "# with h5py.File(files[0], 'r') as h5file:\n",
    "#     # Load the entire HDF5 file into a DataTree\n",
    "#     data_tree = dt.DataTree.from_hdf5(h5file)\n",
    "\n",
    "# # Now `data_tree` is a DataTree object containing the structure and data of the HDF5 file\n",
    "# # You can navigate and manipulate this tree structure as needed\n",
    "\n",
    "# # For example, to print the contents of the DataTree\n",
    "# print(data_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb7921-a63e-48c2-8215-20ceda095180",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # create empty lists\n",
    "# lon=[]\n",
    "# lat=[]\n",
    "# h_corr=[]\n",
    "# sigma_h=[]\n",
    "\n",
    "# # fill lists\n",
    "# for file in files:\n",
    "#     print(file)\n",
    "#     try:\n",
    "#         for pair in ['pt1','pt2','pt3']:\n",
    "#             lons, lats, hh, ss=read_ATL11(file, pair)\n",
    "#             lon += [lons]\n",
    "#             lat += [lats]\n",
    "#             h_corr += [hh]\n",
    "#             sigma_h += [ss]\n",
    "#     except Exception as E:\n",
    "#         pass\n",
    "\n",
    "# # concatenate lists\n",
    "# lon=np.concatenate(lon)\n",
    "# lat=np.concatenate(lat)\n",
    "# h_corr=np.concatenate(h_corr, axis=0)\n",
    "# sigma_h=np.concatenate(sigma_h, axis=0)\n",
    "# x,y=ll2ps(lon,lat) # transform geodetic lon, lat to polar stereographic x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dd3fa5-b94f-4831-82e5-b4f889ee0b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5dd4c7-cb4d-4e98-a6e5-2396278c2647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find ICESat-2 ATL15 r003 data granules\n",
    "results = earthaccess.search_data(\n",
    "    doi='10.5067/ATLAS/ATL15.003',\n",
    "    # short_name='ATL15',\n",
    "    # version='003',\n",
    "    bounding_box=(180, -90, -180, -60),  # (lower_left_lon, lower_left_lat , upper_right_lon, upper_right_lat))\n",
    "    cloud_hosted=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6223853-0e77-4e12-8ba1-a5dcde961801",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open data granules as s3 files to stream\n",
    "files = earthaccess.open(results)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8acc8a-b616-446b-bd14-41e6c26c46f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# After viewing files, index the files you wish to open\n",
    "print(files[15])\n",
    "print(files[3])\n",
    "print(files[9])\n",
    "print(files[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e8bdda-d8c0-415b-87b2-fae741baa41d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open each file, which are quadrants in polar stereographic coordinations around the Geographic South Pole\n",
    "ATL15_A1 = xr.open_dataset(files[15], group='delta_h')\n",
    "ATL15_A2 = xr.open_dataset(files[3], group='delta_h')\n",
    "ATL15_A3 = xr.open_dataset(files[9], group='delta_h')\n",
    "ATL15_A4 = xr.open_dataset(files[11], group='delta_h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da23c068-f494-45bc-814e-e76e22b4729b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Open locally stored files when NSIDC cloud access isn't working\n",
    "# ATL15_A1 = xr.open_dataset(DATA_DIR + '/altimetry/ICESat2/ATL15.003-Ant/ATL15_A1_0318_01km_003_01.nc', group='delta_h')\n",
    "# ATL15_A2 = xr.open_dataset(DATA_DIR + '/altimetry/ICESat2/ATL15.003-Ant/ATL15_A2_0318_01km_003_01.nc', group='delta_h')\n",
    "# ATL15_A3 = xr.open_dataset(DATA_DIR + '/altimetry/ICESat2/ATL15.003-Ant/ATL15_A3_0318_01km_003_01.nc', group='delta_h')\n",
    "# ATL15_A4 = xr.open_dataset(DATA_DIR + '/altimetry/ICESat2/ATL15.003-Ant/ATL15_A4_0318_01km_003_01.nc', group='delta_h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c4b4f9-6096-40de-92bb-12a1d9688890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the variables to keep\n",
    "variables_to_keep = ['time', 'y', 'x', 'delta_h', 'data_count']\n",
    "\n",
    "# List of xarray datasets\n",
    "datasets = [ATL15_A1, ATL15_A2, ATL15_A3, ATL15_A4]\n",
    "\n",
    "# Function to drop variables not in variables_to_keep from a dataset\n",
    "def drop_unwanted_variables(dataset):\n",
    "    variables_to_drop = [var for var in dataset.variables if var not in variables_to_keep]\n",
    "    return dataset.drop_vars(variables_to_drop)\n",
    "\n",
    "# Apply the function to each dataset\n",
    "ATL15_A1, ATL15_A2, ATL15_A3, ATL15_A4 = [drop_unwanted_variables(ds) for ds in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f875fb41-ec6f-4e03-a659-e7e7273b98df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use xarray concatenation to stitch two quadrants togethers\n",
    "# Use xarray index selecting to occlude the duplicated x=0 vector of data\n",
    "ATL15_A12 = xr.concat([ATL15_A2.isel(x=slice(0,-1)), ATL15_A1], dim=\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372eec1e-3eb5-42d8-94ee-a2de74316803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use xarray concatenation to stitch two quadrants togethers\n",
    "# Use xarray index selecting to occlude the duplicated x=0 vector of data\n",
    "ATL15_A34 = xr.concat([ATL15_A3.isel(x=slice(0,-1)), ATL15_A4], dim='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d14e491-fd5e-424b-ae91-75ba59381ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use xarray concatenation to stitch two quadrants togethers\n",
    "# Use xarray index selecting to occlude the duplicated x=0 vector of data\n",
    "ATL15_dh = xr.concat([ATL15_A34.isel(y=slice(0,-1)), ATL15_A12], dim='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d62934f-4e13-404d-b2b3-4338f498cfd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete variables to reduce memory consumption\n",
    "del ATL15_A1, ATL15_A12, ATL15_A2, ATL15_A3, ATL15_A34, ATL15_A4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b88e5-6957-4f63-b9d9-70591d18af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add datasets attributes\n",
    "ATL15_dh.attrs['identifier_product_DOI'] = '10.5067/ATLAS/ATL15.003'\n",
    "ATL15_dh.attrs['shortName'] = 'ATL15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d571618-924b-410a-a202-29c72aced243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ATL15_dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0bae96-66b2-4c2e-8c49-5623c3fde835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scripps Grounding Line\n",
    "# https://doi.pangaea.de/10.1594/PANGAEA.819147\n",
    "Scripps_gl = gpd.read_file(DATA_DIR + \n",
    "    '/boundaries/Depoorter2013/Antarctica_masks/scripps_antarctica_polygons_v1.shp')\n",
    "\n",
    "# Isolate only land ice\n",
    "Scripps_landice = Scripps_gl[Scripps_gl['Id_text'] == 'Grounded ice or land']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ba0a3-7cc4-430c-ab7e-3604365c59cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import MODIS Mosaic of Antarctica surface imagery\n",
    "# https://nsidc.org/data/nsidc-0730/versions/1\n",
    "# Relocate to data_dir\n",
    "# Open into an xarray.DataArray\n",
    "# moa_lowres = DATA_DIR + '/surface_imagery/MODIS_MOA/2014/moa750_2014_hp1_v01.tif' \n",
    "# moa_lowres_da = rioxarray.open_rasterio(moa_lowres)\n",
    "\n",
    "moa_highres = DATA_DIR + '/surface_imagery/MODIS_MOA/2014/moa125_2014_hp1_v01.tif' \n",
    "moa_highres_da = rioxarray.open_rasterio(moa_highres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a112e9f7-d8bc-4010-b547-3b63a05a4948",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df19fcb6-5ea0-4d70-946d-53e960a9bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip altimetry datasets to grounding line to limit analysis to only below grounded ice\n",
    "CS2_Smith2017.rio.write_crs(3031, inplace=True)\n",
    "CS2_Smith2017 = CS2_Smith2017.rio.clip(Scripps_landice.geometry.values, Scripps_landice.crs, drop=False)\n",
    "ATL15_dh.rio.write_crs(3031, inplace=True)\n",
    "ATL15_dh = ATL15_dh.rio.clip(Scripps_landice.geometry.values, Scripps_landice.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05b1c92-b737-4461-8341-5b7dfbd39349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset1=CS2_Smith2017\n",
    "dataset2=ATL15_dh\n",
    "\n",
    "# Create empty lists to store data\n",
    "cyc_start_dates = []\n",
    "cyc_end_dates = []\n",
    "midcyc_dates = []\n",
    "\n",
    "for idx in range(len(dataset1.delta_h[:33])):\n",
    "    # Smith and others, 2017 method CryoSat-2 SARIn data\n",
    "    if dataset1.identifier_product_DOI == 'doi:10.5194/tc-11-451-2017':\n",
    "        cyc_start_date = dataset1.time.values[idx]\n",
    "        cyc_end_date = dataset1.time.values[idx+1]\n",
    "        midcyc_days = cyc_end_date - cyc_start_date\n",
    "        midcyc_date = cyc_start_date + midcyc_days/2\n",
    "        cyc_start_dates += [cyc_start_date]\n",
    "        cyc_end_dates += [cyc_end_date]\n",
    "        midcyc_dates += [midcyc_date]\n",
    "    # Cryo-TEMPO-EOLIS Swath Thematic Gridded Product \n",
    "    elif dataset1.Title == 'Land Ice Elevation Thematic Gridded Product':\n",
    "        date_time_str = '70-01-01'\n",
    "        date_time_obj = datetime.datetime.strptime(date_time_str, '%y-%m-%d')\n",
    "        cyc_start_date = date_time_obj + datetime.timedelta(seconds = ds_sub.time.values[idx].astype(float))\n",
    "        cyc_end_date = date_time_obj + datetime.timedelta(seconds = ds_sub.time.values[idx+1].astype(float))\n",
    "        midcyc_days = cyc_end_date - cyc_start_date\n",
    "        midcyc_date = cyc_start_date + midcyc_days/2\n",
    "        cyc_start_dates += [cyc_start_date]\n",
    "        cyc_end_dates += [cyc_end_date]\n",
    "        midcyc_dates += [midcyc_date]\n",
    "for idx in range(len(dataset2.delta_h)-1):\n",
    "    # ICESat-2 ATL15 r003\n",
    "    if dataset2.identifier_product_DOI == '10.5067/ATLAS/ATL15.003':    \n",
    "        cyc_start_date = dataset2.time.values[idx]\n",
    "        cyc_end_date = dataset2.time.values[idx+1]\n",
    "        midcyc_days = cyc_end_date - cyc_start_date\n",
    "        midcyc_date = cyc_start_date + midcyc_days/2\n",
    "        cyc_start_dates += [cyc_start_date]\n",
    "        cyc_end_dates += [cyc_end_date]\n",
    "        midcyc_dates += [midcyc_date]\n",
    "\n",
    "# Concatenate list into pandas dataframe\n",
    "cyc_dates = pd.DataFrame({'cyc_start_dates': cyc_start_dates, 'midcyc_dates': midcyc_dates, 'cyc_end_dates': cyc_end_dates})\n",
    "\n",
    "# Store cycle dates list as csv for use in other notebooks\n",
    "cyc_dates.to_csv('output/cycle_dates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f638644a-e8ec-4dcb-97cf-726c97744575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Ensure dates are correct\n",
    "# for idx in range(len(midcyc_dates)):\n",
    "#     if idx <= 32:\n",
    "#         print('idx:', idx, 'CS2 era')\n",
    "#         print('cyc_start_date:', CS2_Smith2017.time[idx].values)\n",
    "#         print('mid_cyc_date:', midcyc_dates[idx])\n",
    "#         print('cyc_end_date:', CS2_Smith2017.time[idx+1].values)\n",
    "#     elif idx > 32:\n",
    "#         print('idx:', idx, 'IS2 era')\n",
    "#         # Subtract 33 (32th idx because 0th idx) from idx to start over with new dataset\n",
    "#         print('cyc_start_date:', ATL15_dh.time[idx-33].values)\n",
    "#         print('mid_start_date:', midcyc_dates[idx])\n",
    "#         print('cyc_end_date:', ATL15_dh.time[(idx+1)-33].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596962e3-e1e3-47c9-b19e-62d368182982",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Analysis of previously identified lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429e1ca6-74fb-4945-b353-206a57285013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in range(0, len(lake_locations)):\n",
    "    # Isolate lake from Siegfried & Fricker, 2018 inventory as geodataframe using slicing\n",
    "    lake_gdf = lake_locations.iloc[idx:idx+1]\n",
    "    print('Working on', lake_gdf['name'].values[0])\n",
    "    \n",
    "    # Plot data counts\n",
    "    plot_data_counts(lake_gdf=lake_gdf, dataset1=CS2_Smith2017, dataset2=ATL15_dh)\n",
    "\n",
    "    # Plot height change with various area multiple polygons to decide limit of evolving outlines search\n",
    "    plot_height_changes(lake_gdf=lake_gdf, dataset1=CS2_Smith2017, dataset2=ATL15_dh)\n",
    "    \n",
    "    # Clear the output of each index\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ce093-8427-4dd8-8cb3-c39ba331cdb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa449926-35fa-4d65-b5e6-d2c0bd664bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default lake analysis used an area multiple of three to generate evolving outlines\n",
    "# and an area multiple of two was used as the limit of evolving outline search when \n",
    "# finding the percentage of evolving outlines that intersected with the limit of investigation\n",
    "# A larger area multiple was used to generate the evolving outlines such that some percentage\n",
    "# would intersect with the limit of investigation to determine the adjustments to the level used to\n",
    "# generate the outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d72385-b2d2-4e13-bcbf-fcb8407f6d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# idx = 0\n",
    "# initial_level = 0.1  # Starting level\n",
    "# area_multiple_search_extents = range(2, 10)  # From 2 to 10 inclusive\n",
    "# dataset1=CS2_Smith2017\n",
    "# # dataset1='none'\n",
    "# dataset2=ATL15_dh\n",
    "\n",
    "# # Select lake\n",
    "# lake_gdf = SiegfriedFricker2018_outlines.iloc[idx:idx+1]\n",
    "# lake_name = lake_gdf.name.values[0]\n",
    "# print('Working on', lake_name)\n",
    "\n",
    "# # Initialize DataFrame to store results\n",
    "# results_df = pd.DataFrame(columns=['area_multiple_search_extent', 'Level'])\n",
    "\n",
    "# # Make output folders\n",
    "# create_folder(OUTPUT_DIR + '/{}'.format('find_evolving_outlines'))\n",
    "# create_folder(OUTPUT_DIR + '/find_evolving_outlines/{}'.format(lake_name))\n",
    "\n",
    "# for area_multiple_search_extent in area_multiple_search_extents:\n",
    "#     level = initial_level  # Reset level for each new area_multiple_search_extent\n",
    "#     within_fraction = 0.0  # Reset within_fraction for each new area_multiple_search_extent\n",
    "    \n",
    "#     while within_fraction < 0.90 and level <= 2.0:\n",
    "#         # Find evolving outlines\n",
    "#         outlines_gdf = find_evolving_outlines(lake_gdf=lake_gdf, \n",
    "#             area_multiple_search_extent=area_multiple_search_extent, level=level, \n",
    "#             dataset1=dataset1, dataset2=dataset2)\n",
    "        \n",
    "#         # Define lake polygon and buffered geodataframe as before\n",
    "#         lake_poly = lake_gdf.iloc[0].geometry\n",
    "#         buffered_lake_gdf = muliple_area_buffer(polygon=lake_poly, \n",
    "#             area_multiple=area_multiple_search_extent)\n",
    "        \n",
    "#         # Check which evolving outlines are within or overlap\n",
    "#         within = gpd.sjoin(outlines_gdf, buffered_lake_gdf, predicate='within')\n",
    "#         overlaps = gpd.sjoin(outlines_gdf, buffered_lake_gdf, predicate='overlaps')\n",
    "\n",
    "#         # Calculate within_fraction as before\n",
    "#         if (len(within) + len(overlaps)) > 0:\n",
    "#             within_fraction = np.round((len(within) / (len(within) + len(overlaps))), 2)\n",
    "#         else:\n",
    "#             print('No outlines found at this level')\n",
    "#             break\n",
    "        \n",
    "#         print(f\"Extent: {area_multiple_search_extent}, Level: {level}, Within: {round(within_fraction*100)}%\")\n",
    "        \n",
    "#         if within_fraction >= 0.90:\n",
    "#             break  # Exit loop if condition is met\n",
    "        \n",
    "#         level += 0.01  # Increase level for next iteration\n",
    "#         level = min(np.round(level, 2), 1.5)  # Ensure level does not exceed 1.5 and is rounded to two decimal places\n",
    "    \n",
    "#     # # Store the results in a new row and then concatenate into the results DataFrame\n",
    "#     new_row = pd.DataFrame({'area_multiple_search_extent': [area_multiple_search_extent], 'Level': [level]})\n",
    "#     results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "# # # Play sound to indicate complete   \n",
    "# # play_sound()\n",
    "\n",
    "# # Display the DataFrame\n",
    "# print(results_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# end_time = time.time()\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(\"Elapsed time:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a31ea8-2f92-464f-9f61-9fe11de4009e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Select dataframe the row with the minimum level to capture the most signal\n",
    "# # If there are multiple rows with the same minimum level, select the row with \n",
    "# # the smaller area_multiple_search_extent\n",
    "\n",
    "# # Sort the DataFrame first by 'Resulting_Level' and then by 'area_multiple_search_extent', \n",
    "# # both in ascending order\n",
    "# sorted_df = results_df.sort_values(by=['Resulting_Level', 'area_multiple_search_extent'], \n",
    "#     ascending=[True, True])\n",
    "\n",
    "# # Select the first row of the sorted DataFrame\n",
    "# selected_row = sorted_df.iloc[0]\n",
    "\n",
    "# # Display the selected row\n",
    "# print(selected_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f30451a-e1fc-4823-b06d-1b7762cf43f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Repeat with the selected level \n",
    "# outlines_gdf = find_evolving_outlines_and_plot(lake_gdf=lake_gdf, \n",
    "#     area_multiple_search_extent=selected_row['area_multiple_search_extent'],\n",
    "#     level=selected_row['Level'], dataset1=dataset1, dataset2=dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04744b01-9382-41ec-938c-8f4223494d70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Plot outlines\n",
    "# plot_evolving_outlines(lake_gdf=lake_gdf, outlines_gdf=outlines_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c78b4-bd27-4aca-beee-f2e57e16c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Needed for CookE2 (but maybe it needs larger buffer\n",
    "# # # Simplify centroid column to string to allow GeoJSON export\n",
    "# # for column in gdf.columns:\n",
    "# #     if column != 'geometry':  # Exclude the geometry column\n",
    "# #         filtered_gdf[column] = gdf[column].astype(str)\n",
    "        \n",
    "# # Clean up outlines by just looking at those intersecting with original static lake outline or evolving outlines that do so\n",
    "# filtered_gdf = extract_intersecting_polygons(outlines_gdf, lake_gdf['geometry'].values[0]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd7a53-fec0-4bb6-812c-3b844fc25609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Plot outlines\n",
    "# plot_evolving_outlines(lake_gdf, filtered_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f25e24-cf1b-4c7f-ad57-88fe9f8fc06d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(filtered_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605698e0-b2bf-41b7-a9cc-5df2d2d4883e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Simplify centroid column to string to allow GeoJSON export\n",
    "# for column in filtered_gdf.columns:\n",
    "#     if column != 'geometry':  # Exclude the geometry column\n",
    "#         filtered_gdf[column] = filtered_gdf[column].astype(str)\n",
    "\n",
    "# # Export evolving outlines GeoDataFrame to GeoJSON\n",
    "# filtered_gdf.to_file(filename='outlines/evolving_outlines/{}.geojson'.format(lake_gdf['name'].values[0]), driver='GeoJSON')\n",
    "# # gdf.to_file(filename='evolving_outlines/test.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af01f2f0-a52c-4f99-944f-6a7649b5f72c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Test opening works\n",
    "# gdf_open = gpd.read_file('outlines/evolving_outlines/{}.geojson'.format(lake_gdf['name'].values[0]))\n",
    "# gdf_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "b4315d5f-7605-49cb-9f4d-c2b6de0c037e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in range(0, len(lake_locations)):\n",
    "\n",
    "    if idx in [0, 23, 53, 112]:  # Skip previously analyzed lakes\n",
    "        break\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    initial_level = 0.1  # Starting level\n",
    "    area_multiple_search_extents = range(2, 11)  # From 2 to 10 inclusive\n",
    "    if lake_locations['CS2_SARIn'][idx] == False:\n",
    "        dataset1='none'\n",
    "    elif lake_locations['CS2_SARIn'][idx] == True:\n",
    "        dataset1=CS2_Smith2017\n",
    "    dataset2=ATL15_dh\n",
    "\n",
    "    # Select lake\n",
    "    lake_gdf = lake_locations.iloc[idx:idx+1]\n",
    "    lake_name = lake_gdf.name.values[0]\n",
    "    print('Working on', lake_name)\n",
    "\n",
    "    # Initialize DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=['area_multiple_search_extent', 'level'])\n",
    "\n",
    "    for area_multiple_search_extent in area_multiple_search_extents:\n",
    "        level = initial_level  # Reset level for each new area_multiple_search_extent\n",
    "        within_fraction = 0.0  # Reset within_fraction for each new area_multiple_search_extent\n",
    "        level_increment = 0.05  # Initial level increment\n",
    "\n",
    "        while within_fraction < 0.90 and level <= 2.0:\n",
    "            # Find evolving outlines\n",
    "            outlines_gdf = find_evolving_outlines(lake_gdf=lake_gdf, \n",
    "                area_multiple_search_extent=area_multiple_search_extent, level=level, \n",
    "                dataset1=dataset1, dataset2=dataset2)\n",
    "\n",
    "            # Define lake polygon and buffered geodataframe as before\n",
    "            lake_poly = lake_gdf.iloc[0].geometry\n",
    "            buffered_lake_gdf = muliple_area_buffer(polygon=lake_poly, \n",
    "                area_multiple=area_multiple_search_extent)\n",
    "\n",
    "            # Check which evolving outlines are within or overlap\n",
    "            within = gpd.sjoin(outlines_gdf, buffered_lake_gdf, predicate='within')\n",
    "            overlaps = gpd.sjoin(outlines_gdf, buffered_lake_gdf, predicate='overlaps')\n",
    "\n",
    "            # Calculate within_fraction as before\n",
    "            if (len(within) + len(overlaps)) > 0:\n",
    "                within_fraction = np.round((len(within) / (len(within) + len(overlaps))), 2)\n",
    "            else:\n",
    "                print('No outlines found at this level')\n",
    "                break\n",
    "\n",
    "            print(f\"Extent: {area_multiple_search_extent}, Level: {level}, Within: {round(within_fraction*100)}%\")\n",
    "\n",
    "            # Once within_fraction is greater specified percent,\n",
    "            if within_fraction >= 0.80:\n",
    "                level_increment = 0.01  # Increase level increment\n",
    "\n",
    "            # Once within_fraction is greater specified percent,\n",
    "            if within_fraction >= 0.90:\n",
    "                break  # Exit loop if condition is met\n",
    "\n",
    "            level += level_increment\n",
    "            level = np.round(level, 2)\n",
    "            \n",
    "        # # Store the results in a new row and then concatenate into the results DataFrame\n",
    "        new_row = pd.DataFrame({'area_multiple_search_extent': [area_multiple_search_extent], 'level': [level]})\n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(results_df)\n",
    "    \n",
    "    # Select dataframe the row with the minimum level to capture the most signal\n",
    "    # If there are multiple rows with the same minimum level, select the row with \n",
    "    # the smaller area_multiple_search_extent\n",
    "\n",
    "    # Sort the DataFrame first by 'Resulting_Level' and then by 'area_multiple_search_extent', \n",
    "    # both in ascending order\n",
    "    sorted_df = results_df.sort_values(by=['level', 'area_multiple_search_extent'], \n",
    "        ascending=[True, True])\n",
    "\n",
    "    # Select the first row of the sorted DataFrame\n",
    "    selected_row = sorted_df.iloc[0]\n",
    "\n",
    "    # Display the selected row\n",
    "    print(selected_row)\n",
    "\n",
    "    # Repeat with the selected level (first make output folders)\n",
    "    create_folder(OUTPUT_DIR + '/{}'.format('find_evolving_outlines'))\n",
    "    create_folder(OUTPUT_DIR + '/find_evolving_outlines/{}'.format(lake_name))\n",
    "    outlines_gdf = find_evolving_outlines(lake_gdf=lake_gdf, \n",
    "        area_multiple_search_extent=selected_row['area_multiple_search_extent'],\n",
    "        level=selected_row['level'], dataset1=dataset1, dataset2=dataset2, plot=True)\n",
    "\n",
    "    # Ensure at least two evolving outlines intersect with each other before proceeding\n",
    "    if check_intersections(gdf) == True:\n",
    "    \n",
    "        # Clean up outlines by just looking at those intersecting with original static lake outline or evolving outlines that do so\n",
    "        filtered_gdf = extract_intersecting_polygons(outlines_gdf, lake_gdf['geometry'].values[0])\n",
    "\n",
    "        # Plot and save fig of evolving outlines\n",
    "        create_folder(OUTPUT_DIR + '/{}'.format('plot_evolving_outlines'))\n",
    "        plot_evolving_outlines(lake_gdf=lake_gdf, outlines_gdf=filtered_gdf)\n",
    "\n",
    "        # # Simplify centroid column to string to allow GeoJSON export\n",
    "        # for column in filtered_gdf.columns:\n",
    "        #     if column != 'geometry':  # Exclude the geometry column\n",
    "        #         filtered_gdf[column] = filtered_gdf[column].astype(str)\n",
    "\n",
    "        # Export evolving outlines GeoDataFrame to GeoJSON\n",
    "        filtered_gdf.to_file(filename='outlines/evolving_outlines/{}.geojson'.format(lake_gdf['name'].values[0]), driver='GeoJSON')\n",
    "\n",
    "    # Otherwise move onto next lake\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f\"The cell ran in {np.round(duration,1)/60} mins.\")\n",
    "    \n",
    "    \n",
    "    # Play sound to indicate a lake is complete   \n",
    "    # play_sound()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938bcd2d-3328-4568-b951-e5fa1fcf978d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923fdb9e-a65e-4fd5-9364-e2282e1c55fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "a501b201-b1a3-4535-945e-71fcef1f00d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '/home/jovyan/1_outlines_candidates/output/FigS1_lake_reexamination_methods.ipynb/plot_evolving_outlines' already exists.\n",
      "Folder '/home/jovyan/1_outlines_candidates/output/FigS1_lake_reexamination_methods.ipynb/plot_evolving_outlines' already exists.\n",
      "Folder '/home/jovyan/1_outlines_candidates/output/FigS1_lake_reexamination_methods.ipynb/plot_evolving_outlines' already exists.\n"
     ]
    }
   ],
   "source": [
    "# replotting evovling outlines after func modifications\n",
    "for idx in [23, 53, 112]:\n",
    "\n",
    "    # Select lake\n",
    "    lake_gdf = lake_locations.iloc[idx:idx+1]\n",
    "    \n",
    "    # Test opening works\n",
    "    filtered_gdf = gpd.read_file('outlines/evolving_outlines/{}.geojson'.format(lake_gdf['name'].values[0]))\n",
    "    \n",
    "    # Plot and save fig of evolving outlines\n",
    "    create_folder(OUTPUT_DIR + '/{}'.format('plot_evolving_outlines'))\n",
    "    plot_evolving_outlines(lake_gdf=lake_gdf, outlines_gdf=filtered_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c67fc6-646b-47f7-8aec-1b8065dfd682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c79b87c-b315-4c7f-ba39-dd4e3d69fa84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6726d47-2b8f-4fa2-9d56-ef3354cdd080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for idx in range(0, len(SiegfriedFricker2018_outlines)):\n",
    "\n",
    "idx = 112\n",
    "\n",
    "# Isolate lake from Siegfried & Fricker, 2018 inventory as geodataframe using slicing\n",
    "lake_gdf = SiegfriedFricker2018_outlines.iloc[idx:idx+1]\n",
    "print('Working on', lake_gdf['name'].values[0])\n",
    "\n",
    "# Load evolving outlines into geodataframe\n",
    "outlines_gdf = gpd.read_file('outlines/evolving_outlines/{}.geojson'.format(lake_gdf['name'].values[0]))\n",
    "\n",
    "# Find delta heights and delta volume displacements using static outlines and compare to evolving outlines\n",
    "compare_evolving_static_outlines(lake_gdf=lake_gdf, outlines_gdf=outlines_gdf, dataset1=CS2_Smith2017, dataset2=ATL15_dh)\n",
    "\n",
    "# Clear the output of each index\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c88a731-d0a8-48f1-af63-04633a89a60b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a583f25d-9678-4996-850a-58a0e3c45262",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a5eab8-f9d7-47e7-b8b7-7ec2af526116",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hard coding compare_evolving_static_outlines function to debug\n",
    "\n",
    "# Isolate lake from Siegfried & Fricker, 2018 inventory as geodataframe using slicing\n",
    "idx = 112\n",
    "lake_gdf = SiegfriedFricker2018_outlines.iloc[idx:idx+1]\n",
    "\n",
    "outlines_gdf = gpd.read_file('outlines/evolving_outlines/{}.geojson'.format(lake_gdf['name'].values[0]))\n",
    "\n",
    "# Define lake name and polygon and buffered polygon to use as off-lake region\n",
    "lake_name = lake_gdf.name.values[0]\n",
    "static_outline = lake_gdf.iloc[0].geometry\n",
    "static_outline_buffered = muliple_area_buffer(static_outline, 2)\n",
    "evolving_outlines_unary_union = outlines_gdf.unary_union\n",
    "evolving_outlines_unary_union_buffered = muliple_area_buffer(evolving_outlines_unary_union, 2)\n",
    "\n",
    "# Combine static outline with evolving outlines from outlines_gdf\n",
    "combined_outline = unary_union([static_outline] + list(outlines_gdf.geometry))\n",
    "\n",
    "# If you need to buffer the combined outline, do it here\n",
    "combined_outline_buffered = muliple_area_buffer(combined_outline, 2)\n",
    "\n",
    "# Establish x_min, x_max, y_min, y_max from the combined outline\n",
    "# x_min, y_min, x_max, y_max = static_outline.bounds\n",
    "x_min, y_min, x_max, y_max = combined_outline.bounds\n",
    "del combined_outline\n",
    "x_buffer = abs(x_max-x_min)*0.2\n",
    "y_buffer = abs(y_max-y_min)*0.2\n",
    "\n",
    "# Clipping datasets\n",
    "if dataset1 != 'none':\n",
    "    dataset1_static_outline_clipped = dataset1.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "    dataset1_static_outline_clipped = dataset1.rio.clip([static_outline], dataset1.rio.crs)\n",
    "    dataset1_static_region_clipped = dataset1.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "    dataset1_static_region_clipped = dataset1.rio.clip([static_outline_buffered.geometry[0]], dataset1.rio.crs)\n",
    "    dataset1_static_region_clipped = dataset1.rio.clip([static_outline], dataset1.rio.crs, invert=True)\n",
    "    dataset1_evolving_region_clipped = dataset1.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "    dataset1_evolving_region_clipped = dataset1.rio.clip([evolving_outlines_unary_union_buffered.geometry[0]], dataset1.rio.crs)\n",
    "    dataset1_evolving_region_clipped = dataset1.rio.clip([evolving_outlines_unary_union], dataset1.rio.crs, invert=True)\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "dataset2_static_outline_clipped = dataset2.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "dataset2_static_outline_clipped = dataset1.rio.clip([static_outline], dataset2.rio.crs)\n",
    "dataset2_static_region_clipped = dataset2.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "dataset2_static_region_clipped = dataset2.rio.clip([static_outline_buffered.geometry[0]], dataset2.rio.crs)\n",
    "dataset2_static_region_clipped = dataset2.rio.clip([static_outline], dataset1.rio.crs, invert=True)\n",
    "dataset2_evolving_region_clipped = dataset2.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "dataset2_evolving_region_clipped = dataset2.rio.clip([evolving_outlines_unary_union_buffered.geometry[0]], dataset2.rio.crs)\n",
    "dataset2_evolving_region_clipped = dataset2.rio.clip([evolving_outlines_unary_union], dataset2.rio.crs, invert=True)\n",
    "\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# Create empty list to store polygons, areas, perimeters and dates\n",
    "static_outline_dhs = []\n",
    "static_region_dhs = []\n",
    "static_outline_dhs_corr = []\n",
    "static_outline_dvols_corr = []\n",
    "evolving_outlines_dareas = []\n",
    "evolving_outlines_dhs = []\n",
    "evolving_region_dhs = []\n",
    "evolving_outlines_dhs_corr = []\n",
    "evolving_outlines_dvols_corr = []\n",
    "midcyc_datetimes = []\n",
    "\n",
    "# Initialize no_data value\n",
    "no_data = np.nan\n",
    "\n",
    "# Calculate cycle-to-cycle dHeight at each cycle of the spliced data sets\n",
    "# for idx in range(len(midcyc_dates)-1):  \n",
    "for idx, dt in enumerate(cyc_dates['midcyc_dates'], 0):\n",
    "    gc.collect()\n",
    "    print(idx)\n",
    "    # For midcyc_dates indexes <= 32, use CryoSat-2 dataset for cycle-to-cycle dHeight\n",
    "    # This covers the CryoSat-2 era before ICESat-2 launch (2010-08-17 to 2018-08-17)\n",
    "    if idx <= 32:\n",
    "        if dataset1 != 'none':\n",
    "            dh_static_outline_clipped = dataset1_static_outline_clipped.delta_h[idx+1,:,:]-dataset1_static_outline_clipped.delta_h[idx,:,:]\n",
    "            dh_static_region_clipped = dataset1_static_region_clipped.delta_h[idx+1,:,:]-dataset1_static_region_clipped.delta_h[idx,:,:]\n",
    "            # dhdt_clipped.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "            # Filter rows that match the current time slice\n",
    "            outlines_gdf_dt_sub = outlines_gdf[outlines_gdf['midcyc_datetime'] == dt]\n",
    "            \n",
    "            # Check if outlines_gdf_dt_sub has any rows before proceeding\n",
    "            if not outlines_gdf_dt_sub.empty:\n",
    "                dataset1_evolving_outlines_clipped = dataset1.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "                dataset1_evolving_outlines_clipped = dataset1.rio.clip(outlines_gdf_dt_sub.geometry, dataset1.rio.crs)\n",
    "                dh_evolving_outlines_clipped = dataset1_evolving_outlines_clipped.delta_h[idx+1,:,:]-dataset1_evolving_outlines_clipped.delta_h[idx,:,:]\n",
    "                dh_evolving_region_clipped = dataset1_evolving_region_clipped.delta_h[idx+1,:,:]-dataset1_evolving_region_clipped.delta_h[idx,:,:]\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "    # For midcyc_dates indexes > 32, use ICESat-2 dataset for cycle-to-cycle dHeight\n",
    "    # This covers the ICESat-2 ATL06 v003 (2018-11-16 to 2023-04-02)\n",
    "    elif idx > 32: #& idx <=49:  # Less one because cycle-to-cycle differencing reduces length by one\n",
    "        # Subtract 33 from idx to start over at index zero with new dataset\n",
    "        dh_static_outline_clipped = dataset2_static_outline_clipped.delta_h[(idx-33)+1,:,:]-dataset2_static_outline_clipped.delta_h[(idx-33),:,:]\n",
    "        dh_static_region_clipped = dataset2_static_region_clipped.delta_h[(idx-33)+1,:,:]-dataset2_static_region_clipped.delta_h[(idx-33),:,:]\n",
    "        # dhdt_clipped.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "        # Filter rows that match the current time slice\n",
    "        outlines_gdf_dt_sub = outlines_gdf[outlines_gdf['midcyc_datetime'] == dt]\n",
    "        \n",
    "        # Check if outlines_gdf_dt_sub has any rows before proceeding\n",
    "        if not outlines_gdf_dt_sub.empty:\n",
    "            dataset2_evolving_outlines_clipped = dataset2.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "            dataset2_evolving_outlines_clipped = dataset2.rio.clip(outlines_gdf_dt_sub.geometry, dataset2.rio.crs)\n",
    "            dh_evolving_outlines_clipped = dataset2_evolving_outlines_clipped.delta_h[(idx-33)+1,:,:]-dataset2_evolving_outlines_clipped.delta_h[(idx-33),:,:]\n",
    "            dh_evolving_region_clipped = dataset2_evolving_region_clipped.delta_h[(idx-33)+1,:,:]-dataset2_evolving_region_clipped.delta_h[(idx-33),:,:]\n",
    "        # elif outlines_gdf_dt_sub.empty:\n",
    "            # dataset2_evolving_outlines_clipped = no_data\n",
    "            # dataset2_evolving_outlines_clipped = no_data\n",
    "            # dh_evolving_outlines_clipped = no_data\n",
    "            # dh_evolving_region_clipped = no_data\n",
    "            \n",
    "    # Store data into lists\n",
    "    static_outline_dh = np.nanmean(dh_static_outline_clipped)\n",
    "    static_region_dh = np.nanmean(dh_static_region_clipped)\n",
    "    static_outline_dh_corr = static_outline_dh - static_region_dh\n",
    "    static_outline_dvol_corr = static_outline_dh_corr*static_outline.area\n",
    "\n",
    "    static_outline_dhs += [static_outline_dh]\n",
    "    static_region_dhs += [static_region_dh]\n",
    "    static_outline_dhs_corr += [static_outline_dh_corr]\n",
    "    static_outline_dvols_corr += [static_outline_dvol_corr]\n",
    "\n",
    "    evolving_outlines_darea = sum(outlines_gdf_dt_sub.geometry.area)\n",
    "    evolving_outlines_dh = np.nanmean(dh_evolving_outlines_clipped)\n",
    "    evolving_region_dh = np.nanmean(dh_evolving_region_clipped)\n",
    "    evolving_outlines_dh_corr = evolving_outlines_dh - evolving_region_dh\n",
    "    evolving_outlines_dvol_corr = evolving_outlines_dh_corr*sum(outlines_gdf_dt_sub.geometry.area)\n",
    "\n",
    "    evolving_outlines_dareas += [evolving_outlines_darea]\n",
    "    evolving_outlines_dhs += [evolving_outlines_dh]\n",
    "    evolving_region_dhs += [evolving_region_dh]\n",
    "    evolving_outlines_dhs_corr += [evolving_outlines_dh_corr]\n",
    "    evolving_outlines_dvols_corr += [evolving_outlines_dvol_corr]\n",
    "\n",
    "    midcyc_datetimes += [midcyc_dates[idx]]\n",
    "\n",
    "# gc.collect()\n",
    "# Store polygons in dataframe for further analysis\n",
    "d = {'static_outline_dh (m)': static_outline_dhs, \n",
    "     'static_region_dh (m)': static_region_dhs,\n",
    "     'static_outline_dh_corr (m)': static_outline_dhs_corr,\n",
    "     'static_outline_dvol_corr (m^3)': static_outline_dvols_corr,\n",
    "     'evolving_outlines_darea (m^2)': evolving_outlines_dareas,\n",
    "     'evolving_outlines_dh (m)': evolving_outlines_dhs,\n",
    "     'evolving_region_dh (m)': evolving_region_dhs,\n",
    "     'evolving_outlines_dh_corr (m)': evolving_outlines_dhs_corr,\n",
    "     'evolving_outlines_dvol_corr (m^3)': evolving_outlines_dvols_corr,\n",
    "     'midcyc_datetime': midcyc_datetimes}\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "# Export dataframe to csv\n",
    "df.to_csv('outlines/compare_evolving_static_outlines/{}.csv'.format(lake_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a5b835-ec86-44f0-a826-08468b89ac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aa85dd-eec2-4eed-bb3b-42e9a02bcb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.axhline(lake_gdf.geometry.area.values[0])\n",
    "ax.plot(df.midcyc_datetime, df['evolving_outlines_darea (m^2)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fd1723-9f13-4a47-b854-df70a55acf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(df.midcyc_datetime, df['static_outline_dh (m)'])\n",
    "ax.plot(df.midcyc_datetime, df['evolving_outlines_dh (m)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308b976d-fd9c-41cb-9a33-432101a3377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(df.midcyc_datetime, df['static_outline_dvol_corr (m^3)'])\n",
    "ax.plot(df.midcyc_datetime, df['evolving_outlines_dvol_corr (m^3)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8381a789-6dd8-4a4d-8838-e61234c37071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c059b478-b0d9-4cc2-b7a3-a195305ef531",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fig. S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89d2ef4-2675-46b9-a0d3-109671221ab0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROI_poly = buffered_poly\n",
    "print(type(buffered_poly))\n",
    "print(type(ROI_poly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a4007e-064b-4b7a-9163-705fa3fb5927",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROI_poly = buffered_poly\n",
    "dataset1 = CS2_dh\n",
    "dataset2 = ATL15_dh\n",
    "\n",
    "# Clipping datasets\n",
    "dataset1_clipped = dataset1.rio.clip(ROI_poly, dataset1.rio.crs)\n",
    "dataset2_clipped = dataset2.rio.clip(ROI_poly, dataset2.rio.crs)\n",
    "\n",
    "# Extract min and max of x and y for dataset1\n",
    "min_x1 = dataset1_clipped.x.min().item()\n",
    "max_x1 = dataset1_clipped.x.max().item()\n",
    "min_y1 = dataset1_clipped.y.min().item()\n",
    "max_y1 = dataset1_clipped.y.max().item()\n",
    "\n",
    "# Extract min and max of x and y for dataset2\n",
    "min_x2 = dataset2_clipped.x.min().item()\n",
    "max_x2 = dataset2_clipped.x.max().item()\n",
    "min_y2 = dataset2_clipped.y.min().item()\n",
    "max_y2 = dataset2_clipped.y.max().item()\n",
    "\n",
    "# Check if the coordinates match\n",
    "if min_x1 != min_x2 or max_x1 != max_x2 or min_y1 != min_y2 or max_y1 != max_y2:\n",
    "    raise ValueError(\"Dataset1 and Dataset2 do not have matching x, y min, max coordinates\")\n",
    "\n",
    "# Establish x_min, x_max, y_min, y_max, \n",
    "x_min = min_x1\n",
    "x_max = max_x1\n",
    "y_min = min_y1\n",
    "y_max = max_y1\n",
    "\n",
    "# Subsetting datasets\n",
    "# Subset datasets to region of interest for plotting\n",
    "buffer = 4000\n",
    "mask_x = (dataset1.x >= x_min-buffer) & (dataset1.x <= x_max+buffer)\n",
    "mask_y = (dataset1.y >= y_min-buffer) & (dataset1.y <= y_max+buffer)\n",
    "dataset1_subset = dataset1.where(mask_x & mask_y, drop=True)\n",
    "mask_x = (dataset2.x >= x_min-buffer) & (dataset2.x <= x_max+buffer)\n",
    "mask_y = (dataset2.y >= y_min-buffer) & (dataset2.y <= y_max+buffer)\n",
    "dataset2_subset = dataset2.where(mask_x & mask_y, drop=True)\n",
    "mask_x = (moa_highres_da.x >= x_min-buffer) & (moa_highres_da.x <= x_max+buffer)\n",
    "mask_y = (moa_highres_da.y >= y_min-buffer) & (moa_highres_da.y <= y_max+buffer)\n",
    "moa_highres_da_subset = moa_highres_da.where(mask_x & mask_y, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7937af91-7689-4a40-b36c-fa8c58a1808e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modify find contours function to achieve something like less than 5% of contours intersecting with buffer\n",
    "# gdf = find_evolving_outlines(ROI['name'].values[0], buffered_poly, 0.5, CS2_dh, ATL15_dh)\n",
    "# gdf = find_evolving_outlines(ROI['name'].values[0], buffered_poly, 0.5, CS2_dh, ATL15_dh)\n",
    "gdf = find_evolving_outlines('Slessor_2_3_23', buffered_poly, 0.5, CS2_dh, ATL15_dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f68e0c-a180-4b49-a3a9-7faeb8ee5e61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Try other lakes\n",
    "\n",
    "# Plot Fig. 2\n",
    "fig, ax = plt.subplots(3,1, sharex=True, figsize=(5.5,16.5))\n",
    "\n",
    "\n",
    "# Panel A - Plot uplift filling event\n",
    "# Specify the time value you want to plot\n",
    "specified_date = datetime.date(2010, 8, 17)\n",
    "gdf_subset[(gdf_subset['date'] == specified_date) & (gdf_subset['dh (m)'] > 0)].boundary.plot(ax=ax[0], color='blue')\n",
    "gdf_subset[(gdf_subset['date'] == specified_date) & (gdf_subset['dh (m)'] < 0)].boundary.plot(ax=ax[0], color='red')\n",
    "\n",
    "# Calculate the absolute difference between each time in the dataset and the specified time\n",
    "time_diff = np.abs(midcyc_dates - np.datetime64(specified_date))\n",
    "# Find the index of the minimum difference\n",
    "nearest_time_index = time_diff.argmin().item()\n",
    "if nearest_time_index <= 32:\n",
    "    dhdt = dataset1_subset.cyc_to_cyc_delta_h[nearest_time_index,:,:]\n",
    "elif nearest_time_index > 32:\n",
    "    # Subtract 33 from idx to start over with new dataset\n",
    "    dhdt = dataset2_subset.cyc_to_cyc_delta_h[(nearest_time_index-33),:,:]\n",
    "\n",
    "# Plot gridded height change data\n",
    "divnorm=colors.TwoSlopeNorm(vmin=-1.5, vcenter=0., vmax=1.5)  \n",
    "img = ax[0].imshow(dhdt, extent=[x_min-buffer, x_max+buffer, y_min-buffer, y_max+buffer], origin='upper', cmap='coolwarm_r', \n",
    "                   # norm=colors.CenteredNorm(),\n",
    "                   norm=divnorm)\n",
    "\n",
    "# Plot buffered polygon showing extent of evolving outline search\n",
    "ROI_color = 'magenta'\n",
    "gpd.GeoSeries(buffered_poly).boundary.plot(ax=ax[0], color=ROI_color)\n",
    "\n",
    "# Create an axes on the right side of ax1 for the colorbar\n",
    "cax = fig.add_axes([ax[0].get_position().x1 + 0.15, ax[0].get_position().y0 - 0.092, 0.03, ax[0].get_position().height])\n",
    "fig.colorbar(img, cax=cax).set_label('height change [m]', size=12)\n",
    "\n",
    "# Annotate time slice\n",
    "ax[0].annotate('height change: {} to {}'.format(datetime64_to_fractional_year(cyc_start_dates[nearest_time_index]),\n",
    "    datetime64_to_fractional_year(cyc_end_dates[nearest_time_index])), \n",
    "    xy=(-421e3,1009e3), xycoords='data', fontsize=14)\n",
    "\n",
    "\n",
    "# Panel B - Plot subsidence draining event\n",
    "# specified_date = datetime.date(2021, 2, 15)\n",
    "specified_date = datetime.date(2020, 2, 16)\n",
    "# specified_date = datetime.date(2015, 8, 17)\n",
    "\n",
    "gdf_subset[(gdf_subset['date'] == specified_date) & (gdf_subset['dh (m)'] > 0)].boundary.plot(ax=ax[1], color='blue')\n",
    "gdf_subset[(gdf_subset['date'] == specified_date) & (gdf_subset['dh (m)'] < 0)].boundary.plot(ax=ax[1], color='red')\n",
    "\n",
    "# Calculate the absolute difference between each time in the dataset and the specified time\n",
    "time_diff = np.abs(midcyc_dates - np.datetime64(specified_date))\n",
    "# Find the index of the minimum difference\n",
    "nearest_time_index = time_diff.argmin().item()\n",
    "if nearest_time_index <= 32:\n",
    "    dhdt = dataset1_subset.delta_h[nearest_time_index+1,:,:]-dataset1_subset.delta_h[nearest_time_index,:,:]\n",
    "elif nearest_time_index > 32:\n",
    "    # Subtract 33 from idx to start over with new dataset\n",
    "    dhdt = dataset2_subset.delta_h[(nearest_time_index-33)+1,:,:]-dataset2_subset.delta_h[(nearest_time_index-33),:,:]\n",
    "\n",
    "# Plot gridded height change data\n",
    "img = ax[1].imshow(dhdt, extent=[x_min-buffer, x_max+buffer, y_min-buffer, y_max+buffer], origin='lower', cmap='coolwarm_r', \n",
    "                   # norm=colors.CenteredNorm(),\n",
    "                   norm=divnorm)\n",
    "gpd.GeoSeries(buffered_poly).boundary.plot(ax=ax[1], color=ROI_color)\n",
    "\n",
    "ax[1].annotate('height change: {} to {}'.format(datetime64_to_fractional_year(cyc_start_dates[nearest_time_index]),\n",
    "    datetime64_to_fractional_year(cyc_end_dates[nearest_time_index])), \n",
    "    xy=(-421e3,1009e3), xycoords='data', fontsize=14)\n",
    "\n",
    "\n",
    "# Panel C - Plot outlines in aggregate vs. two past static delineations\n",
    "# Plot MOA imagery  \n",
    "ax[2].imshow(moa_highres_da_subset[0,:,:], cmap=\"gray\", clim=[14000, 17000], extent=[x_min-buffer, x_max+buffer, y_min-buffer, y_max+buffer])\n",
    "\n",
    "# Pick colormap and make continuous cmap discrete for evolving outlines\n",
    "colormap = 'plasma'\n",
    "continuous_cmap = matplotlib.colormaps[colormap]\n",
    "discrete_cmap = colors.ListedColormap(continuous_cmap(np.linspace(0, 1, len(midcyc_dates)-1)))\n",
    "\n",
    "# Norm to time variable\n",
    "norm = plt.Normalize(mdates.date2num(midcyc_dates[0]), \n",
    "                     mdates.date2num(midcyc_dates[-1]))\n",
    "\n",
    "# Use for loop to store each time slice as line segment to use in legend\n",
    "# And plot each outline in the geopandas dataframe and color by date\n",
    "lines = []  # list of lines to be used for the legend\n",
    "for idx, dt in enumerate(midcyc_dates, 0):\n",
    "    x = 1; y = 1\n",
    "    line, = ax[2].plot(x, y, color=discrete_cmap(norm(mdates.date2num(midcyc_dates[idx]))), linewidth=3)\n",
    "    lines.append(line)\n",
    "    \n",
    "    # Filter rows that match the current time slice\n",
    "    gdf_subset_dt = gdf_subset[gdf_subset['datetime'] == dt]\n",
    "\n",
    "    # Plotting the subset\n",
    "    gdf_subset_dt.plot(ax=ax[2], edgecolor=discrete_cmap(norm(mdates.date2num(midcyc_dates[idx]))), facecolor='none')\n",
    "\n",
    "    \n",
    "# All panels\n",
    "# Label axes\n",
    "ax[2].set_xlabel('x [km]', size=16)\n",
    "ax[1].set_ylabel('y [km]', size=16)\n",
    "\n",
    "# ax[0].annotate('A', xy=(-425e3,1049e3), xycoords='data', fontsize=30)\n",
    "# ax[1].annotate('B', xy=(-425e3,1049e3), xycoords='data', fontsize=30)\n",
    "# ax[2].annotate('C', xy=(-425e3,1049e3), xycoords='data', fontsize=30)\n",
    "ax[0].annotate('A', xy=(0.02, 0.9), xycoords='axes fraction', fontsize=30)\n",
    "ax[1].annotate('B', xy=(0.02, 0.9), xycoords='axes fraction', fontsize=30)\n",
    "ax[2].annotate('C', xy=(0.02, 0.9), xycoords='axes fraction', fontsize=30)\n",
    "\n",
    "# Create lines for legend\n",
    "S09_color = 'lightseagreen'\n",
    "SF18_color = 'teal'\n",
    "Smith2009 = plt.Line2D((0, 1), (0, 0), color=S09_color, linestyle=(0, (1, 2)), linewidth=3)\n",
    "SiegfriedFricker2018 = plt.Line2D((0, 1), (0, 0), color=SF18_color, linestyle=(0, (1, 1)), linewidth=3)\n",
    "ROI = plt.Line2D((0, 1), (0, 0), color=ROI_color, linestyle='solid', linewidth=3)\n",
    "uplift = plt.Line2D((0, 1), (0, 0), color='blue', linewidth=3)\n",
    "subsidence = plt.Line2D((0, 1), (0, 0), color='red', linewidth=3)\n",
    "\n",
    "# Create legends\n",
    "ax[0].legend([Smith2009, SiegfriedFricker2018, ROI, uplift], \n",
    "           ['static outline [10]',\n",
    "            'static outline [13]', \n",
    "            # 'evolving outline ({} m threshold)'.format(threshold)], \n",
    "            'evolving outline search limit [this study]',\n",
    "            'evolving outline - uplift [this study]'], \n",
    "             loc='upper right') \n",
    "\n",
    "ax[1].legend([subsidence],\n",
    "           ['evolving outline - subsidence [this study]'], \n",
    "             loc='upper right')\n",
    "\n",
    "legend = ax[2].legend([tuple(lines)], ['evolving outlines [this study]'],\n",
    "    handlelength=3, handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n",
    "    loc='upper center')\n",
    "legend.get_frame().set_linewidth(0.0)\n",
    "ax[2].patch.set_alpha(1)\n",
    "\n",
    "# Create colorbar \n",
    "m = plt.cm.ScalarMappable(cmap=discrete_cmap)\n",
    "m.set_array(np.array([datetime64_to_fractional_year(date) for date in midcyc_dates[0:]]))\n",
    "cax = inset_axes(ax[2],\n",
    "                 width=\"100%\",\n",
    "                 height=\"2.5%\",\n",
    "                 loc=3,\n",
    "                 bbox_to_anchor=[0,-0.14,1,1],\n",
    "                 bbox_transform=ax[2].transAxes,\n",
    "                 borderpad=0,\n",
    "                 )\n",
    "cbar=fig.colorbar(m, ticks=np.array([2010,2012,2014,2016,2018,2020,2022]), \n",
    "             cax=cax, orientation='horizontal').set_label('evolving outline year', size=15)\n",
    "\n",
    "# Plot inset map\n",
    "axIns = ax[0].inset_axes([0.01, 0.01, 0.3, 0.3]) # [left, bottom, width, height] (fractional axes coordinates)\n",
    "axIns.set_aspect('equal')\n",
    "moa_2014_coastline.plot(ax=axIns, color='gray', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "moa_2014_groundingline.plot(ax=axIns, color='ghostwhite', edgecolor='k', linewidth=0.1, zorder=3)\n",
    "axIns.axis('off')\n",
    "# # Plot black rectangle to indicate location\n",
    "# rect = Rectangle((x_min, y_min), (x_max-x_min), (y_max-y_min), fill=False, linewidth=2, color='k', zorder=3)\n",
    "# axIns.add_artist(rect)\n",
    "# Plot red star to indicate location\n",
    "axIns.scatter(((x_max+x_min)/2), ((y_max+y_min)/2), marker='*', \n",
    "    linewidth=1, edgecolor='k', facecolor='r', s=100, zorder=3)\n",
    "\n",
    "# # Add annotation to the opposite side of the colorbar\n",
    "# cbar.ax.text(1.1, 0.5, 'CryoSat-2 era', va='center', ha='left', transform=cbar.ax.transAxes)\n",
    "\n",
    "for i in ax: \n",
    "    S09_outlines.boundary.plot(ax=i, edgecolor=S09_color, facecolor='none', linestyle=(0, (1, 2)), linewidth=3, alpha=1, zorder=0)\n",
    "    SF18_outlines_SF18only.boundary.plot(ax=i, edgecolor=SF18_color, facecolor='none', linestyle=(0, (1, 1)), linewidth=3, alpha=1, zorder=0)\n",
    "    \n",
    "    # Change polar stereographic m to km\n",
    "    km_scale = 1e3\n",
    "    ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    i.xaxis.set_major_formatter(ticks_x)\n",
    "    ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "    i.yaxis.set_major_formatter(ticks_y)\n",
    "    \n",
    "    # i.set(xlim=(x_min-buffer, x_max+buffer), ylim=(y_min-buffer, y_max+buffer))   \n",
    "    i.set(xlim=(x_min-buffer, x_max+buffer), ylim=(y_min-buffer, y_max+buffer))   \n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9029d96-c400-4346-abab-fd6ce96b5474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a382fc-3e70-4dd8-bb49-ef539b386c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b587357-145a-48d9-96b0-7aac94ccedbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe41da-fd2d-4890-a858-d6ac61e90535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I can't figure out why data_counts plot looks sparse when you plot at continental scale\n",
    "plt.close()\n",
    "for idx in range(145, len(lake_locations)):\n",
    "\n",
    "    # Define lake name and polygon\n",
    "    lake_name = lake_gdf.name.values[0]\n",
    "    lake_poly = lake_gdf.iloc[0].geometry\n",
    "\n",
    "    # # Create buffered polygons for various multiples of lake area to find which\n",
    "    # # best emcompasses the height change signals at previously identified lakes\n",
    "    buffered_poly_2x = muliple_area_buffer(lake_poly, 2)\n",
    "    # buffered_poly_3x = muliple_area_buffer(lake_poly, 3)\n",
    "    # buffered_poly_4x = muliple_area_buffer(lake_poly, 4)\n",
    "    # buffered_poly_5x = muliple_area_buffer(lake_poly, 5)\n",
    "\n",
    "    # Establish x_min, x_max, y_min, y_max\n",
    "    x_min, y_min, x_max, y_max = buffered_poly_2x.iloc[0].geometry.bounds\n",
    "    x_buffer = abs(x_max-x_min)*10\n",
    "    y_buffer = abs(y_max-y_min)*10\n",
    "\n",
    "    # Subsetting datasets\n",
    "    mask_x = (dataset1.x >= x_min - x_buffer) & (dataset1.x <= x_max + x_buffer)\n",
    "    mask_y = (dataset1.y >= y_min - y_buffer) & (dataset1.y <= y_max + y_buffer)\n",
    "    dataset1_subset = dataset1.where(mask_x & mask_y, drop=True)\n",
    "    mask_x = (dataset2.x >= x_min - x_buffer) & (dataset2.x <= x_max + x_buffer)\n",
    "    mask_y = (dataset2.y >= y_min - y_buffer) & (dataset2.y <= y_max + y_buffer)\n",
    "    dataset2_subset = dataset2.where(mask_x & mask_y, drop=True)\n",
    "\n",
    "    # Make output folders\n",
    "    create_folder(OUTPUT_DIR + '/{}'.format('plot_data_counts'))\n",
    "    create_folder(OUTPUT_DIR + '/plot_data_counts/{}'.format(lake_name))\n",
    "    print('Now creating/saving plots...')\n",
    "\n",
    "    # Create lines for legend\n",
    "    S09_color = 'cyan'\n",
    "    SF18_color  = 'darkcyan'\n",
    "    lakes_notSF18_color = 'deepskyblue'\n",
    "    Smith2009 = plt.Line2D((0, 1), (0, 0), color=S09_color, linestyle=(0, (1, 3)), linewidth=2)\n",
    "    SiegfriedFricker2018 = plt.Line2D((0, 1), (0, 0), color=SF18_color, linestyle=(0, (1, 2)), linewidth=2)\n",
    "    lakes_notSF18 = plt.Line2D((0, 1), (0, 0), color=lakes_notSF18_color, linestyle=(0, (1, 1)), linewidth=2)\n",
    "\n",
    "    # Calculate cycle-to-cycle dHeight at each cycle of the spliced data sets\n",
    "    for idx in range(33,34):#len(midcyc_dates)):\n",
    "        # For midcyc_dates indexes <= 32, use CryoSat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the CryoSat-2 era before ICESat-2 launch (2010-08-17 to 2018-08-17)\n",
    "        if idx <= 32:\n",
    "            count_subset = dataset1_subset['data_count'][idx,:,:]\n",
    "            count_subset.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "        # For midcyc_dates indexes > 32, use ICESat-2 dataset for cycle-to-cycle dHeight\n",
    "        # This covers the ICESat-2 era (2018-11-16 to most recently available data)\n",
    "        elif idx > 32:\n",
    "            # Subtract 33 from idx to start over at index zero with new dataset\n",
    "            count_subset = dataset2_subset['data_count'][(idx-33),:,:]\n",
    "            count_subset.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "\n",
    "        # Only plot arrays that have data in them; some time slices have no data because no CryoSat-2 SARIn coverage\n",
    "        # Returns True if there is at least one non-NaN value in data_stacked, and False if all values are NaN\n",
    "        # if np.any(~np.isnan(count_subset)):\n",
    "        # Create fig, ax\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        # Plot figure\n",
    "        img = ax.imshow(count_subset, extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer], \n",
    "            origin='lower', cmap='viridis')\n",
    "\n",
    "        buffered_poly_2x.boundary.plot(ax=ax, edgecolor='r', facecolor='none', linewidth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3de926-fc8f-4f81-bcc5-8d005970cb74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only plot arrays that have data in them; some time slices have no data because no CryoSat-2 SARIn coverage\n",
    "# Returns True if there is at least one non-NaN value in data_stacked, and False if all values are NaN\n",
    "# if np.any(~np.isnan(count_subset)):\n",
    "plt.close()\n",
    "# Create fig, ax\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "idx=44\n",
    "\n",
    "# Plot figure\n",
    "img = ax.imshow(dataset2['data_count'][(idx-33),:,:], \n",
    "    # extent=[x_min-x_buffer, x_max+x_buffer, y_min-y_buffer, y_max+y_buffer],\n",
    "    extent=[dataset2.x.min(), dataset2.x.max(), dataset2.y.min(), dataset2.y.max()], \n",
    "    origin='lower', \n",
    "    # cmap='viridis'\n",
    "    )\n",
    "\n",
    "buffered_poly_2x.boundary.plot(ax=ax, edgecolor='r', facecolor='none', linewidth=5)\n",
    "plt.show()\n",
    "Scripps_landice.boundary.plot(ax=ax, edgecolor='blue', facecolor='none', linewidth=0.5)\n",
    "\n",
    "# Add colorbar \n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "fig.colorbar(img, cax=cax).set_label('data counts', size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac66a1-541e-4dc1-8e1d-54ada1b4ea59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
